05/15/2022 19:00:19 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=False, bsz_list=[4], cache_dir='/data/qin/cache/', checkpoint='None', cuda='4', dataset='nlp_forest_single', debug=False, dev_file='data', do_lowercase=False, do_predict=True, do_train=True, eval_period=50, freeze_embeds=False, gradient_accumulation_steps=2, identifier='T5-large-multitask-nopara2para-5e-1-4-20', learning_rate=0.5, learning_rate_list=[0.5], lm_adapted_path='/data/qin/lm_adapted_t5model/torch_ckpt/large/pytorch_model.bin', local_rank=0, log_step=10, max_grad_norm=1.0, max_input_length=512, max_output_length=128, model='google/t5-v1_1-large', num_beams=4, num_train_epochs=1000.0, output_dir='models/T5-large-multitask-nopara2para-5e-1-4-20/singletask-paws', predict_batch_size=16, predict_checkpoint='best-model.pt', prefix='', prompt_number=100, quiet=False, seed=42, task_dir='data/paws/', task_name='paws', test_file='data', total_steps=3000, train_batch_size=4, train_file='data', wait_step=10000000000, warmup_steps=50, weight_decay=1e-05)
05/15/2022 19:00:19 - INFO - __main__ - models/T5-large-multitask-nopara2para-5e-1-4-20/singletask-paws
06/22/2022 13:21:01 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-multitask-nopara2para-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-nopara2para-5e-1-4-20-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
06/22/2022 13:21:01 - INFO - __main__ - models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws
06/22/2022 13:21:01 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-multitask-nopara2para-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-nopara2para-5e-1-4-20-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
06/22/2022 13:21:01 - INFO - __main__ - models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws
06/22/2022 13:21:01 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/22/2022 13:21:01 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/22/2022 13:21:01 - INFO - __main__ - args.device: cuda:1
06/22/2022 13:21:01 - INFO - __main__ - args.device: cuda:0
06/22/2022 13:21:01 - INFO - __main__ - Using 2 gpus
06/22/2022 13:21:01 - INFO - __main__ - Using 2 gpus
06/22/2022 13:21:01 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/22/2022 13:21:01 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/22/2022 13:21:06 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.5, bsz=8 ...
06/22/2022 13:21:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:21:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:21:07 - INFO - __main__ - Printing 3 examples
06/22/2022 13:21:07 - INFO - __main__ - Printing 3 examples
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:21:07 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:21:07 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:21:07 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:21:07 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:21:07 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:21:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:21:07 - INFO - __main__ - Printing 3 examples
06/22/2022 13:21:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/22/2022 13:21:07 - INFO - __main__ - Printing 3 examples
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/22/2022 13:21:07 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:21:07 - INFO - __main__ - ['1']
06/22/2022 13:21:07 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:21:07 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:21:07 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:21:07 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:21:07 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:21:13 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:21:13 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:21:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:21:13 - INFO - __main__ - Starting training!
06/22/2022 13:21:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:21:19 - INFO - __main__ - Starting training!
06/22/2022 13:21:21 - INFO - __main__ - Step 10 Global step 10 Train loss 3.72 on epoch=4
06/22/2022 13:21:22 - INFO - __main__ - Step 20 Global step 20 Train loss 2.27 on epoch=9
06/22/2022 13:21:23 - INFO - __main__ - Step 30 Global step 30 Train loss 1.20 on epoch=14
06/22/2022 13:21:24 - INFO - __main__ - Step 40 Global step 40 Train loss 0.79 on epoch=19
06/22/2022 13:21:26 - INFO - __main__ - Step 50 Global step 50 Train loss 0.74 on epoch=24
06/22/2022 13:21:26 - INFO - __main__ - Global step 50 Train loss 1.74 Classification-F1 0.3333333333333333 on epoch=24
06/22/2022 13:21:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/22/2022 13:21:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=29
06/22/2022 13:21:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=34
06/22/2022 13:21:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=39
06/22/2022 13:21:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=44
06/22/2022 13:21:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=49
06/22/2022 13:21:33 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 13:21:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=54
06/22/2022 13:21:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
06/22/2022 13:21:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=64
06/22/2022 13:21:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
06/22/2022 13:21:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
06/22/2022 13:21:39 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 13:21:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=79
06/22/2022 13:21:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
06/22/2022 13:21:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
06/22/2022 13:21:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.36 on epoch=94
06/22/2022 13:21:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
06/22/2022 13:21:46 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 13:21:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=104
06/22/2022 13:21:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=109
06/22/2022 13:21:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=114
06/22/2022 13:21:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
06/22/2022 13:21:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
06/22/2022 13:21:52 - INFO - __main__ - Global step 250 Train loss 0.33 Classification-F1 0.3992490613266583 on epoch=124
06/22/2022 13:21:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=124, global_step=250
06/22/2022 13:21:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
06/22/2022 13:21:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
06/22/2022 13:21:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=139
06/22/2022 13:21:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
06/22/2022 13:21:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
06/22/2022 13:21:59 - INFO - __main__ - Global step 300 Train loss 0.31 Classification-F1 0.5134502923976608 on epoch=149
06/22/2022 13:21:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5134502923976608 on epoch=149, global_step=300
06/22/2022 13:22:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
06/22/2022 13:22:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
06/22/2022 13:22:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
06/22/2022 13:22:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.32 on epoch=169
06/22/2022 13:22:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
06/22/2022 13:22:05 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.5588547189819725 on epoch=174
06/22/2022 13:22:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.5588547189819725 on epoch=174, global_step=350
06/22/2022 13:22:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
06/22/2022 13:22:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=184
06/22/2022 13:22:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=189
06/22/2022 13:22:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.16 on epoch=194
06/22/2022 13:22:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=199
06/22/2022 13:22:12 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.5555555555555556 on epoch=199
06/22/2022 13:22:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
06/22/2022 13:22:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.15 on epoch=209
06/22/2022 13:22:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.13 on epoch=214
06/22/2022 13:22:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
06/22/2022 13:22:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
06/22/2022 13:22:18 - INFO - __main__ - Global step 450 Train loss 0.12 Classification-F1 0.6190476190476191 on epoch=224
06/22/2022 13:22:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.6190476190476191 on epoch=224, global_step=450
06/22/2022 13:22:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
06/22/2022 13:22:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
06/22/2022 13:22:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.05 on epoch=239
06/22/2022 13:22:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
06/22/2022 13:22:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
06/22/2022 13:22:25 - INFO - __main__ - Global step 500 Train loss 0.07 Classification-F1 0.5733333333333335 on epoch=249
06/22/2022 13:22:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
06/22/2022 13:22:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
06/22/2022 13:22:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
06/22/2022 13:22:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
06/22/2022 13:22:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
06/22/2022 13:22:31 - INFO - __main__ - Global step 550 Train loss 0.04 Classification-F1 0.6862745098039216 on epoch=274
06/22/2022 13:22:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6190476190476191 -> 0.6862745098039216 on epoch=274, global_step=550
06/22/2022 13:22:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
06/22/2022 13:22:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
06/22/2022 13:22:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
06/22/2022 13:22:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
06/22/2022 13:22:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
06/22/2022 13:22:38 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.5901477832512315 on epoch=299
06/22/2022 13:22:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
06/22/2022 13:22:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
06/22/2022 13:22:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
06/22/2022 13:22:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
06/22/2022 13:22:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/22/2022 13:22:45 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.6559139784946237 on epoch=324
06/22/2022 13:22:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
06/22/2022 13:22:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
06/22/2022 13:22:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
06/22/2022 13:22:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
06/22/2022 13:22:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
06/22/2022 13:22:51 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=349
06/22/2022 13:22:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
06/22/2022 13:22:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/22/2022 13:22:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
06/22/2022 13:22:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
06/22/2022 13:22:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/22/2022 13:22:58 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=374
06/22/2022 13:22:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/22/2022 13:23:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/22/2022 13:23:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
06/22/2022 13:23:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/22/2022 13:23:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
06/22/2022 13:23:04 - INFO - __main__ - Global step 800 Train loss 0.02 Classification-F1 0.5555555555555556 on epoch=399
06/22/2022 13:23:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
06/22/2022 13:23:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/22/2022 13:23:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
06/22/2022 13:23:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
06/22/2022 13:23:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/22/2022 13:23:11 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=424
06/22/2022 13:23:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
06/22/2022 13:23:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/22/2022 13:23:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/22/2022 13:23:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
06/22/2022 13:23:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 13:23:18 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=449
06/22/2022 13:23:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/22/2022 13:23:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/22/2022 13:23:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
06/22/2022 13:23:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/22/2022 13:23:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/22/2022 13:23:24 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=474
06/22/2022 13:23:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/22/2022 13:23:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/22/2022 13:23:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/22/2022 13:23:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/22/2022 13:23:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 13:23:31 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=499
06/22/2022 13:23:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/22/2022 13:23:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/22/2022 13:23:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/22/2022 13:23:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/22/2022 13:23:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/22/2022 13:23:38 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.5625 on epoch=524
06/22/2022 13:23:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/22/2022 13:23:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 13:23:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 13:23:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/22/2022 13:23:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 13:23:44 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=549
06/22/2022 13:23:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 13:23:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 13:23:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/22/2022 13:23:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 13:23:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/22/2022 13:23:51 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=574
06/22/2022 13:23:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/22/2022 13:23:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 13:23:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 13:23:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 13:23:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 13:23:57 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=599
06/22/2022 13:23:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 13:24:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 13:24:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
06/22/2022 13:24:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 13:24:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/22/2022 13:24:04 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=624
06/22/2022 13:24:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 13:24:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 13:24:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 13:24:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 13:24:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 13:24:11 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=649
06/22/2022 13:24:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 13:24:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 13:24:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 13:24:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 13:24:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 13:24:17 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=674
06/22/2022 13:24:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 13:24:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 13:24:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 13:24:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 13:24:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 13:24:24 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=699
06/22/2022 13:24:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 13:24:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 13:24:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 13:24:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 13:24:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 13:24:31 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=724
06/22/2022 13:24:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 13:24:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 13:24:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 13:24:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 13:24:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 13:24:37 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=749
06/22/2022 13:24:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 13:24:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 13:24:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 13:24:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 13:24:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 13:24:44 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=774
06/22/2022 13:24:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 13:24:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 13:24:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 13:24:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 13:24:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 13:24:50 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=799
06/22/2022 13:24:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 13:24:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 13:24:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 13:24:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 13:24:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 13:24:57 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=824
06/22/2022 13:24:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 13:24:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 13:25:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 13:25:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 13:25:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 13:25:03 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=849
06/22/2022 13:25:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 13:25:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 13:25:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 13:25:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 13:25:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 13:25:10 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=874
06/22/2022 13:25:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 13:25:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 13:25:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 13:25:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 13:25:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 13:25:16 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=899
06/22/2022 13:25:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 13:25:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 13:25:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 13:25:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 13:25:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 13:25:23 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=924
06/22/2022 13:25:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 13:25:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 13:25:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 13:25:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 13:25:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 13:25:30 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=949
06/22/2022 13:25:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 13:25:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 13:25:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 13:25:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 13:25:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 13:25:36 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=974
06/22/2022 13:25:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 13:25:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 13:25:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 13:25:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 13:25:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 13:25:43 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=999
06/22/2022 13:25:43 - INFO - __main__ - save last model!
06/22/2022 13:25:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 13:25:43 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 13:25:43 - INFO - __main__ - Printing 3 examples
06/22/2022 13:25:43 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 13:25:43 - INFO - __main__ - ['0']
06/22/2022 13:25:43 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 13:25:43 - INFO - __main__ - ['1']
06/22/2022 13:25:43 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 13:25:43 - INFO - __main__ - ['1']
06/22/2022 13:25:43 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:25:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:25:44 - INFO - __main__ - Printing 3 examples
06/22/2022 13:25:44 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/22/2022 13:25:44 - INFO - __main__ - ['1']
06/22/2022 13:25:44 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/22/2022 13:25:44 - INFO - __main__ - ['1']
06/22/2022 13:25:44 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/22/2022 13:25:44 - INFO - __main__ - ['1']
06/22/2022 13:25:44 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:25:44 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:25:44 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:25:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:25:44 - INFO - __main__ - Printing 3 examples
06/22/2022 13:25:44 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/22/2022 13:25:44 - INFO - __main__ - ['1']
06/22/2022 13:25:44 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/22/2022 13:25:44 - INFO - __main__ - ['1']
06/22/2022 13:25:44 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/22/2022 13:25:44 - INFO - __main__ - ['1']
06/22/2022 13:25:44 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:25:44 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:25:44 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:25:47 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:25:49 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:25:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:25:49 - INFO - __main__ - Starting training!
06/22/2022 13:25:55 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 13:27:27 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_100_0.5_8_predictions.txt
06/22/2022 13:27:27 - INFO - __main__ - Classification-F1 on test data: 0.4992
06/22/2022 13:27:27 - INFO - __main__ - prefix=paws_16_100, lr=0.5, bsz=8, dev_performance=0.6862745098039216, test_performance=0.49921794994879676
06/22/2022 13:27:27 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.4, bsz=8 ...
06/22/2022 13:27:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:27:28 - INFO - __main__ - Printing 3 examples
06/22/2022 13:27:28 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/22/2022 13:27:28 - INFO - __main__ - ['1']
06/22/2022 13:27:28 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/22/2022 13:27:28 - INFO - __main__ - ['1']
06/22/2022 13:27:28 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/22/2022 13:27:28 - INFO - __main__ - ['1']
06/22/2022 13:27:28 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:27:28 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:27:28 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:27:28 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:27:28 - INFO - __main__ - Printing 3 examples
06/22/2022 13:27:28 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/22/2022 13:27:28 - INFO - __main__ - ['1']
06/22/2022 13:27:28 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/22/2022 13:27:28 - INFO - __main__ - ['1']
06/22/2022 13:27:28 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/22/2022 13:27:28 - INFO - __main__ - ['1']
06/22/2022 13:27:28 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:27:28 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:27:28 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:27:34 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:27:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:27:34 - INFO - __main__ - Starting training!
06/22/2022 13:27:35 - INFO - __main__ - Step 10 Global step 10 Train loss 4.16 on epoch=4
06/22/2022 13:27:37 - INFO - __main__ - Step 20 Global step 20 Train loss 2.68 on epoch=9
06/22/2022 13:27:38 - INFO - __main__ - Step 30 Global step 30 Train loss 1.89 on epoch=14
06/22/2022 13:27:39 - INFO - __main__ - Step 40 Global step 40 Train loss 1.62 on epoch=19
06/22/2022 13:27:40 - INFO - __main__ - Step 50 Global step 50 Train loss 1.07 on epoch=24
06/22/2022 13:27:41 - INFO - __main__ - Global step 50 Train loss 2.28 Classification-F1 0.3816425120772947 on epoch=24
06/22/2022 13:27:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3816425120772947 on epoch=24, global_step=50
06/22/2022 13:27:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.81 on epoch=29
06/22/2022 13:27:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.65 on epoch=34
06/22/2022 13:27:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=39
06/22/2022 13:27:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=44
06/22/2022 13:27:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=49
06/22/2022 13:27:47 - INFO - __main__ - Global step 100 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 13:27:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=54
06/22/2022 13:27:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=59
06/22/2022 13:27:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=64
06/22/2022 13:27:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=69
06/22/2022 13:27:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=74
06/22/2022 13:27:54 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 13:27:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=79
06/22/2022 13:27:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=84
06/22/2022 13:27:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
06/22/2022 13:27:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=94
06/22/2022 13:28:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=99
06/22/2022 13:28:00 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 13:28:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=104
06/22/2022 13:28:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=109
06/22/2022 13:28:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=114
06/22/2022 13:28:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/22/2022 13:28:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
06/22/2022 13:28:07 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 13:28:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
06/22/2022 13:28:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=134
06/22/2022 13:28:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
06/22/2022 13:28:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
06/22/2022 13:28:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=149
06/22/2022 13:28:13 - INFO - __main__ - Global step 300 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=149
06/22/2022 13:28:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=154
06/22/2022 13:28:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=159
06/22/2022 13:28:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=164
06/22/2022 13:28:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=169
06/22/2022 13:28:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=174
06/22/2022 13:28:20 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=174
06/22/2022 13:28:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=179
06/22/2022 13:28:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=184
06/22/2022 13:28:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=189
06/22/2022 13:28:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=194
06/22/2022 13:28:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=199
06/22/2022 13:28:26 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.3992490613266583 on epoch=199
06/22/2022 13:28:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.3992490613266583 on epoch=199, global_step=400
06/22/2022 13:28:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=204
06/22/2022 13:28:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=209
06/22/2022 13:28:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=214
06/22/2022 13:28:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=219
06/22/2022 13:28:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=224
06/22/2022 13:28:32 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.5134502923976608 on epoch=224
06/22/2022 13:28:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5134502923976608 on epoch=224, global_step=450
06/22/2022 13:28:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=229
06/22/2022 13:28:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=234
06/22/2022 13:28:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=239
06/22/2022 13:28:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=244
06/22/2022 13:28:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=249
06/22/2022 13:28:39 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.5134502923976608 on epoch=249
06/22/2022 13:28:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=254
06/22/2022 13:28:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=259
06/22/2022 13:28:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
06/22/2022 13:28:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=269
06/22/2022 13:28:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=274
06/22/2022 13:28:45 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.4589371980676329 on epoch=274
06/22/2022 13:28:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=279
06/22/2022 13:28:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=284
06/22/2022 13:28:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
06/22/2022 13:28:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=294
06/22/2022 13:28:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
06/22/2022 13:28:52 - INFO - __main__ - Global step 600 Train loss 0.28 Classification-F1 0.5134502923976608 on epoch=299
06/22/2022 13:28:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=304
06/22/2022 13:28:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
06/22/2022 13:28:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=314
06/22/2022 13:28:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=319
06/22/2022 13:28:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
06/22/2022 13:28:58 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.5636363636363637 on epoch=324
06/22/2022 13:28:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.5636363636363637 on epoch=324, global_step=650
06/22/2022 13:29:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=329
06/22/2022 13:29:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=334
06/22/2022 13:29:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=339
06/22/2022 13:29:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=344
06/22/2022 13:29:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=349
06/22/2022 13:29:05 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.5151515151515151 on epoch=349
06/22/2022 13:29:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=354
06/22/2022 13:29:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=359
06/22/2022 13:29:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=364
06/22/2022 13:29:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=369
06/22/2022 13:29:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
06/22/2022 13:29:11 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.49090909090909085 on epoch=374
06/22/2022 13:29:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=379
06/22/2022 13:29:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
06/22/2022 13:29:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=389
06/22/2022 13:29:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
06/22/2022 13:29:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=399
06/22/2022 13:29:18 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.5270935960591133 on epoch=399
06/22/2022 13:29:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
06/22/2022 13:29:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=409
06/22/2022 13:29:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=414
06/22/2022 13:29:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
06/22/2022 13:29:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/22/2022 13:29:24 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.5933528836754642 on epoch=424
06/22/2022 13:29:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5636363636363637 -> 0.5933528836754642 on epoch=424, global_step=850
06/22/2022 13:29:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/22/2022 13:29:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
06/22/2022 13:29:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/22/2022 13:29:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
06/22/2022 13:29:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=449
06/22/2022 13:29:31 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.5555555555555556 on epoch=449
06/22/2022 13:29:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/22/2022 13:29:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
06/22/2022 13:29:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
06/22/2022 13:29:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/22/2022 13:29:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/22/2022 13:29:37 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.5835835835835835 on epoch=474
06/22/2022 13:29:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/22/2022 13:29:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/22/2022 13:29:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
06/22/2022 13:29:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=494
06/22/2022 13:29:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
06/22/2022 13:29:44 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.5270935960591133 on epoch=499
06/22/2022 13:29:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
06/22/2022 13:29:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/22/2022 13:29:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
06/22/2022 13:29:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 13:29:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=524
06/22/2022 13:29:50 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.5 on epoch=524
06/22/2022 13:29:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
06/22/2022 13:29:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/22/2022 13:29:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
06/22/2022 13:29:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/22/2022 13:29:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 13:29:57 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=549
06/22/2022 13:29:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/22/2022 13:29:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/22/2022 13:30:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 13:30:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 13:30:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/22/2022 13:30:03 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=574
06/22/2022 13:30:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 13:30:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/22/2022 13:30:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/22/2022 13:30:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/22/2022 13:30:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 13:30:10 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=599
06/22/2022 13:30:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 13:30:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 13:30:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/22/2022 13:30:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/22/2022 13:30:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 13:30:17 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4980392156862745 on epoch=624
06/22/2022 13:30:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 13:30:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 13:30:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 13:30:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 13:30:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 13:30:23 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=649
06/22/2022 13:30:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 13:30:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/22/2022 13:30:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 13:30:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 13:30:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/22/2022 13:30:30 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4980392156862745 on epoch=674
06/22/2022 13:30:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 13:30:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/22/2022 13:30:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/22/2022 13:30:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 13:30:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/22/2022 13:30:36 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=699
06/22/2022 13:30:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 13:30:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 13:30:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 13:30:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/22/2022 13:30:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/22/2022 13:30:43 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=724
06/22/2022 13:30:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 13:30:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 13:30:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 13:30:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 13:30:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 13:30:49 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=749
06/22/2022 13:30:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 13:30:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 13:30:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 13:30:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/22/2022 13:30:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/22/2022 13:30:56 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=774
06/22/2022 13:30:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 13:30:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 13:31:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 13:31:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 13:31:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 13:31:03 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=799
06/22/2022 13:31:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 13:31:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 13:31:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/22/2022 13:31:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 13:31:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 13:31:09 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=824
06/22/2022 13:31:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 13:31:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/22/2022 13:31:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
06/22/2022 13:31:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 13:31:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/22/2022 13:31:16 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=849
06/22/2022 13:31:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/22/2022 13:31:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 13:31:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 13:31:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 13:31:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 13:31:22 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=874
06/22/2022 13:31:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 13:31:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 13:31:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 13:31:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 13:31:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 13:31:29 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=899
06/22/2022 13:31:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 13:31:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/22/2022 13:31:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 13:31:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 13:31:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 13:31:35 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=924
06/22/2022 13:31:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 13:31:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 13:31:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 13:31:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 13:31:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/22/2022 13:31:42 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5 on epoch=949
06/22/2022 13:31:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 13:31:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 13:31:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 13:31:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 13:31:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 13:31:49 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=974
06/22/2022 13:31:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 13:31:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 13:31:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 13:31:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 13:31:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 13:31:55 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=999
06/22/2022 13:31:55 - INFO - __main__ - save last model!
06/22/2022 13:31:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 13:31:55 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 13:31:55 - INFO - __main__ - Printing 3 examples
06/22/2022 13:31:55 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 13:31:55 - INFO - __main__ - ['0']
06/22/2022 13:31:55 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 13:31:55 - INFO - __main__ - ['1']
06/22/2022 13:31:55 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 13:31:55 - INFO - __main__ - ['1']
06/22/2022 13:31:55 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:31:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:31:56 - INFO - __main__ - Printing 3 examples
06/22/2022 13:31:56 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/22/2022 13:31:56 - INFO - __main__ - ['1']
06/22/2022 13:31:56 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/22/2022 13:31:56 - INFO - __main__ - ['1']
06/22/2022 13:31:56 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/22/2022 13:31:56 - INFO - __main__ - ['1']
06/22/2022 13:31:56 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:31:56 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:31:56 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:31:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:31:56 - INFO - __main__ - Printing 3 examples
06/22/2022 13:31:56 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/22/2022 13:31:56 - INFO - __main__ - ['1']
06/22/2022 13:31:56 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/22/2022 13:31:56 - INFO - __main__ - ['1']
06/22/2022 13:31:56 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/22/2022 13:31:56 - INFO - __main__ - ['1']
06/22/2022 13:31:56 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:31:56 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:31:56 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:31:59 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:32:01 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:32:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:32:01 - INFO - __main__ - Starting training!
06/22/2022 13:32:07 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 13:33:51 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_100_0.4_8_predictions.txt
06/22/2022 13:33:51 - INFO - __main__ - Classification-F1 on test data: 0.4948
06/22/2022 13:33:51 - INFO - __main__ - prefix=paws_16_100, lr=0.4, bsz=8, dev_performance=0.5933528836754642, test_performance=0.49477185791709066
06/22/2022 13:33:51 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.3, bsz=8 ...
06/22/2022 13:33:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:33:52 - INFO - __main__ - Printing 3 examples
06/22/2022 13:33:52 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/22/2022 13:33:52 - INFO - __main__ - ['1']
06/22/2022 13:33:52 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/22/2022 13:33:52 - INFO - __main__ - ['1']
06/22/2022 13:33:52 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/22/2022 13:33:52 - INFO - __main__ - ['1']
06/22/2022 13:33:52 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:33:52 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:33:52 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:33:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:33:52 - INFO - __main__ - Printing 3 examples
06/22/2022 13:33:52 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/22/2022 13:33:52 - INFO - __main__ - ['1']
06/22/2022 13:33:52 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/22/2022 13:33:52 - INFO - __main__ - ['1']
06/22/2022 13:33:52 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/22/2022 13:33:52 - INFO - __main__ - ['1']
06/22/2022 13:33:52 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:33:52 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:33:52 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:33:58 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:33:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:33:58 - INFO - __main__ - Starting training!
06/22/2022 13:34:00 - INFO - __main__ - Step 10 Global step 10 Train loss 4.23 on epoch=4
06/22/2022 13:34:01 - INFO - __main__ - Step 20 Global step 20 Train loss 3.14 on epoch=9
06/22/2022 13:34:02 - INFO - __main__ - Step 30 Global step 30 Train loss 2.24 on epoch=14
06/22/2022 13:34:03 - INFO - __main__ - Step 40 Global step 40 Train loss 1.48 on epoch=19
06/22/2022 13:34:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.98 on epoch=24
06/22/2022 13:34:05 - INFO - __main__ - Global step 50 Train loss 2.42 Classification-F1 0.3191489361702127 on epoch=24
06/22/2022 13:34:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3191489361702127 on epoch=24, global_step=50
06/22/2022 13:34:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.80 on epoch=29
06/22/2022 13:34:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.76 on epoch=34
06/22/2022 13:34:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.69 on epoch=39
06/22/2022 13:34:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=44
06/22/2022 13:34:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=49
06/22/2022 13:34:11 - INFO - __main__ - Global step 100 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 13:34:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3191489361702127 -> 0.3333333333333333 on epoch=49, global_step=100
06/22/2022 13:34:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=54
06/22/2022 13:34:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=59
06/22/2022 13:34:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=64
06/22/2022 13:34:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=69
06/22/2022 13:34:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=74
06/22/2022 13:34:18 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 13:34:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=79
06/22/2022 13:34:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=84
06/22/2022 13:34:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=89
06/22/2022 13:34:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=94
06/22/2022 13:34:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
06/22/2022 13:34:25 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 13:34:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=104
06/22/2022 13:34:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=109
06/22/2022 13:34:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=114
06/22/2022 13:34:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=119
06/22/2022 13:34:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=124
06/22/2022 13:34:31 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 13:34:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=129
06/22/2022 13:34:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=134
06/22/2022 13:34:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=139
06/22/2022 13:34:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=144
06/22/2022 13:34:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
06/22/2022 13:34:38 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.4589371980676329 on epoch=149
06/22/2022 13:34:38 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=149, global_step=300
06/22/2022 13:34:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.34 on epoch=154
06/22/2022 13:34:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=159
06/22/2022 13:34:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=164
06/22/2022 13:34:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=169
06/22/2022 13:34:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
06/22/2022 13:34:44 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.3992490613266583 on epoch=174
06/22/2022 13:34:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=179
06/22/2022 13:34:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=184
06/22/2022 13:34:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=189
06/22/2022 13:34:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=194
06/22/2022 13:34:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=199
06/22/2022 13:34:51 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.4589371980676329 on epoch=199
06/22/2022 13:34:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=204
06/22/2022 13:34:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=209
06/22/2022 13:34:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=214
06/22/2022 13:34:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.30 on epoch=219
06/22/2022 13:34:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=224
06/22/2022 13:34:57 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.4589371980676329 on epoch=224
06/22/2022 13:34:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
06/22/2022 13:35:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
06/22/2022 13:35:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=239
06/22/2022 13:35:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=244
06/22/2022 13:35:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=249
06/22/2022 13:35:04 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.4589371980676329 on epoch=249
06/22/2022 13:35:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.32 on epoch=254
06/22/2022 13:35:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=259
06/22/2022 13:35:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=264
06/22/2022 13:35:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=269
06/22/2022 13:35:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=274
06/22/2022 13:35:10 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.5636363636363637 on epoch=274
06/22/2022 13:35:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5636363636363637 on epoch=274, global_step=550
06/22/2022 13:35:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
06/22/2022 13:35:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
06/22/2022 13:35:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
06/22/2022 13:35:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
06/22/2022 13:35:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
06/22/2022 13:35:17 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.5465587044534412 on epoch=299
06/22/2022 13:35:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=304
06/22/2022 13:35:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=309
06/22/2022 13:35:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=314
06/22/2022 13:35:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=319
06/22/2022 13:35:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=324
06/22/2022 13:35:24 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.5555555555555556 on epoch=324
06/22/2022 13:35:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=329
06/22/2022 13:35:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.17 on epoch=334
06/22/2022 13:35:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=339
06/22/2022 13:35:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=344
06/22/2022 13:35:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=349
06/22/2022 13:35:30 - INFO - __main__ - Global step 700 Train loss 0.17 Classification-F1 0.5307917888563051 on epoch=349
06/22/2022 13:35:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=354
06/22/2022 13:35:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=359
06/22/2022 13:35:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=364
06/22/2022 13:35:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
06/22/2022 13:35:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=374
06/22/2022 13:35:37 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.4920634920634921 on epoch=374
06/22/2022 13:35:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
06/22/2022 13:35:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
06/22/2022 13:35:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=389
06/22/2022 13:35:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
06/22/2022 13:35:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
06/22/2022 13:35:43 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.5195195195195195 on epoch=399
06/22/2022 13:35:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
06/22/2022 13:35:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=409
06/22/2022 13:35:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=414
06/22/2022 13:35:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
06/22/2022 13:35:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/22/2022 13:35:50 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.464039408866995 on epoch=424
06/22/2022 13:35:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
06/22/2022 13:35:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
06/22/2022 13:35:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/22/2022 13:35:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/22/2022 13:35:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
06/22/2022 13:35:57 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.464039408866995 on epoch=449
06/22/2022 13:35:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/22/2022 13:35:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/22/2022 13:36:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 13:36:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
06/22/2022 13:36:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/22/2022 13:36:03 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=474
06/22/2022 13:36:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/22/2022 13:36:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/22/2022 13:36:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/22/2022 13:36:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
06/22/2022 13:36:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/22/2022 13:36:10 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=499
06/22/2022 13:36:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 13:36:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/22/2022 13:36:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 13:36:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 13:36:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
06/22/2022 13:36:16 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=524
06/22/2022 13:36:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/22/2022 13:36:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
06/22/2022 13:36:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 13:36:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/22/2022 13:36:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
06/22/2022 13:36:23 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=549
06/22/2022 13:36:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/22/2022 13:36:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/22/2022 13:36:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 13:36:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 13:36:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 13:36:30 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=574
06/22/2022 13:36:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 13:36:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/22/2022 13:36:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/22/2022 13:36:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/22/2022 13:36:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 13:36:36 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=599
06/22/2022 13:36:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 13:36:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 13:36:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
06/22/2022 13:36:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/22/2022 13:36:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/22/2022 13:36:43 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=624
06/22/2022 13:36:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 13:36:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 13:36:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 13:36:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 13:36:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/22/2022 13:36:49 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=649
06/22/2022 13:36:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 13:36:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 13:36:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/22/2022 13:36:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 13:36:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 13:36:56 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=674
06/22/2022 13:36:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 13:36:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/22/2022 13:37:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 13:37:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 13:37:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
06/22/2022 13:37:02 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=699
06/22/2022 13:37:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 13:37:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/22/2022 13:37:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 13:37:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 13:37:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 13:37:09 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=724
06/22/2022 13:37:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/22/2022 13:37:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 13:37:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/22/2022 13:37:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 13:37:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=749
06/22/2022 13:37:16 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=749
06/22/2022 13:37:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 13:37:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/22/2022 13:37:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 13:37:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 13:37:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/22/2022 13:37:22 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=774
06/22/2022 13:37:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/22/2022 13:37:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 13:37:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 13:37:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 13:37:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 13:37:29 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=799
06/22/2022 13:37:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 13:37:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 13:37:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 13:37:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 13:37:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 13:37:35 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=824
06/22/2022 13:37:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 13:37:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 13:37:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/22/2022 13:37:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 13:37:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 13:37:42 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=849
06/22/2022 13:37:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 13:37:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 13:37:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 13:37:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 13:37:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 13:37:49 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=874
06/22/2022 13:37:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 13:37:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 13:37:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 13:37:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 13:37:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 13:37:55 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=899
06/22/2022 13:37:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 13:37:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 13:37:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 13:38:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 13:38:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 13:38:02 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=924
06/22/2022 13:38:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 13:38:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 13:38:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 13:38:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 13:38:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 13:38:09 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=949
06/22/2022 13:38:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 13:38:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 13:38:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 13:38:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 13:38:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 13:38:15 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=974
06/22/2022 13:38:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 13:38:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/22/2022 13:38:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 13:38:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 13:38:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 13:38:22 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=999
06/22/2022 13:38:22 - INFO - __main__ - save last model!
06/22/2022 13:38:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 13:38:22 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 13:38:22 - INFO - __main__ - Printing 3 examples
06/22/2022 13:38:22 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 13:38:22 - INFO - __main__ - ['0']
06/22/2022 13:38:22 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 13:38:22 - INFO - __main__ - ['1']
06/22/2022 13:38:22 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 13:38:22 - INFO - __main__ - ['1']
06/22/2022 13:38:22 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:38:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:38:22 - INFO - __main__ - Printing 3 examples
06/22/2022 13:38:22 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/22/2022 13:38:22 - INFO - __main__ - ['1']
06/22/2022 13:38:22 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/22/2022 13:38:22 - INFO - __main__ - ['1']
06/22/2022 13:38:22 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/22/2022 13:38:22 - INFO - __main__ - ['1']
06/22/2022 13:38:22 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:38:22 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:38:22 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:38:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:38:22 - INFO - __main__ - Printing 3 examples
06/22/2022 13:38:22 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/22/2022 13:38:22 - INFO - __main__ - ['1']
06/22/2022 13:38:22 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/22/2022 13:38:22 - INFO - __main__ - ['1']
06/22/2022 13:38:22 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/22/2022 13:38:22 - INFO - __main__ - ['1']
06/22/2022 13:38:22 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:38:22 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:38:23 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:38:26 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:38:28 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:38:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:38:28 - INFO - __main__ - Starting training!
06/22/2022 13:38:34 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 13:40:15 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_100_0.3_8_predictions.txt
06/22/2022 13:40:15 - INFO - __main__ - Classification-F1 on test data: 0.3366
06/22/2022 13:40:15 - INFO - __main__ - prefix=paws_16_100, lr=0.3, bsz=8, dev_performance=0.5636363636363637, test_performance=0.33664067582616325
06/22/2022 13:40:15 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.2, bsz=8 ...
06/22/2022 13:40:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:40:16 - INFO - __main__ - Printing 3 examples
06/22/2022 13:40:16 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/22/2022 13:40:16 - INFO - __main__ - ['1']
06/22/2022 13:40:16 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/22/2022 13:40:16 - INFO - __main__ - ['1']
06/22/2022 13:40:16 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/22/2022 13:40:16 - INFO - __main__ - ['1']
06/22/2022 13:40:16 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:40:16 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:40:16 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:40:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:40:17 - INFO - __main__ - Printing 3 examples
06/22/2022 13:40:17 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/22/2022 13:40:17 - INFO - __main__ - ['1']
06/22/2022 13:40:17 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/22/2022 13:40:17 - INFO - __main__ - ['1']
06/22/2022 13:40:17 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/22/2022 13:40:17 - INFO - __main__ - ['1']
06/22/2022 13:40:17 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:40:17 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:40:17 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:40:22 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:40:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:40:22 - INFO - __main__ - Starting training!
06/22/2022 13:40:23 - INFO - __main__ - Step 10 Global step 10 Train loss 4.49 on epoch=4
06/22/2022 13:40:25 - INFO - __main__ - Step 20 Global step 20 Train loss 3.45 on epoch=9
06/22/2022 13:40:26 - INFO - __main__ - Step 30 Global step 30 Train loss 2.89 on epoch=14
06/22/2022 13:40:27 - INFO - __main__ - Step 40 Global step 40 Train loss 2.19 on epoch=19
06/22/2022 13:40:28 - INFO - __main__ - Step 50 Global step 50 Train loss 1.88 on epoch=24
06/22/2022 13:40:29 - INFO - __main__ - Global step 50 Train loss 2.98 Classification-F1 0.3333333333333333 on epoch=24
06/22/2022 13:40:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/22/2022 13:40:30 - INFO - __main__ - Step 60 Global step 60 Train loss 1.41 on epoch=29
06/22/2022 13:40:31 - INFO - __main__ - Step 70 Global step 70 Train loss 1.06 on epoch=34
06/22/2022 13:40:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=39
06/22/2022 13:40:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=44
06/22/2022 13:40:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.75 on epoch=49
06/22/2022 13:40:35 - INFO - __main__ - Global step 100 Train loss 1.00 Classification-F1 0.37662337662337664 on epoch=49
06/22/2022 13:40:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.37662337662337664 on epoch=49, global_step=100
06/22/2022 13:40:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.73 on epoch=54
06/22/2022 13:40:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.72 on epoch=59
06/22/2022 13:40:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=64
06/22/2022 13:40:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=69
06/22/2022 13:40:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=74
06/22/2022 13:40:42 - INFO - __main__ - Global step 150 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 13:40:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=79
06/22/2022 13:40:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=84
06/22/2022 13:40:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=89
06/22/2022 13:40:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=94
06/22/2022 13:40:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=99
06/22/2022 13:40:48 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 13:40:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=104
06/22/2022 13:40:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=109
06/22/2022 13:40:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=114
06/22/2022 13:40:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=119
06/22/2022 13:40:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.52 on epoch=124
06/22/2022 13:40:54 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 13:40:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=129
06/22/2022 13:40:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=134
06/22/2022 13:40:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=139
06/22/2022 13:40:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=144
06/22/2022 13:41:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=149
06/22/2022 13:41:01 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=149
06/22/2022 13:41:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=154
06/22/2022 13:41:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=159
06/22/2022 13:41:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=164
06/22/2022 13:41:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=169
06/22/2022 13:41:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
06/22/2022 13:41:08 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=174
06/22/2022 13:41:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=179
06/22/2022 13:41:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=184
06/22/2022 13:41:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=189
06/22/2022 13:41:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=194
06/22/2022 13:41:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=199
06/22/2022 13:41:14 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=199
06/22/2022 13:41:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=204
06/22/2022 13:41:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=209
06/22/2022 13:41:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=214
06/22/2022 13:41:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=219
06/22/2022 13:41:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=224
06/22/2022 13:41:21 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3992490613266583 on epoch=224
06/22/2022 13:41:21 - INFO - __main__ - Saving model with best Classification-F1: 0.37662337662337664 -> 0.3992490613266583 on epoch=224, global_step=450
06/22/2022 13:41:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=229
06/22/2022 13:41:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=234
06/22/2022 13:41:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=239
06/22/2022 13:41:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=244
06/22/2022 13:41:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=249
06/22/2022 13:41:27 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=249
06/22/2022 13:41:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=254
06/22/2022 13:41:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=259
06/22/2022 13:41:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=264
06/22/2022 13:41:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
06/22/2022 13:41:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=274
06/22/2022 13:41:34 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.3992490613266583 on epoch=274
06/22/2022 13:41:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=279
06/22/2022 13:41:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=284
06/22/2022 13:41:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
06/22/2022 13:41:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=294
06/22/2022 13:41:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=299
06/22/2022 13:41:40 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.3992490613266583 on epoch=299
06/22/2022 13:41:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=304
06/22/2022 13:41:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=309
06/22/2022 13:41:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=314
06/22/2022 13:41:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=319
06/22/2022 13:41:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=324
06/22/2022 13:41:47 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.4589371980676329 on epoch=324
06/22/2022 13:41:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4589371980676329 on epoch=324, global_step=650
06/22/2022 13:41:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=329
06/22/2022 13:41:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=334
06/22/2022 13:41:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=339
06/22/2022 13:41:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=344
06/22/2022 13:41:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=349
06/22/2022 13:41:53 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.4589371980676329 on epoch=349
06/22/2022 13:41:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=354
06/22/2022 13:41:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=359
06/22/2022 13:41:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=364
06/22/2022 13:41:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=369
06/22/2022 13:42:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=374
06/22/2022 13:42:00 - INFO - __main__ - Global step 750 Train loss 0.31 Classification-F1 0.5134502923976608 on epoch=374
06/22/2022 13:42:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5134502923976608 on epoch=374, global_step=750
06/22/2022 13:42:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=379
06/22/2022 13:42:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=384
06/22/2022 13:42:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=389
06/22/2022 13:42:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=394
06/22/2022 13:42:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=399
06/22/2022 13:42:07 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.539313399778516 on epoch=399
06/22/2022 13:42:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.539313399778516 on epoch=399, global_step=800
06/22/2022 13:42:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=404
06/22/2022 13:42:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=409
06/22/2022 13:42:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=414
06/22/2022 13:42:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=419
06/22/2022 13:42:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.31 on epoch=424
06/22/2022 13:42:13 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.5636363636363637 on epoch=424
06/22/2022 13:42:13 - INFO - __main__ - Saving model with best Classification-F1: 0.539313399778516 -> 0.5636363636363637 on epoch=424, global_step=850
06/22/2022 13:42:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=429
06/22/2022 13:42:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=434
06/22/2022 13:42:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=439
06/22/2022 13:42:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=444
06/22/2022 13:42:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=449
06/22/2022 13:42:20 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.5151515151515151 on epoch=449
06/22/2022 13:42:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=454
06/22/2022 13:42:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=459
06/22/2022 13:42:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=464
06/22/2022 13:42:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=469
06/22/2022 13:42:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=474
06/22/2022 13:42:26 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.5636363636363637 on epoch=474
06/22/2022 13:42:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=479
06/22/2022 13:42:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=484
06/22/2022 13:42:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=489
06/22/2022 13:42:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=494
06/22/2022 13:42:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=499
06/22/2022 13:42:33 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.5333333333333333 on epoch=499
06/22/2022 13:42:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=504
06/22/2022 13:42:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=509
06/22/2022 13:42:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=514
06/22/2022 13:42:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=519
06/22/2022 13:42:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=524
06/22/2022 13:42:39 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.4909862142099682 on epoch=524
06/22/2022 13:42:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=529
06/22/2022 13:42:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=534
06/22/2022 13:42:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=539
06/22/2022 13:42:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=544
06/22/2022 13:42:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=549
06/22/2022 13:42:46 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.5270935960591133 on epoch=549
06/22/2022 13:42:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=554
06/22/2022 13:42:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=559
06/22/2022 13:42:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=564
06/22/2022 13:42:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=569
06/22/2022 13:42:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=574
06/22/2022 13:42:53 - INFO - __main__ - Global step 1150 Train loss 0.13 Classification-F1 0.5307917888563051 on epoch=574
06/22/2022 13:42:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=579
06/22/2022 13:42:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=584
06/22/2022 13:42:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=589
06/22/2022 13:42:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=594
06/22/2022 13:42:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
06/22/2022 13:42:59 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.5270935960591133 on epoch=599
06/22/2022 13:43:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=604
06/22/2022 13:43:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=609
06/22/2022 13:43:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=614
06/22/2022 13:43:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=619
06/22/2022 13:43:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=624
06/22/2022 13:43:06 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.5555555555555556 on epoch=624
06/22/2022 13:43:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
06/22/2022 13:43:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=634
06/22/2022 13:43:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
06/22/2022 13:43:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
06/22/2022 13:43:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=649
06/22/2022 13:43:13 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.5307917888563051 on epoch=649
06/22/2022 13:43:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=654
06/22/2022 13:43:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
06/22/2022 13:43:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
06/22/2022 13:43:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
06/22/2022 13:43:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=674
06/22/2022 13:43:19 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.4920634920634921 on epoch=674
06/22/2022 13:43:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
06/22/2022 13:43:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/22/2022 13:43:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
06/22/2022 13:43:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=694
06/22/2022 13:43:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/22/2022 13:43:26 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.5607843137254902 on epoch=699
06/22/2022 13:43:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
06/22/2022 13:43:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
06/22/2022 13:43:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
06/22/2022 13:43:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
06/22/2022 13:43:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
06/22/2022 13:43:32 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.5270935960591133 on epoch=724
06/22/2022 13:43:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/22/2022 13:43:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
06/22/2022 13:43:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
06/22/2022 13:43:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
06/22/2022 13:43:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/22/2022 13:43:39 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=749
06/22/2022 13:43:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
06/22/2022 13:43:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
06/22/2022 13:43:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
06/22/2022 13:43:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
06/22/2022 13:43:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/22/2022 13:43:46 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.5933528836754642 on epoch=774
06/22/2022 13:43:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5636363636363637 -> 0.5933528836754642 on epoch=774, global_step=1550
06/22/2022 13:43:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/22/2022 13:43:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
06/22/2022 13:43:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 13:43:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
06/22/2022 13:43:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/22/2022 13:43:53 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.5933528836754642 on epoch=799
06/22/2022 13:43:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 13:43:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
06/22/2022 13:43:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 13:43:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
06/22/2022 13:43:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
06/22/2022 13:43:59 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.5625 on epoch=824
06/22/2022 13:44:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
06/22/2022 13:44:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/22/2022 13:44:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
06/22/2022 13:44:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/22/2022 13:44:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 13:44:06 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.5625 on epoch=849
06/22/2022 13:44:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/22/2022 13:44:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/22/2022 13:44:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/22/2022 13:44:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
06/22/2022 13:44:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/22/2022 13:44:13 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=874
06/22/2022 13:44:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 13:44:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=884
06/22/2022 13:44:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/22/2022 13:44:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/22/2022 13:44:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 13:44:19 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.5625 on epoch=899
06/22/2022 13:44:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/22/2022 13:44:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/22/2022 13:44:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/22/2022 13:44:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/22/2022 13:44:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/22/2022 13:44:26 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=924
06/22/2022 13:44:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 13:44:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 13:44:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/22/2022 13:44:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/22/2022 13:44:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
06/22/2022 13:44:33 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5625 on epoch=949
06/22/2022 13:44:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/22/2022 13:44:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
06/22/2022 13:44:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 13:44:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/22/2022 13:44:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/22/2022 13:44:40 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=974
06/22/2022 13:44:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
06/22/2022 13:44:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/22/2022 13:44:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
06/22/2022 13:44:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/22/2022 13:44:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/22/2022 13:44:46 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=999
06/22/2022 13:44:46 - INFO - __main__ - save last model!
06/22/2022 13:44:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 13:44:46 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 13:44:46 - INFO - __main__ - Printing 3 examples
06/22/2022 13:44:46 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 13:44:46 - INFO - __main__ - ['0']
06/22/2022 13:44:46 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 13:44:46 - INFO - __main__ - ['1']
06/22/2022 13:44:46 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 13:44:46 - INFO - __main__ - ['1']
06/22/2022 13:44:46 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:44:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:44:47 - INFO - __main__ - Printing 3 examples
06/22/2022 13:44:47 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/22/2022 13:44:47 - INFO - __main__ - ['1']
06/22/2022 13:44:47 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/22/2022 13:44:47 - INFO - __main__ - ['1']
06/22/2022 13:44:47 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/22/2022 13:44:47 - INFO - __main__ - ['1']
06/22/2022 13:44:47 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:44:47 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:44:47 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:44:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:44:47 - INFO - __main__ - Printing 3 examples
06/22/2022 13:44:47 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/22/2022 13:44:47 - INFO - __main__ - ['1']
06/22/2022 13:44:47 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/22/2022 13:44:47 - INFO - __main__ - ['1']
06/22/2022 13:44:47 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/22/2022 13:44:47 - INFO - __main__ - ['1']
06/22/2022 13:44:47 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:44:47 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:44:47 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:44:50 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:44:52 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:44:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:44:53 - INFO - __main__ - Starting training!
06/22/2022 13:44:58 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 13:46:48 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_100_0.2_8_predictions.txt
06/22/2022 13:46:48 - INFO - __main__ - Classification-F1 on test data: 0.5072
06/22/2022 13:46:48 - INFO - __main__ - prefix=paws_16_100, lr=0.2, bsz=8, dev_performance=0.5933528836754642, test_performance=0.5071888287978357
06/22/2022 13:46:48 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.5, bsz=8 ...
06/22/2022 13:46:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:46:49 - INFO - __main__ - Printing 3 examples
06/22/2022 13:46:49 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/22/2022 13:46:49 - INFO - __main__ - ['1']
06/22/2022 13:46:49 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/22/2022 13:46:49 - INFO - __main__ - ['1']
06/22/2022 13:46:49 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/22/2022 13:46:49 - INFO - __main__ - ['1']
06/22/2022 13:46:49 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:46:49 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:46:49 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:46:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:46:49 - INFO - __main__ - Printing 3 examples
06/22/2022 13:46:49 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/22/2022 13:46:49 - INFO - __main__ - ['1']
06/22/2022 13:46:49 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/22/2022 13:46:49 - INFO - __main__ - ['1']
06/22/2022 13:46:49 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/22/2022 13:46:49 - INFO - __main__ - ['1']
06/22/2022 13:46:49 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:46:49 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:46:49 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:46:55 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:46:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:46:55 - INFO - __main__ - Starting training!
06/22/2022 13:46:56 - INFO - __main__ - Step 10 Global step 10 Train loss 4.04 on epoch=4
06/22/2022 13:46:58 - INFO - __main__ - Step 20 Global step 20 Train loss 2.33 on epoch=9
06/22/2022 13:46:59 - INFO - __main__ - Step 30 Global step 30 Train loss 1.14 on epoch=14
06/22/2022 13:47:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.80 on epoch=19
06/22/2022 13:47:01 - INFO - __main__ - Step 50 Global step 50 Train loss 0.65 on epoch=24
06/22/2022 13:47:02 - INFO - __main__ - Global step 50 Train loss 1.79 Classification-F1 0.36374269005847953 on epoch=24
06/22/2022 13:47:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.36374269005847953 on epoch=24, global_step=50
06/22/2022 13:47:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.62 on epoch=29
06/22/2022 13:47:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=34
06/22/2022 13:47:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=39
06/22/2022 13:47:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=44
06/22/2022 13:47:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=49
06/22/2022 13:47:08 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 13:47:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
06/22/2022 13:47:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=59
06/22/2022 13:47:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
06/22/2022 13:47:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=69
06/22/2022 13:47:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
06/22/2022 13:47:15 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3816425120772947 on epoch=74
06/22/2022 13:47:15 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.3816425120772947 on epoch=74, global_step=150
06/22/2022 13:47:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=79
06/22/2022 13:47:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=84
06/22/2022 13:47:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
06/22/2022 13:47:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=94
06/22/2022 13:47:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=99
06/22/2022 13:47:21 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 13:47:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
06/22/2022 13:47:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
06/22/2022 13:47:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
06/22/2022 13:47:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/22/2022 13:47:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=124
06/22/2022 13:47:28 - INFO - __main__ - Global step 250 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 13:47:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.35 on epoch=129
06/22/2022 13:47:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
06/22/2022 13:47:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=139
06/22/2022 13:47:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=144
06/22/2022 13:47:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=149
06/22/2022 13:47:35 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.36374269005847953 on epoch=149
06/22/2022 13:47:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/22/2022 13:47:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
06/22/2022 13:47:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=164
06/22/2022 13:47:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/22/2022 13:47:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=174
06/22/2022 13:47:41 - INFO - __main__ - Global step 350 Train loss 0.31 Classification-F1 0.46843853820598 on epoch=174
06/22/2022 13:47:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.46843853820598 on epoch=174, global_step=350
06/22/2022 13:47:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=179
06/22/2022 13:47:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=184
06/22/2022 13:47:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=189
06/22/2022 13:47:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=194
06/22/2022 13:47:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
06/22/2022 13:47:48 - INFO - __main__ - Global step 400 Train loss 0.30 Classification-F1 0.46843853820598 on epoch=199
06/22/2022 13:47:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/22/2022 13:47:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
06/22/2022 13:47:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
06/22/2022 13:47:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
06/22/2022 13:47:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=224
06/22/2022 13:47:54 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.5333333333333333 on epoch=224
06/22/2022 13:47:54 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5333333333333333 on epoch=224, global_step=450
06/22/2022 13:47:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=229
06/22/2022 13:47:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
06/22/2022 13:47:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=239
06/22/2022 13:47:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=244
06/22/2022 13:48:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=249
06/22/2022 13:48:01 - INFO - __main__ - Global step 500 Train loss 0.17 Classification-F1 0.464039408866995 on epoch=249
06/22/2022 13:48:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
06/22/2022 13:48:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=259
06/22/2022 13:48:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
06/22/2022 13:48:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
06/22/2022 13:48:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
06/22/2022 13:48:07 - INFO - __main__ - Global step 550 Train loss 0.09 Classification-F1 0.4909862142099682 on epoch=274
06/22/2022 13:48:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
06/22/2022 13:48:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
06/22/2022 13:48:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
06/22/2022 13:48:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
06/22/2022 13:48:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
06/22/2022 13:48:14 - INFO - __main__ - Global step 600 Train loss 0.04 Classification-F1 0.5607843137254902 on epoch=299
06/22/2022 13:48:14 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5607843137254902 on epoch=299, global_step=600
06/22/2022 13:48:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
06/22/2022 13:48:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
06/22/2022 13:48:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
06/22/2022 13:48:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
06/22/2022 13:48:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/22/2022 13:48:20 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.5195195195195195 on epoch=324
06/22/2022 13:48:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
06/22/2022 13:48:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
06/22/2022 13:48:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
06/22/2022 13:48:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
06/22/2022 13:48:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
06/22/2022 13:48:27 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.5625 on epoch=349
06/22/2022 13:48:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.5625 on epoch=349, global_step=700
06/22/2022 13:48:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
06/22/2022 13:48:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/22/2022 13:48:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
06/22/2022 13:48:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/22/2022 13:48:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/22/2022 13:48:34 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.5625 on epoch=374
06/22/2022 13:48:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/22/2022 13:48:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/22/2022 13:48:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
06/22/2022 13:48:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/22/2022 13:48:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
06/22/2022 13:48:40 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=399
06/22/2022 13:48:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
06/22/2022 13:48:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/22/2022 13:48:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/22/2022 13:48:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
06/22/2022 13:48:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
06/22/2022 13:48:47 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=424
06/22/2022 13:48:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
06/22/2022 13:48:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
06/22/2022 13:48:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/22/2022 13:48:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/22/2022 13:48:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 13:48:54 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.5733333333333335 on epoch=449
06/22/2022 13:48:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5625 -> 0.5733333333333335 on epoch=449, global_step=900
06/22/2022 13:48:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
06/22/2022 13:48:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/22/2022 13:48:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 13:48:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/22/2022 13:49:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/22/2022 13:49:00 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.5625 on epoch=474
06/22/2022 13:49:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/22/2022 13:49:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/22/2022 13:49:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
06/22/2022 13:49:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/22/2022 13:49:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 13:49:07 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=499
06/22/2022 13:49:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/22/2022 13:49:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/22/2022 13:49:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/22/2022 13:49:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/22/2022 13:49:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/22/2022 13:49:13 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.5625 on epoch=524
06/22/2022 13:49:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/22/2022 13:49:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 13:49:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/22/2022 13:49:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/22/2022 13:49:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 13:49:20 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=549
06/22/2022 13:49:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5733333333333335 -> 0.5933528836754642 on epoch=549, global_step=1100
06/22/2022 13:49:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 13:49:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 13:49:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 13:49:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 13:49:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 13:49:26 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.5625 on epoch=574
06/22/2022 13:49:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/22/2022 13:49:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 13:49:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 13:49:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 13:49:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 13:49:33 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5835835835835835 on epoch=599
06/22/2022 13:49:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 13:49:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 13:49:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 13:49:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 13:49:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 13:49:39 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=624
06/22/2022 13:49:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 13:49:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 13:49:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 13:49:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 13:49:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
06/22/2022 13:49:46 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=649
06/22/2022 13:49:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 13:49:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 13:49:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 13:49:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 13:49:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 13:49:52 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=674
06/22/2022 13:49:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 13:49:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 13:49:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 13:49:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 13:49:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 13:49:59 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=699
06/22/2022 13:50:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
06/22/2022 13:50:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 13:50:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 13:50:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 13:50:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 13:50:05 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=724
06/22/2022 13:50:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 13:50:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 13:50:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 13:50:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 13:50:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 13:50:12 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.6190476190476191 on epoch=749
06/22/2022 13:50:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6190476190476191 on epoch=749, global_step=1500
06/22/2022 13:50:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 13:50:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 13:50:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 13:50:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 13:50:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 13:50:18 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=774
06/22/2022 13:50:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/22/2022 13:50:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 13:50:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 13:50:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 13:50:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 13:50:25 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.6235294117647059 on epoch=799
06/22/2022 13:50:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6190476190476191 -> 0.6235294117647059 on epoch=799, global_step=1600
06/22/2022 13:50:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 13:50:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 13:50:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 13:50:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 13:50:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 13:50:31 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=824
06/22/2022 13:50:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 13:50:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 13:50:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 13:50:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 13:50:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 13:50:38 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.6235294117647059 on epoch=849
06/22/2022 13:50:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 13:50:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 13:50:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 13:50:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 13:50:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 13:50:44 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.6235294117647059 on epoch=874
06/22/2022 13:50:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 13:50:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 13:50:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 13:50:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 13:50:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 13:50:51 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.6532019704433498 on epoch=899
06/22/2022 13:50:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6235294117647059 -> 0.6532019704433498 on epoch=899, global_step=1800
06/22/2022 13:50:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 13:50:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 13:50:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 13:50:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 13:50:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 13:50:57 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=924
06/22/2022 13:50:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 13:51:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 13:51:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 13:51:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 13:51:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 13:51:04 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=949
06/22/2022 13:51:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 13:51:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 13:51:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 13:51:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 13:51:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 13:51:11 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.6190476190476191 on epoch=974
06/22/2022 13:51:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 13:51:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 13:51:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 13:51:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 13:51:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 13:51:17 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=999
06/22/2022 13:51:17 - INFO - __main__ - save last model!
06/22/2022 13:51:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 13:51:17 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 13:51:17 - INFO - __main__ - Printing 3 examples
06/22/2022 13:51:17 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 13:51:17 - INFO - __main__ - ['0']
06/22/2022 13:51:17 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 13:51:17 - INFO - __main__ - ['1']
06/22/2022 13:51:17 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 13:51:17 - INFO - __main__ - ['1']
06/22/2022 13:51:17 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:51:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:51:18 - INFO - __main__ - Printing 3 examples
06/22/2022 13:51:18 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/22/2022 13:51:18 - INFO - __main__ - ['1']
06/22/2022 13:51:18 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/22/2022 13:51:18 - INFO - __main__ - ['1']
06/22/2022 13:51:18 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/22/2022 13:51:18 - INFO - __main__ - ['1']
06/22/2022 13:51:18 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:51:18 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:51:18 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:51:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:51:18 - INFO - __main__ - Printing 3 examples
06/22/2022 13:51:18 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/22/2022 13:51:18 - INFO - __main__ - ['1']
06/22/2022 13:51:18 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/22/2022 13:51:18 - INFO - __main__ - ['1']
06/22/2022 13:51:18 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/22/2022 13:51:18 - INFO - __main__ - ['1']
06/22/2022 13:51:18 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:51:18 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:51:18 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:51:21 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:51:23 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:51:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:51:23 - INFO - __main__ - Starting training!
06/22/2022 13:51:29 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 13:53:07 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_13_0.5_8_predictions.txt
06/22/2022 13:53:07 - INFO - __main__ - Classification-F1 on test data: 0.5138
06/22/2022 13:53:07 - INFO - __main__ - prefix=paws_16_13, lr=0.5, bsz=8, dev_performance=0.6532019704433498, test_performance=0.5137984850668436
06/22/2022 13:53:07 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.4, bsz=8 ...
06/22/2022 13:53:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:53:08 - INFO - __main__ - Printing 3 examples
06/22/2022 13:53:08 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/22/2022 13:53:08 - INFO - __main__ - ['1']
06/22/2022 13:53:08 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/22/2022 13:53:08 - INFO - __main__ - ['1']
06/22/2022 13:53:08 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/22/2022 13:53:08 - INFO - __main__ - ['1']
06/22/2022 13:53:08 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:53:08 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:53:08 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:53:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:53:08 - INFO - __main__ - Printing 3 examples
06/22/2022 13:53:08 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/22/2022 13:53:08 - INFO - __main__ - ['1']
06/22/2022 13:53:08 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/22/2022 13:53:08 - INFO - __main__ - ['1']
06/22/2022 13:53:08 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/22/2022 13:53:08 - INFO - __main__ - ['1']
06/22/2022 13:53:08 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:53:08 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:53:08 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:53:13 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:53:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:53:13 - INFO - __main__ - Starting training!
06/22/2022 13:53:15 - INFO - __main__ - Step 10 Global step 10 Train loss 3.82 on epoch=4
06/22/2022 13:53:16 - INFO - __main__ - Step 20 Global step 20 Train loss 2.51 on epoch=9
06/22/2022 13:53:17 - INFO - __main__ - Step 30 Global step 30 Train loss 1.53 on epoch=14
06/22/2022 13:53:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.88 on epoch=19
06/22/2022 13:53:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.67 on epoch=24
06/22/2022 13:53:20 - INFO - __main__ - Global step 50 Train loss 1.88 Classification-F1 0.3333333333333333 on epoch=24
06/22/2022 13:53:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/22/2022 13:53:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=29
06/22/2022 13:53:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=34
06/22/2022 13:53:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=39
06/22/2022 13:53:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=44
06/22/2022 13:53:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
06/22/2022 13:53:26 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.3191489361702127 on epoch=49
06/22/2022 13:53:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=54
06/22/2022 13:53:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=59
06/22/2022 13:53:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=64
06/22/2022 13:53:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=69
06/22/2022 13:53:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=74
06/22/2022 13:53:33 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 13:53:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
06/22/2022 13:53:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.35 on epoch=84
06/22/2022 13:53:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
06/22/2022 13:53:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=94
06/22/2022 13:53:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=99
06/22/2022 13:53:39 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3191489361702127 on epoch=99
06/22/2022 13:53:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=104
06/22/2022 13:53:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=109
06/22/2022 13:53:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
06/22/2022 13:53:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/22/2022 13:53:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=124
06/22/2022 13:53:46 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.39756367663344405 on epoch=124
06/22/2022 13:53:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39756367663344405 on epoch=124, global_step=250
06/22/2022 13:53:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=129
06/22/2022 13:53:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
06/22/2022 13:53:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=139
06/22/2022 13:53:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=144
06/22/2022 13:53:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=149
06/22/2022 13:53:52 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.4181818181818182 on epoch=149
06/22/2022 13:53:52 - INFO - __main__ - Saving model with best Classification-F1: 0.39756367663344405 -> 0.4181818181818182 on epoch=149, global_step=300
06/22/2022 13:53:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=154
06/22/2022 13:53:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
06/22/2022 13:53:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=164
06/22/2022 13:53:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=169
06/22/2022 13:53:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=174
06/22/2022 13:53:59 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.46843853820598 on epoch=174
06/22/2022 13:53:59 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.46843853820598 on epoch=174, global_step=350
06/22/2022 13:54:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=179
06/22/2022 13:54:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=184
06/22/2022 13:54:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=189
06/22/2022 13:54:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=194
06/22/2022 13:54:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=199
06/22/2022 13:54:05 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.46843853820598 on epoch=199
06/22/2022 13:54:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=204
06/22/2022 13:54:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
06/22/2022 13:54:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=214
06/22/2022 13:54:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
06/22/2022 13:54:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/22/2022 13:54:12 - INFO - __main__ - Global step 450 Train loss 0.28 Classification-F1 0.49090909090909085 on epoch=224
06/22/2022 13:54:12 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.49090909090909085 on epoch=224, global_step=450
06/22/2022 13:54:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
06/22/2022 13:54:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=234
06/22/2022 13:54:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=239
06/22/2022 13:54:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=244
06/22/2022 13:54:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=249
06/22/2022 13:54:18 - INFO - __main__ - Global step 500 Train loss 0.29 Classification-F1 0.5151515151515151 on epoch=249
06/22/2022 13:54:18 - INFO - __main__ - Saving model with best Classification-F1: 0.49090909090909085 -> 0.5151515151515151 on epoch=249, global_step=500
06/22/2022 13:54:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=254
06/22/2022 13:54:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
06/22/2022 13:54:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
06/22/2022 13:54:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=269
06/22/2022 13:54:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
06/22/2022 13:54:25 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.4458874458874459 on epoch=274
06/22/2022 13:54:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
06/22/2022 13:54:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
06/22/2022 13:54:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=289
06/22/2022 13:54:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=294
06/22/2022 13:54:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=299
06/22/2022 13:54:31 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.5333333333333333 on epoch=299
06/22/2022 13:54:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5333333333333333 on epoch=299, global_step=600
06/22/2022 13:54:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=304
06/22/2022 13:54:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=309
06/22/2022 13:54:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=314
06/22/2022 13:54:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
06/22/2022 13:54:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=324
06/22/2022 13:54:38 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.5076923076923077 on epoch=324
06/22/2022 13:54:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=329
06/22/2022 13:54:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=334
06/22/2022 13:54:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=339
06/22/2022 13:54:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=344
06/22/2022 13:54:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=349
06/22/2022 13:54:44 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.4817813765182186 on epoch=349
06/22/2022 13:54:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=354
06/22/2022 13:54:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=359
06/22/2022 13:54:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=364
06/22/2022 13:54:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=369
06/22/2022 13:54:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=374
06/22/2022 13:54:51 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.5076923076923077 on epoch=374
06/22/2022 13:54:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
06/22/2022 13:54:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=384
06/22/2022 13:54:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=389
06/22/2022 13:54:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
06/22/2022 13:54:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=399
06/22/2022 13:54:58 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.5076923076923077 on epoch=399
06/22/2022 13:54:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
06/22/2022 13:55:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
06/22/2022 13:55:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=414
06/22/2022 13:55:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/22/2022 13:55:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/22/2022 13:55:04 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.4285714285714286 on epoch=424
06/22/2022 13:55:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/22/2022 13:55:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
06/22/2022 13:55:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
06/22/2022 13:55:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/22/2022 13:55:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/22/2022 13:55:11 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.4920634920634921 on epoch=449
06/22/2022 13:55:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
06/22/2022 13:55:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
06/22/2022 13:55:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
06/22/2022 13:55:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/22/2022 13:55:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
06/22/2022 13:55:17 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.464039408866995 on epoch=474
06/22/2022 13:55:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
06/22/2022 13:55:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/22/2022 13:55:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
06/22/2022 13:55:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
06/22/2022 13:55:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
06/22/2022 13:55:24 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.464039408866995 on epoch=499
06/22/2022 13:55:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
06/22/2022 13:55:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/22/2022 13:55:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
06/22/2022 13:55:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/22/2022 13:55:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/22/2022 13:55:30 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=524
06/22/2022 13:55:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/22/2022 13:55:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
06/22/2022 13:55:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 13:55:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/22/2022 13:55:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 13:55:37 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=549
06/22/2022 13:55:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/22/2022 13:55:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/22/2022 13:55:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 13:55:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
06/22/2022 13:55:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 13:55:43 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=574
06/22/2022 13:55:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/22/2022 13:55:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/22/2022 13:55:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/22/2022 13:55:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
06/22/2022 13:55:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 13:55:50 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=599
06/22/2022 13:55:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 13:55:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 13:55:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/22/2022 13:55:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/22/2022 13:55:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 13:55:57 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=624
06/22/2022 13:55:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 13:55:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 13:56:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/22/2022 13:56:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 13:56:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/22/2022 13:56:03 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=649
06/22/2022 13:56:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 13:56:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/22/2022 13:56:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 13:56:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 13:56:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 13:56:10 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=674
06/22/2022 13:56:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 13:56:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 13:56:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 13:56:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/22/2022 13:56:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 13:56:16 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.4285714285714286 on epoch=699
06/22/2022 13:56:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 13:56:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/22/2022 13:56:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 13:56:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 13:56:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 13:56:23 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=724
06/22/2022 13:56:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 13:56:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 13:56:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/22/2022 13:56:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 13:56:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 13:56:29 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=749
06/22/2022 13:56:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 13:56:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 13:56:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 13:56:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
06/22/2022 13:56:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 13:56:35 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=774
06/22/2022 13:56:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 13:56:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 13:56:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 13:56:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 13:56:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 13:56:42 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=799
06/22/2022 13:56:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 13:56:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
06/22/2022 13:56:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 13:56:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 13:56:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 13:56:49 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=824
06/22/2022 13:56:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 13:56:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 13:56:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 13:56:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 13:56:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 13:56:55 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=849
06/22/2022 13:56:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 13:56:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 13:56:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 13:57:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 13:57:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 13:57:02 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=874
06/22/2022 13:57:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 13:57:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 13:57:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 13:57:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 13:57:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 13:57:08 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=899
06/22/2022 13:57:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 13:57:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 13:57:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 13:57:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 13:57:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 13:57:14 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5465587044534412 on epoch=924
06/22/2022 13:57:14 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5465587044534412 on epoch=924, global_step=1850
06/22/2022 13:57:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/22/2022 13:57:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 13:57:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/22/2022 13:57:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 13:57:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 13:57:21 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5076923076923077 on epoch=949
06/22/2022 13:57:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 13:57:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 13:57:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 13:57:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 13:57:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 13:57:27 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4666666666666667 on epoch=974
06/22/2022 13:57:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 13:57:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 13:57:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 13:57:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 13:57:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 13:57:34 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=999
06/22/2022 13:57:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5465587044534412 -> 0.5555555555555556 on epoch=999, global_step=2000
06/22/2022 13:57:34 - INFO - __main__ - save last model!
06/22/2022 13:57:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 13:57:34 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 13:57:34 - INFO - __main__ - Printing 3 examples
06/22/2022 13:57:34 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 13:57:34 - INFO - __main__ - ['0']
06/22/2022 13:57:34 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 13:57:34 - INFO - __main__ - ['1']
06/22/2022 13:57:34 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 13:57:34 - INFO - __main__ - ['1']
06/22/2022 13:57:34 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:57:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:57:34 - INFO - __main__ - Printing 3 examples
06/22/2022 13:57:34 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/22/2022 13:57:34 - INFO - __main__ - ['1']
06/22/2022 13:57:35 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/22/2022 13:57:35 - INFO - __main__ - ['1']
06/22/2022 13:57:35 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/22/2022 13:57:35 - INFO - __main__ - ['1']
06/22/2022 13:57:35 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:57:35 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:57:35 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:57:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:57:35 - INFO - __main__ - Printing 3 examples
06/22/2022 13:57:35 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/22/2022 13:57:35 - INFO - __main__ - ['1']
06/22/2022 13:57:35 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/22/2022 13:57:35 - INFO - __main__ - ['1']
06/22/2022 13:57:35 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/22/2022 13:57:35 - INFO - __main__ - ['1']
06/22/2022 13:57:35 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:57:35 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:57:35 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:57:38 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:57:40 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:57:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:57:40 - INFO - __main__ - Starting training!
06/22/2022 13:57:46 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 13:59:19 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_13_0.4_8_predictions.txt
06/22/2022 13:59:19 - INFO - __main__ - Classification-F1 on test data: 0.0639
06/22/2022 13:59:19 - INFO - __main__ - prefix=paws_16_13, lr=0.4, bsz=8, dev_performance=0.5555555555555556, test_performance=0.06385326301734412
06/22/2022 13:59:19 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.3, bsz=8 ...
06/22/2022 13:59:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:59:20 - INFO - __main__ - Printing 3 examples
06/22/2022 13:59:20 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/22/2022 13:59:20 - INFO - __main__ - ['1']
06/22/2022 13:59:20 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/22/2022 13:59:20 - INFO - __main__ - ['1']
06/22/2022 13:59:20 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/22/2022 13:59:20 - INFO - __main__ - ['1']
06/22/2022 13:59:20 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:59:20 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:59:20 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 13:59:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 13:59:20 - INFO - __main__ - Printing 3 examples
06/22/2022 13:59:20 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/22/2022 13:59:20 - INFO - __main__ - ['1']
06/22/2022 13:59:20 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/22/2022 13:59:20 - INFO - __main__ - ['1']
06/22/2022 13:59:20 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/22/2022 13:59:20 - INFO - __main__ - ['1']
06/22/2022 13:59:20 - INFO - __main__ - Tokenizing Input ...
06/22/2022 13:59:20 - INFO - __main__ - Tokenizing Output ...
06/22/2022 13:59:20 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 13:59:25 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 13:59:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 13:59:26 - INFO - __main__ - Starting training!
06/22/2022 13:59:27 - INFO - __main__ - Step 10 Global step 10 Train loss 4.06 on epoch=4
06/22/2022 13:59:28 - INFO - __main__ - Step 20 Global step 20 Train loss 3.17 on epoch=9
06/22/2022 13:59:30 - INFO - __main__ - Step 30 Global step 30 Train loss 2.13 on epoch=14
06/22/2022 13:59:31 - INFO - __main__ - Step 40 Global step 40 Train loss 1.38 on epoch=19
06/22/2022 13:59:32 - INFO - __main__ - Step 50 Global step 50 Train loss 1.04 on epoch=24
06/22/2022 13:59:32 - INFO - __main__ - Global step 50 Train loss 2.36 Classification-F1 0.4285714285714286 on epoch=24
06/22/2022 13:59:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4285714285714286 on epoch=24, global_step=50
06/22/2022 13:59:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.74 on epoch=29
06/22/2022 13:59:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=34
06/22/2022 13:59:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=39
06/22/2022 13:59:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=44
06/22/2022 13:59:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=49
06/22/2022 13:59:39 - INFO - __main__ - Global step 100 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 13:59:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=54
06/22/2022 13:59:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=59
06/22/2022 13:59:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=64
06/22/2022 13:59:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=69
06/22/2022 13:59:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=74
06/22/2022 13:59:45 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 13:59:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=79
06/22/2022 13:59:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=84
06/22/2022 13:59:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
06/22/2022 13:59:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=94
06/22/2022 13:59:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=99
06/22/2022 13:59:52 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 13:59:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=104
06/22/2022 13:59:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=109
06/22/2022 13:59:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
06/22/2022 13:59:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=119
06/22/2022 13:59:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=124
06/22/2022 13:59:58 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 14:00:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
06/22/2022 14:00:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=134
06/22/2022 14:00:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=139
06/22/2022 14:00:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=144
06/22/2022 14:00:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=149
06/22/2022 14:00:05 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=149
06/22/2022 14:00:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=154
06/22/2022 14:00:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
06/22/2022 14:00:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=164
06/22/2022 14:00:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=169
06/22/2022 14:00:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=174
06/22/2022 14:00:11 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.3191489361702127 on epoch=174
06/22/2022 14:00:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=179
06/22/2022 14:00:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=184
06/22/2022 14:00:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=189
06/22/2022 14:00:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=194
06/22/2022 14:00:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=199
06/22/2022 14:00:18 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=199
06/22/2022 14:00:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.32 on epoch=204
06/22/2022 14:00:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=209
06/22/2022 14:00:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=214
06/22/2022 14:00:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=219
06/22/2022 14:00:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=224
06/22/2022 14:00:24 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.4554554554554554 on epoch=224
06/22/2022 14:00:24 - INFO - __main__ - Saving model with best Classification-F1: 0.4285714285714286 -> 0.4554554554554554 on epoch=224, global_step=450
06/22/2022 14:00:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=229
06/22/2022 14:00:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=234
06/22/2022 14:00:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=239
06/22/2022 14:00:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=244
06/22/2022 14:00:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=249
06/22/2022 14:00:31 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.4458874458874459 on epoch=249
06/22/2022 14:00:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=254
06/22/2022 14:00:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=259
06/22/2022 14:00:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=264
06/22/2022 14:00:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
06/22/2022 14:00:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=274
06/22/2022 14:00:37 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.4458874458874459 on epoch=274
06/22/2022 14:00:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=279
06/22/2022 14:00:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
06/22/2022 14:00:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=289
06/22/2022 14:00:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=294
06/22/2022 14:00:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=299
06/22/2022 14:00:44 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.4231177094379639 on epoch=299
06/22/2022 14:00:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=304
06/22/2022 14:00:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=309
06/22/2022 14:00:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=314
06/22/2022 14:00:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=319
06/22/2022 14:00:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=324
06/22/2022 14:00:50 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.4231177094379639 on epoch=324
06/22/2022 14:00:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=329
06/22/2022 14:00:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=334
06/22/2022 14:00:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=339
06/22/2022 14:00:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
06/22/2022 14:00:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
06/22/2022 14:00:57 - INFO - __main__ - Global step 700 Train loss 0.26 Classification-F1 0.4817813765182186 on epoch=349
06/22/2022 14:00:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.4817813765182186 on epoch=349, global_step=700
06/22/2022 14:00:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=354
06/22/2022 14:00:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=359
06/22/2022 14:01:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=364
06/22/2022 14:01:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
06/22/2022 14:01:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
06/22/2022 14:01:03 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.5076923076923077 on epoch=374
06/22/2022 14:01:03 - INFO - __main__ - Saving model with best Classification-F1: 0.4817813765182186 -> 0.5076923076923077 on epoch=374, global_step=750
06/22/2022 14:01:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=379
06/22/2022 14:01:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=384
06/22/2022 14:01:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=389
06/22/2022 14:01:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=394
06/22/2022 14:01:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=399
06/22/2022 14:01:10 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.5076923076923077 on epoch=399
06/22/2022 14:01:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=404
06/22/2022 14:01:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=409
06/22/2022 14:01:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=414
06/22/2022 14:01:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=419
06/22/2022 14:01:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=424
06/22/2022 14:01:16 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.4554554554554554 on epoch=424
06/22/2022 14:01:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=429
06/22/2022 14:01:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=434
06/22/2022 14:01:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=439
06/22/2022 14:01:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=444
06/22/2022 14:01:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=449
06/22/2022 14:01:23 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.4920634920634921 on epoch=449
06/22/2022 14:01:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=454
06/22/2022 14:01:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=459
06/22/2022 14:01:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
06/22/2022 14:01:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
06/22/2022 14:01:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=474
06/22/2022 14:01:29 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.5733333333333335 on epoch=474
06/22/2022 14:01:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5733333333333335 on epoch=474, global_step=950
06/22/2022 14:01:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/22/2022 14:01:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
06/22/2022 14:01:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
06/22/2022 14:01:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
06/22/2022 14:01:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=499
06/22/2022 14:01:36 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.4980392156862745 on epoch=499
06/22/2022 14:01:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
06/22/2022 14:01:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/22/2022 14:01:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/22/2022 14:01:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
06/22/2022 14:01:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
06/22/2022 14:01:43 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.5307917888563051 on epoch=524
06/22/2022 14:01:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
06/22/2022 14:01:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
06/22/2022 14:01:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
06/22/2022 14:01:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/22/2022 14:01:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/22/2022 14:01:49 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.5465587044534412 on epoch=549
06/22/2022 14:01:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/22/2022 14:01:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/22/2022 14:01:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
06/22/2022 14:01:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 14:01:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 14:01:55 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.5465587044534412 on epoch=574
06/22/2022 14:01:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/22/2022 14:01:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
06/22/2022 14:01:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
06/22/2022 14:02:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
06/22/2022 14:02:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
06/22/2022 14:02:02 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=599
06/22/2022 14:02:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 14:02:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 14:02:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/22/2022 14:02:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/22/2022 14:02:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/22/2022 14:02:09 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.5835835835835835 on epoch=624
06/22/2022 14:02:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5733333333333335 -> 0.5835835835835835 on epoch=624, global_step=1250
06/22/2022 14:02:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 14:02:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
06/22/2022 14:02:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 14:02:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
06/22/2022 14:02:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
06/22/2022 14:02:15 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4980392156862745 on epoch=649
06/22/2022 14:02:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
06/22/2022 14:02:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/22/2022 14:02:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/22/2022 14:02:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 14:02:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/22/2022 14:02:21 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.5 on epoch=674
06/22/2022 14:02:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/22/2022 14:02:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/22/2022 14:02:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/22/2022 14:02:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/22/2022 14:02:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 14:02:28 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=699
06/22/2022 14:02:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 14:02:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 14:02:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/22/2022 14:02:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/22/2022 14:02:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 14:02:34 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.5465587044534412 on epoch=724
06/22/2022 14:02:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/22/2022 14:02:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 14:02:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 14:02:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 14:02:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 14:02:41 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=749
06/22/2022 14:02:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 14:02:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/22/2022 14:02:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 14:02:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/22/2022 14:02:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 14:02:47 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=774
06/22/2022 14:02:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 14:02:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 14:02:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 14:02:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/22/2022 14:02:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 14:02:54 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=799
06/22/2022 14:02:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 14:02:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 14:02:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 14:02:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 14:03:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
06/22/2022 14:03:01 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=824
06/22/2022 14:03:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 14:03:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/22/2022 14:03:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
06/22/2022 14:03:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 14:03:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 14:03:07 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=849
06/22/2022 14:03:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 14:03:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/22/2022 14:03:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 14:03:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/22/2022 14:03:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 14:03:14 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5 on epoch=874
06/22/2022 14:03:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 14:03:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 14:03:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/22/2022 14:03:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 14:03:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 14:03:20 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=899
06/22/2022 14:03:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 14:03:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 14:03:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/22/2022 14:03:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 14:03:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/22/2022 14:03:27 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5 on epoch=924
06/22/2022 14:03:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 14:03:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 14:03:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 14:03:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 14:03:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/22/2022 14:03:33 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5 on epoch=949
06/22/2022 14:03:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/22/2022 14:03:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 14:03:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 14:03:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 14:03:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 14:03:39 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5835835835835835 on epoch=974
06/22/2022 14:03:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 14:03:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 14:03:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 14:03:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 14:03:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/22/2022 14:03:46 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5 on epoch=999
06/22/2022 14:03:46 - INFO - __main__ - save last model!
06/22/2022 14:03:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 14:03:46 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 14:03:46 - INFO - __main__ - Printing 3 examples
06/22/2022 14:03:46 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 14:03:46 - INFO - __main__ - ['0']
06/22/2022 14:03:46 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 14:03:46 - INFO - __main__ - ['1']
06/22/2022 14:03:46 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 14:03:46 - INFO - __main__ - ['1']
06/22/2022 14:03:46 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:03:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:03:47 - INFO - __main__ - Printing 3 examples
06/22/2022 14:03:47 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/22/2022 14:03:47 - INFO - __main__ - ['1']
06/22/2022 14:03:47 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/22/2022 14:03:47 - INFO - __main__ - ['1']
06/22/2022 14:03:47 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/22/2022 14:03:47 - INFO - __main__ - ['1']
06/22/2022 14:03:47 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:03:47 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:03:47 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:03:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:03:47 - INFO - __main__ - Printing 3 examples
06/22/2022 14:03:47 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/22/2022 14:03:47 - INFO - __main__ - ['1']
06/22/2022 14:03:47 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/22/2022 14:03:47 - INFO - __main__ - ['1']
06/22/2022 14:03:47 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/22/2022 14:03:47 - INFO - __main__ - ['1']
06/22/2022 14:03:47 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:03:47 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:03:47 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:03:50 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:03:52 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:03:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:03:52 - INFO - __main__ - Starting training!
06/22/2022 14:03:58 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 14:05:32 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_13_0.3_8_predictions.txt
06/22/2022 14:05:32 - INFO - __main__ - Classification-F1 on test data: 0.5041
06/22/2022 14:05:32 - INFO - __main__ - prefix=paws_16_13, lr=0.3, bsz=8, dev_performance=0.5835835835835835, test_performance=0.50411346210807
06/22/2022 14:05:32 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.2, bsz=8 ...
06/22/2022 14:05:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:05:33 - INFO - __main__ - Printing 3 examples
06/22/2022 14:05:33 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/22/2022 14:05:33 - INFO - __main__ - ['1']
06/22/2022 14:05:33 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/22/2022 14:05:33 - INFO - __main__ - ['1']
06/22/2022 14:05:33 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/22/2022 14:05:33 - INFO - __main__ - ['1']
06/22/2022 14:05:33 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:05:33 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:05:33 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:05:33 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:05:33 - INFO - __main__ - Printing 3 examples
06/22/2022 14:05:33 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/22/2022 14:05:33 - INFO - __main__ - ['1']
06/22/2022 14:05:33 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/22/2022 14:05:33 - INFO - __main__ - ['1']
06/22/2022 14:05:33 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/22/2022 14:05:33 - INFO - __main__ - ['1']
06/22/2022 14:05:33 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:05:33 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:05:33 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:05:39 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:05:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:05:39 - INFO - __main__ - Starting training!
06/22/2022 14:05:40 - INFO - __main__ - Step 10 Global step 10 Train loss 4.35 on epoch=4
06/22/2022 14:05:42 - INFO - __main__ - Step 20 Global step 20 Train loss 3.76 on epoch=9
06/22/2022 14:05:43 - INFO - __main__ - Step 30 Global step 30 Train loss 2.95 on epoch=14
06/22/2022 14:05:44 - INFO - __main__ - Step 40 Global step 40 Train loss 2.25 on epoch=19
06/22/2022 14:05:45 - INFO - __main__ - Step 50 Global step 50 Train loss 1.77 on epoch=24
06/22/2022 14:05:46 - INFO - __main__ - Global step 50 Train loss 3.02 Classification-F1 0.3333333333333333 on epoch=24
06/22/2022 14:05:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/22/2022 14:05:47 - INFO - __main__ - Step 60 Global step 60 Train loss 1.37 on epoch=29
06/22/2022 14:05:48 - INFO - __main__ - Step 70 Global step 70 Train loss 1.13 on epoch=34
06/22/2022 14:05:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=39
06/22/2022 14:05:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.76 on epoch=44
06/22/2022 14:05:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.73 on epoch=49
06/22/2022 14:05:52 - INFO - __main__ - Global step 100 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 14:05:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=54
06/22/2022 14:05:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.61 on epoch=59
06/22/2022 14:05:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=64
06/22/2022 14:05:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.56 on epoch=69
06/22/2022 14:05:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=74
06/22/2022 14:05:59 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.3191489361702127 on epoch=74
06/22/2022 14:06:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=79
06/22/2022 14:06:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=84
06/22/2022 14:06:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=89
06/22/2022 14:06:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=94
06/22/2022 14:06:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=99
06/22/2022 14:06:05 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 14:06:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=104
06/22/2022 14:06:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=109
06/22/2022 14:06:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=114
06/22/2022 14:06:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=119
06/22/2022 14:06:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=124
06/22/2022 14:06:12 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 14:06:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=129
06/22/2022 14:06:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=134
06/22/2022 14:06:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=139
06/22/2022 14:06:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=144
06/22/2022 14:06:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=149
06/22/2022 14:06:18 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=149
06/22/2022 14:06:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=154
06/22/2022 14:06:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=159
06/22/2022 14:06:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=164
06/22/2022 14:06:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=169
06/22/2022 14:06:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=174
06/22/2022 14:06:24 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=174
06/22/2022 14:06:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=179
06/22/2022 14:06:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=184
06/22/2022 14:06:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=189
06/22/2022 14:06:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
06/22/2022 14:06:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=199
06/22/2022 14:06:31 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=199
06/22/2022 14:06:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=204
06/22/2022 14:06:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=209
06/22/2022 14:06:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=214
06/22/2022 14:06:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=219
06/22/2022 14:06:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=224
06/22/2022 14:06:37 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.36374269005847953 on epoch=224
06/22/2022 14:06:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36374269005847953 on epoch=224, global_step=450
06/22/2022 14:06:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=229
06/22/2022 14:06:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=234
06/22/2022 14:06:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=239
06/22/2022 14:06:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=244
06/22/2022 14:06:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=249
06/22/2022 14:06:44 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.46843853820598 on epoch=249
06/22/2022 14:06:44 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.46843853820598 on epoch=249, global_step=500
06/22/2022 14:06:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=254
06/22/2022 14:06:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=259
06/22/2022 14:06:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=264
06/22/2022 14:06:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
06/22/2022 14:06:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=274
06/22/2022 14:06:50 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.4181818181818182 on epoch=274
06/22/2022 14:06:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=279
06/22/2022 14:06:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=284
06/22/2022 14:06:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=289
06/22/2022 14:06:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=294
06/22/2022 14:06:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=299
06/22/2022 14:06:57 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.4458874458874459 on epoch=299
06/22/2022 14:06:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=304
06/22/2022 14:06:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.30 on epoch=309
06/22/2022 14:07:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=314
06/22/2022 14:07:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=319
06/22/2022 14:07:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=324
06/22/2022 14:07:03 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.4231177094379639 on epoch=324
06/22/2022 14:07:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=329
06/22/2022 14:07:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=334
06/22/2022 14:07:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=339
06/22/2022 14:07:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=344
06/22/2022 14:07:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=349
06/22/2022 14:07:10 - INFO - __main__ - Global step 700 Train loss 0.30 Classification-F1 0.4231177094379639 on epoch=349
06/22/2022 14:07:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=354
06/22/2022 14:07:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=359
06/22/2022 14:07:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=364
06/22/2022 14:07:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=369
06/22/2022 14:07:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=374
06/22/2022 14:07:16 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.4666666666666667 on epoch=374
06/22/2022 14:07:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=379
06/22/2022 14:07:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=384
06/22/2022 14:07:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=389
06/22/2022 14:07:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=394
06/22/2022 14:07:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
06/22/2022 14:07:23 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.4666666666666667 on epoch=399
06/22/2022 14:07:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=404
06/22/2022 14:07:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=409
06/22/2022 14:07:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=414
06/22/2022 14:07:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
06/22/2022 14:07:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=424
06/22/2022 14:07:29 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.5076923076923077 on epoch=424
06/22/2022 14:07:29 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5076923076923077 on epoch=424, global_step=850
06/22/2022 14:07:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=429
06/22/2022 14:07:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=434
06/22/2022 14:07:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=439
06/22/2022 14:07:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=444
06/22/2022 14:07:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=449
06/22/2022 14:07:36 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.5076923076923077 on epoch=449
06/22/2022 14:07:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=454
06/22/2022 14:07:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=459
06/22/2022 14:07:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
06/22/2022 14:07:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=469
06/22/2022 14:07:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=474
06/22/2022 14:07:42 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.4666666666666667 on epoch=474
06/22/2022 14:07:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=479
06/22/2022 14:07:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=484
06/22/2022 14:07:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=489
06/22/2022 14:07:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=494
06/22/2022 14:07:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=499
06/22/2022 14:07:49 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.5333333333333333 on epoch=499
06/22/2022 14:07:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5333333333333333 on epoch=499, global_step=1000
06/22/2022 14:07:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=504
06/22/2022 14:07:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=509
06/22/2022 14:07:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=514
06/22/2022 14:07:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=519
06/22/2022 14:07:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=524
06/22/2022 14:07:55 - INFO - __main__ - Global step 1050 Train loss 0.16 Classification-F1 0.5588547189819725 on epoch=524
06/22/2022 14:07:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5588547189819725 on epoch=524, global_step=1050
06/22/2022 14:07:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=529
06/22/2022 14:07:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=534
06/22/2022 14:07:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=539
06/22/2022 14:08:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=544
06/22/2022 14:08:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=549
06/22/2022 14:08:02 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.6190476190476191 on epoch=549
06/22/2022 14:08:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.6190476190476191 on epoch=549, global_step=1100
06/22/2022 14:08:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=554
06/22/2022 14:08:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=559
06/22/2022 14:08:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=564
06/22/2022 14:08:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=569
06/22/2022 14:08:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=574
06/22/2022 14:08:08 - INFO - __main__ - Global step 1150 Train loss 0.12 Classification-F1 0.6476476476476476 on epoch=574
06/22/2022 14:08:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6190476190476191 -> 0.6476476476476476 on epoch=574, global_step=1150
06/22/2022 14:08:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=579
06/22/2022 14:08:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
06/22/2022 14:08:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=589
06/22/2022 14:08:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=594
06/22/2022 14:08:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
06/22/2022 14:08:15 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.6190476190476191 on epoch=599
06/22/2022 14:08:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
06/22/2022 14:08:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=609
06/22/2022 14:08:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=614
06/22/2022 14:08:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
06/22/2022 14:08:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
06/22/2022 14:08:21 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.6000000000000001 on epoch=624
06/22/2022 14:08:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=629
06/22/2022 14:08:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=634
06/22/2022 14:08:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=639
06/22/2022 14:08:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
06/22/2022 14:08:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=649
06/22/2022 14:08:28 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.6113360323886641 on epoch=649
06/22/2022 14:08:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=654
06/22/2022 14:08:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=659
06/22/2022 14:08:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
06/22/2022 14:08:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
06/22/2022 14:08:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=674
06/22/2022 14:08:34 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.5835835835835835 on epoch=674
06/22/2022 14:08:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=679
06/22/2022 14:08:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
06/22/2022 14:08:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=689
06/22/2022 14:08:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
06/22/2022 14:08:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
06/22/2022 14:08:41 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6235294117647059 on epoch=699
06/22/2022 14:08:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=704
06/22/2022 14:08:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/22/2022 14:08:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
06/22/2022 14:08:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
06/22/2022 14:08:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/22/2022 14:08:47 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.6190476190476191 on epoch=724
06/22/2022 14:08:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
06/22/2022 14:08:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
06/22/2022 14:08:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=739
06/22/2022 14:08:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
06/22/2022 14:08:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/22/2022 14:08:54 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.5835835835835835 on epoch=749
06/22/2022 14:08:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
06/22/2022 14:08:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
06/22/2022 14:08:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
06/22/2022 14:08:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
06/22/2022 14:09:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/22/2022 14:09:00 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6113360323886641 on epoch=774
06/22/2022 14:09:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=779
06/22/2022 14:09:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
06/22/2022 14:09:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 14:09:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/22/2022 14:09:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/22/2022 14:09:07 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.5835835835835835 on epoch=799
06/22/2022 14:09:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
06/22/2022 14:09:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
06/22/2022 14:09:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 14:09:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
06/22/2022 14:09:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=824
06/22/2022 14:09:13 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.5270935960591133 on epoch=824
06/22/2022 14:09:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
06/22/2022 14:09:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
06/22/2022 14:09:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
06/22/2022 14:09:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/22/2022 14:09:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 14:09:20 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.5076923076923077 on epoch=849
06/22/2022 14:09:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/22/2022 14:09:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/22/2022 14:09:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/22/2022 14:09:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
06/22/2022 14:09:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/22/2022 14:09:26 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=874
06/22/2022 14:09:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 14:09:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=884
06/22/2022 14:09:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
06/22/2022 14:09:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/22/2022 14:09:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 14:09:33 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.4817813765182186 on epoch=899
06/22/2022 14:09:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/22/2022 14:09:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
06/22/2022 14:09:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/22/2022 14:09:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
06/22/2022 14:09:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/22/2022 14:09:39 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=924
06/22/2022 14:09:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/22/2022 14:09:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 14:09:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/22/2022 14:09:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 14:09:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/22/2022 14:09:46 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=949
06/22/2022 14:09:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=954
06/22/2022 14:09:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
06/22/2022 14:09:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 14:09:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/22/2022 14:09:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
06/22/2022 14:09:52 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.5607843137254902 on epoch=974
06/22/2022 14:09:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/22/2022 14:09:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/22/2022 14:09:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 14:09:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/22/2022 14:09:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/22/2022 14:09:58 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=999
06/22/2022 14:09:58 - INFO - __main__ - save last model!
06/22/2022 14:09:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 14:09:58 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 14:09:58 - INFO - __main__ - Printing 3 examples
06/22/2022 14:09:59 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 14:09:59 - INFO - __main__ - ['0']
06/22/2022 14:09:59 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 14:09:59 - INFO - __main__ - ['1']
06/22/2022 14:09:59 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 14:09:59 - INFO - __main__ - ['1']
06/22/2022 14:09:59 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:09:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:09:59 - INFO - __main__ - Printing 3 examples
06/22/2022 14:09:59 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/22/2022 14:09:59 - INFO - __main__ - ['1']
06/22/2022 14:09:59 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/22/2022 14:09:59 - INFO - __main__ - ['1']
06/22/2022 14:09:59 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/22/2022 14:09:59 - INFO - __main__ - ['1']
06/22/2022 14:09:59 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:09:59 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:09:59 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:09:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:09:59 - INFO - __main__ - Printing 3 examples
06/22/2022 14:09:59 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/22/2022 14:09:59 - INFO - __main__ - ['1']
06/22/2022 14:09:59 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/22/2022 14:09:59 - INFO - __main__ - ['1']
06/22/2022 14:09:59 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/22/2022 14:09:59 - INFO - __main__ - ['1']
06/22/2022 14:09:59 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:09:59 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:09:59 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:10:03 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:10:04 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:10:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:10:05 - INFO - __main__ - Starting training!
06/22/2022 14:10:10 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 14:11:50 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_13_0.2_8_predictions.txt
06/22/2022 14:11:51 - INFO - __main__ - Classification-F1 on test data: 0.1973
06/22/2022 14:11:51 - INFO - __main__ - prefix=paws_16_13, lr=0.2, bsz=8, dev_performance=0.6476476476476476, test_performance=0.19725137298362272
06/22/2022 14:11:51 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.5, bsz=8 ...
06/22/2022 14:11:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:11:52 - INFO - __main__ - Printing 3 examples
06/22/2022 14:11:52 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/22/2022 14:11:52 - INFO - __main__ - ['1']
06/22/2022 14:11:52 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/22/2022 14:11:52 - INFO - __main__ - ['1']
06/22/2022 14:11:52 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/22/2022 14:11:52 - INFO - __main__ - ['1']
06/22/2022 14:11:52 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:11:52 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:11:52 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:11:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:11:52 - INFO - __main__ - Printing 3 examples
06/22/2022 14:11:52 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/22/2022 14:11:52 - INFO - __main__ - ['1']
06/22/2022 14:11:52 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/22/2022 14:11:52 - INFO - __main__ - ['1']
06/22/2022 14:11:52 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/22/2022 14:11:52 - INFO - __main__ - ['1']
06/22/2022 14:11:52 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:11:52 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:11:52 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:11:57 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:11:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:11:57 - INFO - __main__ - Starting training!
06/22/2022 14:11:59 - INFO - __main__ - Step 10 Global step 10 Train loss 3.71 on epoch=4
06/22/2022 14:12:00 - INFO - __main__ - Step 20 Global step 20 Train loss 2.06 on epoch=9
06/22/2022 14:12:01 - INFO - __main__ - Step 30 Global step 30 Train loss 1.11 on epoch=14
06/22/2022 14:12:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.80 on epoch=19
06/22/2022 14:12:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.63 on epoch=24
06/22/2022 14:12:04 - INFO - __main__ - Global step 50 Train loss 1.66 Classification-F1 0.3333333333333333 on epoch=24
06/22/2022 14:12:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/22/2022 14:12:05 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=29
06/22/2022 14:12:06 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=34
06/22/2022 14:12:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=39
06/22/2022 14:12:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
06/22/2022 14:12:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=49
06/22/2022 14:12:10 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 14:12:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=54
06/22/2022 14:12:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=59
06/22/2022 14:12:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=64
06/22/2022 14:12:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=69
06/22/2022 14:12:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=74
06/22/2022 14:12:17 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 14:12:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=79
06/22/2022 14:12:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=84
06/22/2022 14:12:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
06/22/2022 14:12:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
06/22/2022 14:12:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=99
06/22/2022 14:12:23 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 14:12:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=104
06/22/2022 14:12:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
06/22/2022 14:12:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=114
06/22/2022 14:12:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=119
06/22/2022 14:12:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=124
06/22/2022 14:12:30 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.36374269005847953 on epoch=124
06/22/2022 14:12:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36374269005847953 on epoch=124, global_step=250
06/22/2022 14:12:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
06/22/2022 14:12:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
06/22/2022 14:12:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
06/22/2022 14:12:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.34 on epoch=144
06/22/2022 14:12:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=149
06/22/2022 14:12:36 - INFO - __main__ - Global step 300 Train loss 0.33 Classification-F1 0.3454545454545454 on epoch=149
06/22/2022 14:12:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/22/2022 14:12:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=159
06/22/2022 14:12:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=164
06/22/2022 14:12:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/22/2022 14:12:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
06/22/2022 14:12:43 - INFO - __main__ - Global step 350 Train loss 0.29 Classification-F1 0.39999999999999997 on epoch=174
06/22/2022 14:12:43 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.39999999999999997 on epoch=174, global_step=350
06/22/2022 14:12:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
06/22/2022 14:12:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
06/22/2022 14:12:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
06/22/2022 14:12:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
06/22/2022 14:12:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
06/22/2022 14:12:49 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.2748768472906404 on epoch=199
06/22/2022 14:12:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=204
06/22/2022 14:12:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=209
06/22/2022 14:12:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.17 on epoch=214
06/22/2022 14:12:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=219
06/22/2022 14:12:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.18 on epoch=224
06/22/2022 14:12:56 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.33793103448275863 on epoch=224
06/22/2022 14:12:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.16 on epoch=229
06/22/2022 14:12:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=234
06/22/2022 14:12:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.15 on epoch=239
06/22/2022 14:13:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
06/22/2022 14:13:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=249
06/22/2022 14:13:02 - INFO - __main__ - Global step 500 Train loss 0.15 Classification-F1 0.39999999999999997 on epoch=249
06/22/2022 14:13:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=254
06/22/2022 14:13:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
06/22/2022 14:13:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
06/22/2022 14:13:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
06/22/2022 14:13:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=274
06/22/2022 14:13:09 - INFO - __main__ - Global step 550 Train loss 0.11 Classification-F1 0.34310850439882695 on epoch=274
06/22/2022 14:13:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=279
06/22/2022 14:13:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
06/22/2022 14:13:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
06/22/2022 14:13:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
06/22/2022 14:13:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
06/22/2022 14:13:15 - INFO - __main__ - Global step 600 Train loss 0.06 Classification-F1 0.39139139139139134 on epoch=299
06/22/2022 14:13:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
06/22/2022 14:13:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
06/22/2022 14:13:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/22/2022 14:13:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
06/22/2022 14:13:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/22/2022 14:13:22 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.375 on epoch=324
06/22/2022 14:13:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
06/22/2022 14:13:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
06/22/2022 14:13:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
06/22/2022 14:13:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
06/22/2022 14:13:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
06/22/2022 14:13:28 - INFO - __main__ - Global step 700 Train loss 0.03 Classification-F1 0.41700404858299595 on epoch=349
06/22/2022 14:13:28 - INFO - __main__ - Saving model with best Classification-F1: 0.39999999999999997 -> 0.41700404858299595 on epoch=349, global_step=700
06/22/2022 14:13:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
06/22/2022 14:13:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
06/22/2022 14:13:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/22/2022 14:13:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
06/22/2022 14:13:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/22/2022 14:13:35 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.3650793650793651 on epoch=374
06/22/2022 14:13:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/22/2022 14:13:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/22/2022 14:13:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
06/22/2022 14:13:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
06/22/2022 14:13:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/22/2022 14:13:41 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.34310850439882695 on epoch=399
06/22/2022 14:13:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
06/22/2022 14:13:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/22/2022 14:13:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/22/2022 14:13:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/22/2022 14:13:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/22/2022 14:13:48 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.4009852216748768 on epoch=424
06/22/2022 14:13:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/22/2022 14:13:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/22/2022 14:13:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
06/22/2022 14:13:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/22/2022 14:13:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 14:13:54 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.33793103448275863 on epoch=449
06/22/2022 14:13:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/22/2022 14:13:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/22/2022 14:13:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=464
06/22/2022 14:13:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/22/2022 14:14:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/22/2022 14:14:01 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.39139139139139134 on epoch=474
06/22/2022 14:14:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/22/2022 14:14:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
06/22/2022 14:14:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/22/2022 14:14:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/22/2022 14:14:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 14:14:07 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=499
06/22/2022 14:14:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
06/22/2022 14:14:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/22/2022 14:14:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 14:14:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/22/2022 14:14:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/22/2022 14:14:14 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=524
06/22/2022 14:14:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/22/2022 14:14:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 14:14:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 14:14:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/22/2022 14:14:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 14:14:20 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.2904761904761905 on epoch=549
06/22/2022 14:14:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 14:14:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 14:14:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/22/2022 14:14:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 14:14:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/22/2022 14:14:27 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.375 on epoch=574
06/22/2022 14:14:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
06/22/2022 14:14:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 14:14:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 14:14:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 14:14:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 14:14:33 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=599
06/22/2022 14:14:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 14:14:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 14:14:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 14:14:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/22/2022 14:14:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 14:14:40 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=624
06/22/2022 14:14:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 14:14:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 14:14:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 14:14:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 14:14:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/22/2022 14:14:46 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.41700404858299595 on epoch=649
06/22/2022 14:14:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 14:14:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 14:14:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 14:14:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 14:14:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/22/2022 14:14:53 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=674
06/22/2022 14:14:53 - INFO - __main__ - Saving model with best Classification-F1: 0.41700404858299595 -> 0.4554554554554554 on epoch=674, global_step=1350
06/22/2022 14:14:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 14:14:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 14:14:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 14:14:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 14:14:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 14:14:59 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=699
06/22/2022 14:15:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 14:15:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 14:15:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 14:15:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 14:15:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 14:15:06 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=724
06/22/2022 14:15:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 14:15:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 14:15:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 14:15:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 14:15:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 14:15:13 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=749
06/22/2022 14:15:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.4682306940371457 on epoch=749, global_step=1500
06/22/2022 14:15:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 14:15:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 14:15:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 14:15:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 14:15:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 14:15:19 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=774
06/22/2022 14:15:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 14:15:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 14:15:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 14:15:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 14:15:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 14:15:25 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=799
06/22/2022 14:15:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 14:15:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 14:15:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 14:15:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 14:15:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 14:15:32 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=824
06/22/2022 14:15:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 14:15:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/22/2022 14:15:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 14:15:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 14:15:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 14:15:38 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5 on epoch=849
06/22/2022 14:15:39 - INFO - __main__ - Saving model with best Classification-F1: 0.4682306940371457 -> 0.5 on epoch=849, global_step=1700
06/22/2022 14:15:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/22/2022 14:15:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 14:15:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 14:15:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/22/2022 14:15:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 14:15:45 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=874
06/22/2022 14:15:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 14:15:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 14:15:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 14:15:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 14:15:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 14:15:52 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=899
06/22/2022 14:15:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 14:15:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 14:15:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 14:15:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 14:15:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 14:15:58 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=924
06/22/2022 14:15:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 14:16:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 14:16:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 14:16:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 14:16:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/22/2022 14:16:05 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=949
06/22/2022 14:16:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 14:16:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 14:16:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 14:16:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 14:16:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 14:16:11 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=974
06/22/2022 14:16:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 14:16:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 14:16:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 14:16:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 14:16:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 14:16:18 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=999
06/22/2022 14:16:18 - INFO - __main__ - save last model!
06/22/2022 14:16:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 14:16:18 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 14:16:18 - INFO - __main__ - Printing 3 examples
06/22/2022 14:16:18 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 14:16:18 - INFO - __main__ - ['0']
06/22/2022 14:16:18 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 14:16:18 - INFO - __main__ - ['1']
06/22/2022 14:16:18 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 14:16:18 - INFO - __main__ - ['1']
06/22/2022 14:16:18 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:16:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:16:19 - INFO - __main__ - Printing 3 examples
06/22/2022 14:16:19 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/22/2022 14:16:19 - INFO - __main__ - ['1']
06/22/2022 14:16:19 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/22/2022 14:16:19 - INFO - __main__ - ['1']
06/22/2022 14:16:19 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/22/2022 14:16:19 - INFO - __main__ - ['1']
06/22/2022 14:16:19 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:16:19 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:16:19 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:16:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:16:19 - INFO - __main__ - Printing 3 examples
06/22/2022 14:16:19 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/22/2022 14:16:19 - INFO - __main__ - ['1']
06/22/2022 14:16:19 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/22/2022 14:16:19 - INFO - __main__ - ['1']
06/22/2022 14:16:19 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/22/2022 14:16:19 - INFO - __main__ - ['1']
06/22/2022 14:16:19 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:16:19 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:16:19 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:16:22 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:16:24 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:16:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:16:24 - INFO - __main__ - Starting training!
06/22/2022 14:16:30 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 14:17:58 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_21_0.5_8_predictions.txt
06/22/2022 14:17:58 - INFO - __main__ - Classification-F1 on test data: 0.1378
06/22/2022 14:17:58 - INFO - __main__ - prefix=paws_16_21, lr=0.5, bsz=8, dev_performance=0.5, test_performance=0.13777894654145909
06/22/2022 14:17:58 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.4, bsz=8 ...
06/22/2022 14:17:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:17:59 - INFO - __main__ - Printing 3 examples
06/22/2022 14:17:59 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/22/2022 14:17:59 - INFO - __main__ - ['1']
06/22/2022 14:17:59 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/22/2022 14:17:59 - INFO - __main__ - ['1']
06/22/2022 14:17:59 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/22/2022 14:17:59 - INFO - __main__ - ['1']
06/22/2022 14:17:59 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:17:59 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:17:59 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:17:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:17:59 - INFO - __main__ - Printing 3 examples
06/22/2022 14:17:59 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/22/2022 14:17:59 - INFO - __main__ - ['1']
06/22/2022 14:17:59 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/22/2022 14:17:59 - INFO - __main__ - ['1']
06/22/2022 14:17:59 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/22/2022 14:17:59 - INFO - __main__ - ['1']
06/22/2022 14:17:59 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:17:59 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:17:59 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:18:04 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:18:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:18:04 - INFO - __main__ - Starting training!
06/22/2022 14:18:06 - INFO - __main__ - Step 10 Global step 10 Train loss 4.00 on epoch=4
06/22/2022 14:18:07 - INFO - __main__ - Step 20 Global step 20 Train loss 2.71 on epoch=9
06/22/2022 14:18:08 - INFO - __main__ - Step 30 Global step 30 Train loss 1.60 on epoch=14
06/22/2022 14:18:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=19
06/22/2022 14:18:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.76 on epoch=24
06/22/2022 14:18:11 - INFO - __main__ - Global step 50 Train loss 2.01 Classification-F1 0.3333333333333333 on epoch=24
06/22/2022 14:18:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/22/2022 14:18:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.76 on epoch=29
06/22/2022 14:18:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=34
06/22/2022 14:18:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=39
06/22/2022 14:18:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=44
06/22/2022 14:18:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=49
06/22/2022 14:18:17 - INFO - __main__ - Global step 100 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 14:18:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=54
06/22/2022 14:18:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=59
06/22/2022 14:18:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
06/22/2022 14:18:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=69
06/22/2022 14:18:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=74
06/22/2022 14:18:24 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 14:18:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
06/22/2022 14:18:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
06/22/2022 14:18:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=89
06/22/2022 14:18:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=94
06/22/2022 14:18:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=99
06/22/2022 14:18:30 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 14:18:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=104
06/22/2022 14:18:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=109
06/22/2022 14:18:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/22/2022 14:18:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=119
06/22/2022 14:18:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=124
06/22/2022 14:18:37 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 14:18:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
06/22/2022 14:18:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=134
06/22/2022 14:18:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=139
06/22/2022 14:18:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=144
06/22/2022 14:18:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=149
06/22/2022 14:18:43 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=149
06/22/2022 14:18:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=154
06/22/2022 14:18:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=159
06/22/2022 14:18:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=164
06/22/2022 14:18:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=169
06/22/2022 14:18:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=174
06/22/2022 14:18:50 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.4181818181818182 on epoch=174
06/22/2022 14:18:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4181818181818182 on epoch=174, global_step=350
06/22/2022 14:18:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=179
06/22/2022 14:18:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=184
06/22/2022 14:18:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=189
06/22/2022 14:18:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=194
06/22/2022 14:18:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=199
06/22/2022 14:18:56 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.4231177094379639 on epoch=199
06/22/2022 14:18:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.4231177094379639 on epoch=199, global_step=400
06/22/2022 14:18:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=204
06/22/2022 14:18:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=209
06/22/2022 14:19:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=214
06/22/2022 14:19:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=219
06/22/2022 14:19:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
06/22/2022 14:19:03 - INFO - __main__ - Global step 450 Train loss 0.30 Classification-F1 0.4181818181818182 on epoch=224
06/22/2022 14:19:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
06/22/2022 14:19:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=234
06/22/2022 14:19:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
06/22/2022 14:19:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=244
06/22/2022 14:19:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
06/22/2022 14:19:09 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.4420512820512821 on epoch=249
06/22/2022 14:19:09 - INFO - __main__ - Saving model with best Classification-F1: 0.4231177094379639 -> 0.4420512820512821 on epoch=249, global_step=500
06/22/2022 14:19:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=254
06/22/2022 14:19:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
06/22/2022 14:19:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
06/22/2022 14:19:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=269
06/22/2022 14:19:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=274
06/22/2022 14:19:16 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.39999999999999997 on epoch=274
06/22/2022 14:19:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
06/22/2022 14:19:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=284
06/22/2022 14:19:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=289
06/22/2022 14:19:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
06/22/2022 14:19:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.13 on epoch=299
06/22/2022 14:19:22 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.5270935960591133 on epoch=299
06/22/2022 14:19:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4420512820512821 -> 0.5270935960591133 on epoch=299, global_step=600
06/22/2022 14:19:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
06/22/2022 14:19:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=309
06/22/2022 14:19:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=314
06/22/2022 14:19:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=319
06/22/2022 14:19:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=324
06/22/2022 14:19:29 - INFO - __main__ - Global step 650 Train loss 0.11 Classification-F1 0.4980392156862745 on epoch=324
06/22/2022 14:19:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
06/22/2022 14:19:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=334
06/22/2022 14:19:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=339
06/22/2022 14:19:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=344
06/22/2022 14:19:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
06/22/2022 14:19:35 - INFO - __main__ - Global step 700 Train loss 0.08 Classification-F1 0.5270935960591133 on epoch=349
06/22/2022 14:19:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.09 on epoch=354
06/22/2022 14:19:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=359
06/22/2022 14:19:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=364
06/22/2022 14:19:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=369
06/22/2022 14:19:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
06/22/2022 14:19:41 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.4375 on epoch=374
06/22/2022 14:19:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
06/22/2022 14:19:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=384
06/22/2022 14:19:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
06/22/2022 14:19:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
06/22/2022 14:19:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
06/22/2022 14:19:48 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.4682306940371457 on epoch=399
06/22/2022 14:19:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/22/2022 14:19:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/22/2022 14:19:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
06/22/2022 14:19:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
06/22/2022 14:19:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/22/2022 14:19:54 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.4375 on epoch=424
06/22/2022 14:19:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/22/2022 14:19:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
06/22/2022 14:19:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/22/2022 14:19:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=444
06/22/2022 14:20:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=449
06/22/2022 14:20:01 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.5270935960591133 on epoch=449
06/22/2022 14:20:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/22/2022 14:20:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
06/22/2022 14:20:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
06/22/2022 14:20:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
06/22/2022 14:20:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/22/2022 14:20:07 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.4375 on epoch=474
06/22/2022 14:20:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/22/2022 14:20:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
06/22/2022 14:20:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/22/2022 14:20:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
06/22/2022 14:20:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/22/2022 14:20:14 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.4375 on epoch=499
06/22/2022 14:20:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 14:20:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/22/2022 14:20:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 14:20:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
06/22/2022 14:20:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=524
06/22/2022 14:20:20 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.4375 on epoch=524
06/22/2022 14:20:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/22/2022 14:20:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/22/2022 14:20:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
06/22/2022 14:20:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
06/22/2022 14:20:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 14:20:26 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.4682306940371457 on epoch=549
06/22/2022 14:20:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 14:20:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/22/2022 14:20:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
06/22/2022 14:20:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 14:20:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/22/2022 14:20:33 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.4375 on epoch=574
06/22/2022 14:20:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 14:20:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/22/2022 14:20:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
06/22/2022 14:20:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/22/2022 14:20:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 14:20:39 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=599
06/22/2022 14:20:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 14:20:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 14:20:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 14:20:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 14:20:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
06/22/2022 14:20:46 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=624
06/22/2022 14:20:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 14:20:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 14:20:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 14:20:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 14:20:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/22/2022 14:20:52 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=649
06/22/2022 14:20:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 14:20:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/22/2022 14:20:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 14:20:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 14:20:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 14:20:59 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4375 on epoch=674
06/22/2022 14:21:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 14:21:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
06/22/2022 14:21:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/22/2022 14:21:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/22/2022 14:21:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 14:21:05 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.4375 on epoch=699
06/22/2022 14:21:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 14:21:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 14:21:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
06/22/2022 14:21:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 14:21:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/22/2022 14:21:12 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=724
06/22/2022 14:21:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/22/2022 14:21:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 14:21:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/22/2022 14:21:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 14:21:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 14:21:18 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=749
06/22/2022 14:21:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 14:21:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 14:21:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 14:21:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 14:21:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 14:21:25 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=774
06/22/2022 14:21:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
06/22/2022 14:21:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 14:21:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 14:21:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 14:21:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
06/22/2022 14:21:31 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=799
06/22/2022 14:21:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 14:21:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/22/2022 14:21:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/22/2022 14:21:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/22/2022 14:21:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 14:21:38 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=824
06/22/2022 14:21:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 14:21:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/22/2022 14:21:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
06/22/2022 14:21:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/22/2022 14:21:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 14:21:44 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=849
06/22/2022 14:21:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 14:21:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 14:21:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 14:21:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 14:21:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 14:21:51 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=874
06/22/2022 14:21:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 14:21:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 14:21:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/22/2022 14:21:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 14:21:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 14:21:57 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=899
06/22/2022 14:21:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/22/2022 14:22:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 14:22:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 14:22:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 14:22:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 14:22:04 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=924
06/22/2022 14:22:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 14:22:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 14:22:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 14:22:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 14:22:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 14:22:10 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=949
06/22/2022 14:22:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 14:22:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 14:22:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 14:22:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/22/2022 14:22:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 14:22:17 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=974
06/22/2022 14:22:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 14:22:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=984
06/22/2022 14:22:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 14:22:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 14:22:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 14:22:23 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=999
06/22/2022 14:22:23 - INFO - __main__ - save last model!
06/22/2022 14:22:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 14:22:23 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 14:22:23 - INFO - __main__ - Printing 3 examples
06/22/2022 14:22:23 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 14:22:23 - INFO - __main__ - ['0']
06/22/2022 14:22:23 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 14:22:23 - INFO - __main__ - ['1']
06/22/2022 14:22:23 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 14:22:23 - INFO - __main__ - ['1']
06/22/2022 14:22:23 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:22:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:22:24 - INFO - __main__ - Printing 3 examples
06/22/2022 14:22:24 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/22/2022 14:22:24 - INFO - __main__ - ['1']
06/22/2022 14:22:24 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/22/2022 14:22:24 - INFO - __main__ - ['1']
06/22/2022 14:22:24 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/22/2022 14:22:24 - INFO - __main__ - ['1']
06/22/2022 14:22:24 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:22:24 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:22:24 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:22:24 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:22:24 - INFO - __main__ - Printing 3 examples
06/22/2022 14:22:24 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/22/2022 14:22:24 - INFO - __main__ - ['1']
06/22/2022 14:22:24 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/22/2022 14:22:24 - INFO - __main__ - ['1']
06/22/2022 14:22:24 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/22/2022 14:22:24 - INFO - __main__ - ['1']
06/22/2022 14:22:24 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:22:24 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:22:24 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:22:27 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:22:29 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:22:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:22:29 - INFO - __main__ - Starting training!
06/22/2022 14:22:35 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 14:24:04 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_21_0.4_8_predictions.txt
06/22/2022 14:24:04 - INFO - __main__ - Classification-F1 on test data: 0.3349
06/22/2022 14:24:04 - INFO - __main__ - prefix=paws_16_21, lr=0.4, bsz=8, dev_performance=0.5270935960591133, test_performance=0.3349194324053435
06/22/2022 14:24:04 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.3, bsz=8 ...
06/22/2022 14:24:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:24:05 - INFO - __main__ - Printing 3 examples
06/22/2022 14:24:05 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/22/2022 14:24:05 - INFO - __main__ - ['1']
06/22/2022 14:24:05 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/22/2022 14:24:05 - INFO - __main__ - ['1']
06/22/2022 14:24:05 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/22/2022 14:24:05 - INFO - __main__ - ['1']
06/22/2022 14:24:05 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:24:05 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:24:05 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:24:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:24:05 - INFO - __main__ - Printing 3 examples
06/22/2022 14:24:05 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/22/2022 14:24:05 - INFO - __main__ - ['1']
06/22/2022 14:24:05 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/22/2022 14:24:05 - INFO - __main__ - ['1']
06/22/2022 14:24:05 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/22/2022 14:24:05 - INFO - __main__ - ['1']
06/22/2022 14:24:05 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:24:05 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:24:05 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:24:10 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:24:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:24:10 - INFO - __main__ - Starting training!
06/22/2022 14:24:12 - INFO - __main__ - Step 10 Global step 10 Train loss 4.22 on epoch=4
06/22/2022 14:24:13 - INFO - __main__ - Step 20 Global step 20 Train loss 2.99 on epoch=9
06/22/2022 14:24:14 - INFO - __main__ - Step 30 Global step 30 Train loss 2.20 on epoch=14
06/22/2022 14:24:15 - INFO - __main__ - Step 40 Global step 40 Train loss 1.52 on epoch=19
06/22/2022 14:24:17 - INFO - __main__ - Step 50 Global step 50 Train loss 1.11 on epoch=24
06/22/2022 14:24:17 - INFO - __main__ - Global step 50 Train loss 2.41 Classification-F1 0.6532019704433498 on epoch=24
06/22/2022 14:24:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.6532019704433498 on epoch=24, global_step=50
06/22/2022 14:24:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.81 on epoch=29
06/22/2022 14:24:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=34
06/22/2022 14:24:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.74 on epoch=39
06/22/2022 14:24:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.70 on epoch=44
06/22/2022 14:24:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=49
06/22/2022 14:24:24 - INFO - __main__ - Global step 100 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 14:24:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.61 on epoch=54
06/22/2022 14:24:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=59
06/22/2022 14:24:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=64
06/22/2022 14:24:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=69
06/22/2022 14:24:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=74
06/22/2022 14:24:30 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 14:24:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=79
06/22/2022 14:24:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=84
06/22/2022 14:24:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
06/22/2022 14:24:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=94
06/22/2022 14:24:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=99
06/22/2022 14:24:37 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 14:24:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=104
06/22/2022 14:24:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=109
06/22/2022 14:24:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=114
06/22/2022 14:24:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
06/22/2022 14:24:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=124
06/22/2022 14:24:43 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 14:24:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=129
06/22/2022 14:24:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=134
06/22/2022 14:24:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
06/22/2022 14:24:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=144
06/22/2022 14:24:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=149
06/22/2022 14:24:49 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=149
06/22/2022 14:24:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=154
06/22/2022 14:24:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=159
06/22/2022 14:24:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=164
06/22/2022 14:24:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=169
06/22/2022 14:24:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
06/22/2022 14:24:56 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=174
06/22/2022 14:24:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=179
06/22/2022 14:24:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.29 on epoch=184
06/22/2022 14:25:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.33 on epoch=189
06/22/2022 14:25:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
06/22/2022 14:25:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=199
06/22/2022 14:25:02 - INFO - __main__ - Global step 400 Train loss 0.33 Classification-F1 0.4385964912280702 on epoch=199
06/22/2022 14:25:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=204
06/22/2022 14:25:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=209
06/22/2022 14:25:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=214
06/22/2022 14:25:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=219
06/22/2022 14:25:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=224
06/22/2022 14:25:09 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.4231177094379639 on epoch=224
06/22/2022 14:25:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=229
06/22/2022 14:25:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
06/22/2022 14:25:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=239
06/22/2022 14:25:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=244
06/22/2022 14:25:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=249
06/22/2022 14:25:15 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.39756367663344405 on epoch=249
06/22/2022 14:25:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=254
06/22/2022 14:25:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=259
06/22/2022 14:25:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=264
06/22/2022 14:25:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=269
06/22/2022 14:25:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=274
06/22/2022 14:25:22 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.3764102564102564 on epoch=274
06/22/2022 14:25:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
06/22/2022 14:25:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
06/22/2022 14:25:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=289
06/22/2022 14:25:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=294
06/22/2022 14:25:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
06/22/2022 14:25:28 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.39756367663344405 on epoch=299
06/22/2022 14:25:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=304
06/22/2022 14:25:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
06/22/2022 14:25:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=314
06/22/2022 14:25:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=319
06/22/2022 14:25:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=324
06/22/2022 14:25:35 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.37662337662337664 on epoch=324
06/22/2022 14:25:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=329
06/22/2022 14:25:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=334
06/22/2022 14:25:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=339
06/22/2022 14:25:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=344
06/22/2022 14:25:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=349
06/22/2022 14:25:41 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.4420512820512821 on epoch=349
06/22/2022 14:25:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=354
06/22/2022 14:25:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=359
06/22/2022 14:25:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
06/22/2022 14:25:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=369
06/22/2022 14:25:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=374
06/22/2022 14:25:48 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.41700404858299595 on epoch=374
06/22/2022 14:25:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
06/22/2022 14:25:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
06/22/2022 14:25:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=389
06/22/2022 14:25:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
06/22/2022 14:25:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=399
06/22/2022 14:25:54 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.43529411764705883 on epoch=399
06/22/2022 14:25:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=404
06/22/2022 14:25:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=409
06/22/2022 14:25:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=414
06/22/2022 14:25:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
06/22/2022 14:26:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
06/22/2022 14:26:01 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.40566959921798634 on epoch=424
06/22/2022 14:26:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
06/22/2022 14:26:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
06/22/2022 14:26:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
06/22/2022 14:26:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=444
06/22/2022 14:26:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/22/2022 14:26:07 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.4554554554554554 on epoch=449
06/22/2022 14:26:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=454
06/22/2022 14:26:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=459
06/22/2022 14:26:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
06/22/2022 14:26:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/22/2022 14:26:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/22/2022 14:26:14 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.4554554554554554 on epoch=474
06/22/2022 14:26:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/22/2022 14:26:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
06/22/2022 14:26:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
06/22/2022 14:26:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
06/22/2022 14:26:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
06/22/2022 14:26:20 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.4420512820512821 on epoch=499
06/22/2022 14:26:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
06/22/2022 14:26:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
06/22/2022 14:26:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/22/2022 14:26:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/22/2022 14:26:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
06/22/2022 14:26:27 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.40566959921798634 on epoch=524
06/22/2022 14:26:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
06/22/2022 14:26:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
06/22/2022 14:26:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 14:26:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=544
06/22/2022 14:26:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
06/22/2022 14:26:33 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.4285714285714286 on epoch=549
06/22/2022 14:26:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
06/22/2022 14:26:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/22/2022 14:26:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 14:26:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/22/2022 14:26:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 14:26:40 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.40566959921798634 on epoch=574
06/22/2022 14:26:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 14:26:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/22/2022 14:26:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/22/2022 14:26:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/22/2022 14:26:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
06/22/2022 14:26:46 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.4420512820512821 on epoch=599
06/22/2022 14:26:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
06/22/2022 14:26:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
06/22/2022 14:26:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/22/2022 14:26:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/22/2022 14:26:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
06/22/2022 14:26:53 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.4009852216748768 on epoch=624
06/22/2022 14:26:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 14:26:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 14:26:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
06/22/2022 14:26:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 14:26:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/22/2022 14:26:59 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=649
06/22/2022 14:27:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 14:27:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/22/2022 14:27:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/22/2022 14:27:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 14:27:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/22/2022 14:27:05 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.4817813765182186 on epoch=674
06/22/2022 14:27:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 14:27:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/22/2022 14:27:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/22/2022 14:27:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/22/2022 14:27:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/22/2022 14:27:12 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=699
06/22/2022 14:27:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 14:27:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/22/2022 14:27:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/22/2022 14:27:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/22/2022 14:27:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/22/2022 14:27:18 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=724
06/22/2022 14:27:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/22/2022 14:27:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/22/2022 14:27:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/22/2022 14:27:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
06/22/2022 14:27:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/22/2022 14:27:25 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=749
06/22/2022 14:27:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 14:27:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
06/22/2022 14:27:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 14:27:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/22/2022 14:27:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/22/2022 14:27:31 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.4420512820512821 on epoch=774
06/22/2022 14:27:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/22/2022 14:27:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 14:27:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 14:27:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/22/2022 14:27:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/22/2022 14:27:38 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.375 on epoch=799
06/22/2022 14:27:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 14:27:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/22/2022 14:27:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 14:27:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 14:27:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 14:27:44 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=824
06/22/2022 14:27:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 14:27:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 14:27:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/22/2022 14:27:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 14:27:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 14:27:51 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4375 on epoch=849
06/22/2022 14:27:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=854
06/22/2022 14:27:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
06/22/2022 14:27:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 14:27:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 14:27:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/22/2022 14:27:57 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=874
06/22/2022 14:27:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 14:27:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/22/2022 14:28:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/22/2022 14:28:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 14:28:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 14:28:03 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.375 on epoch=899
06/22/2022 14:28:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 14:28:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/22/2022 14:28:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/22/2022 14:28:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 14:28:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/22/2022 14:28:10 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=924
06/22/2022 14:28:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 14:28:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 14:28:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 14:28:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/22/2022 14:28:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 14:28:16 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.40566959921798634 on epoch=949
06/22/2022 14:28:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 14:28:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 14:28:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 14:28:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 14:28:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 14:28:23 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.37254901960784315 on epoch=974
06/22/2022 14:28:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/22/2022 14:28:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 14:28:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 14:28:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/22/2022 14:28:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/22/2022 14:28:29 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=999
06/22/2022 14:28:29 - INFO - __main__ - save last model!
06/22/2022 14:28:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 14:28:29 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 14:28:29 - INFO - __main__ - Printing 3 examples
06/22/2022 14:28:29 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 14:28:29 - INFO - __main__ - ['0']
06/22/2022 14:28:29 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 14:28:29 - INFO - __main__ - ['1']
06/22/2022 14:28:29 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 14:28:29 - INFO - __main__ - ['1']
06/22/2022 14:28:29 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:28:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:28:30 - INFO - __main__ - Printing 3 examples
06/22/2022 14:28:30 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/22/2022 14:28:30 - INFO - __main__ - ['1']
06/22/2022 14:28:30 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/22/2022 14:28:30 - INFO - __main__ - ['1']
06/22/2022 14:28:30 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/22/2022 14:28:30 - INFO - __main__ - ['1']
06/22/2022 14:28:30 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:28:30 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:28:30 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:28:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:28:30 - INFO - __main__ - Printing 3 examples
06/22/2022 14:28:30 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/22/2022 14:28:30 - INFO - __main__ - ['1']
06/22/2022 14:28:30 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/22/2022 14:28:30 - INFO - __main__ - ['1']
06/22/2022 14:28:30 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/22/2022 14:28:30 - INFO - __main__ - ['1']
06/22/2022 14:28:30 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:28:30 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:28:30 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:28:33 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:28:35 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:28:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:28:36 - INFO - __main__ - Starting training!
06/22/2022 14:28:41 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 14:30:10 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_21_0.3_8_predictions.txt
06/22/2022 14:30:10 - INFO - __main__ - Classification-F1 on test data: 0.4941
06/22/2022 14:30:10 - INFO - __main__ - prefix=paws_16_21, lr=0.3, bsz=8, dev_performance=0.6532019704433498, test_performance=0.4941380241398986
06/22/2022 14:30:10 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.2, bsz=8 ...
06/22/2022 14:30:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:30:11 - INFO - __main__ - Printing 3 examples
06/22/2022 14:30:11 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/22/2022 14:30:11 - INFO - __main__ - ['1']
06/22/2022 14:30:11 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/22/2022 14:30:11 - INFO - __main__ - ['1']
06/22/2022 14:30:11 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/22/2022 14:30:11 - INFO - __main__ - ['1']
06/22/2022 14:30:11 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:30:11 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:30:11 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:30:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:30:11 - INFO - __main__ - Printing 3 examples
06/22/2022 14:30:11 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/22/2022 14:30:11 - INFO - __main__ - ['1']
06/22/2022 14:30:11 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/22/2022 14:30:11 - INFO - __main__ - ['1']
06/22/2022 14:30:11 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/22/2022 14:30:11 - INFO - __main__ - ['1']
06/22/2022 14:30:11 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:30:11 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:30:11 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:30:16 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:30:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:30:16 - INFO - __main__ - Starting training!
06/22/2022 14:30:18 - INFO - __main__ - Step 10 Global step 10 Train loss 4.33 on epoch=4
06/22/2022 14:30:19 - INFO - __main__ - Step 20 Global step 20 Train loss 3.48 on epoch=9
06/22/2022 14:30:20 - INFO - __main__ - Step 30 Global step 30 Train loss 2.83 on epoch=14
06/22/2022 14:30:22 - INFO - __main__ - Step 40 Global step 40 Train loss 2.20 on epoch=19
06/22/2022 14:30:23 - INFO - __main__ - Step 50 Global step 50 Train loss 1.84 on epoch=24
06/22/2022 14:30:23 - INFO - __main__ - Global step 50 Train loss 2.94 Classification-F1 0.3333333333333333 on epoch=24
06/22/2022 14:30:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/22/2022 14:30:25 - INFO - __main__ - Step 60 Global step 60 Train loss 1.39 on epoch=29
06/22/2022 14:30:26 - INFO - __main__ - Step 70 Global step 70 Train loss 1.11 on epoch=34
06/22/2022 14:30:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=39
06/22/2022 14:30:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=44
06/22/2022 14:30:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.79 on epoch=49
06/22/2022 14:30:30 - INFO - __main__ - Global step 100 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 14:30:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.72 on epoch=54
06/22/2022 14:30:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.63 on epoch=59
06/22/2022 14:30:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=64
06/22/2022 14:30:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=69
06/22/2022 14:30:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=74
06/22/2022 14:30:36 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 14:30:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=79
06/22/2022 14:30:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=84
06/22/2022 14:30:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=89
06/22/2022 14:30:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.61 on epoch=94
06/22/2022 14:30:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=99
06/22/2022 14:30:43 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 14:30:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=104
06/22/2022 14:30:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=109
06/22/2022 14:30:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=114
06/22/2022 14:30:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=119
06/22/2022 14:30:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=124
06/22/2022 14:30:49 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 14:30:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
06/22/2022 14:30:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=134
06/22/2022 14:30:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=139
06/22/2022 14:30:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=144
06/22/2022 14:30:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=149
06/22/2022 14:30:56 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=149
06/22/2022 14:30:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=154
06/22/2022 14:30:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=159
06/22/2022 14:30:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=164
06/22/2022 14:31:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=169
06/22/2022 14:31:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=174
06/22/2022 14:31:02 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=174
06/22/2022 14:31:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=179
06/22/2022 14:31:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=184
06/22/2022 14:31:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=189
06/22/2022 14:31:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=194
06/22/2022 14:31:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=199
06/22/2022 14:31:09 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=199
06/22/2022 14:31:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=204
06/22/2022 14:31:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=209
06/22/2022 14:31:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=214
06/22/2022 14:31:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=219
06/22/2022 14:31:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=224
06/22/2022 14:31:15 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=224
06/22/2022 14:31:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=229
06/22/2022 14:31:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=234
06/22/2022 14:31:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=239
06/22/2022 14:31:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=244
06/22/2022 14:31:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=249
06/22/2022 14:31:22 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=249
06/22/2022 14:31:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=254
06/22/2022 14:31:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=259
06/22/2022 14:31:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=264
06/22/2022 14:31:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=269
06/22/2022 14:31:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=274
06/22/2022 14:31:28 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=274
06/22/2022 14:31:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=274, global_step=550
06/22/2022 14:31:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=279
06/22/2022 14:31:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=284
06/22/2022 14:31:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
06/22/2022 14:31:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=294
06/22/2022 14:31:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=299
06/22/2022 14:31:34 - INFO - __main__ - Global step 600 Train loss 0.32 Classification-F1 0.4589371980676329 on epoch=299
06/22/2022 14:31:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4589371980676329 on epoch=299, global_step=600
06/22/2022 14:31:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=304
06/22/2022 14:31:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=309
06/22/2022 14:31:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=314
06/22/2022 14:31:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=319
06/22/2022 14:31:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=324
06/22/2022 14:31:41 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.39756367663344405 on epoch=324
06/22/2022 14:31:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=329
06/22/2022 14:31:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=334
06/22/2022 14:31:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=339
06/22/2022 14:31:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=344
06/22/2022 14:31:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=349
06/22/2022 14:31:47 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.4181818181818182 on epoch=349
06/22/2022 14:31:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=354
06/22/2022 14:31:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=359
06/22/2022 14:31:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=364
06/22/2022 14:31:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
06/22/2022 14:31:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=374
06/22/2022 14:31:54 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.39756367663344405 on epoch=374
06/22/2022 14:31:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=379
06/22/2022 14:31:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=384
06/22/2022 14:31:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=389
06/22/2022 14:31:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=394
06/22/2022 14:32:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=399
06/22/2022 14:32:00 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.4231177094379639 on epoch=399
06/22/2022 14:32:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=404
06/22/2022 14:32:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=409
06/22/2022 14:32:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=414
06/22/2022 14:32:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=419
06/22/2022 14:32:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=424
06/22/2022 14:32:07 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.4420512820512821 on epoch=424
06/22/2022 14:32:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=429
06/22/2022 14:32:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=434
06/22/2022 14:32:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=439
06/22/2022 14:32:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=444
06/22/2022 14:32:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=449
06/22/2022 14:32:13 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.4920634920634921 on epoch=449
06/22/2022 14:32:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.4920634920634921 on epoch=449, global_step=900
06/22/2022 14:32:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=454
06/22/2022 14:32:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=459
06/22/2022 14:32:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=464
06/22/2022 14:32:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=469
06/22/2022 14:32:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=474
06/22/2022 14:32:20 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.4920634920634921 on epoch=474
06/22/2022 14:32:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=479
06/22/2022 14:32:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=484
06/22/2022 14:32:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=489
06/22/2022 14:32:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=494
06/22/2022 14:32:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=499
06/22/2022 14:32:26 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.4375 on epoch=499
06/22/2022 14:32:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=504
06/22/2022 14:32:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=509
06/22/2022 14:32:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.13 on epoch=514
06/22/2022 14:32:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=519
06/22/2022 14:32:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=524
06/22/2022 14:32:33 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.3522267206477733 on epoch=524
06/22/2022 14:32:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=529
06/22/2022 14:32:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=534
06/22/2022 14:32:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=539
06/22/2022 14:32:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=544
06/22/2022 14:32:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
06/22/2022 14:32:39 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.43529411764705883 on epoch=549
06/22/2022 14:32:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=554
06/22/2022 14:32:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
06/22/2022 14:32:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=564
06/22/2022 14:32:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=569
06/22/2022 14:32:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=574
06/22/2022 14:32:45 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.43529411764705883 on epoch=574
06/22/2022 14:32:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=579
06/22/2022 14:32:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=584
06/22/2022 14:32:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=589
06/22/2022 14:32:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=594
06/22/2022 14:32:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
06/22/2022 14:32:52 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.43529411764705883 on epoch=599
06/22/2022 14:32:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
06/22/2022 14:32:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
06/22/2022 14:32:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
06/22/2022 14:32:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=619
06/22/2022 14:32:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
06/22/2022 14:32:58 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.4009852216748768 on epoch=624
06/22/2022 14:32:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/22/2022 14:33:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=634
06/22/2022 14:33:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/22/2022 14:33:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=644
06/22/2022 14:33:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
06/22/2022 14:33:05 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.4682306940371457 on epoch=649
06/22/2022 14:33:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/22/2022 14:33:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
06/22/2022 14:33:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
06/22/2022 14:33:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
06/22/2022 14:33:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
06/22/2022 14:33:11 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.43529411764705883 on epoch=674
06/22/2022 14:33:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=679
06/22/2022 14:33:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/22/2022 14:33:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
06/22/2022 14:33:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
06/22/2022 14:33:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=699
06/22/2022 14:33:18 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.43529411764705883 on epoch=699
06/22/2022 14:33:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
06/22/2022 14:33:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
06/22/2022 14:33:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
06/22/2022 14:33:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
06/22/2022 14:33:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
06/22/2022 14:33:24 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.43529411764705883 on epoch=724
06/22/2022 14:33:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
06/22/2022 14:33:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
06/22/2022 14:33:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=739
06/22/2022 14:33:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 14:33:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/22/2022 14:33:31 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.4375 on epoch=749
06/22/2022 14:33:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
06/22/2022 14:33:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/22/2022 14:33:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
06/22/2022 14:33:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/22/2022 14:33:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/22/2022 14:33:37 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.43529411764705883 on epoch=774
06/22/2022 14:33:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
06/22/2022 14:33:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
06/22/2022 14:33:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=789
06/22/2022 14:33:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/22/2022 14:33:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
06/22/2022 14:33:44 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.43529411764705883 on epoch=799
06/22/2022 14:33:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
06/22/2022 14:33:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/22/2022 14:33:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
06/22/2022 14:33:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/22/2022 14:33:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
06/22/2022 14:33:50 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.4375 on epoch=824
06/22/2022 14:33:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
06/22/2022 14:33:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/22/2022 14:33:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
06/22/2022 14:33:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/22/2022 14:33:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
06/22/2022 14:33:57 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=849
06/22/2022 14:33:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/22/2022 14:33:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/22/2022 14:34:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/22/2022 14:34:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/22/2022 14:34:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/22/2022 14:34:03 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=874
06/22/2022 14:34:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=879
06/22/2022 14:34:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
06/22/2022 14:34:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/22/2022 14:34:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
06/22/2022 14:34:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 14:34:09 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.3650793650793651 on epoch=899
06/22/2022 14:34:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
06/22/2022 14:34:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/22/2022 14:34:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/22/2022 14:34:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/22/2022 14:34:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/22/2022 14:34:16 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=924
06/22/2022 14:34:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
06/22/2022 14:34:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 14:34:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=939
06/22/2022 14:34:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/22/2022 14:34:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 14:34:23 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.4682306940371457 on epoch=949
06/22/2022 14:34:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/22/2022 14:34:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/22/2022 14:34:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 14:34:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/22/2022 14:34:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
06/22/2022 14:34:29 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=974
06/22/2022 14:34:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/22/2022 14:34:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/22/2022 14:34:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 14:34:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/22/2022 14:34:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=999
06/22/2022 14:34:35 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=999
06/22/2022 14:34:35 - INFO - __main__ - save last model!
06/22/2022 14:34:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 14:34:36 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 14:34:36 - INFO - __main__ - Printing 3 examples
06/22/2022 14:34:36 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 14:34:36 - INFO - __main__ - ['0']
06/22/2022 14:34:36 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 14:34:36 - INFO - __main__ - ['1']
06/22/2022 14:34:36 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 14:34:36 - INFO - __main__ - ['1']
06/22/2022 14:34:36 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:34:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:34:36 - INFO - __main__ - Printing 3 examples
06/22/2022 14:34:36 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/22/2022 14:34:36 - INFO - __main__ - ['1']
06/22/2022 14:34:36 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/22/2022 14:34:36 - INFO - __main__ - ['1']
06/22/2022 14:34:36 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/22/2022 14:34:36 - INFO - __main__ - ['1']
06/22/2022 14:34:36 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:34:36 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:34:36 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:34:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:34:36 - INFO - __main__ - Printing 3 examples
06/22/2022 14:34:36 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/22/2022 14:34:36 - INFO - __main__ - ['1']
06/22/2022 14:34:36 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/22/2022 14:34:36 - INFO - __main__ - ['1']
06/22/2022 14:34:36 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/22/2022 14:34:36 - INFO - __main__ - ['1']
06/22/2022 14:34:36 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:34:36 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:34:36 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:34:40 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:34:42 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:34:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:34:42 - INFO - __main__ - Starting training!
06/22/2022 14:34:47 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 14:36:17 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_21_0.2_8_predictions.txt
06/22/2022 14:36:17 - INFO - __main__ - Classification-F1 on test data: 0.5040
06/22/2022 14:36:17 - INFO - __main__ - prefix=paws_16_21, lr=0.2, bsz=8, dev_performance=0.4920634920634921, test_performance=0.504031458576056
06/22/2022 14:36:17 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.5, bsz=8 ...
06/22/2022 14:36:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:36:18 - INFO - __main__ - Printing 3 examples
06/22/2022 14:36:18 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/22/2022 14:36:18 - INFO - __main__ - ['1']
06/22/2022 14:36:18 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/22/2022 14:36:18 - INFO - __main__ - ['1']
06/22/2022 14:36:18 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/22/2022 14:36:18 - INFO - __main__ - ['1']
06/22/2022 14:36:18 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:36:18 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:36:18 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:36:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:36:18 - INFO - __main__ - Printing 3 examples
06/22/2022 14:36:18 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/22/2022 14:36:18 - INFO - __main__ - ['1']
06/22/2022 14:36:18 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/22/2022 14:36:18 - INFO - __main__ - ['1']
06/22/2022 14:36:18 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/22/2022 14:36:18 - INFO - __main__ - ['1']
06/22/2022 14:36:18 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:36:18 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:36:18 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:36:24 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:36:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:36:24 - INFO - __main__ - Starting training!
06/22/2022 14:36:25 - INFO - __main__ - Step 10 Global step 10 Train loss 3.78 on epoch=4
06/22/2022 14:36:27 - INFO - __main__ - Step 20 Global step 20 Train loss 2.30 on epoch=9
06/22/2022 14:36:28 - INFO - __main__ - Step 30 Global step 30 Train loss 1.30 on epoch=14
06/22/2022 14:36:29 - INFO - __main__ - Step 40 Global step 40 Train loss 0.83 on epoch=19
06/22/2022 14:36:30 - INFO - __main__ - Step 50 Global step 50 Train loss 0.61 on epoch=24
06/22/2022 14:36:30 - INFO - __main__ - Global step 50 Train loss 1.76 Classification-F1 0.6825396825396826 on epoch=24
06/22/2022 14:36:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.6825396825396826 on epoch=24, global_step=50
06/22/2022 14:36:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.74 on epoch=29
06/22/2022 14:36:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=34
06/22/2022 14:36:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=39
06/22/2022 14:36:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=44
06/22/2022 14:36:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=49
06/22/2022 14:36:37 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.5134502923976608 on epoch=49
06/22/2022 14:36:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
06/22/2022 14:36:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=59
06/22/2022 14:36:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=64
06/22/2022 14:36:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
06/22/2022 14:36:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/22/2022 14:36:43 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 14:36:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
06/22/2022 14:36:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=84
06/22/2022 14:36:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
06/22/2022 14:36:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
06/22/2022 14:36:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
06/22/2022 14:36:50 - INFO - __main__ - Global step 200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 14:36:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=104
06/22/2022 14:36:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
06/22/2022 14:36:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=114
06/22/2022 14:36:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
06/22/2022 14:36:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=124
06/22/2022 14:36:56 - INFO - __main__ - Global step 250 Train loss 0.31 Classification-F1 0.3992490613266583 on epoch=124
06/22/2022 14:36:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
06/22/2022 14:36:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
06/22/2022 14:37:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
06/22/2022 14:37:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
06/22/2022 14:37:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/22/2022 14:37:03 - INFO - __main__ - Global step 300 Train loss 0.27 Classification-F1 0.4589371980676329 on epoch=149
06/22/2022 14:37:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=154
06/22/2022 14:37:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=159
06/22/2022 14:37:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
06/22/2022 14:37:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=169
06/22/2022 14:37:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=174
06/22/2022 14:37:09 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.4285714285714286 on epoch=174
06/22/2022 14:37:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.22 on epoch=179
06/22/2022 14:37:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=184
06/22/2022 14:37:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=189
06/22/2022 14:37:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=194
06/22/2022 14:37:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
06/22/2022 14:37:16 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.5270935960591133 on epoch=199
06/22/2022 14:37:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
06/22/2022 14:37:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=209
06/22/2022 14:37:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
06/22/2022 14:37:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=219
06/22/2022 14:37:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
06/22/2022 14:37:22 - INFO - __main__ - Global step 450 Train loss 0.11 Classification-F1 0.43529411764705883 on epoch=224
06/22/2022 14:37:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.11 on epoch=229
06/22/2022 14:37:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
06/22/2022 14:37:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.10 on epoch=239
06/22/2022 14:37:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
06/22/2022 14:37:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
06/22/2022 14:37:29 - INFO - __main__ - Global step 500 Train loss 0.08 Classification-F1 0.4285714285714286 on epoch=249
06/22/2022 14:37:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.06 on epoch=254
06/22/2022 14:37:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
06/22/2022 14:37:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
06/22/2022 14:37:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
06/22/2022 14:37:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
06/22/2022 14:37:35 - INFO - __main__ - Global step 550 Train loss 0.05 Classification-F1 0.4980392156862745 on epoch=274
06/22/2022 14:37:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
06/22/2022 14:37:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
06/22/2022 14:37:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
06/22/2022 14:37:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
06/22/2022 14:37:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
06/22/2022 14:37:41 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.5307917888563051 on epoch=299
06/22/2022 14:37:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
06/22/2022 14:37:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
06/22/2022 14:37:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
06/22/2022 14:37:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
06/22/2022 14:37:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
06/22/2022 14:37:48 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.5901477832512315 on epoch=324
06/22/2022 14:37:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
06/22/2022 14:37:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
06/22/2022 14:37:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
06/22/2022 14:37:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
06/22/2022 14:37:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
06/22/2022 14:37:54 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=349
06/22/2022 14:37:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
06/22/2022 14:37:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/22/2022 14:37:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/22/2022 14:37:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/22/2022 14:38:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/22/2022 14:38:01 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.6000000000000001 on epoch=374
06/22/2022 14:38:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/22/2022 14:38:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/22/2022 14:38:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
06/22/2022 14:38:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
06/22/2022 14:38:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
06/22/2022 14:38:07 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=399
06/22/2022 14:38:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
06/22/2022 14:38:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
06/22/2022 14:38:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/22/2022 14:38:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
06/22/2022 14:38:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/22/2022 14:38:14 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.5625 on epoch=424
06/22/2022 14:38:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/22/2022 14:38:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/22/2022 14:38:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/22/2022 14:38:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
06/22/2022 14:38:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
06/22/2022 14:38:20 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.5625 on epoch=449
06/22/2022 14:38:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
06/22/2022 14:38:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/22/2022 14:38:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
06/22/2022 14:38:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/22/2022 14:38:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/22/2022 14:38:26 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.5625 on epoch=474
06/22/2022 14:38:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/22/2022 14:38:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/22/2022 14:38:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
06/22/2022 14:38:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/22/2022 14:38:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 14:38:33 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=499
06/22/2022 14:38:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/22/2022 14:38:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/22/2022 14:38:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/22/2022 14:38:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/22/2022 14:38:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/22/2022 14:38:39 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=524
06/22/2022 14:38:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/22/2022 14:38:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 14:38:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/22/2022 14:38:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/22/2022 14:38:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 14:38:46 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=549
06/22/2022 14:38:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 14:38:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 14:38:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/22/2022 14:38:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 14:38:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/22/2022 14:38:52 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=574
06/22/2022 14:38:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/22/2022 14:38:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 14:38:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 14:38:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 14:38:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 14:38:59 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=599
06/22/2022 14:39:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 14:39:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 14:39:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 14:39:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 14:39:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 14:39:05 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=624
06/22/2022 14:39:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 14:39:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 14:39:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 14:39:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 14:39:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 14:39:12 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.5625 on epoch=649
06/22/2022 14:39:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/22/2022 14:39:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 14:39:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 14:39:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 14:39:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 14:39:18 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=674
06/22/2022 14:39:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 14:39:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 14:39:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 14:39:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 14:39:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 14:39:25 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=699
06/22/2022 14:39:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 14:39:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 14:39:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 14:39:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 14:39:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 14:39:31 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5 on epoch=724
06/22/2022 14:39:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 14:39:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 14:39:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 14:39:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 14:39:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 14:39:37 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=749
06/22/2022 14:39:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 14:39:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 14:39:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 14:39:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 14:39:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 14:39:44 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=774
06/22/2022 14:39:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 14:39:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 14:39:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 14:39:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 14:39:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 14:39:50 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=799
06/22/2022 14:39:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 14:39:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 14:39:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 14:39:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 14:39:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 14:39:57 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5 on epoch=824
06/22/2022 14:39:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 14:39:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 14:40:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 14:40:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 14:40:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 14:40:03 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=849
06/22/2022 14:40:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 14:40:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 14:40:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 14:40:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 14:40:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 14:40:10 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=874
06/22/2022 14:40:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 14:40:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 14:40:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 14:40:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 14:40:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 14:40:16 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=899
06/22/2022 14:40:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 14:40:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 14:40:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 14:40:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 14:40:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 14:40:22 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=924
06/22/2022 14:40:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 14:40:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 14:40:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 14:40:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 14:40:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/22/2022 14:40:29 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=949
06/22/2022 14:40:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 14:40:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=959
06/22/2022 14:40:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 14:40:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 14:40:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 14:40:35 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=974
06/22/2022 14:40:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 14:40:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 14:40:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 14:40:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 14:40:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 14:40:42 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=999
06/22/2022 14:40:42 - INFO - __main__ - save last model!
06/22/2022 14:40:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 14:40:42 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 14:40:42 - INFO - __main__ - Printing 3 examples
06/22/2022 14:40:42 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 14:40:42 - INFO - __main__ - ['0']
06/22/2022 14:40:42 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 14:40:42 - INFO - __main__ - ['1']
06/22/2022 14:40:42 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 14:40:42 - INFO - __main__ - ['1']
06/22/2022 14:40:42 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:40:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:40:42 - INFO - __main__ - Printing 3 examples
06/22/2022 14:40:42 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/22/2022 14:40:42 - INFO - __main__ - ['1']
06/22/2022 14:40:42 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/22/2022 14:40:42 - INFO - __main__ - ['1']
06/22/2022 14:40:42 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/22/2022 14:40:42 - INFO - __main__ - ['1']
06/22/2022 14:40:42 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:40:42 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:40:42 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:40:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:40:43 - INFO - __main__ - Printing 3 examples
06/22/2022 14:40:43 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/22/2022 14:40:43 - INFO - __main__ - ['1']
06/22/2022 14:40:43 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/22/2022 14:40:43 - INFO - __main__ - ['1']
06/22/2022 14:40:43 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/22/2022 14:40:43 - INFO - __main__ - ['1']
06/22/2022 14:40:43 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:40:43 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:40:43 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:40:46 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:40:48 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:40:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:40:48 - INFO - __main__ - Starting training!
06/22/2022 14:40:54 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 14:42:22 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_42_0.5_8_predictions.txt
06/22/2022 14:42:22 - INFO - __main__ - Classification-F1 on test data: 0.3332
06/22/2022 14:42:22 - INFO - __main__ - prefix=paws_16_42, lr=0.5, bsz=8, dev_performance=0.6825396825396826, test_performance=0.3332068229572284
06/22/2022 14:42:22 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.4, bsz=8 ...
06/22/2022 14:42:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:42:23 - INFO - __main__ - Printing 3 examples
06/22/2022 14:42:23 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/22/2022 14:42:23 - INFO - __main__ - ['1']
06/22/2022 14:42:23 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/22/2022 14:42:23 - INFO - __main__ - ['1']
06/22/2022 14:42:23 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/22/2022 14:42:23 - INFO - __main__ - ['1']
06/22/2022 14:42:23 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:42:23 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:42:23 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:42:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:42:23 - INFO - __main__ - Printing 3 examples
06/22/2022 14:42:23 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/22/2022 14:42:23 - INFO - __main__ - ['1']
06/22/2022 14:42:23 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/22/2022 14:42:23 - INFO - __main__ - ['1']
06/22/2022 14:42:23 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/22/2022 14:42:23 - INFO - __main__ - ['1']
06/22/2022 14:42:23 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:42:23 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:42:23 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:42:28 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:42:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:42:29 - INFO - __main__ - Starting training!
06/22/2022 14:42:30 - INFO - __main__ - Step 10 Global step 10 Train loss 4.00 on epoch=4
06/22/2022 14:42:31 - INFO - __main__ - Step 20 Global step 20 Train loss 2.50 on epoch=9
06/22/2022 14:42:33 - INFO - __main__ - Step 30 Global step 30 Train loss 1.58 on epoch=14
06/22/2022 14:42:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=19
06/22/2022 14:42:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.73 on epoch=24
06/22/2022 14:42:35 - INFO - __main__ - Global step 50 Train loss 1.95 Classification-F1 0.3816425120772947 on epoch=24
06/22/2022 14:42:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3816425120772947 on epoch=24, global_step=50
06/22/2022 14:42:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=29
06/22/2022 14:42:38 - INFO - __main__ - Step 70 Global step 70 Train loss 0.60 on epoch=34
06/22/2022 14:42:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=39
06/22/2022 14:42:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=44
06/22/2022 14:42:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
06/22/2022 14:42:42 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 14:42:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=54
06/22/2022 14:42:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
06/22/2022 14:42:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=64
06/22/2022 14:42:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=69
06/22/2022 14:42:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/22/2022 14:42:48 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 14:42:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=79
06/22/2022 14:42:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=84
06/22/2022 14:42:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=89
06/22/2022 14:42:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=94
06/22/2022 14:42:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
06/22/2022 14:42:55 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.4589371980676329 on epoch=99
06/22/2022 14:42:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.4589371980676329 on epoch=99, global_step=200
06/22/2022 14:42:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=104
06/22/2022 14:42:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=109
06/22/2022 14:42:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
06/22/2022 14:43:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=119
06/22/2022 14:43:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=124
06/22/2022 14:43:01 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.464039408866995 on epoch=124
06/22/2022 14:43:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.464039408866995 on epoch=124, global_step=250
06/22/2022 14:43:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=129
06/22/2022 14:43:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=134
06/22/2022 14:43:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=139
06/22/2022 14:43:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
06/22/2022 14:43:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
06/22/2022 14:43:08 - INFO - __main__ - Global step 300 Train loss 0.36 Classification-F1 0.4589371980676329 on epoch=149
06/22/2022 14:43:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=154
06/22/2022 14:43:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.30 on epoch=159
06/22/2022 14:43:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=164
06/22/2022 14:43:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=169
06/22/2022 14:43:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
06/22/2022 14:43:14 - INFO - __main__ - Global step 350 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=174
06/22/2022 14:43:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/22/2022 14:43:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=184
06/22/2022 14:43:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=189
06/22/2022 14:43:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=194
06/22/2022 14:43:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=199
06/22/2022 14:43:21 - INFO - __main__ - Global step 400 Train loss 0.30 Classification-F1 0.3191489361702127 on epoch=199
06/22/2022 14:43:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/22/2022 14:43:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=209
06/22/2022 14:43:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.30 on epoch=214
06/22/2022 14:43:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
06/22/2022 14:43:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
06/22/2022 14:43:27 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.46843853820598 on epoch=224
06/22/2022 14:43:27 - INFO - __main__ - Saving model with best Classification-F1: 0.464039408866995 -> 0.46843853820598 on epoch=224, global_step=450
06/22/2022 14:43:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
06/22/2022 14:43:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
06/22/2022 14:43:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
06/22/2022 14:43:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
06/22/2022 14:43:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.17 on epoch=249
06/22/2022 14:43:34 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.36374269005847953 on epoch=249
06/22/2022 14:43:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
06/22/2022 14:43:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=259
06/22/2022 14:43:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=264
06/22/2022 14:43:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=269
06/22/2022 14:43:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
06/22/2022 14:43:40 - INFO - __main__ - Global step 550 Train loss 0.18 Classification-F1 0.43529411764705883 on epoch=274
06/22/2022 14:43:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=279
06/22/2022 14:43:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=284
06/22/2022 14:43:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=289
06/22/2022 14:43:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
06/22/2022 14:43:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=299
06/22/2022 14:43:47 - INFO - __main__ - Global step 600 Train loss 0.13 Classification-F1 0.5076923076923077 on epoch=299
06/22/2022 14:43:47 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5076923076923077 on epoch=299, global_step=600
06/22/2022 14:43:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=304
06/22/2022 14:43:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=309
06/22/2022 14:43:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=314
06/22/2022 14:43:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=319
06/22/2022 14:43:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
06/22/2022 14:43:54 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.43529411764705883 on epoch=324
06/22/2022 14:43:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=329
06/22/2022 14:43:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
06/22/2022 14:43:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=339
06/22/2022 14:43:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=344
06/22/2022 14:44:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
06/22/2022 14:44:00 - INFO - __main__ - Global step 700 Train loss 0.10 Classification-F1 0.4666666666666667 on epoch=349
06/22/2022 14:44:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
06/22/2022 14:44:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=359
06/22/2022 14:44:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
06/22/2022 14:44:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
06/22/2022 14:44:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
06/22/2022 14:44:07 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.5625 on epoch=374
06/22/2022 14:44:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5625 on epoch=374, global_step=750
06/22/2022 14:44:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
06/22/2022 14:44:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
06/22/2022 14:44:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
06/22/2022 14:44:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
06/22/2022 14:44:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
06/22/2022 14:44:13 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.5195195195195195 on epoch=399
06/22/2022 14:44:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
06/22/2022 14:44:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/22/2022 14:44:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
06/22/2022 14:44:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/22/2022 14:44:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/22/2022 14:44:20 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.5 on epoch=424
06/22/2022 14:44:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
06/22/2022 14:44:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
06/22/2022 14:44:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/22/2022 14:44:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/22/2022 14:44:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
06/22/2022 14:44:26 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.5733333333333335 on epoch=449
06/22/2022 14:44:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5625 -> 0.5733333333333335 on epoch=449, global_step=900
06/22/2022 14:44:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
06/22/2022 14:44:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
06/22/2022 14:44:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
06/22/2022 14:44:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
06/22/2022 14:44:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/22/2022 14:44:33 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.5465587044534412 on epoch=474
06/22/2022 14:44:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
06/22/2022 14:44:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
06/22/2022 14:44:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/22/2022 14:44:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
06/22/2022 14:44:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/22/2022 14:44:39 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.5465587044534412 on epoch=499
06/22/2022 14:44:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 14:44:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
06/22/2022 14:44:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 14:44:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 14:44:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/22/2022 14:44:46 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=524
06/22/2022 14:44:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5733333333333335 -> 0.5933528836754642 on epoch=524, global_step=1050
06/22/2022 14:44:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/22/2022 14:44:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
06/22/2022 14:44:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 14:44:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/22/2022 14:44:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 14:44:52 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=549
06/22/2022 14:44:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
06/22/2022 14:44:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 14:44:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 14:44:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
06/22/2022 14:44:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/22/2022 14:44:58 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.5607843137254902 on epoch=574
06/22/2022 14:45:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
06/22/2022 14:45:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/22/2022 14:45:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 14:45:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/22/2022 14:45:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/22/2022 14:45:05 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=599
06/22/2022 14:45:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 14:45:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/22/2022 14:45:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 14:45:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/22/2022 14:45:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/22/2022 14:45:11 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=624
06/22/2022 14:45:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 14:45:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 14:45:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 14:45:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 14:45:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/22/2022 14:45:18 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=649
06/22/2022 14:45:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 14:45:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 14:45:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 14:45:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 14:45:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 14:45:24 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=674
06/22/2022 14:45:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/22/2022 14:45:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/22/2022 14:45:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 14:45:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 14:45:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/22/2022 14:45:31 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=699
06/22/2022 14:45:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 14:45:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 14:45:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=714
06/22/2022 14:45:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 14:45:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 14:45:37 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=724
06/22/2022 14:45:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 14:45:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/22/2022 14:45:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=739
06/22/2022 14:45:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 14:45:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 14:45:44 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5901477832512315 on epoch=749
06/22/2022 14:45:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 14:45:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 14:45:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 14:45:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 14:45:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 14:45:50 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.6235294117647059 on epoch=774
06/22/2022 14:45:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6235294117647059 on epoch=774, global_step=1550
06/22/2022 14:45:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 14:45:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 14:45:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
06/22/2022 14:45:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 14:45:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 14:45:56 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5835835835835835 on epoch=799
06/22/2022 14:45:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 14:45:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 14:46:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 14:46:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 14:46:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 14:46:03 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=824
06/22/2022 14:46:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 14:46:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 14:46:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 14:46:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/22/2022 14:46:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 14:46:09 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=849
06/22/2022 14:46:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/22/2022 14:46:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 14:46:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 14:46:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 14:46:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 14:46:16 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=874
06/22/2022 14:46:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 14:46:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 14:46:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 14:46:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 14:46:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 14:46:22 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=899
06/22/2022 14:46:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
06/22/2022 14:46:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
06/22/2022 14:46:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 14:46:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 14:46:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 14:46:29 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=924
06/22/2022 14:46:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 14:46:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/22/2022 14:46:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 14:46:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 14:46:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 14:46:35 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=949
06/22/2022 14:46:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 14:46:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 14:46:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 14:46:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 14:46:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 14:46:41 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=974
06/22/2022 14:46:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 14:46:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 14:46:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 14:46:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 14:46:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 14:46:48 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=999
06/22/2022 14:46:48 - INFO - __main__ - save last model!
06/22/2022 14:46:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 14:46:48 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 14:46:48 - INFO - __main__ - Printing 3 examples
06/22/2022 14:46:48 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 14:46:48 - INFO - __main__ - ['0']
06/22/2022 14:46:48 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 14:46:48 - INFO - __main__ - ['1']
06/22/2022 14:46:48 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 14:46:48 - INFO - __main__ - ['1']
06/22/2022 14:46:48 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:46:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:46:49 - INFO - __main__ - Printing 3 examples
06/22/2022 14:46:49 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/22/2022 14:46:49 - INFO - __main__ - ['1']
06/22/2022 14:46:49 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/22/2022 14:46:49 - INFO - __main__ - ['1']
06/22/2022 14:46:49 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/22/2022 14:46:49 - INFO - __main__ - ['1']
06/22/2022 14:46:49 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:46:49 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:46:49 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:46:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:46:49 - INFO - __main__ - Printing 3 examples
06/22/2022 14:46:49 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/22/2022 14:46:49 - INFO - __main__ - ['1']
06/22/2022 14:46:49 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/22/2022 14:46:49 - INFO - __main__ - ['1']
06/22/2022 14:46:49 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/22/2022 14:46:49 - INFO - __main__ - ['1']
06/22/2022 14:46:49 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:46:49 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:46:49 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:46:52 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:46:54 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:46:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:46:54 - INFO - __main__ - Starting training!
06/22/2022 14:47:00 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 14:48:28 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_42_0.4_8_predictions.txt
06/22/2022 14:48:28 - INFO - __main__ - Classification-F1 on test data: 0.1982
06/22/2022 14:48:29 - INFO - __main__ - prefix=paws_16_42, lr=0.4, bsz=8, dev_performance=0.6235294117647059, test_performance=0.19823537589814302
06/22/2022 14:48:29 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.3, bsz=8 ...
06/22/2022 14:48:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:48:30 - INFO - __main__ - Printing 3 examples
06/22/2022 14:48:30 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/22/2022 14:48:30 - INFO - __main__ - ['1']
06/22/2022 14:48:30 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/22/2022 14:48:30 - INFO - __main__ - ['1']
06/22/2022 14:48:30 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/22/2022 14:48:30 - INFO - __main__ - ['1']
06/22/2022 14:48:30 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:48:30 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:48:30 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:48:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:48:30 - INFO - __main__ - Printing 3 examples
06/22/2022 14:48:30 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/22/2022 14:48:30 - INFO - __main__ - ['1']
06/22/2022 14:48:30 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/22/2022 14:48:30 - INFO - __main__ - ['1']
06/22/2022 14:48:30 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/22/2022 14:48:30 - INFO - __main__ - ['1']
06/22/2022 14:48:30 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:48:30 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:48:30 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:48:36 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:48:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:48:36 - INFO - __main__ - Starting training!
06/22/2022 14:48:38 - INFO - __main__ - Step 10 Global step 10 Train loss 4.35 on epoch=4
06/22/2022 14:48:39 - INFO - __main__ - Step 20 Global step 20 Train loss 3.01 on epoch=9
06/22/2022 14:48:40 - INFO - __main__ - Step 30 Global step 30 Train loss 2.17 on epoch=14
06/22/2022 14:48:41 - INFO - __main__ - Step 40 Global step 40 Train loss 1.57 on epoch=19
06/22/2022 14:48:43 - INFO - __main__ - Step 50 Global step 50 Train loss 1.09 on epoch=24
06/22/2022 14:48:43 - INFO - __main__ - Global step 50 Train loss 2.44 Classification-F1 0.3333333333333333 on epoch=24
06/22/2022 14:48:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/22/2022 14:48:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.82 on epoch=29
06/22/2022 14:48:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.79 on epoch=34
06/22/2022 14:48:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=39
06/22/2022 14:48:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=44
06/22/2022 14:48:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=49
06/22/2022 14:48:50 - INFO - __main__ - Global step 100 Train loss 0.67 Classification-F1 0.3191489361702127 on epoch=49
06/22/2022 14:48:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=54
06/22/2022 14:48:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=59
06/22/2022 14:48:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
06/22/2022 14:48:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=69
06/22/2022 14:48:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=74
06/22/2022 14:48:56 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 14:48:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
06/22/2022 14:48:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=84
06/22/2022 14:49:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=89
06/22/2022 14:49:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=94
06/22/2022 14:49:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
06/22/2022 14:49:03 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 14:49:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=104
06/22/2022 14:49:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=109
06/22/2022 14:49:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/22/2022 14:49:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=119
06/22/2022 14:49:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=124
06/22/2022 14:49:10 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 14:49:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=129
06/22/2022 14:49:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
06/22/2022 14:49:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
06/22/2022 14:49:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=144
06/22/2022 14:49:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=149
06/22/2022 14:49:16 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=149
06/22/2022 14:49:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.33 on epoch=154
06/22/2022 14:49:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=159
06/22/2022 14:49:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=164
06/22/2022 14:49:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
06/22/2022 14:49:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=174
06/22/2022 14:49:23 - INFO - __main__ - Global step 350 Train loss 0.32 Classification-F1 0.4589371980676329 on epoch=174
06/22/2022 14:49:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=174, global_step=350
06/22/2022 14:49:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
06/22/2022 14:49:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.29 on epoch=184
06/22/2022 14:49:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=189
06/22/2022 14:49:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=194
06/22/2022 14:49:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=199
06/22/2022 14:49:30 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.4589371980676329 on epoch=199
06/22/2022 14:49:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=204
06/22/2022 14:49:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
06/22/2022 14:49:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=214
06/22/2022 14:49:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
06/22/2022 14:49:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/22/2022 14:49:36 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=224
06/22/2022 14:49:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
06/22/2022 14:49:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=234
06/22/2022 14:49:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=239
06/22/2022 14:49:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
06/22/2022 14:49:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=249
06/22/2022 14:49:43 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.46843853820598 on epoch=249
06/22/2022 14:49:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.46843853820598 on epoch=249, global_step=500
06/22/2022 14:49:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=254
06/22/2022 14:49:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=259
06/22/2022 14:49:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=264
06/22/2022 14:49:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
06/22/2022 14:49:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=274
06/22/2022 14:49:50 - INFO - __main__ - Global step 550 Train loss 0.18 Classification-F1 0.5733333333333335 on epoch=274
06/22/2022 14:49:50 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5733333333333335 on epoch=274, global_step=550
06/22/2022 14:49:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=279
06/22/2022 14:49:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=284
06/22/2022 14:49:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
06/22/2022 14:49:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=294
06/22/2022 14:49:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
06/22/2022 14:49:56 - INFO - __main__ - Global step 600 Train loss 0.16 Classification-F1 0.4458874458874459 on epoch=299
06/22/2022 14:49:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=304
06/22/2022 14:49:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=309
06/22/2022 14:50:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=314
06/22/2022 14:50:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=319
06/22/2022 14:50:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=324
06/22/2022 14:50:03 - INFO - __main__ - Global step 650 Train loss 0.13 Classification-F1 0.4920634920634921 on epoch=324
06/22/2022 14:50:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=329
06/22/2022 14:50:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
06/22/2022 14:50:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
06/22/2022 14:50:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
06/22/2022 14:50:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=349
06/22/2022 14:50:09 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.5555555555555556 on epoch=349
06/22/2022 14:50:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
06/22/2022 14:50:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
06/22/2022 14:50:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=364
06/22/2022 14:50:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=369
06/22/2022 14:50:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
06/22/2022 14:50:16 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.5555555555555556 on epoch=374
06/22/2022 14:50:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=379
06/22/2022 14:50:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
06/22/2022 14:50:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=389
06/22/2022 14:50:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
06/22/2022 14:50:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
06/22/2022 14:50:22 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.5555555555555556 on epoch=399
06/22/2022 14:50:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
06/22/2022 14:50:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/22/2022 14:50:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
06/22/2022 14:50:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=419
06/22/2022 14:50:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/22/2022 14:50:29 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.5270935960591133 on epoch=424
06/22/2022 14:50:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
06/22/2022 14:50:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/22/2022 14:50:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/22/2022 14:50:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/22/2022 14:50:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
06/22/2022 14:50:35 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.5307917888563051 on epoch=449
06/22/2022 14:50:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/22/2022 14:50:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/22/2022 14:50:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 14:50:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/22/2022 14:50:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/22/2022 14:50:42 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=474
06/22/2022 14:50:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/22/2022 14:50:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/22/2022 14:50:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/22/2022 14:50:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
06/22/2022 14:50:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/22/2022 14:50:48 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=499
06/22/2022 14:50:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 14:50:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/22/2022 14:50:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 14:50:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 14:50:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/22/2022 14:50:55 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=524
06/22/2022 14:50:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/22/2022 14:50:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/22/2022 14:50:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/22/2022 14:50:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/22/2022 14:51:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/22/2022 14:51:01 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5625 on epoch=549
06/22/2022 14:51:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 14:51:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 14:51:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/22/2022 14:51:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 14:51:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/22/2022 14:51:08 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=574
06/22/2022 14:51:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/22/2022 14:51:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/22/2022 14:51:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/22/2022 14:51:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/22/2022 14:51:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 14:51:14 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=599
06/22/2022 14:51:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/22/2022 14:51:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 14:51:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/22/2022 14:51:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/22/2022 14:51:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 14:51:20 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=624
06/22/2022 14:51:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 14:51:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 14:51:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 14:51:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/22/2022 14:51:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/22/2022 14:51:27 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.5625 on epoch=649
06/22/2022 14:51:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
06/22/2022 14:51:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 14:51:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 14:51:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
06/22/2022 14:51:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 14:51:33 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=674
06/22/2022 14:51:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 14:51:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 14:51:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 14:51:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 14:51:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 14:51:40 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.5 on epoch=699
06/22/2022 14:51:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 14:51:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 14:51:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/22/2022 14:51:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 14:51:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 14:51:46 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5625 on epoch=724
06/22/2022 14:51:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 14:51:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=734
06/22/2022 14:51:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 14:51:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 14:51:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=749
06/22/2022 14:51:53 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=749
06/22/2022 14:51:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 14:51:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/22/2022 14:51:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 14:51:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 14:51:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/22/2022 14:51:59 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=774
06/22/2022 14:51:59 - INFO - __main__ - Saving model with best Classification-F1: 0.5733333333333335 -> 0.5933528836754642 on epoch=774, global_step=1550
06/22/2022 14:52:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 14:52:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 14:52:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/22/2022 14:52:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 14:52:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/22/2022 14:52:05 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5835835835835835 on epoch=799
06/22/2022 14:52:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 14:52:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=809
06/22/2022 14:52:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 14:52:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 14:52:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 14:52:12 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=824
06/22/2022 14:52:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 14:52:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 14:52:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/22/2022 14:52:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/22/2022 14:52:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 14:52:18 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=849
06/22/2022 14:52:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 14:52:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 14:52:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 14:52:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 14:52:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 14:52:25 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=874
06/22/2022 14:52:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 14:52:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
06/22/2022 14:52:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 14:52:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 14:52:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 14:52:31 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.5465587044534412 on epoch=899
06/22/2022 14:52:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
06/22/2022 14:52:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 14:52:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 14:52:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 14:52:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 14:52:38 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=924
06/22/2022 14:52:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 14:52:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
06/22/2022 14:52:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 14:52:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 14:52:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 14:52:44 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=949
06/22/2022 14:52:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 14:52:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 14:52:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 14:52:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 14:52:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/22/2022 14:52:51 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=974
06/22/2022 14:52:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 14:52:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 14:52:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 14:52:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 14:52:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 14:52:57 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=999
06/22/2022 14:52:57 - INFO - __main__ - save last model!
06/22/2022 14:52:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 14:52:57 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 14:52:57 - INFO - __main__ - Printing 3 examples
06/22/2022 14:52:57 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 14:52:57 - INFO - __main__ - ['0']
06/22/2022 14:52:57 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 14:52:57 - INFO - __main__ - ['1']
06/22/2022 14:52:57 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 14:52:57 - INFO - __main__ - ['1']
06/22/2022 14:52:57 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:52:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:52:58 - INFO - __main__ - Printing 3 examples
06/22/2022 14:52:58 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/22/2022 14:52:58 - INFO - __main__ - ['1']
06/22/2022 14:52:58 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/22/2022 14:52:58 - INFO - __main__ - ['1']
06/22/2022 14:52:58 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/22/2022 14:52:58 - INFO - __main__ - ['1']
06/22/2022 14:52:58 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:52:58 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:52:58 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:52:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:52:58 - INFO - __main__ - Printing 3 examples
06/22/2022 14:52:58 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/22/2022 14:52:58 - INFO - __main__ - ['1']
06/22/2022 14:52:58 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/22/2022 14:52:58 - INFO - __main__ - ['1']
06/22/2022 14:52:58 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/22/2022 14:52:58 - INFO - __main__ - ['1']
06/22/2022 14:52:58 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:52:58 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:52:58 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:53:01 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:53:03 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:53:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:53:03 - INFO - __main__ - Starting training!
06/22/2022 14:53:09 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 14:54:39 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_42_0.3_8_predictions.txt
06/22/2022 14:54:39 - INFO - __main__ - Classification-F1 on test data: 0.2502
06/22/2022 14:54:39 - INFO - __main__ - prefix=paws_16_42, lr=0.3, bsz=8, dev_performance=0.5933528836754642, test_performance=0.25018152440169544
06/22/2022 14:54:39 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.2, bsz=8 ...
06/22/2022 14:54:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:54:40 - INFO - __main__ - Printing 3 examples
06/22/2022 14:54:40 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/22/2022 14:54:40 - INFO - __main__ - ['1']
06/22/2022 14:54:40 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/22/2022 14:54:40 - INFO - __main__ - ['1']
06/22/2022 14:54:40 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/22/2022 14:54:40 - INFO - __main__ - ['1']
06/22/2022 14:54:40 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:54:40 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:54:40 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:54:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:54:40 - INFO - __main__ - Printing 3 examples
06/22/2022 14:54:40 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/22/2022 14:54:40 - INFO - __main__ - ['1']
06/22/2022 14:54:40 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/22/2022 14:54:40 - INFO - __main__ - ['1']
06/22/2022 14:54:40 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/22/2022 14:54:40 - INFO - __main__ - ['1']
06/22/2022 14:54:40 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:54:40 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:54:40 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:54:45 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:54:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:54:45 - INFO - __main__ - Starting training!
06/22/2022 14:54:47 - INFO - __main__ - Step 10 Global step 10 Train loss 4.49 on epoch=4
06/22/2022 14:54:48 - INFO - __main__ - Step 20 Global step 20 Train loss 3.71 on epoch=9
06/22/2022 14:54:49 - INFO - __main__ - Step 30 Global step 30 Train loss 2.83 on epoch=14
06/22/2022 14:54:50 - INFO - __main__ - Step 40 Global step 40 Train loss 2.26 on epoch=19
06/22/2022 14:54:52 - INFO - __main__ - Step 50 Global step 50 Train loss 1.89 on epoch=24
06/22/2022 14:54:52 - INFO - __main__ - Global step 50 Train loss 3.04 Classification-F1 0.21276595744680848 on epoch=24
06/22/2022 14:54:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21276595744680848 on epoch=24, global_step=50
06/22/2022 14:54:53 - INFO - __main__ - Step 60 Global step 60 Train loss 1.36 on epoch=29
06/22/2022 14:54:55 - INFO - __main__ - Step 70 Global step 70 Train loss 1.02 on epoch=34
06/22/2022 14:54:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=39
06/22/2022 14:54:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=44
06/22/2022 14:54:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.67 on epoch=49
06/22/2022 14:54:58 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.5933528836754642 on epoch=49
06/22/2022 14:54:58 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.5933528836754642 on epoch=49, global_step=100
06/22/2022 14:55:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.63 on epoch=54
06/22/2022 14:55:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.66 on epoch=59
06/22/2022 14:55:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=64
06/22/2022 14:55:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=69
06/22/2022 14:55:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=74
06/22/2022 14:55:05 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.5151515151515151 on epoch=74
06/22/2022 14:55:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=79
06/22/2022 14:55:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=84
06/22/2022 14:55:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=89
06/22/2022 14:55:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=94
06/22/2022 14:55:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=99
06/22/2022 14:55:11 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.3816425120772947 on epoch=99
06/22/2022 14:55:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=104
06/22/2022 14:55:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=109
06/22/2022 14:55:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=114
06/22/2022 14:55:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=119
06/22/2022 14:55:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=124
06/22/2022 14:55:18 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 14:55:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=129
06/22/2022 14:55:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=134
06/22/2022 14:55:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
06/22/2022 14:55:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=144
06/22/2022 14:55:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=149
06/22/2022 14:55:24 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=149
06/22/2022 14:55:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=154
06/22/2022 14:55:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=159
06/22/2022 14:55:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=164
06/22/2022 14:55:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=169
06/22/2022 14:55:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=174
06/22/2022 14:55:31 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=174
06/22/2022 14:55:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=179
06/22/2022 14:55:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=184
06/22/2022 14:55:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=189
06/22/2022 14:55:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=194
06/22/2022 14:55:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.32 on epoch=199
06/22/2022 14:55:37 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=199
06/22/2022 14:55:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=204
06/22/2022 14:55:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=209
06/22/2022 14:55:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=214
06/22/2022 14:55:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=219
06/22/2022 14:55:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=224
06/22/2022 14:55:44 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=224
06/22/2022 14:55:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=229
06/22/2022 14:55:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
06/22/2022 14:55:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=239
06/22/2022 14:55:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=244
06/22/2022 14:55:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=249
06/22/2022 14:55:50 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.3992490613266583 on epoch=249
06/22/2022 14:55:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=254
06/22/2022 14:55:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=259
06/22/2022 14:55:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=264
06/22/2022 14:55:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
06/22/2022 14:55:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=274
06/22/2022 14:55:56 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=274
06/22/2022 14:55:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=279
06/22/2022 14:55:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=284
06/22/2022 14:56:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=289
06/22/2022 14:56:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=294
06/22/2022 14:56:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=299
06/22/2022 14:56:03 - INFO - __main__ - Global step 600 Train loss 0.30 Classification-F1 0.3816425120772947 on epoch=299
06/22/2022 14:56:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=304
06/22/2022 14:56:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=309
06/22/2022 14:56:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=314
06/22/2022 14:56:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=319
06/22/2022 14:56:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
06/22/2022 14:56:09 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.4385964912280702 on epoch=324
06/22/2022 14:56:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=329
06/22/2022 14:56:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=334
06/22/2022 14:56:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=339
06/22/2022 14:56:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=344
06/22/2022 14:56:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
06/22/2022 14:56:16 - INFO - __main__ - Global step 700 Train loss 0.24 Classification-F1 0.6000000000000001 on epoch=349
06/22/2022 14:56:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6000000000000001 on epoch=349, global_step=700
06/22/2022 14:56:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=354
06/22/2022 14:56:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=359
06/22/2022 14:56:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
06/22/2022 14:56:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=369
06/22/2022 14:56:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=374
06/22/2022 14:56:22 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.5076923076923077 on epoch=374
06/22/2022 14:56:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
06/22/2022 14:56:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.18 on epoch=384
06/22/2022 14:56:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
06/22/2022 14:56:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=394
06/22/2022 14:56:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=399
06/22/2022 14:56:29 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.5151515151515151 on epoch=399
06/22/2022 14:56:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=404
06/22/2022 14:56:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=409
06/22/2022 14:56:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=414
06/22/2022 14:56:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=419
06/22/2022 14:56:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=424
06/22/2022 14:56:35 - INFO - __main__ - Global step 850 Train loss 0.14 Classification-F1 0.5465587044534412 on epoch=424
06/22/2022 14:56:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=429
06/22/2022 14:56:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=434
06/22/2022 14:56:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=439
06/22/2022 14:56:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=444
06/22/2022 14:56:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=449
06/22/2022 14:56:41 - INFO - __main__ - Global step 900 Train loss 0.14 Classification-F1 0.4920634920634921 on epoch=449
06/22/2022 14:56:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=454
06/22/2022 14:56:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=459
06/22/2022 14:56:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=464
06/22/2022 14:56:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
06/22/2022 14:56:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=474
06/22/2022 14:56:48 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.5195195195195195 on epoch=474
06/22/2022 14:56:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=479
06/22/2022 14:56:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=484
06/22/2022 14:56:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=489
06/22/2022 14:56:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=494
06/22/2022 14:56:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=499
06/22/2022 14:56:54 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.5607843137254902 on epoch=499
06/22/2022 14:56:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
06/22/2022 14:56:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
06/22/2022 14:56:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
06/22/2022 14:56:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
06/22/2022 14:57:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=524
06/22/2022 14:57:01 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.5333333333333333 on epoch=524
06/22/2022 14:57:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
06/22/2022 14:57:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
06/22/2022 14:57:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=539
06/22/2022 14:57:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
06/22/2022 14:57:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
06/22/2022 14:57:07 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.5195195195195195 on epoch=549
06/22/2022 14:57:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/22/2022 14:57:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
06/22/2022 14:57:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
06/22/2022 14:57:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
06/22/2022 14:57:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
06/22/2022 14:57:14 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.5076923076923077 on epoch=574
06/22/2022 14:57:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/22/2022 14:57:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/22/2022 14:57:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
06/22/2022 14:57:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
06/22/2022 14:57:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
06/22/2022 14:57:20 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.5465587044534412 on epoch=599
06/22/2022 14:57:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 14:57:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/22/2022 14:57:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/22/2022 14:57:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=619
06/22/2022 14:57:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/22/2022 14:57:26 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.5333333333333333 on epoch=624
06/22/2022 14:57:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/22/2022 14:57:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
06/22/2022 14:57:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
06/22/2022 14:57:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
06/22/2022 14:57:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/22/2022 14:57:33 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.5333333333333333 on epoch=649
06/22/2022 14:57:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
06/22/2022 14:57:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/22/2022 14:57:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/22/2022 14:57:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 14:57:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
06/22/2022 14:57:39 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.5076923076923077 on epoch=674
06/22/2022 14:57:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/22/2022 14:57:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/22/2022 14:57:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/22/2022 14:57:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
06/22/2022 14:57:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/22/2022 14:57:46 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=699
06/22/2022 14:57:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
06/22/2022 14:57:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/22/2022 14:57:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
06/22/2022 14:57:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
06/22/2022 14:57:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
06/22/2022 14:57:52 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.5555555555555556 on epoch=724
06/22/2022 14:57:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/22/2022 14:57:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
06/22/2022 14:57:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/22/2022 14:57:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 14:57:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/22/2022 14:57:59 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=749
06/22/2022 14:58:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 14:58:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/22/2022 14:58:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
06/22/2022 14:58:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/22/2022 14:58:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/22/2022 14:58:05 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=774
06/22/2022 14:58:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/22/2022 14:58:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
06/22/2022 14:58:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 14:58:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/22/2022 14:58:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 14:58:12 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=799
06/22/2022 14:58:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 14:58:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/22/2022 14:58:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 14:58:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/22/2022 14:58:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 14:58:18 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=824
06/22/2022 14:58:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/22/2022 14:58:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 14:58:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 14:58:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=844
06/22/2022 14:58:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/22/2022 14:58:24 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=849
06/22/2022 14:58:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 14:58:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/22/2022 14:58:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/22/2022 14:58:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 14:58:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 14:58:31 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=874
06/22/2022 14:58:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
06/22/2022 14:58:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/22/2022 14:58:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 14:58:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 14:58:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 14:58:37 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=899
06/22/2022 14:58:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 14:58:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 14:58:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/22/2022 14:58:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/22/2022 14:58:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 14:58:44 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=924
06/22/2022 14:58:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 14:58:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 14:58:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 14:58:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/22/2022 14:58:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 14:58:50 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=949
06/22/2022 14:58:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 14:58:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 14:58:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 14:58:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/22/2022 14:58:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 14:58:57 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=974
06/22/2022 14:58:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 14:58:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/22/2022 14:59:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 14:59:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/22/2022 14:59:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 14:59:03 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=999
06/22/2022 14:59:03 - INFO - __main__ - save last model!
06/22/2022 14:59:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 14:59:03 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 14:59:03 - INFO - __main__ - Printing 3 examples
06/22/2022 14:59:03 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 14:59:03 - INFO - __main__ - ['0']
06/22/2022 14:59:03 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 14:59:03 - INFO - __main__ - ['1']
06/22/2022 14:59:03 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 14:59:03 - INFO - __main__ - ['1']
06/22/2022 14:59:03 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:59:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:59:04 - INFO - __main__ - Printing 3 examples
06/22/2022 14:59:04 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/22/2022 14:59:04 - INFO - __main__ - ['0']
06/22/2022 14:59:04 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/22/2022 14:59:04 - INFO - __main__ - ['0']
06/22/2022 14:59:04 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/22/2022 14:59:04 - INFO - __main__ - ['0']
06/22/2022 14:59:04 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:59:04 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:59:04 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 14:59:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 14:59:04 - INFO - __main__ - Printing 3 examples
06/22/2022 14:59:04 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/22/2022 14:59:04 - INFO - __main__ - ['0']
06/22/2022 14:59:04 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/22/2022 14:59:04 - INFO - __main__ - ['0']
06/22/2022 14:59:04 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/22/2022 14:59:04 - INFO - __main__ - ['0']
06/22/2022 14:59:04 - INFO - __main__ - Tokenizing Input ...
06/22/2022 14:59:04 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:59:04 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 14:59:07 - INFO - __main__ - Tokenizing Output ...
06/22/2022 14:59:09 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 14:59:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 14:59:09 - INFO - __main__ - Starting training!
06/22/2022 14:59:15 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 15:00:43 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_42_0.2_8_predictions.txt
06/22/2022 15:00:43 - INFO - __main__ - Classification-F1 on test data: 0.3155
06/22/2022 15:00:44 - INFO - __main__ - prefix=paws_16_42, lr=0.2, bsz=8, dev_performance=0.6000000000000001, test_performance=0.3155461803450166
06/22/2022 15:00:44 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.5, bsz=8 ...
06/22/2022 15:00:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:00:44 - INFO - __main__ - Printing 3 examples
06/22/2022 15:00:44 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/22/2022 15:00:44 - INFO - __main__ - ['0']
06/22/2022 15:00:44 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/22/2022 15:00:45 - INFO - __main__ - ['0']
06/22/2022 15:00:45 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/22/2022 15:00:45 - INFO - __main__ - ['0']
06/22/2022 15:00:45 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:00:45 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:00:45 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 15:00:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:00:45 - INFO - __main__ - Printing 3 examples
06/22/2022 15:00:45 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/22/2022 15:00:45 - INFO - __main__ - ['0']
06/22/2022 15:00:45 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/22/2022 15:00:45 - INFO - __main__ - ['0']
06/22/2022 15:00:45 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/22/2022 15:00:45 - INFO - __main__ - ['0']
06/22/2022 15:00:45 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:00:45 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:00:45 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 15:00:50 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 15:00:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 15:00:50 - INFO - __main__ - Starting training!
06/22/2022 15:00:52 - INFO - __main__ - Step 10 Global step 10 Train loss 3.69 on epoch=4
06/22/2022 15:00:53 - INFO - __main__ - Step 20 Global step 20 Train loss 2.26 on epoch=9
06/22/2022 15:00:54 - INFO - __main__ - Step 30 Global step 30 Train loss 1.15 on epoch=14
06/22/2022 15:00:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.68 on epoch=19
06/22/2022 15:00:56 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=24
06/22/2022 15:00:57 - INFO - __main__ - Global step 50 Train loss 1.67 Classification-F1 0.3992490613266583 on epoch=24
06/22/2022 15:00:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3992490613266583 on epoch=24, global_step=50
06/22/2022 15:00:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=29
06/22/2022 15:00:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=34
06/22/2022 15:01:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=39
06/22/2022 15:01:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=44
06/22/2022 15:01:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
06/22/2022 15:01:03 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.4385964912280702 on epoch=49
06/22/2022 15:01:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4385964912280702 on epoch=49, global_step=100
06/22/2022 15:01:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
06/22/2022 15:01:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
06/22/2022 15:01:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=64
06/22/2022 15:01:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=69
06/22/2022 15:01:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=74
06/22/2022 15:01:10 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=74
06/22/2022 15:01:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=79
06/22/2022 15:01:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=84
06/22/2022 15:01:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
06/22/2022 15:01:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.34 on epoch=94
06/22/2022 15:01:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.31 on epoch=99
06/22/2022 15:01:16 - INFO - __main__ - Global step 200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 15:01:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
06/22/2022 15:01:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=109
06/22/2022 15:01:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
06/22/2022 15:01:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=119
06/22/2022 15:01:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.32 on epoch=124
06/22/2022 15:01:23 - INFO - __main__ - Global step 250 Train loss 0.36 Classification-F1 0.39756367663344405 on epoch=124
06/22/2022 15:01:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
06/22/2022 15:01:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.32 on epoch=134
06/22/2022 15:01:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
06/22/2022 15:01:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
06/22/2022 15:01:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
06/22/2022 15:01:29 - INFO - __main__ - Global step 300 Train loss 0.30 Classification-F1 0.43529411764705883 on epoch=149
06/22/2022 15:01:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
06/22/2022 15:01:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
06/22/2022 15:01:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
06/22/2022 15:01:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
06/22/2022 15:01:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=174
06/22/2022 15:01:36 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.39139139139139134 on epoch=174
06/22/2022 15:01:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
06/22/2022 15:01:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=184
06/22/2022 15:01:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=189
06/22/2022 15:01:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=194
06/22/2022 15:01:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
06/22/2022 15:01:43 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.4285714285714286 on epoch=199
06/22/2022 15:01:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
06/22/2022 15:01:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=209
06/22/2022 15:01:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
06/22/2022 15:01:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.17 on epoch=219
06/22/2022 15:01:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
06/22/2022 15:01:49 - INFO - __main__ - Global step 450 Train loss 0.13 Classification-F1 0.464039408866995 on epoch=224
06/22/2022 15:01:49 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.464039408866995 on epoch=224, global_step=450
06/22/2022 15:01:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=229
06/22/2022 15:01:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
06/22/2022 15:01:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
06/22/2022 15:01:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
06/22/2022 15:01:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
06/22/2022 15:01:56 - INFO - __main__ - Global step 500 Train loss 0.10 Classification-F1 0.4420512820512821 on epoch=249
06/22/2022 15:01:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
06/22/2022 15:01:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
06/22/2022 15:01:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
06/22/2022 15:02:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
06/22/2022 15:02:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
06/22/2022 15:02:02 - INFO - __main__ - Global step 550 Train loss 0.06 Classification-F1 0.4285714285714286 on epoch=274
06/22/2022 15:02:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
06/22/2022 15:02:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
06/22/2022 15:02:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
06/22/2022 15:02:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
06/22/2022 15:02:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
06/22/2022 15:02:08 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.39139139139139134 on epoch=299
06/22/2022 15:02:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
06/22/2022 15:02:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
06/22/2022 15:02:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/22/2022 15:02:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
06/22/2022 15:02:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/22/2022 15:02:15 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.4285714285714286 on epoch=324
06/22/2022 15:02:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
06/22/2022 15:02:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
06/22/2022 15:02:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
06/22/2022 15:02:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
06/22/2022 15:02:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
06/22/2022 15:02:21 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.4285714285714286 on epoch=349
06/22/2022 15:02:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
06/22/2022 15:02:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
06/22/2022 15:02:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/22/2022 15:02:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/22/2022 15:02:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
06/22/2022 15:02:28 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.4285714285714286 on epoch=374
06/22/2022 15:02:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
06/22/2022 15:02:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/22/2022 15:02:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
06/22/2022 15:02:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/22/2022 15:02:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/22/2022 15:02:34 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=399
06/22/2022 15:02:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
06/22/2022 15:02:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/22/2022 15:02:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
06/22/2022 15:02:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
06/22/2022 15:02:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/22/2022 15:02:41 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=424
06/22/2022 15:02:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/22/2022 15:02:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/22/2022 15:02:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/22/2022 15:02:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
06/22/2022 15:02:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
06/22/2022 15:02:47 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=449
06/22/2022 15:02:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/22/2022 15:02:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/22/2022 15:02:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
06/22/2022 15:02:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/22/2022 15:02:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/22/2022 15:02:54 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=474
06/22/2022 15:02:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/22/2022 15:02:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/22/2022 15:02:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/22/2022 15:02:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/22/2022 15:03:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 15:03:00 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=499
06/22/2022 15:03:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
06/22/2022 15:03:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
06/22/2022 15:03:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/22/2022 15:03:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/22/2022 15:03:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/22/2022 15:03:07 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.4285714285714286 on epoch=524
06/22/2022 15:03:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/22/2022 15:03:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/22/2022 15:03:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/22/2022 15:03:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/22/2022 15:03:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 15:03:13 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=549
06/22/2022 15:03:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/22/2022 15:03:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 15:03:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 15:03:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 15:03:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/22/2022 15:03:20 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=574
06/22/2022 15:03:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
06/22/2022 15:03:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 15:03:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/22/2022 15:03:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 15:03:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 15:03:26 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=599
06/22/2022 15:03:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
06/22/2022 15:03:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 15:03:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 15:03:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 15:03:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 15:03:33 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=624
06/22/2022 15:03:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 15:03:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/22/2022 15:03:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/22/2022 15:03:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 15:03:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 15:03:39 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=649
06/22/2022 15:03:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/22/2022 15:03:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/22/2022 15:03:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 15:03:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/22/2022 15:03:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 15:03:45 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=674
06/22/2022 15:03:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/22/2022 15:03:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 15:03:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/22/2022 15:03:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 15:03:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 15:03:52 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=699
06/22/2022 15:03:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 15:03:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/22/2022 15:03:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/22/2022 15:03:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/22/2022 15:03:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
06/22/2022 15:03:58 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=724
06/22/2022 15:03:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/22/2022 15:04:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/22/2022 15:04:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 15:04:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 15:04:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 15:04:05 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=749
06/22/2022 15:04:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 15:04:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/22/2022 15:04:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 15:04:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 15:04:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 15:04:11 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=774
06/22/2022 15:04:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 15:04:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 15:04:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
06/22/2022 15:04:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 15:04:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 15:04:18 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=799
06/22/2022 15:04:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 15:04:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/22/2022 15:04:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 15:04:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 15:04:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 15:04:24 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=824
06/22/2022 15:04:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 15:04:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 15:04:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 15:04:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 15:04:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/22/2022 15:04:31 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=849
06/22/2022 15:04:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 15:04:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 15:04:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/22/2022 15:04:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 15:04:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 15:04:37 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=874
06/22/2022 15:04:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/22/2022 15:04:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 15:04:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 15:04:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 15:04:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 15:04:43 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=899
06/22/2022 15:04:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=904
06/22/2022 15:04:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 15:04:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 15:04:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 15:04:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 15:04:50 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=924
06/22/2022 15:04:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 15:04:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 15:04:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 15:04:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 15:04:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 15:04:56 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=949
06/22/2022 15:04:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 15:04:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 15:05:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 15:05:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 15:05:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/22/2022 15:05:03 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=974
06/22/2022 15:05:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 15:05:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 15:05:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 15:05:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 15:05:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 15:05:09 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=999
06/22/2022 15:05:09 - INFO - __main__ - save last model!
06/22/2022 15:05:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 15:05:09 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 15:05:09 - INFO - __main__ - Printing 3 examples
06/22/2022 15:05:09 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 15:05:09 - INFO - __main__ - ['0']
06/22/2022 15:05:09 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 15:05:09 - INFO - __main__ - ['1']
06/22/2022 15:05:09 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 15:05:09 - INFO - __main__ - ['1']
06/22/2022 15:05:09 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:05:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:05:10 - INFO - __main__ - Printing 3 examples
06/22/2022 15:05:10 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/22/2022 15:05:10 - INFO - __main__ - ['0']
06/22/2022 15:05:10 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/22/2022 15:05:10 - INFO - __main__ - ['0']
06/22/2022 15:05:10 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/22/2022 15:05:10 - INFO - __main__ - ['0']
06/22/2022 15:05:10 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:05:10 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:05:10 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 15:05:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:05:10 - INFO - __main__ - Printing 3 examples
06/22/2022 15:05:10 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/22/2022 15:05:10 - INFO - __main__ - ['0']
06/22/2022 15:05:10 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/22/2022 15:05:10 - INFO - __main__ - ['0']
06/22/2022 15:05:10 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/22/2022 15:05:10 - INFO - __main__ - ['0']
06/22/2022 15:05:10 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:05:10 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:05:10 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 15:05:13 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:05:16 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 15:05:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 15:05:16 - INFO - __main__ - Starting training!
06/22/2022 15:05:21 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 15:06:51 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_87_0.5_8_predictions.txt
06/22/2022 15:06:51 - INFO - __main__ - Classification-F1 on test data: 0.5042
06/22/2022 15:06:51 - INFO - __main__ - prefix=paws_16_87, lr=0.5, bsz=8, dev_performance=0.464039408866995, test_performance=0.50423453827715
06/22/2022 15:06:51 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.4, bsz=8 ...
06/22/2022 15:06:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:06:52 - INFO - __main__ - Printing 3 examples
06/22/2022 15:06:52 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/22/2022 15:06:52 - INFO - __main__ - ['0']
06/22/2022 15:06:52 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/22/2022 15:06:52 - INFO - __main__ - ['0']
06/22/2022 15:06:52 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/22/2022 15:06:52 - INFO - __main__ - ['0']
06/22/2022 15:06:52 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:06:52 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:06:52 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 15:06:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:06:52 - INFO - __main__ - Printing 3 examples
06/22/2022 15:06:52 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/22/2022 15:06:52 - INFO - __main__ - ['0']
06/22/2022 15:06:52 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/22/2022 15:06:52 - INFO - __main__ - ['0']
06/22/2022 15:06:52 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/22/2022 15:06:52 - INFO - __main__ - ['0']
06/22/2022 15:06:52 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:06:52 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:06:52 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 15:06:57 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 15:06:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 15:06:58 - INFO - __main__ - Starting training!
06/22/2022 15:06:59 - INFO - __main__ - Step 10 Global step 10 Train loss 3.91 on epoch=4
06/22/2022 15:07:00 - INFO - __main__ - Step 20 Global step 20 Train loss 2.41 on epoch=9
06/22/2022 15:07:02 - INFO - __main__ - Step 30 Global step 30 Train loss 1.51 on epoch=14
06/22/2022 15:07:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.88 on epoch=19
06/22/2022 15:07:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.69 on epoch=24
06/22/2022 15:07:04 - INFO - __main__ - Global step 50 Train loss 1.88 Classification-F1 0.4589371980676329 on epoch=24
06/22/2022 15:07:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4589371980676329 on epoch=24, global_step=50
06/22/2022 15:07:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=29
06/22/2022 15:07:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=34
06/22/2022 15:07:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=39
06/22/2022 15:07:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
06/22/2022 15:07:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
06/22/2022 15:07:11 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=49
06/22/2022 15:07:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=54
06/22/2022 15:07:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=59
06/22/2022 15:07:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
06/22/2022 15:07:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=69
06/22/2022 15:07:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/22/2022 15:07:17 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=74
06/22/2022 15:07:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=79
06/22/2022 15:07:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=84
06/22/2022 15:07:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=89
06/22/2022 15:07:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=94
06/22/2022 15:07:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=99
06/22/2022 15:07:24 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3992490613266583 on epoch=99
06/22/2022 15:07:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=104
06/22/2022 15:07:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
06/22/2022 15:07:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=114
06/22/2022 15:07:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.33 on epoch=119
06/22/2022 15:07:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=124
06/22/2022 15:07:30 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 15:07:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
06/22/2022 15:07:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
06/22/2022 15:07:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
06/22/2022 15:07:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
06/22/2022 15:07:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=149
06/22/2022 15:07:37 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.36374269005847953 on epoch=149
06/22/2022 15:07:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=154
06/22/2022 15:07:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=159
06/22/2022 15:07:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=164
06/22/2022 15:07:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=169
06/22/2022 15:07:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=174
06/22/2022 15:07:43 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.37254901960784315 on epoch=174
06/22/2022 15:07:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
06/22/2022 15:07:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=184
06/22/2022 15:07:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=189
06/22/2022 15:07:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=194
06/22/2022 15:07:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=199
06/22/2022 15:07:50 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.3454545454545454 on epoch=199
06/22/2022 15:07:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=204
06/22/2022 15:07:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=209
06/22/2022 15:07:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
06/22/2022 15:07:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
06/22/2022 15:07:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
06/22/2022 15:07:56 - INFO - __main__ - Global step 450 Train loss 0.26 Classification-F1 0.30980392156862746 on epoch=224
06/22/2022 15:07:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
06/22/2022 15:07:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=234
06/22/2022 15:08:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
06/22/2022 15:08:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
06/22/2022 15:08:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
06/22/2022 15:08:02 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3764102564102564 on epoch=249
06/22/2022 15:08:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
06/22/2022 15:08:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=259
06/22/2022 15:08:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=264
06/22/2022 15:08:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=269
06/22/2022 15:08:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.15 on epoch=274
06/22/2022 15:08:09 - INFO - __main__ - Global step 550 Train loss 0.19 Classification-F1 0.3522267206477733 on epoch=274
06/22/2022 15:08:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.18 on epoch=279
06/22/2022 15:08:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=284
06/22/2022 15:08:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
06/22/2022 15:08:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=294
06/22/2022 15:08:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=299
06/22/2022 15:08:15 - INFO - __main__ - Global step 600 Train loss 0.14 Classification-F1 0.3764102564102564 on epoch=299
06/22/2022 15:08:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=304
06/22/2022 15:08:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=309
06/22/2022 15:08:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=314
06/22/2022 15:08:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=319
06/22/2022 15:08:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=324
06/22/2022 15:08:22 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.4554554554554554 on epoch=324
06/22/2022 15:08:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
06/22/2022 15:08:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
06/22/2022 15:08:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
06/22/2022 15:08:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
06/22/2022 15:08:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=349
06/22/2022 15:08:28 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.4920634920634921 on epoch=349
06/22/2022 15:08:28 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.4920634920634921 on epoch=349, global_step=700
06/22/2022 15:08:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
06/22/2022 15:08:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
06/22/2022 15:08:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
06/22/2022 15:08:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
06/22/2022 15:08:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
06/22/2022 15:08:35 - INFO - __main__ - Global step 750 Train loss 0.04 Classification-F1 0.4920634920634921 on epoch=374
06/22/2022 15:08:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/22/2022 15:08:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
06/22/2022 15:08:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
06/22/2022 15:08:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
06/22/2022 15:08:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/22/2022 15:08:41 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.4554554554554554 on epoch=399
06/22/2022 15:08:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/22/2022 15:08:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/22/2022 15:08:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/22/2022 15:08:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/22/2022 15:08:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
06/22/2022 15:08:48 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=424
06/22/2022 15:08:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
06/22/2022 15:08:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/22/2022 15:08:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/22/2022 15:08:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/22/2022 15:08:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/22/2022 15:08:54 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=449
06/22/2022 15:08:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/22/2022 15:08:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
06/22/2022 15:08:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/22/2022 15:08:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/22/2022 15:09:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/22/2022 15:09:01 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=474
06/22/2022 15:09:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/22/2022 15:09:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/22/2022 15:09:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/22/2022 15:09:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/22/2022 15:09:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/22/2022 15:09:07 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=499
06/22/2022 15:09:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/22/2022 15:09:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/22/2022 15:09:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/22/2022 15:09:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/22/2022 15:09:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
06/22/2022 15:09:14 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=524
06/22/2022 15:09:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/22/2022 15:09:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/22/2022 15:09:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/22/2022 15:09:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/22/2022 15:09:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/22/2022 15:09:20 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=549
06/22/2022 15:09:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5270935960591133 on epoch=549, global_step=1100
06/22/2022 15:09:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/22/2022 15:09:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/22/2022 15:09:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/22/2022 15:09:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/22/2022 15:09:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/22/2022 15:09:27 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=574
06/22/2022 15:09:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
06/22/2022 15:09:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/22/2022 15:09:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/22/2022 15:09:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/22/2022 15:09:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/22/2022 15:09:33 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=599
06/22/2022 15:09:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/22/2022 15:09:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/22/2022 15:09:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/22/2022 15:09:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/22/2022 15:09:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/22/2022 15:09:40 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=624
06/22/2022 15:09:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/22/2022 15:09:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/22/2022 15:09:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/22/2022 15:09:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/22/2022 15:09:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/22/2022 15:09:46 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=649
06/22/2022 15:09:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/22/2022 15:09:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/22/2022 15:09:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/22/2022 15:09:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/22/2022 15:09:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/22/2022 15:09:53 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=674
06/22/2022 15:09:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 15:09:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/22/2022 15:09:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/22/2022 15:09:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/22/2022 15:09:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/22/2022 15:09:59 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=699
06/22/2022 15:10:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/22/2022 15:10:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/22/2022 15:10:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/22/2022 15:10:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/22/2022 15:10:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/22/2022 15:10:06 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=724
06/22/2022 15:10:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/22/2022 15:10:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
06/22/2022 15:10:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/22/2022 15:10:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/22/2022 15:10:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/22/2022 15:10:12 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=749
06/22/2022 15:10:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/22/2022 15:10:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/22/2022 15:10:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/22/2022 15:10:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/22/2022 15:10:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/22/2022 15:10:19 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=774
06/22/2022 15:10:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/22/2022 15:10:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/22/2022 15:10:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 15:10:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/22/2022 15:10:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/22/2022 15:10:25 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=799
06/22/2022 15:10:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/22/2022 15:10:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/22/2022 15:10:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/22/2022 15:10:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/22/2022 15:10:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/22/2022 15:10:32 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=824
06/22/2022 15:10:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 15:10:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/22/2022 15:10:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/22/2022 15:10:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/22/2022 15:10:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 15:10:38 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=849
06/22/2022 15:10:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/22/2022 15:10:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/22/2022 15:10:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/22/2022 15:10:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/22/2022 15:10:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 15:10:45 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=874
06/22/2022 15:10:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 15:10:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/22/2022 15:10:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 15:10:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 15:10:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/22/2022 15:10:51 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=899
06/22/2022 15:10:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/22/2022 15:10:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/22/2022 15:10:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/22/2022 15:10:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/22/2022 15:10:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 15:10:58 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=924
06/22/2022 15:10:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/22/2022 15:11:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 15:11:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/22/2022 15:11:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/22/2022 15:11:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/22/2022 15:11:04 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=949
06/22/2022 15:11:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/22/2022 15:11:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 15:11:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 15:11:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 15:11:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 15:11:11 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.2540322580645162 on epoch=974
06/22/2022 15:11:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 15:11:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 15:11:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 15:11:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/22/2022 15:11:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 15:11:17 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.22844827586206895 on epoch=999
06/22/2022 15:11:17 - INFO - __main__ - save last model!
06/22/2022 15:11:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 15:11:17 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 15:11:17 - INFO - __main__ - Printing 3 examples
06/22/2022 15:11:17 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 15:11:17 - INFO - __main__ - ['0']
06/22/2022 15:11:17 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 15:11:17 - INFO - __main__ - ['1']
06/22/2022 15:11:17 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 15:11:17 - INFO - __main__ - ['1']
06/22/2022 15:11:17 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:11:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:11:18 - INFO - __main__ - Printing 3 examples
06/22/2022 15:11:18 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/22/2022 15:11:18 - INFO - __main__ - ['0']
06/22/2022 15:11:18 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/22/2022 15:11:18 - INFO - __main__ - ['0']
06/22/2022 15:11:18 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/22/2022 15:11:18 - INFO - __main__ - ['0']
06/22/2022 15:11:18 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:11:18 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:11:18 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 15:11:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:11:18 - INFO - __main__ - Printing 3 examples
06/22/2022 15:11:18 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/22/2022 15:11:18 - INFO - __main__ - ['0']
06/22/2022 15:11:18 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/22/2022 15:11:18 - INFO - __main__ - ['0']
06/22/2022 15:11:18 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/22/2022 15:11:18 - INFO - __main__ - ['0']
06/22/2022 15:11:18 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:11:18 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:11:18 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 15:11:22 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:11:24 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 15:11:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 15:11:24 - INFO - __main__ - Starting training!
06/22/2022 15:11:29 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 15:13:02 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_87_0.4_8_predictions.txt
06/22/2022 15:13:02 - INFO - __main__ - Classification-F1 on test data: 0.1203
06/22/2022 15:13:02 - INFO - __main__ - prefix=paws_16_87, lr=0.4, bsz=8, dev_performance=0.5270935960591133, test_performance=0.12033816415693749
06/22/2022 15:13:02 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.3, bsz=8 ...
06/22/2022 15:13:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:13:04 - INFO - __main__ - Printing 3 examples
06/22/2022 15:13:04 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/22/2022 15:13:04 - INFO - __main__ - ['0']
06/22/2022 15:13:04 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/22/2022 15:13:04 - INFO - __main__ - ['0']
06/22/2022 15:13:04 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/22/2022 15:13:04 - INFO - __main__ - ['0']
06/22/2022 15:13:04 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:13:04 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:13:04 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 15:13:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:13:04 - INFO - __main__ - Printing 3 examples
06/22/2022 15:13:04 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/22/2022 15:13:04 - INFO - __main__ - ['0']
06/22/2022 15:13:04 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/22/2022 15:13:04 - INFO - __main__ - ['0']
06/22/2022 15:13:04 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/22/2022 15:13:04 - INFO - __main__ - ['0']
06/22/2022 15:13:04 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:13:04 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:13:04 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 15:13:09 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 15:13:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 15:13:09 - INFO - __main__ - Starting training!
06/22/2022 15:13:11 - INFO - __main__ - Step 10 Global step 10 Train loss 4.05 on epoch=4
06/22/2022 15:13:12 - INFO - __main__ - Step 20 Global step 20 Train loss 2.99 on epoch=9
06/22/2022 15:13:13 - INFO - __main__ - Step 30 Global step 30 Train loss 2.02 on epoch=14
06/22/2022 15:13:14 - INFO - __main__ - Step 40 Global step 40 Train loss 1.25 on epoch=19
06/22/2022 15:13:16 - INFO - __main__ - Step 50 Global step 50 Train loss 0.90 on epoch=24
06/22/2022 15:13:16 - INFO - __main__ - Global step 50 Train loss 2.24 Classification-F1 0.35819819819819826 on epoch=24
06/22/2022 15:13:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.35819819819819826 on epoch=24, global_step=50
06/22/2022 15:13:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.80 on epoch=29
06/22/2022 15:13:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.69 on epoch=34
06/22/2022 15:13:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=39
06/22/2022 15:13:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=44
06/22/2022 15:13:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
06/22/2022 15:13:22 - INFO - __main__ - Global step 100 Train loss 0.63 Classification-F1 0.4817813765182186 on epoch=49
06/22/2022 15:13:22 - INFO - __main__ - Saving model with best Classification-F1: 0.35819819819819826 -> 0.4817813765182186 on epoch=49, global_step=100
06/22/2022 15:13:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=54
06/22/2022 15:13:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=59
06/22/2022 15:13:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
06/22/2022 15:13:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
06/22/2022 15:13:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=74
06/22/2022 15:13:29 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.3552492046659597 on epoch=74
06/22/2022 15:13:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=79
06/22/2022 15:13:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=84
06/22/2022 15:13:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
06/22/2022 15:13:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=94
06/22/2022 15:13:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=99
06/22/2022 15:13:35 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.3992490613266583 on epoch=99
06/22/2022 15:13:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=104
06/22/2022 15:13:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=109
06/22/2022 15:13:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=114
06/22/2022 15:13:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=119
06/22/2022 15:13:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=124
06/22/2022 15:13:42 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=124
06/22/2022 15:13:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=129
06/22/2022 15:13:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=134
06/22/2022 15:13:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
06/22/2022 15:13:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=144
06/22/2022 15:13:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=149
06/22/2022 15:13:48 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.4231177094379639 on epoch=149
06/22/2022 15:13:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/22/2022 15:13:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=159
06/22/2022 15:13:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=164
06/22/2022 15:13:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=169
06/22/2022 15:13:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=174
06/22/2022 15:13:55 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.5835835835835835 on epoch=174
06/22/2022 15:13:55 - INFO - __main__ - Saving model with best Classification-F1: 0.4817813765182186 -> 0.5835835835835835 on epoch=174, global_step=350
06/22/2022 15:13:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=179
06/22/2022 15:13:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=184
06/22/2022 15:13:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=189
06/22/2022 15:13:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
06/22/2022 15:14:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=199
06/22/2022 15:14:01 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.40566959921798634 on epoch=199
06/22/2022 15:14:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=204
06/22/2022 15:14:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=209
06/22/2022 15:14:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.30 on epoch=214
06/22/2022 15:14:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=219
06/22/2022 15:14:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=224
06/22/2022 15:14:07 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.464039408866995 on epoch=224
06/22/2022 15:14:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.31 on epoch=229
06/22/2022 15:14:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
06/22/2022 15:14:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=239
06/22/2022 15:14:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=244
06/22/2022 15:14:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=249
06/22/2022 15:14:14 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.464039408866995 on epoch=249
06/22/2022 15:14:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=254
06/22/2022 15:14:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=259
06/22/2022 15:14:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
06/22/2022 15:14:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
06/22/2022 15:14:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
06/22/2022 15:14:20 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.4920634920634921 on epoch=274
06/22/2022 15:14:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=279
06/22/2022 15:14:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
06/22/2022 15:14:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
06/22/2022 15:14:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=294
06/22/2022 15:14:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
06/22/2022 15:14:27 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.39999999999999997 on epoch=299
06/22/2022 15:14:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=304
06/22/2022 15:14:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=309
06/22/2022 15:14:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=314
06/22/2022 15:14:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
06/22/2022 15:14:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=324
06/22/2022 15:14:33 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.5465587044534412 on epoch=324
06/22/2022 15:14:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=329
06/22/2022 15:14:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=334
06/22/2022 15:14:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
06/22/2022 15:14:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=344
06/22/2022 15:14:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=349
06/22/2022 15:14:40 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.40566959921798634 on epoch=349
06/22/2022 15:14:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=354
06/22/2022 15:14:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.15 on epoch=359
06/22/2022 15:14:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=364
06/22/2022 15:14:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=369
06/22/2022 15:14:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=374
06/22/2022 15:14:46 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.5076923076923077 on epoch=374
06/22/2022 15:14:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=379
06/22/2022 15:14:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=384
06/22/2022 15:14:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=389
06/22/2022 15:14:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=394
06/22/2022 15:14:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=399
06/22/2022 15:14:53 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.5465587044534412 on epoch=399
06/22/2022 15:14:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=404
06/22/2022 15:14:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=409
06/22/2022 15:14:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=414
06/22/2022 15:14:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=419
06/22/2022 15:14:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=424
06/22/2022 15:14:59 - INFO - __main__ - Global step 850 Train loss 0.13 Classification-F1 0.4909862142099682 on epoch=424
06/22/2022 15:15:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=429
06/22/2022 15:15:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=434
06/22/2022 15:15:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=439
06/22/2022 15:15:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=444
06/22/2022 15:15:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=449
06/22/2022 15:15:06 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.5733333333333335 on epoch=449
06/22/2022 15:15:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=454
06/22/2022 15:15:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=459
06/22/2022 15:15:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=464
06/22/2022 15:15:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
06/22/2022 15:15:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=474
06/22/2022 15:15:12 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.5733333333333335 on epoch=474
06/22/2022 15:15:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=479
06/22/2022 15:15:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
06/22/2022 15:15:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=489
06/22/2022 15:15:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=494
06/22/2022 15:15:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=499
06/22/2022 15:15:19 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.5465587044534412 on epoch=499
06/22/2022 15:15:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=504
06/22/2022 15:15:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
06/22/2022 15:15:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
06/22/2022 15:15:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
06/22/2022 15:15:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
06/22/2022 15:15:25 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.5465587044534412 on epoch=524
06/22/2022 15:15:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=529
06/22/2022 15:15:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=534
06/22/2022 15:15:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
06/22/2022 15:15:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=544
06/22/2022 15:15:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=549
06/22/2022 15:15:32 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.4909862142099682 on epoch=549
06/22/2022 15:15:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=554
06/22/2022 15:15:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
06/22/2022 15:15:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
06/22/2022 15:15:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
06/22/2022 15:15:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=574
06/22/2022 15:15:38 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.5465587044534412 on epoch=574
06/22/2022 15:15:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
06/22/2022 15:15:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
06/22/2022 15:15:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=589
06/22/2022 15:15:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=594
06/22/2022 15:15:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=599
06/22/2022 15:15:45 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.4920634920634921 on epoch=599
06/22/2022 15:15:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
06/22/2022 15:15:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
06/22/2022 15:15:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
06/22/2022 15:15:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/22/2022 15:15:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/22/2022 15:15:51 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.4920634920634921 on epoch=624
06/22/2022 15:15:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
06/22/2022 15:15:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
06/22/2022 15:15:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
06/22/2022 15:15:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
06/22/2022 15:15:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
06/22/2022 15:15:58 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.5195195195195195 on epoch=649
06/22/2022 15:15:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=654
06/22/2022 15:16:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
06/22/2022 15:16:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
06/22/2022 15:16:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
06/22/2022 15:16:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/22/2022 15:16:04 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.4920634920634921 on epoch=674
06/22/2022 15:16:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
06/22/2022 15:16:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
06/22/2022 15:16:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/22/2022 15:16:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
06/22/2022 15:16:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/22/2022 15:16:11 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.4980392156862745 on epoch=699
06/22/2022 15:16:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/22/2022 15:16:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
06/22/2022 15:16:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
06/22/2022 15:16:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
06/22/2022 15:16:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/22/2022 15:16:17 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=724
06/22/2022 15:16:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/22/2022 15:16:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/22/2022 15:16:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
06/22/2022 15:16:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
06/22/2022 15:16:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/22/2022 15:16:24 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=749
06/22/2022 15:16:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
06/22/2022 15:16:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
06/22/2022 15:16:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 15:16:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/22/2022 15:16:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/22/2022 15:16:30 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=774
06/22/2022 15:16:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/22/2022 15:16:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
06/22/2022 15:16:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=789
06/22/2022 15:16:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/22/2022 15:16:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=799
06/22/2022 15:16:37 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.4920634920634921 on epoch=799
06/22/2022 15:16:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=804
06/22/2022 15:16:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/22/2022 15:16:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 15:16:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/22/2022 15:16:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
06/22/2022 15:16:43 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.464039408866995 on epoch=824
06/22/2022 15:16:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
06/22/2022 15:16:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/22/2022 15:16:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/22/2022 15:16:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/22/2022 15:16:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 15:16:50 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=849
06/22/2022 15:16:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/22/2022 15:16:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=859
06/22/2022 15:16:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/22/2022 15:16:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/22/2022 15:16:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/22/2022 15:16:56 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.4420512820512821 on epoch=874
06/22/2022 15:16:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
06/22/2022 15:16:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
06/22/2022 15:17:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/22/2022 15:17:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/22/2022 15:17:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/22/2022 15:17:03 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.41700404858299595 on epoch=899
06/22/2022 15:17:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/22/2022 15:17:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/22/2022 15:17:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/22/2022 15:17:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/22/2022 15:17:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
06/22/2022 15:17:09 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=924
06/22/2022 15:17:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/22/2022 15:17:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 15:17:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/22/2022 15:17:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/22/2022 15:17:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=949
06/22/2022 15:17:16 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=949
06/22/2022 15:17:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/22/2022 15:17:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 15:17:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/22/2022 15:17:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 15:17:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/22/2022 15:17:22 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=974
06/22/2022 15:17:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 15:17:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 15:17:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/22/2022 15:17:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/22/2022 15:17:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 15:17:28 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=999
06/22/2022 15:17:28 - INFO - __main__ - save last model!
06/22/2022 15:17:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 15:17:28 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 15:17:28 - INFO - __main__ - Printing 3 examples
06/22/2022 15:17:28 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 15:17:28 - INFO - __main__ - ['0']
06/22/2022 15:17:28 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 15:17:28 - INFO - __main__ - ['1']
06/22/2022 15:17:28 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 15:17:28 - INFO - __main__ - ['1']
06/22/2022 15:17:29 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:17:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:17:29 - INFO - __main__ - Printing 3 examples
06/22/2022 15:17:29 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/22/2022 15:17:29 - INFO - __main__ - ['0']
06/22/2022 15:17:29 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/22/2022 15:17:29 - INFO - __main__ - ['0']
06/22/2022 15:17:29 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/22/2022 15:17:29 - INFO - __main__ - ['0']
06/22/2022 15:17:29 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:17:29 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:17:29 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 15:17:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:17:29 - INFO - __main__ - Printing 3 examples
06/22/2022 15:17:29 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/22/2022 15:17:29 - INFO - __main__ - ['0']
06/22/2022 15:17:29 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/22/2022 15:17:29 - INFO - __main__ - ['0']
06/22/2022 15:17:29 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/22/2022 15:17:29 - INFO - __main__ - ['0']
06/22/2022 15:17:29 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:17:29 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:17:29 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 15:17:33 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:17:34 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 15:17:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 15:17:35 - INFO - __main__ - Starting training!
06/22/2022 15:17:40 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 15:19:17 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_87_0.3_8_predictions.txt
06/22/2022 15:19:17 - INFO - __main__ - Classification-F1 on test data: 0.4932
06/22/2022 15:19:17 - INFO - __main__ - prefix=paws_16_87, lr=0.3, bsz=8, dev_performance=0.5835835835835835, test_performance=0.49317820853627264
06/22/2022 15:19:17 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.2, bsz=8 ...
06/22/2022 15:19:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:19:18 - INFO - __main__ - Printing 3 examples
06/22/2022 15:19:18 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/22/2022 15:19:18 - INFO - __main__ - ['0']
06/22/2022 15:19:18 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/22/2022 15:19:18 - INFO - __main__ - ['0']
06/22/2022 15:19:18 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/22/2022 15:19:18 - INFO - __main__ - ['0']
06/22/2022 15:19:18 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:19:18 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:19:18 - INFO - __main__ - Loaded 32 examples from train data
06/22/2022 15:19:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/22/2022 15:19:18 - INFO - __main__ - Printing 3 examples
06/22/2022 15:19:18 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/22/2022 15:19:18 - INFO - __main__ - ['0']
06/22/2022 15:19:18 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/22/2022 15:19:18 - INFO - __main__ - ['0']
06/22/2022 15:19:18 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/22/2022 15:19:18 - INFO - __main__ - ['0']
06/22/2022 15:19:18 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:19:18 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:19:18 - INFO - __main__ - Loaded 32 examples from dev data
06/22/2022 15:19:23 - INFO - __main__ - load prompt embedding from ckpt
06/22/2022 15:19:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/22/2022 15:19:23 - INFO - __main__ - Starting training!
06/22/2022 15:19:25 - INFO - __main__ - Step 10 Global step 10 Train loss 4.18 on epoch=4
06/22/2022 15:19:26 - INFO - __main__ - Step 20 Global step 20 Train loss 3.58 on epoch=9
06/22/2022 15:19:27 - INFO - __main__ - Step 30 Global step 30 Train loss 2.94 on epoch=14
06/22/2022 15:19:28 - INFO - __main__ - Step 40 Global step 40 Train loss 2.17 on epoch=19
06/22/2022 15:19:30 - INFO - __main__ - Step 50 Global step 50 Train loss 1.64 on epoch=24
06/22/2022 15:19:30 - INFO - __main__ - Global step 50 Train loss 2.90 Classification-F1 0.3333333333333333 on epoch=24
06/22/2022 15:19:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/22/2022 15:19:31 - INFO - __main__ - Step 60 Global step 60 Train loss 1.20 on epoch=29
06/22/2022 15:19:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=34
06/22/2022 15:19:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.80 on epoch=39
06/22/2022 15:19:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=44
06/22/2022 15:19:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=49
06/22/2022 15:19:37 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.4589371980676329 on epoch=49
06/22/2022 15:19:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=49, global_step=100
06/22/2022 15:19:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=54
06/22/2022 15:19:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.64 on epoch=59
06/22/2022 15:19:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=64
06/22/2022 15:19:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=69
06/22/2022 15:19:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=74
06/22/2022 15:19:43 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.4181818181818182 on epoch=74
06/22/2022 15:19:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=79
06/22/2022 15:19:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
06/22/2022 15:19:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=89
06/22/2022 15:19:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=94
06/22/2022 15:19:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=99
06/22/2022 15:19:49 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=99
06/22/2022 15:19:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=104
06/22/2022 15:19:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=109
06/22/2022 15:19:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=114
06/22/2022 15:19:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=119
06/22/2022 15:19:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=124
06/22/2022 15:19:56 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=124
06/22/2022 15:19:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=129
06/22/2022 15:19:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
06/22/2022 15:20:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=139
06/22/2022 15:20:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=144
06/22/2022 15:20:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=149
06/22/2022 15:20:02 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=149
06/22/2022 15:20:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=154
06/22/2022 15:20:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=159
06/22/2022 15:20:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=164
06/22/2022 15:20:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=169
06/22/2022 15:20:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
06/22/2022 15:20:09 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3992490613266583 on epoch=174
06/22/2022 15:20:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=179
06/22/2022 15:20:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=184
06/22/2022 15:20:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=189
06/22/2022 15:20:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
06/22/2022 15:20:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=199
06/22/2022 15:20:15 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.5151515151515151 on epoch=199
06/22/2022 15:20:15 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5151515151515151 on epoch=199, global_step=400
06/22/2022 15:20:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=204
06/22/2022 15:20:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=209
06/22/2022 15:20:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=214
06/22/2022 15:20:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=219
06/22/2022 15:20:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=224
06/22/2022 15:20:22 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.5134502923976608 on epoch=224
06/22/2022 15:20:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=229
06/22/2022 15:20:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=234
06/22/2022 15:20:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=239
06/22/2022 15:20:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=244
06/22/2022 15:20:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=249
06/22/2022 15:20:28 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.46843853820598 on epoch=249
06/22/2022 15:20:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=254
06/22/2022 15:20:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=259
06/22/2022 15:20:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=264
06/22/2022 15:20:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=269
06/22/2022 15:20:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=274
06/22/2022 15:20:35 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.5195195195195195 on epoch=274
06/22/2022 15:20:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5195195195195195 on epoch=274, global_step=550
06/22/2022 15:20:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=279
06/22/2022 15:20:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=284
06/22/2022 15:20:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=289
06/22/2022 15:20:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=294
06/22/2022 15:20:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=299
06/22/2022 15:20:41 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.5901477832512315 on epoch=299
06/22/2022 15:20:41 - INFO - __main__ - Saving model with best Classification-F1: 0.5195195195195195 -> 0.5901477832512315 on epoch=299, global_step=600
06/22/2022 15:20:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=304
06/22/2022 15:20:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.29 on epoch=309
06/22/2022 15:20:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=314
06/22/2022 15:20:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=319
06/22/2022 15:20:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=324
06/22/2022 15:20:48 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.4817813765182186 on epoch=324
06/22/2022 15:20:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=329
06/22/2022 15:20:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=334
06/22/2022 15:20:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=339
06/22/2022 15:20:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=344
06/22/2022 15:20:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=349
06/22/2022 15:20:54 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.5607843137254902 on epoch=349
06/22/2022 15:20:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=354
06/22/2022 15:20:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=359
06/22/2022 15:20:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=364
06/22/2022 15:20:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=369
06/22/2022 15:21:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=374
06/22/2022 15:21:01 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.4231177094379639 on epoch=374
06/22/2022 15:21:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=379
06/22/2022 15:21:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=384
06/22/2022 15:21:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
06/22/2022 15:21:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=394
06/22/2022 15:21:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=399
06/22/2022 15:21:07 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.3552492046659597 on epoch=399
06/22/2022 15:21:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=404
06/22/2022 15:21:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=409
06/22/2022 15:21:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=414
06/22/2022 15:21:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=419
06/22/2022 15:21:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=424
06/22/2022 15:21:14 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.4666666666666667 on epoch=424
06/22/2022 15:21:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=429
06/22/2022 15:21:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=434
06/22/2022 15:21:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=439
06/22/2022 15:21:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=444
06/22/2022 15:21:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=449
06/22/2022 15:21:20 - INFO - __main__ - Global step 900 Train loss 0.16 Classification-F1 0.4817813765182186 on epoch=449
06/22/2022 15:21:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=454
06/22/2022 15:21:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=459
06/22/2022 15:21:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=464
06/22/2022 15:21:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=469
06/22/2022 15:21:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=474
06/22/2022 15:21:27 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.4666666666666667 on epoch=474
06/22/2022 15:21:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=479
06/22/2022 15:21:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=484
06/22/2022 15:21:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=489
06/22/2022 15:21:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=494
06/22/2022 15:21:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=499
06/22/2022 15:21:33 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.5076923076923077 on epoch=499
06/22/2022 15:21:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=504
06/22/2022 15:21:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
06/22/2022 15:21:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=514
06/22/2022 15:21:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=519
06/22/2022 15:21:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
06/22/2022 15:21:40 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.5076923076923077 on epoch=524
06/22/2022 15:21:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=529
06/22/2022 15:21:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=534
06/22/2022 15:21:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=539
06/22/2022 15:21:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
06/22/2022 15:21:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
06/22/2022 15:21:46 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.4554554554554554 on epoch=549
06/22/2022 15:21:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/22/2022 15:21:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=559
06/22/2022 15:21:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=564
06/22/2022 15:21:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
06/22/2022 15:21:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/22/2022 15:21:53 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.34310850439882695 on epoch=574
06/22/2022 15:21:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=579
06/22/2022 15:21:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
06/22/2022 15:21:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
06/22/2022 15:21:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=594
06/22/2022 15:21:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=599
06/22/2022 15:21:59 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.4285714285714286 on epoch=599
06/22/2022 15:22:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=604
06/22/2022 15:22:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/22/2022 15:22:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/22/2022 15:22:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/22/2022 15:22:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/22/2022 15:22:05 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.4009852216748768 on epoch=624
06/22/2022 15:22:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/22/2022 15:22:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=634
06/22/2022 15:22:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
06/22/2022 15:22:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
06/22/2022 15:22:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
06/22/2022 15:22:12 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.3650793650793651 on epoch=649
06/22/2022 15:22:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
06/22/2022 15:22:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/22/2022 15:22:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
06/22/2022 15:22:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
06/22/2022 15:22:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/22/2022 15:22:18 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.3650793650793651 on epoch=674
06/22/2022 15:22:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/22/2022 15:22:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/22/2022 15:22:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/22/2022 15:22:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/22/2022 15:22:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=699
06/22/2022 15:22:25 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.3650793650793651 on epoch=699
06/22/2022 15:22:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
06/22/2022 15:22:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
06/22/2022 15:22:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
06/22/2022 15:22:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/22/2022 15:22:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/22/2022 15:22:31 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.39139139139139134 on epoch=724
06/22/2022 15:22:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/22/2022 15:22:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
06/22/2022 15:22:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/22/2022 15:22:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/22/2022 15:22:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/22/2022 15:22:38 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=749
06/22/2022 15:22:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/22/2022 15:22:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/22/2022 15:22:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/22/2022 15:22:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
06/22/2022 15:22:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/22/2022 15:22:44 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.39139139139139134 on epoch=774
06/22/2022 15:22:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/22/2022 15:22:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/22/2022 15:22:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/22/2022 15:22:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/22/2022 15:22:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
06/22/2022 15:22:51 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=799
06/22/2022 15:22:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/22/2022 15:22:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/22/2022 15:22:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/22/2022 15:22:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/22/2022 15:22:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
06/22/2022 15:22:57 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=824
06/22/2022 15:22:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/22/2022 15:22:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
06/22/2022 15:23:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/22/2022 15:23:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/22/2022 15:23:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/22/2022 15:23:03 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=849
06/22/2022 15:23:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=854
06/22/2022 15:23:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
06/22/2022 15:23:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/22/2022 15:23:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/22/2022 15:23:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/22/2022 15:23:10 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.3650793650793651 on epoch=874
06/22/2022 15:23:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/22/2022 15:23:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/22/2022 15:23:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/22/2022 15:23:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/22/2022 15:23:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
06/22/2022 15:23:16 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.39139139139139134 on epoch=899
06/22/2022 15:23:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/22/2022 15:23:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/22/2022 15:23:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/22/2022 15:23:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/22/2022 15:23:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/22/2022 15:23:23 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.4285714285714286 on epoch=924
06/22/2022 15:23:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/22/2022 15:23:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/22/2022 15:23:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/22/2022 15:23:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/22/2022 15:23:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/22/2022 15:23:29 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.3650793650793651 on epoch=949
06/22/2022 15:23:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
06/22/2022 15:23:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/22/2022 15:23:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/22/2022 15:23:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/22/2022 15:23:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/22/2022 15:23:36 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.3650793650793651 on epoch=974
06/22/2022 15:23:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/22/2022 15:23:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/22/2022 15:23:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/22/2022 15:23:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/22/2022 15:23:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/22/2022 15:23:42 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.3650793650793651 on epoch=999
06/22/2022 15:23:42 - INFO - __main__ - save last model!
06/22/2022 15:23:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/22/2022 15:23:42 - INFO - __main__ - Start tokenizing ... 8000 instances
06/22/2022 15:23:42 - INFO - __main__ - Printing 3 examples
06/22/2022 15:23:42 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/22/2022 15:23:42 - INFO - __main__ - ['0']
06/22/2022 15:23:42 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/22/2022 15:23:42 - INFO - __main__ - ['1']
06/22/2022 15:23:42 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/22/2022 15:23:42 - INFO - __main__ - ['1']
06/22/2022 15:23:42 - INFO - __main__ - Tokenizing Input ...
06/22/2022 15:23:47 - INFO - __main__ - Tokenizing Output ...
06/22/2022 15:23:54 - INFO - __main__ - Loaded 8000 examples from test data
06/22/2022 15:25:25 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_87_0.2_8_predictions.txt
06/22/2022 15:25:25 - INFO - __main__ - Classification-F1 on test data: 0.5094
06/22/2022 15:25:25 - INFO - __main__ - prefix=paws_16_87, lr=0.2, bsz=8, dev_performance=0.5901477832512315, test_performance=0.5093858615233618
06/24/2022 14:09:23 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-multitask-nopara2para-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-nopara2para-5e-1-4-20-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
06/24/2022 14:09:23 - INFO - __main__ - models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws
06/24/2022 14:09:23 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-multitask-nopara2para-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-nopara2para-5e-1-4-20-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
06/24/2022 14:09:23 - INFO - __main__ - models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws
06/24/2022 14:09:24 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/24/2022 14:09:24 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/24/2022 14:09:24 - INFO - __main__ - args.device: cuda:0
06/24/2022 14:09:24 - INFO - __main__ - Using 2 gpus
06/24/2022 14:09:24 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/24/2022 14:09:24 - INFO - __main__ - args.device: cuda:1
06/24/2022 14:09:24 - INFO - __main__ - Using 2 gpus
06/24/2022 14:09:24 - INFO - __main__ - Fine-tuning the following samples: ['paws_16_100', 'paws_16_13', 'paws_16_21', 'paws_16_42', 'paws_16_87']
06/24/2022 14:09:29 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.5, bsz=8 ...
06/24/2022 14:09:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:09:30 - INFO - __main__ - Printing 3 examples
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:09:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:09:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:09:30 - INFO - __main__ - Printing 3 examples
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:09:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:09:30 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:09:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:09:30 - INFO - __main__ - Printing 3 examples
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:09:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:09:30 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:09:30 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:09:30 - INFO - __main__ - Printing 3 examples
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 14:09:30 - INFO - __main__ - ['1']
06/24/2022 14:09:30 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:09:30 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:09:30 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:09:30 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:09:36 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:09:36 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:09:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:09:36 - INFO - __main__ - Starting training!
06/24/2022 14:09:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:09:42 - INFO - __main__ - Starting training!
06/24/2022 14:09:44 - INFO - __main__ - Step 10 Global step 10 Train loss 3.72 on epoch=4
06/24/2022 14:09:45 - INFO - __main__ - Step 20 Global step 20 Train loss 2.27 on epoch=9
06/24/2022 14:09:46 - INFO - __main__ - Step 30 Global step 30 Train loss 1.20 on epoch=14
06/24/2022 14:09:47 - INFO - __main__ - Step 40 Global step 40 Train loss 0.79 on epoch=19
06/24/2022 14:09:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.74 on epoch=24
06/24/2022 14:09:49 - INFO - __main__ - Global step 50 Train loss 1.74 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 14:09:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/24/2022 14:09:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=29
06/24/2022 14:09:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=34
06/24/2022 14:09:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=39
06/24/2022 14:09:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=44
06/24/2022 14:09:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=49
06/24/2022 14:09:56 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 14:09:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=54
06/24/2022 14:09:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
06/24/2022 14:09:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=64
06/24/2022 14:10:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
06/24/2022 14:10:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
06/24/2022 14:10:02 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 14:10:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=79
06/24/2022 14:10:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
06/24/2022 14:10:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
06/24/2022 14:10:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.36 on epoch=94
06/24/2022 14:10:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
06/24/2022 14:10:09 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 14:10:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=104
06/24/2022 14:10:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=109
06/24/2022 14:10:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=114
06/24/2022 14:10:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
06/24/2022 14:10:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
06/24/2022 14:10:15 - INFO - __main__ - Global step 250 Train loss 0.33 Classification-F1 0.3992490613266583 on epoch=124
06/24/2022 14:10:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=124, global_step=250
06/24/2022 14:10:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
06/24/2022 14:10:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
06/24/2022 14:10:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=139
06/24/2022 14:10:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
06/24/2022 14:10:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
06/24/2022 14:10:22 - INFO - __main__ - Global step 300 Train loss 0.31 Classification-F1 0.5134502923976608 on epoch=149
06/24/2022 14:10:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5134502923976608 on epoch=149, global_step=300
06/24/2022 14:10:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
06/24/2022 14:10:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
06/24/2022 14:10:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
06/24/2022 14:10:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.32 on epoch=169
06/24/2022 14:10:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
06/24/2022 14:10:28 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.5588547189819725 on epoch=174
06/24/2022 14:10:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.5588547189819725 on epoch=174, global_step=350
06/24/2022 14:10:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
06/24/2022 14:10:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=184
06/24/2022 14:10:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=189
06/24/2022 14:10:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.16 on epoch=194
06/24/2022 14:10:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=199
06/24/2022 14:10:35 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.5555555555555556 on epoch=199
06/24/2022 14:10:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
06/24/2022 14:10:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.15 on epoch=209
06/24/2022 14:10:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.13 on epoch=214
06/24/2022 14:10:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
06/24/2022 14:10:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
06/24/2022 14:10:42 - INFO - __main__ - Global step 450 Train loss 0.12 Classification-F1 0.6190476190476191 on epoch=224
06/24/2022 14:10:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.6190476190476191 on epoch=224, global_step=450
06/24/2022 14:10:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
06/24/2022 14:10:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
06/24/2022 14:10:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.05 on epoch=239
06/24/2022 14:10:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
06/24/2022 14:10:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
06/24/2022 14:10:48 - INFO - __main__ - Global step 500 Train loss 0.07 Classification-F1 0.5733333333333335 on epoch=249
06/24/2022 14:10:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
06/24/2022 14:10:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
06/24/2022 14:10:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
06/24/2022 14:10:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
06/24/2022 14:10:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
06/24/2022 14:10:55 - INFO - __main__ - Global step 550 Train loss 0.04 Classification-F1 0.6862745098039216 on epoch=274
06/24/2022 14:10:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6190476190476191 -> 0.6862745098039216 on epoch=274, global_step=550
06/24/2022 14:10:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
06/24/2022 14:10:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
06/24/2022 14:10:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
06/24/2022 14:11:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
06/24/2022 14:11:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
06/24/2022 14:11:02 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.5901477832512315 on epoch=299
06/24/2022 14:11:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
06/24/2022 14:11:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
06/24/2022 14:11:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
06/24/2022 14:11:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
06/24/2022 14:11:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/24/2022 14:11:08 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.6559139784946237 on epoch=324
06/24/2022 14:11:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
06/24/2022 14:11:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
06/24/2022 14:11:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
06/24/2022 14:11:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
06/24/2022 14:11:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
06/24/2022 14:11:15 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=349
06/24/2022 14:11:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
06/24/2022 14:11:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/24/2022 14:11:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
06/24/2022 14:11:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
06/24/2022 14:11:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/24/2022 14:11:22 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=374
06/24/2022 14:11:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/24/2022 14:11:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/24/2022 14:11:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
06/24/2022 14:11:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/24/2022 14:11:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
06/24/2022 14:11:28 - INFO - __main__ - Global step 800 Train loss 0.02 Classification-F1 0.5555555555555556 on epoch=399
06/24/2022 14:11:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
06/24/2022 14:11:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/24/2022 14:11:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
06/24/2022 14:11:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
06/24/2022 14:11:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/24/2022 14:11:35 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=424
06/24/2022 14:11:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
06/24/2022 14:11:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/24/2022 14:11:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/24/2022 14:11:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
06/24/2022 14:11:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 14:11:41 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=449
06/24/2022 14:11:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/24/2022 14:11:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/24/2022 14:11:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
06/24/2022 14:11:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/24/2022 14:11:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/24/2022 14:11:48 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=474
06/24/2022 14:11:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/24/2022 14:11:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/24/2022 14:11:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/24/2022 14:11:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/24/2022 14:11:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 14:11:55 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=499
06/24/2022 14:11:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/24/2022 14:11:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/24/2022 14:11:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/24/2022 14:11:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/24/2022 14:12:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/24/2022 14:12:01 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.5625 on epoch=524
06/24/2022 14:12:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/24/2022 14:12:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 14:12:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 14:12:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/24/2022 14:12:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 14:12:08 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=549
06/24/2022 14:12:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 14:12:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 14:12:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/24/2022 14:12:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 14:12:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/24/2022 14:12:14 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=574
06/24/2022 14:12:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/24/2022 14:12:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 14:12:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 14:12:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 14:12:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 14:12:21 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=599
06/24/2022 14:12:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 14:12:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 14:12:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
06/24/2022 14:12:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 14:12:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/24/2022 14:12:28 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=624
06/24/2022 14:12:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 14:12:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 14:12:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 14:12:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 14:12:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 14:12:34 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=649
06/24/2022 14:12:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 14:12:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 14:12:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 14:12:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 14:12:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 14:12:41 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=674
06/24/2022 14:12:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 14:12:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 14:12:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 14:12:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 14:12:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 14:12:47 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=699
06/24/2022 14:12:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 14:12:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 14:12:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 14:12:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 14:12:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 14:12:53 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=724
06/24/2022 14:12:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 14:12:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 14:12:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 14:12:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 14:13:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 14:13:00 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=749
06/24/2022 14:13:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 14:13:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 14:13:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 14:13:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 14:13:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 14:13:06 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=774
06/24/2022 14:13:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 14:13:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 14:13:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 14:13:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 14:13:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 14:13:13 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=799
06/24/2022 14:13:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 14:13:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 14:13:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 14:13:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 14:13:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 14:13:19 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=824
06/24/2022 14:13:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 14:13:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 14:13:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 14:13:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 14:13:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 14:13:26 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=849
06/24/2022 14:13:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 14:13:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 14:13:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 14:13:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 14:13:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 14:13:32 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=874
06/24/2022 14:13:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 14:13:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 14:13:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 14:13:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 14:13:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 14:13:39 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=899
06/24/2022 14:13:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 14:13:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 14:13:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 14:13:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 14:13:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 14:13:45 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=924
06/24/2022 14:13:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 14:13:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 14:13:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 14:13:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 14:13:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 14:13:51 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=949
06/24/2022 14:13:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 14:13:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 14:13:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 14:13:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 14:13:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 14:13:58 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=974
06/24/2022 14:13:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 14:14:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 14:14:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 14:14:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 14:14:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 14:14:04 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=999
06/24/2022 14:14:04 - INFO - __main__ - save last model!
06/24/2022 14:14:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 14:14:04 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 14:14:04 - INFO - __main__ - Printing 3 examples
06/24/2022 14:14:04 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 14:14:05 - INFO - __main__ - ['0']
06/24/2022 14:14:05 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 14:14:05 - INFO - __main__ - ['1']
06/24/2022 14:14:05 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 14:14:05 - INFO - __main__ - ['1']
06/24/2022 14:14:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:14:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:14:05 - INFO - __main__ - Printing 3 examples
06/24/2022 14:14:05 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 14:14:05 - INFO - __main__ - ['1']
06/24/2022 14:14:05 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 14:14:05 - INFO - __main__ - ['1']
06/24/2022 14:14:05 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 14:14:05 - INFO - __main__ - ['1']
06/24/2022 14:14:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:14:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:14:05 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:14:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:14:05 - INFO - __main__ - Printing 3 examples
06/24/2022 14:14:05 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 14:14:05 - INFO - __main__ - ['1']
06/24/2022 14:14:05 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 14:14:05 - INFO - __main__ - ['1']
06/24/2022 14:14:05 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 14:14:05 - INFO - __main__ - ['1']
06/24/2022 14:14:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:14:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:14:05 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:14:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:14:11 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:14:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:14:11 - INFO - __main__ - Starting training!
06/24/2022 14:14:16 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 14:15:47 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_100_0.5_8_predictions.txt
06/24/2022 14:15:47 - INFO - __main__ - Classification-F1 on test data: 0.4992
06/24/2022 14:15:48 - INFO - __main__ - prefix=paws_16_100, lr=0.5, bsz=8, dev_performance=0.6862745098039216, test_performance=0.49921794994879676
06/24/2022 14:15:48 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.4, bsz=8 ...
06/24/2022 14:15:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:15:49 - INFO - __main__ - Printing 3 examples
06/24/2022 14:15:49 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 14:15:49 - INFO - __main__ - ['1']
06/24/2022 14:15:49 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 14:15:49 - INFO - __main__ - ['1']
06/24/2022 14:15:49 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 14:15:49 - INFO - __main__ - ['1']
06/24/2022 14:15:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:15:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:15:49 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:15:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:15:49 - INFO - __main__ - Printing 3 examples
06/24/2022 14:15:49 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 14:15:49 - INFO - __main__ - ['1']
06/24/2022 14:15:49 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 14:15:49 - INFO - __main__ - ['1']
06/24/2022 14:15:49 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 14:15:49 - INFO - __main__ - ['1']
06/24/2022 14:15:49 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:15:49 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:15:49 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:15:55 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:15:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:15:55 - INFO - __main__ - Starting training!
06/24/2022 14:15:56 - INFO - __main__ - Step 10 Global step 10 Train loss 4.16 on epoch=4
06/24/2022 14:15:58 - INFO - __main__ - Step 20 Global step 20 Train loss 2.68 on epoch=9
06/24/2022 14:15:59 - INFO - __main__ - Step 30 Global step 30 Train loss 1.89 on epoch=14
06/24/2022 14:16:00 - INFO - __main__ - Step 40 Global step 40 Train loss 1.62 on epoch=19
06/24/2022 14:16:01 - INFO - __main__ - Step 50 Global step 50 Train loss 1.07 on epoch=24
06/24/2022 14:16:02 - INFO - __main__ - Global step 50 Train loss 2.28 Classification-F1 0.3816425120772947 on epoch=24
06/24/2022 14:16:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3816425120772947 on epoch=24, global_step=50
06/24/2022 14:16:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.81 on epoch=29
06/24/2022 14:16:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.65 on epoch=34
06/24/2022 14:16:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=39
06/24/2022 14:16:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=44
06/24/2022 14:16:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=49
06/24/2022 14:16:08 - INFO - __main__ - Global step 100 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 14:16:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=54
06/24/2022 14:16:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=59
06/24/2022 14:16:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=64
06/24/2022 14:16:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=69
06/24/2022 14:16:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=74
06/24/2022 14:16:14 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 14:16:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=79
06/24/2022 14:16:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=84
06/24/2022 14:16:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
06/24/2022 14:16:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=94
06/24/2022 14:16:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=99
06/24/2022 14:16:21 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 14:16:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=104
06/24/2022 14:16:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=109
06/24/2022 14:16:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=114
06/24/2022 14:16:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/24/2022 14:16:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
06/24/2022 14:16:27 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 14:16:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
06/24/2022 14:16:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=134
06/24/2022 14:16:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
06/24/2022 14:16:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
06/24/2022 14:16:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=149
06/24/2022 14:16:33 - INFO - __main__ - Global step 300 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 14:16:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=154
06/24/2022 14:16:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=159
06/24/2022 14:16:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=164
06/24/2022 14:16:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=169
06/24/2022 14:16:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=174
06/24/2022 14:16:40 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 14:16:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=179
06/24/2022 14:16:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=184
06/24/2022 14:16:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=189
06/24/2022 14:16:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=194
06/24/2022 14:16:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=199
06/24/2022 14:16:46 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.3992490613266583 on epoch=199
06/24/2022 14:16:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.3992490613266583 on epoch=199, global_step=400
06/24/2022 14:16:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=204
06/24/2022 14:16:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=209
06/24/2022 14:16:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=214
06/24/2022 14:16:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=219
06/24/2022 14:16:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=224
06/24/2022 14:16:53 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.5134502923976608 on epoch=224
06/24/2022 14:16:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5134502923976608 on epoch=224, global_step=450
06/24/2022 14:16:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=229
06/24/2022 14:16:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=234
06/24/2022 14:16:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=239
06/24/2022 14:16:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=244
06/24/2022 14:16:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=249
06/24/2022 14:16:59 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.5134502923976608 on epoch=249
06/24/2022 14:17:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=254
06/24/2022 14:17:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=259
06/24/2022 14:17:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
06/24/2022 14:17:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=269
06/24/2022 14:17:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=274
06/24/2022 14:17:06 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.4589371980676329 on epoch=274
06/24/2022 14:17:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=279
06/24/2022 14:17:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=284
06/24/2022 14:17:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
06/24/2022 14:17:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=294
06/24/2022 14:17:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
06/24/2022 14:17:12 - INFO - __main__ - Global step 600 Train loss 0.28 Classification-F1 0.5134502923976608 on epoch=299
06/24/2022 14:17:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=304
06/24/2022 14:17:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
06/24/2022 14:17:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=314
06/24/2022 14:17:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=319
06/24/2022 14:17:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
06/24/2022 14:17:18 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.5636363636363637 on epoch=324
06/24/2022 14:17:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.5636363636363637 on epoch=324, global_step=650
06/24/2022 14:17:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=329
06/24/2022 14:17:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=334
06/24/2022 14:17:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=339
06/24/2022 14:17:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=344
06/24/2022 14:17:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=349
06/24/2022 14:17:25 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.5151515151515151 on epoch=349
06/24/2022 14:17:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=354
06/24/2022 14:17:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=359
06/24/2022 14:17:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=364
06/24/2022 14:17:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=369
06/24/2022 14:17:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
06/24/2022 14:17:31 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.49090909090909085 on epoch=374
06/24/2022 14:17:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=379
06/24/2022 14:17:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
06/24/2022 14:17:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=389
06/24/2022 14:17:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
06/24/2022 14:17:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=399
06/24/2022 14:17:38 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.5270935960591133 on epoch=399
06/24/2022 14:17:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
06/24/2022 14:17:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=409
06/24/2022 14:17:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=414
06/24/2022 14:17:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
06/24/2022 14:17:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/24/2022 14:17:44 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.5933528836754642 on epoch=424
06/24/2022 14:17:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5636363636363637 -> 0.5933528836754642 on epoch=424, global_step=850
06/24/2022 14:17:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/24/2022 14:17:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
06/24/2022 14:17:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/24/2022 14:17:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
06/24/2022 14:17:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=449
06/24/2022 14:17:51 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.5555555555555556 on epoch=449
06/24/2022 14:17:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/24/2022 14:17:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
06/24/2022 14:17:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
06/24/2022 14:17:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/24/2022 14:17:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/24/2022 14:17:57 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.5835835835835835 on epoch=474
06/24/2022 14:17:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/24/2022 14:17:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/24/2022 14:18:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
06/24/2022 14:18:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=494
06/24/2022 14:18:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
06/24/2022 14:18:03 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.5270935960591133 on epoch=499
06/24/2022 14:18:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
06/24/2022 14:18:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/24/2022 14:18:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
06/24/2022 14:18:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 14:18:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=524
06/24/2022 14:18:10 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.5 on epoch=524
06/24/2022 14:18:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
06/24/2022 14:18:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/24/2022 14:18:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
06/24/2022 14:18:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/24/2022 14:18:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 14:18:16 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=549
06/24/2022 14:18:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/24/2022 14:18:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/24/2022 14:18:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 14:18:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 14:18:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/24/2022 14:18:23 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=574
06/24/2022 14:18:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 14:18:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/24/2022 14:18:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/24/2022 14:18:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/24/2022 14:18:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 14:18:29 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=599
06/24/2022 14:18:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 14:18:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 14:18:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/24/2022 14:18:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/24/2022 14:18:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 14:18:36 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4980392156862745 on epoch=624
06/24/2022 14:18:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 14:18:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 14:18:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 14:18:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 14:18:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 14:18:43 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=649
06/24/2022 14:18:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 14:18:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/24/2022 14:18:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 14:18:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 14:18:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/24/2022 14:18:50 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4980392156862745 on epoch=674
06/24/2022 14:18:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 14:18:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/24/2022 14:18:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/24/2022 14:18:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 14:18:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/24/2022 14:18:56 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=699
06/24/2022 14:18:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 14:18:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 14:19:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 14:19:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/24/2022 14:19:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/24/2022 14:19:03 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=724
06/24/2022 14:19:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 14:19:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 14:19:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 14:19:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 14:19:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 14:19:10 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=749
06/24/2022 14:19:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 14:19:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 14:19:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 14:19:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/24/2022 14:19:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/24/2022 14:19:16 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=774
06/24/2022 14:19:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 14:19:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 14:19:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 14:19:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 14:19:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 14:19:23 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=799
06/24/2022 14:19:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 14:19:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 14:19:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/24/2022 14:19:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 14:19:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 14:19:30 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=824
06/24/2022 14:19:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 14:19:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/24/2022 14:19:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
06/24/2022 14:19:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 14:19:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/24/2022 14:19:36 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=849
06/24/2022 14:19:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/24/2022 14:19:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 14:19:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 14:19:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 14:19:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 14:19:43 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=874
06/24/2022 14:19:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 14:19:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 14:19:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 14:19:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 14:19:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 14:19:50 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=899
06/24/2022 14:19:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 14:19:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/24/2022 14:19:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 14:19:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 14:19:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 14:19:57 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=924
06/24/2022 14:19:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 14:19:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 14:20:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 14:20:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 14:20:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/24/2022 14:20:03 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5 on epoch=949
06/24/2022 14:20:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 14:20:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 14:20:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 14:20:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 14:20:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 14:20:10 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=974
06/24/2022 14:20:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 14:20:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 14:20:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 14:20:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 14:20:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 14:20:17 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=999
06/24/2022 14:20:17 - INFO - __main__ - save last model!
06/24/2022 14:20:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 14:20:17 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 14:20:17 - INFO - __main__ - Printing 3 examples
06/24/2022 14:20:17 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 14:20:17 - INFO - __main__ - ['0']
06/24/2022 14:20:17 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 14:20:17 - INFO - __main__ - ['1']
06/24/2022 14:20:17 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 14:20:17 - INFO - __main__ - ['1']
06/24/2022 14:20:17 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:20:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:20:17 - INFO - __main__ - Printing 3 examples
06/24/2022 14:20:17 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 14:20:17 - INFO - __main__ - ['1']
06/24/2022 14:20:17 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 14:20:17 - INFO - __main__ - ['1']
06/24/2022 14:20:17 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 14:20:17 - INFO - __main__ - ['1']
06/24/2022 14:20:17 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:20:17 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:20:17 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:20:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:20:17 - INFO - __main__ - Printing 3 examples
06/24/2022 14:20:17 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 14:20:17 - INFO - __main__ - ['1']
06/24/2022 14:20:17 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 14:20:17 - INFO - __main__ - ['1']
06/24/2022 14:20:17 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 14:20:17 - INFO - __main__ - ['1']
06/24/2022 14:20:17 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:20:17 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:20:18 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:20:21 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:20:23 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:20:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:20:23 - INFO - __main__ - Starting training!
06/24/2022 14:20:29 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 14:22:10 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_100_0.4_8_predictions.txt
06/24/2022 14:22:10 - INFO - __main__ - Classification-F1 on test data: 0.4948
06/24/2022 14:22:11 - INFO - __main__ - prefix=paws_16_100, lr=0.4, bsz=8, dev_performance=0.5933528836754642, test_performance=0.49477185791709066
06/24/2022 14:22:11 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.3, bsz=8 ...
06/24/2022 14:22:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:22:11 - INFO - __main__ - Printing 3 examples
06/24/2022 14:22:11 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 14:22:11 - INFO - __main__ - ['1']
06/24/2022 14:22:11 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 14:22:11 - INFO - __main__ - ['1']
06/24/2022 14:22:11 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 14:22:11 - INFO - __main__ - ['1']
06/24/2022 14:22:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:22:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:22:12 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:22:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:22:12 - INFO - __main__ - Printing 3 examples
06/24/2022 14:22:12 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 14:22:12 - INFO - __main__ - ['1']
06/24/2022 14:22:12 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 14:22:12 - INFO - __main__ - ['1']
06/24/2022 14:22:12 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 14:22:12 - INFO - __main__ - ['1']
06/24/2022 14:22:12 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:22:12 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:22:12 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:22:17 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:22:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:22:18 - INFO - __main__ - Starting training!
06/24/2022 14:22:19 - INFO - __main__ - Step 10 Global step 10 Train loss 4.23 on epoch=4
06/24/2022 14:22:20 - INFO - __main__ - Step 20 Global step 20 Train loss 3.14 on epoch=9
06/24/2022 14:22:22 - INFO - __main__ - Step 30 Global step 30 Train loss 2.24 on epoch=14
06/24/2022 14:22:23 - INFO - __main__ - Step 40 Global step 40 Train loss 1.48 on epoch=19
06/24/2022 14:22:24 - INFO - __main__ - Step 50 Global step 50 Train loss 0.98 on epoch=24
06/24/2022 14:22:24 - INFO - __main__ - Global step 50 Train loss 2.42 Classification-F1 0.3191489361702127 on epoch=24
06/24/2022 14:22:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3191489361702127 on epoch=24, global_step=50
06/24/2022 14:22:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.80 on epoch=29
06/24/2022 14:22:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.76 on epoch=34
06/24/2022 14:22:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.69 on epoch=39
06/24/2022 14:22:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=44
06/24/2022 14:22:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=49
06/24/2022 14:22:31 - INFO - __main__ - Global step 100 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 14:22:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3191489361702127 -> 0.3333333333333333 on epoch=49, global_step=100
06/24/2022 14:22:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=54
06/24/2022 14:22:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=59
06/24/2022 14:22:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=64
06/24/2022 14:22:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=69
06/24/2022 14:22:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=74
06/24/2022 14:22:37 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 14:22:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=79
06/24/2022 14:22:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=84
06/24/2022 14:22:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=89
06/24/2022 14:22:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=94
06/24/2022 14:22:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
06/24/2022 14:22:44 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 14:22:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=104
06/24/2022 14:22:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=109
06/24/2022 14:22:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=114
06/24/2022 14:22:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=119
06/24/2022 14:22:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=124
06/24/2022 14:22:50 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 14:22:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=129
06/24/2022 14:22:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=134
06/24/2022 14:22:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=139
06/24/2022 14:22:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=144
06/24/2022 14:22:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
06/24/2022 14:22:56 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.4589371980676329 on epoch=149
06/24/2022 14:22:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=149, global_step=300
06/24/2022 14:22:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.34 on epoch=154
06/24/2022 14:22:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=159
06/24/2022 14:23:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=164
06/24/2022 14:23:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=169
06/24/2022 14:23:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
06/24/2022 14:23:03 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.3992490613266583 on epoch=174
06/24/2022 14:23:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=179
06/24/2022 14:23:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=184
06/24/2022 14:23:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=189
06/24/2022 14:23:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=194
06/24/2022 14:23:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=199
06/24/2022 14:23:09 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.4589371980676329 on epoch=199
06/24/2022 14:23:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=204
06/24/2022 14:23:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=209
06/24/2022 14:23:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=214
06/24/2022 14:23:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.30 on epoch=219
06/24/2022 14:23:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=224
06/24/2022 14:23:16 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.4589371980676329 on epoch=224
06/24/2022 14:23:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
06/24/2022 14:23:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
06/24/2022 14:23:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=239
06/24/2022 14:23:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=244
06/24/2022 14:23:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=249
06/24/2022 14:23:22 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.4589371980676329 on epoch=249
06/24/2022 14:23:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.32 on epoch=254
06/24/2022 14:23:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=259
06/24/2022 14:23:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=264
06/24/2022 14:23:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=269
06/24/2022 14:23:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=274
06/24/2022 14:23:29 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.5636363636363637 on epoch=274
06/24/2022 14:23:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5636363636363637 on epoch=274, global_step=550
06/24/2022 14:23:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
06/24/2022 14:23:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
06/24/2022 14:23:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
06/24/2022 14:23:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
06/24/2022 14:23:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
06/24/2022 14:23:35 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.5465587044534412 on epoch=299
06/24/2022 14:23:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=304
06/24/2022 14:23:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=309
06/24/2022 14:23:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=314
06/24/2022 14:23:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=319
06/24/2022 14:23:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=324
06/24/2022 14:23:42 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.5555555555555556 on epoch=324
06/24/2022 14:23:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=329
06/24/2022 14:23:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.17 on epoch=334
06/24/2022 14:23:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=339
06/24/2022 14:23:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=344
06/24/2022 14:23:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=349
06/24/2022 14:23:48 - INFO - __main__ - Global step 700 Train loss 0.17 Classification-F1 0.5307917888563051 on epoch=349
06/24/2022 14:23:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=354
06/24/2022 14:23:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=359
06/24/2022 14:23:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=364
06/24/2022 14:23:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
06/24/2022 14:23:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=374
06/24/2022 14:23:55 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.4920634920634921 on epoch=374
06/24/2022 14:23:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
06/24/2022 14:23:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
06/24/2022 14:23:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=389
06/24/2022 14:24:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
06/24/2022 14:24:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
06/24/2022 14:24:01 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.5195195195195195 on epoch=399
06/24/2022 14:24:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
06/24/2022 14:24:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=409
06/24/2022 14:24:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=414
06/24/2022 14:24:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
06/24/2022 14:24:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/24/2022 14:24:08 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.464039408866995 on epoch=424
06/24/2022 14:24:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
06/24/2022 14:24:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
06/24/2022 14:24:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/24/2022 14:24:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/24/2022 14:24:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
06/24/2022 14:24:14 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.464039408866995 on epoch=449
06/24/2022 14:24:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/24/2022 14:24:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/24/2022 14:24:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 14:24:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
06/24/2022 14:24:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/24/2022 14:24:21 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=474
06/24/2022 14:24:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/24/2022 14:24:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/24/2022 14:24:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/24/2022 14:24:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
06/24/2022 14:24:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/24/2022 14:24:27 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=499
06/24/2022 14:24:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 14:24:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/24/2022 14:24:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 14:24:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 14:24:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
06/24/2022 14:24:34 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=524
06/24/2022 14:24:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/24/2022 14:24:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
06/24/2022 14:24:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 14:24:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/24/2022 14:24:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
06/24/2022 14:24:40 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=549
06/24/2022 14:24:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/24/2022 14:24:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/24/2022 14:24:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 14:24:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 14:24:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 14:24:47 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=574
06/24/2022 14:24:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 14:24:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/24/2022 14:24:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/24/2022 14:24:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/24/2022 14:24:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 14:24:54 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=599
06/24/2022 14:24:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 14:24:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 14:24:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
06/24/2022 14:24:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/24/2022 14:25:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/24/2022 14:25:00 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=624
06/24/2022 14:25:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 14:25:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 14:25:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 14:25:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 14:25:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/24/2022 14:25:07 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=649
06/24/2022 14:25:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 14:25:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 14:25:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/24/2022 14:25:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 14:25:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 14:25:13 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=674
06/24/2022 14:25:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 14:25:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/24/2022 14:25:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 14:25:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 14:25:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
06/24/2022 14:25:20 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=699
06/24/2022 14:25:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 14:25:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/24/2022 14:25:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 14:25:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 14:25:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 14:25:27 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=724
06/24/2022 14:25:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/24/2022 14:25:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 14:25:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/24/2022 14:25:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 14:25:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=749
06/24/2022 14:25:33 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=749
06/24/2022 14:25:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 14:25:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/24/2022 14:25:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 14:25:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 14:25:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/24/2022 14:25:39 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=774
06/24/2022 14:25:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/24/2022 14:25:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 14:25:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 14:25:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 14:25:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 14:25:46 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=799
06/24/2022 14:25:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 14:25:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 14:25:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 14:25:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 14:25:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 14:25:52 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=824
06/24/2022 14:25:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 14:25:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 14:25:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/24/2022 14:25:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 14:25:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 14:25:59 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=849
06/24/2022 14:26:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 14:26:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 14:26:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 14:26:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 14:26:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 14:26:05 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=874
06/24/2022 14:26:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 14:26:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 14:26:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 14:26:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 14:26:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 14:26:12 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=899
06/24/2022 14:26:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 14:26:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 14:26:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 14:26:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 14:26:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 14:26:19 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=924
06/24/2022 14:26:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 14:26:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 14:26:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 14:26:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 14:26:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 14:26:25 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=949
06/24/2022 14:26:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 14:26:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 14:26:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 14:26:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 14:26:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 14:26:32 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=974
06/24/2022 14:26:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 14:26:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/24/2022 14:26:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 14:26:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 14:26:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 14:26:38 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=999
06/24/2022 14:26:38 - INFO - __main__ - save last model!
06/24/2022 14:26:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 14:26:38 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 14:26:38 - INFO - __main__ - Printing 3 examples
06/24/2022 14:26:38 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 14:26:38 - INFO - __main__ - ['0']
06/24/2022 14:26:38 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 14:26:38 - INFO - __main__ - ['1']
06/24/2022 14:26:38 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 14:26:38 - INFO - __main__ - ['1']
06/24/2022 14:26:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:26:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:26:39 - INFO - __main__ - Printing 3 examples
06/24/2022 14:26:39 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 14:26:39 - INFO - __main__ - ['1']
06/24/2022 14:26:39 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 14:26:39 - INFO - __main__ - ['1']
06/24/2022 14:26:39 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 14:26:39 - INFO - __main__ - ['1']
06/24/2022 14:26:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:26:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:26:39 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:26:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:26:39 - INFO - __main__ - Printing 3 examples
06/24/2022 14:26:39 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 14:26:39 - INFO - __main__ - ['1']
06/24/2022 14:26:39 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 14:26:39 - INFO - __main__ - ['1']
06/24/2022 14:26:39 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 14:26:39 - INFO - __main__ - ['1']
06/24/2022 14:26:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:26:39 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:26:39 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:26:42 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:26:45 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:26:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:26:45 - INFO - __main__ - Starting training!
06/24/2022 14:26:50 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 14:28:31 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_100_0.3_8_predictions.txt
06/24/2022 14:28:31 - INFO - __main__ - Classification-F1 on test data: 0.3366
06/24/2022 14:28:31 - INFO - __main__ - prefix=paws_16_100, lr=0.3, bsz=8, dev_performance=0.5636363636363637, test_performance=0.33664067582616325
06/24/2022 14:28:31 - INFO - __main__ - Running ... prefix=paws_16_100, lr=0.2, bsz=8 ...
06/24/2022 14:28:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:28:32 - INFO - __main__ - Printing 3 examples
06/24/2022 14:28:32 - INFO - __main__ -  [paws] sentence 1: Restovich was traded to the Arizona Diamondbacks from the Chicago White Sox on July 27 , 2011 . [SEP] sentence 2: On July 27 , 2011 , Restovich was traded from the Chicago White Sox into Arizona Diamondbacks .
06/24/2022 14:28:32 - INFO - __main__ - ['1']
06/24/2022 14:28:32 - INFO - __main__ -  [paws] sentence 1: `` Thanks to the Facebook generation , anyone by simply mounting a Selfie can become a Harvey Weinstone or a Kevin Spacey '' , he added . [SEP] sentence 2: `` Thanks to the Facebook generation , by simply attaching a selfie , anyone can become a Harvey Weinstein or a Kevin Spacey , '' he added .
06/24/2022 14:28:32 - INFO - __main__ - ['1']
06/24/2022 14:28:32 - INFO - __main__ -  [paws] sentence 1: After spending six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall . [SEP] sentence 2: After six months in Buenos Aires , Forchhammer returned to Christiania in 2010 , where he joined the writing team Future Animals -- Don Stefano and Rissi Royall .
06/24/2022 14:28:32 - INFO - __main__ - ['1']
06/24/2022 14:28:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:28:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:28:32 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:28:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:28:32 - INFO - __main__ - Printing 3 examples
06/24/2022 14:28:32 - INFO - __main__ -  [paws] sentence 1: His parents are Angelina Miers , a prominent artist himself , and Don Luis Toranzos , of Argentina . [SEP] sentence 2: His parents are Angelina Miers , himself a prominent artist , and Don Luis Toranzos from Argentina .
06/24/2022 14:28:32 - INFO - __main__ - ['1']
06/24/2022 14:28:32 - INFO - __main__ -  [paws] sentence 1: Hangars 1 -- 4 were built on the south side of the administration building , while hangars 5 -- 8 were built on the north side . [SEP] sentence 2: On the south side of the administrative building the hangars were built 1 -- 4 , on the north side the hangars 5 -- 8 were built .
06/24/2022 14:28:32 - INFO - __main__ - ['1']
06/24/2022 14:28:32 - INFO - __main__ -  [paws] sentence 1: In Middle Persian sources of the Sassanid period the river is known as `` Wehrōd '' ( lit . [SEP] sentence 2: In the Middle Persian sources of the Sassanid period , the river is known as '' Wehrod `` ( Lit . ) .
06/24/2022 14:28:32 - INFO - __main__ - ['1']
06/24/2022 14:28:32 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:28:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:28:32 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:28:38 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:28:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:28:38 - INFO - __main__ - Starting training!
06/24/2022 14:28:40 - INFO - __main__ - Step 10 Global step 10 Train loss 4.49 on epoch=4
06/24/2022 14:28:41 - INFO - __main__ - Step 20 Global step 20 Train loss 3.45 on epoch=9
06/24/2022 14:28:42 - INFO - __main__ - Step 30 Global step 30 Train loss 2.89 on epoch=14
06/24/2022 14:28:43 - INFO - __main__ - Step 40 Global step 40 Train loss 2.19 on epoch=19
06/24/2022 14:28:45 - INFO - __main__ - Step 50 Global step 50 Train loss 1.88 on epoch=24
06/24/2022 14:28:45 - INFO - __main__ - Global step 50 Train loss 2.98 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 14:28:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/24/2022 14:28:46 - INFO - __main__ - Step 60 Global step 60 Train loss 1.41 on epoch=29
06/24/2022 14:28:48 - INFO - __main__ - Step 70 Global step 70 Train loss 1.06 on epoch=34
06/24/2022 14:28:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=39
06/24/2022 14:28:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=44
06/24/2022 14:28:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.75 on epoch=49
06/24/2022 14:28:52 - INFO - __main__ - Global step 100 Train loss 1.00 Classification-F1 0.37662337662337664 on epoch=49
06/24/2022 14:28:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.37662337662337664 on epoch=49, global_step=100
06/24/2022 14:28:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.73 on epoch=54
06/24/2022 14:28:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.72 on epoch=59
06/24/2022 14:28:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=64
06/24/2022 14:28:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=69
06/24/2022 14:28:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=74
06/24/2022 14:28:58 - INFO - __main__ - Global step 150 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 14:28:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=79
06/24/2022 14:29:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=84
06/24/2022 14:29:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=89
06/24/2022 14:29:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=94
06/24/2022 14:29:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=99
06/24/2022 14:29:05 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 14:29:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=104
06/24/2022 14:29:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=109
06/24/2022 14:29:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=114
06/24/2022 14:29:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=119
06/24/2022 14:29:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.52 on epoch=124
06/24/2022 14:29:11 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 14:29:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=129
06/24/2022 14:29:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=134
06/24/2022 14:29:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=139
06/24/2022 14:29:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=144
06/24/2022 14:29:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=149
06/24/2022 14:29:18 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 14:29:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=154
06/24/2022 14:29:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=159
06/24/2022 14:29:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=164
06/24/2022 14:29:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=169
06/24/2022 14:29:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
06/24/2022 14:29:25 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 14:29:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=179
06/24/2022 14:29:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=184
06/24/2022 14:29:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=189
06/24/2022 14:29:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=194
06/24/2022 14:29:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=199
06/24/2022 14:29:31 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 14:29:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=204
06/24/2022 14:29:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=209
06/24/2022 14:29:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=214
06/24/2022 14:29:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=219
06/24/2022 14:29:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=224
06/24/2022 14:29:38 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3992490613266583 on epoch=224
06/24/2022 14:29:38 - INFO - __main__ - Saving model with best Classification-F1: 0.37662337662337664 -> 0.3992490613266583 on epoch=224, global_step=450
06/24/2022 14:29:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=229
06/24/2022 14:29:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=234
06/24/2022 14:29:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=239
06/24/2022 14:29:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=244
06/24/2022 14:29:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=249
06/24/2022 14:29:44 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 14:29:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=254
06/24/2022 14:29:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=259
06/24/2022 14:29:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=264
06/24/2022 14:29:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
06/24/2022 14:29:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=274
06/24/2022 14:29:51 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.3992490613266583 on epoch=274
06/24/2022 14:29:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=279
06/24/2022 14:29:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=284
06/24/2022 14:29:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
06/24/2022 14:29:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=294
06/24/2022 14:29:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=299
06/24/2022 14:29:58 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.3992490613266583 on epoch=299
06/24/2022 14:29:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=304
06/24/2022 14:30:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=309
06/24/2022 14:30:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=314
06/24/2022 14:30:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=319
06/24/2022 14:30:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=324
06/24/2022 14:30:04 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.4589371980676329 on epoch=324
06/24/2022 14:30:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4589371980676329 on epoch=324, global_step=650
06/24/2022 14:30:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=329
06/24/2022 14:30:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=334
06/24/2022 14:30:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=339
06/24/2022 14:30:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=344
06/24/2022 14:30:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=349
06/24/2022 14:30:11 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.4589371980676329 on epoch=349
06/24/2022 14:30:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=354
06/24/2022 14:30:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=359
06/24/2022 14:30:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=364
06/24/2022 14:30:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=369
06/24/2022 14:30:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=374
06/24/2022 14:30:17 - INFO - __main__ - Global step 750 Train loss 0.31 Classification-F1 0.5134502923976608 on epoch=374
06/24/2022 14:30:18 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5134502923976608 on epoch=374, global_step=750
06/24/2022 14:30:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=379
06/24/2022 14:30:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=384
06/24/2022 14:30:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=389
06/24/2022 14:30:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=394
06/24/2022 14:30:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=399
06/24/2022 14:30:24 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.539313399778516 on epoch=399
06/24/2022 14:30:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.539313399778516 on epoch=399, global_step=800
06/24/2022 14:30:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=404
06/24/2022 14:30:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=409
06/24/2022 14:30:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=414
06/24/2022 14:30:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=419
06/24/2022 14:30:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.31 on epoch=424
06/24/2022 14:30:31 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.5636363636363637 on epoch=424
06/24/2022 14:30:31 - INFO - __main__ - Saving model with best Classification-F1: 0.539313399778516 -> 0.5636363636363637 on epoch=424, global_step=850
06/24/2022 14:30:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=429
06/24/2022 14:30:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=434
06/24/2022 14:30:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=439
06/24/2022 14:30:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=444
06/24/2022 14:30:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=449
06/24/2022 14:30:38 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.5151515151515151 on epoch=449
06/24/2022 14:30:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=454
06/24/2022 14:30:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=459
06/24/2022 14:30:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=464
06/24/2022 14:30:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=469
06/24/2022 14:30:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=474
06/24/2022 14:30:44 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.5636363636363637 on epoch=474
06/24/2022 14:30:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=479
06/24/2022 14:30:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=484
06/24/2022 14:30:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=489
06/24/2022 14:30:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=494
06/24/2022 14:30:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=499
06/24/2022 14:30:51 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.5333333333333333 on epoch=499
06/24/2022 14:30:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=504
06/24/2022 14:30:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=509
06/24/2022 14:30:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=514
06/24/2022 14:30:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=519
06/24/2022 14:30:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=524
06/24/2022 14:30:58 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.4909862142099682 on epoch=524
06/24/2022 14:30:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=529
06/24/2022 14:31:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=534
06/24/2022 14:31:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=539
06/24/2022 14:31:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=544
06/24/2022 14:31:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=549
06/24/2022 14:31:04 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.5270935960591133 on epoch=549
06/24/2022 14:31:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=554
06/24/2022 14:31:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=559
06/24/2022 14:31:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=564
06/24/2022 14:31:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=569
06/24/2022 14:31:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=574
06/24/2022 14:31:11 - INFO - __main__ - Global step 1150 Train loss 0.13 Classification-F1 0.5307917888563051 on epoch=574
06/24/2022 14:31:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=579
06/24/2022 14:31:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=584
06/24/2022 14:31:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=589
06/24/2022 14:31:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=594
06/24/2022 14:31:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
06/24/2022 14:31:18 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.5270935960591133 on epoch=599
06/24/2022 14:31:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=604
06/24/2022 14:31:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=609
06/24/2022 14:31:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=614
06/24/2022 14:31:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=619
06/24/2022 14:31:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=624
06/24/2022 14:31:24 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.5555555555555556 on epoch=624
06/24/2022 14:31:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
06/24/2022 14:31:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=634
06/24/2022 14:31:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
06/24/2022 14:31:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
06/24/2022 14:31:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=649
06/24/2022 14:31:31 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.5307917888563051 on epoch=649
06/24/2022 14:31:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=654
06/24/2022 14:31:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
06/24/2022 14:31:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
06/24/2022 14:31:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
06/24/2022 14:31:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=674
06/24/2022 14:31:37 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.4920634920634921 on epoch=674
06/24/2022 14:31:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
06/24/2022 14:31:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/24/2022 14:31:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
06/24/2022 14:31:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=694
06/24/2022 14:31:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/24/2022 14:31:44 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.5607843137254902 on epoch=699
06/24/2022 14:31:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
06/24/2022 14:31:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
06/24/2022 14:31:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
06/24/2022 14:31:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
06/24/2022 14:31:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
06/24/2022 14:31:50 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.5270935960591133 on epoch=724
06/24/2022 14:31:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/24/2022 14:31:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
06/24/2022 14:31:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
06/24/2022 14:31:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
06/24/2022 14:31:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/24/2022 14:31:57 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=749
06/24/2022 14:31:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
06/24/2022 14:32:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
06/24/2022 14:32:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
06/24/2022 14:32:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
06/24/2022 14:32:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/24/2022 14:32:04 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.5933528836754642 on epoch=774
06/24/2022 14:32:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5636363636363637 -> 0.5933528836754642 on epoch=774, global_step=1550
06/24/2022 14:32:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/24/2022 14:32:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
06/24/2022 14:32:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 14:32:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
06/24/2022 14:32:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/24/2022 14:32:11 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.5933528836754642 on epoch=799
06/24/2022 14:32:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 14:32:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
06/24/2022 14:32:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 14:32:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
06/24/2022 14:32:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
06/24/2022 14:32:18 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.5625 on epoch=824
06/24/2022 14:32:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
06/24/2022 14:32:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/24/2022 14:32:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
06/24/2022 14:32:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/24/2022 14:32:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 14:32:24 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.5625 on epoch=849
06/24/2022 14:32:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/24/2022 14:32:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/24/2022 14:32:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/24/2022 14:32:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
06/24/2022 14:32:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/24/2022 14:32:31 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=874
06/24/2022 14:32:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 14:32:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=884
06/24/2022 14:32:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/24/2022 14:32:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/24/2022 14:32:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 14:32:38 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.5625 on epoch=899
06/24/2022 14:32:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/24/2022 14:32:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/24/2022 14:32:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/24/2022 14:32:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/24/2022 14:32:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/24/2022 14:32:44 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=924
06/24/2022 14:32:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 14:32:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 14:32:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/24/2022 14:32:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/24/2022 14:32:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
06/24/2022 14:32:51 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5625 on epoch=949
06/24/2022 14:32:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/24/2022 14:32:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
06/24/2022 14:32:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 14:32:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/24/2022 14:32:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/24/2022 14:32:58 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=974
06/24/2022 14:32:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
06/24/2022 14:33:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/24/2022 14:33:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
06/24/2022 14:33:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/24/2022 14:33:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/24/2022 14:33:04 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=999
06/24/2022 14:33:04 - INFO - __main__ - save last model!
06/24/2022 14:33:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 14:33:05 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 14:33:05 - INFO - __main__ - Printing 3 examples
06/24/2022 14:33:05 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 14:33:05 - INFO - __main__ - ['0']
06/24/2022 14:33:05 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 14:33:05 - INFO - __main__ - ['1']
06/24/2022 14:33:05 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 14:33:05 - INFO - __main__ - ['1']
06/24/2022 14:33:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:33:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:33:05 - INFO - __main__ - Printing 3 examples
06/24/2022 14:33:05 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 14:33:05 - INFO - __main__ - ['1']
06/24/2022 14:33:05 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 14:33:05 - INFO - __main__ - ['1']
06/24/2022 14:33:05 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 14:33:05 - INFO - __main__ - ['1']
06/24/2022 14:33:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:33:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:33:05 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:33:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:33:05 - INFO - __main__ - Printing 3 examples
06/24/2022 14:33:05 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 14:33:05 - INFO - __main__ - ['1']
06/24/2022 14:33:05 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 14:33:05 - INFO - __main__ - ['1']
06/24/2022 14:33:05 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 14:33:05 - INFO - __main__ - ['1']
06/24/2022 14:33:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:33:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:33:05 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:33:09 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:33:11 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:33:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:33:11 - INFO - __main__ - Starting training!
06/24/2022 14:33:16 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 14:35:06 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_100_0.2_8_predictions.txt
06/24/2022 14:35:06 - INFO - __main__ - Classification-F1 on test data: 0.5072
06/24/2022 14:35:07 - INFO - __main__ - prefix=paws_16_100, lr=0.2, bsz=8, dev_performance=0.5933528836754642, test_performance=0.5071888287978357
06/24/2022 14:35:07 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.5, bsz=8 ...
06/24/2022 14:35:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:35:07 - INFO - __main__ - Printing 3 examples
06/24/2022 14:35:07 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 14:35:07 - INFO - __main__ - ['1']
06/24/2022 14:35:07 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 14:35:07 - INFO - __main__ - ['1']
06/24/2022 14:35:07 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 14:35:07 - INFO - __main__ - ['1']
06/24/2022 14:35:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:35:07 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:35:07 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:35:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:35:07 - INFO - __main__ - Printing 3 examples
06/24/2022 14:35:07 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 14:35:07 - INFO - __main__ - ['1']
06/24/2022 14:35:07 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 14:35:07 - INFO - __main__ - ['1']
06/24/2022 14:35:07 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 14:35:07 - INFO - __main__ - ['1']
06/24/2022 14:35:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:35:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:35:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:35:13 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:35:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:35:13 - INFO - __main__ - Starting training!
06/24/2022 14:35:15 - INFO - __main__ - Step 10 Global step 10 Train loss 4.04 on epoch=4
06/24/2022 14:35:16 - INFO - __main__ - Step 20 Global step 20 Train loss 2.33 on epoch=9
06/24/2022 14:35:17 - INFO - __main__ - Step 30 Global step 30 Train loss 1.14 on epoch=14
06/24/2022 14:35:19 - INFO - __main__ - Step 40 Global step 40 Train loss 0.80 on epoch=19
06/24/2022 14:35:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.65 on epoch=24
06/24/2022 14:35:20 - INFO - __main__ - Global step 50 Train loss 1.79 Classification-F1 0.36374269005847953 on epoch=24
06/24/2022 14:35:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.36374269005847953 on epoch=24, global_step=50
06/24/2022 14:35:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.62 on epoch=29
06/24/2022 14:35:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=34
06/24/2022 14:35:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=39
06/24/2022 14:35:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=44
06/24/2022 14:35:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=49
06/24/2022 14:35:27 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 14:35:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
06/24/2022 14:35:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=59
06/24/2022 14:35:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
06/24/2022 14:35:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=69
06/24/2022 14:35:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
06/24/2022 14:35:33 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3816425120772947 on epoch=74
06/24/2022 14:35:33 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.3816425120772947 on epoch=74, global_step=150
06/24/2022 14:35:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=79
06/24/2022 14:35:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=84
06/24/2022 14:35:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
06/24/2022 14:35:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=94
06/24/2022 14:35:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=99
06/24/2022 14:35:40 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 14:35:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
06/24/2022 14:35:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
06/24/2022 14:35:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
06/24/2022 14:35:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/24/2022 14:35:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=124
06/24/2022 14:35:46 - INFO - __main__ - Global step 250 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 14:35:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.35 on epoch=129
06/24/2022 14:35:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
06/24/2022 14:35:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=139
06/24/2022 14:35:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=144
06/24/2022 14:35:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=149
06/24/2022 14:35:53 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.36374269005847953 on epoch=149
06/24/2022 14:35:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/24/2022 14:35:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
06/24/2022 14:35:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=164
06/24/2022 14:35:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/24/2022 14:35:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=174
06/24/2022 14:35:59 - INFO - __main__ - Global step 350 Train loss 0.31 Classification-F1 0.46843853820598 on epoch=174
06/24/2022 14:35:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.46843853820598 on epoch=174, global_step=350
06/24/2022 14:36:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=179
06/24/2022 14:36:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=184
06/24/2022 14:36:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=189
06/24/2022 14:36:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=194
06/24/2022 14:36:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
06/24/2022 14:36:06 - INFO - __main__ - Global step 400 Train loss 0.30 Classification-F1 0.46843853820598 on epoch=199
06/24/2022 14:36:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/24/2022 14:36:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
06/24/2022 14:36:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
06/24/2022 14:36:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
06/24/2022 14:36:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=224
06/24/2022 14:36:12 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.5333333333333333 on epoch=224
06/24/2022 14:36:12 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5333333333333333 on epoch=224, global_step=450
06/24/2022 14:36:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=229
06/24/2022 14:36:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
06/24/2022 14:36:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=239
06/24/2022 14:36:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=244
06/24/2022 14:36:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=249
06/24/2022 14:36:19 - INFO - __main__ - Global step 500 Train loss 0.17 Classification-F1 0.464039408866995 on epoch=249
06/24/2022 14:36:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
06/24/2022 14:36:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=259
06/24/2022 14:36:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
06/24/2022 14:36:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
06/24/2022 14:36:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
06/24/2022 14:36:25 - INFO - __main__ - Global step 550 Train loss 0.09 Classification-F1 0.4909862142099682 on epoch=274
06/24/2022 14:36:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
06/24/2022 14:36:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
06/24/2022 14:36:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
06/24/2022 14:36:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
06/24/2022 14:36:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
06/24/2022 14:36:32 - INFO - __main__ - Global step 600 Train loss 0.04 Classification-F1 0.5607843137254902 on epoch=299
06/24/2022 14:36:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5607843137254902 on epoch=299, global_step=600
06/24/2022 14:36:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
06/24/2022 14:36:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
06/24/2022 14:36:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
06/24/2022 14:36:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
06/24/2022 14:36:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/24/2022 14:36:38 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.5195195195195195 on epoch=324
06/24/2022 14:36:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
06/24/2022 14:36:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
06/24/2022 14:36:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
06/24/2022 14:36:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
06/24/2022 14:36:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
06/24/2022 14:36:45 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.5625 on epoch=349
06/24/2022 14:36:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.5625 on epoch=349, global_step=700
06/24/2022 14:36:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
06/24/2022 14:36:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/24/2022 14:36:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
06/24/2022 14:36:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/24/2022 14:36:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/24/2022 14:36:52 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.5625 on epoch=374
06/24/2022 14:36:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/24/2022 14:36:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/24/2022 14:36:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
06/24/2022 14:36:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/24/2022 14:36:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
06/24/2022 14:36:59 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=399
06/24/2022 14:37:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
06/24/2022 14:37:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/24/2022 14:37:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/24/2022 14:37:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
06/24/2022 14:37:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
06/24/2022 14:37:05 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=424
06/24/2022 14:37:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
06/24/2022 14:37:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
06/24/2022 14:37:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/24/2022 14:37:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/24/2022 14:37:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 14:37:12 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.5733333333333335 on epoch=449
06/24/2022 14:37:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5625 -> 0.5733333333333335 on epoch=449, global_step=900
06/24/2022 14:37:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
06/24/2022 14:37:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/24/2022 14:37:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 14:37:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/24/2022 14:37:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/24/2022 14:37:19 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.5625 on epoch=474
06/24/2022 14:37:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
06/24/2022 14:37:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/24/2022 14:37:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
06/24/2022 14:37:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/24/2022 14:37:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 14:37:25 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=499
06/24/2022 14:37:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/24/2022 14:37:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/24/2022 14:37:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/24/2022 14:37:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/24/2022 14:37:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/24/2022 14:37:32 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.5625 on epoch=524
06/24/2022 14:37:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/24/2022 14:37:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 14:37:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/24/2022 14:37:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/24/2022 14:37:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 14:37:39 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=549
06/24/2022 14:37:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5733333333333335 -> 0.5933528836754642 on epoch=549, global_step=1100
06/24/2022 14:37:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 14:37:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 14:37:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 14:37:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 14:37:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 14:37:45 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.5625 on epoch=574
06/24/2022 14:37:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/24/2022 14:37:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 14:37:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 14:37:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 14:37:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 14:37:52 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5835835835835835 on epoch=599
06/24/2022 14:37:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 14:37:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 14:37:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 14:37:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 14:37:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 14:37:59 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=624
06/24/2022 14:38:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 14:38:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 14:38:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 14:38:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 14:38:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
06/24/2022 14:38:05 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=649
06/24/2022 14:38:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 14:38:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 14:38:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 14:38:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 14:38:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 14:38:12 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=674
06/24/2022 14:38:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 14:38:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 14:38:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 14:38:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 14:38:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 14:38:18 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=699
06/24/2022 14:38:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
06/24/2022 14:38:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 14:38:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 14:38:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 14:38:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 14:38:25 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=724
06/24/2022 14:38:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 14:38:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 14:38:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 14:38:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 14:38:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 14:38:32 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.6190476190476191 on epoch=749
06/24/2022 14:38:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6190476190476191 on epoch=749, global_step=1500
06/24/2022 14:38:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 14:38:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 14:38:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 14:38:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 14:38:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 14:38:38 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=774
06/24/2022 14:38:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/24/2022 14:38:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 14:38:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 14:38:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 14:38:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 14:38:45 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.6235294117647059 on epoch=799
06/24/2022 14:38:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6190476190476191 -> 0.6235294117647059 on epoch=799, global_step=1600
06/24/2022 14:38:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 14:38:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 14:38:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 14:38:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 14:38:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 14:38:51 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=824
06/24/2022 14:38:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 14:38:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 14:38:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 14:38:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 14:38:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 14:38:58 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.6235294117647059 on epoch=849
06/24/2022 14:38:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 14:39:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 14:39:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 14:39:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 14:39:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 14:39:05 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.6235294117647059 on epoch=874
06/24/2022 14:39:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 14:39:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 14:39:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 14:39:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 14:39:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 14:39:11 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.6532019704433498 on epoch=899
06/24/2022 14:39:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6235294117647059 -> 0.6532019704433498 on epoch=899, global_step=1800
06/24/2022 14:39:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 14:39:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 14:39:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 14:39:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 14:39:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 14:39:18 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=924
06/24/2022 14:39:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 14:39:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 14:39:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 14:39:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 14:39:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 14:39:24 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=949
06/24/2022 14:39:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 14:39:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 14:39:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 14:39:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 14:39:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 14:39:31 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.6190476190476191 on epoch=974
06/24/2022 14:39:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 14:39:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 14:39:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 14:39:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 14:39:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 14:39:37 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=999
06/24/2022 14:39:37 - INFO - __main__ - save last model!
06/24/2022 14:39:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 14:39:38 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 14:39:38 - INFO - __main__ - Printing 3 examples
06/24/2022 14:39:38 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 14:39:38 - INFO - __main__ - ['0']
06/24/2022 14:39:38 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 14:39:38 - INFO - __main__ - ['1']
06/24/2022 14:39:38 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 14:39:38 - INFO - __main__ - ['1']
06/24/2022 14:39:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:39:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:39:38 - INFO - __main__ - Printing 3 examples
06/24/2022 14:39:38 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 14:39:38 - INFO - __main__ - ['1']
06/24/2022 14:39:38 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 14:39:38 - INFO - __main__ - ['1']
06/24/2022 14:39:38 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 14:39:38 - INFO - __main__ - ['1']
06/24/2022 14:39:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:39:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:39:38 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:39:38 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:39:38 - INFO - __main__ - Printing 3 examples
06/24/2022 14:39:38 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 14:39:38 - INFO - __main__ - ['1']
06/24/2022 14:39:38 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 14:39:38 - INFO - __main__ - ['1']
06/24/2022 14:39:38 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 14:39:38 - INFO - __main__ - ['1']
06/24/2022 14:39:38 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:39:38 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:39:38 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:39:42 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:39:44 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:39:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:39:44 - INFO - __main__ - Starting training!
06/24/2022 14:39:49 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 14:41:25 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_13_0.5_8_predictions.txt
06/24/2022 14:41:25 - INFO - __main__ - Classification-F1 on test data: 0.5138
06/24/2022 14:41:25 - INFO - __main__ - prefix=paws_16_13, lr=0.5, bsz=8, dev_performance=0.6532019704433498, test_performance=0.5137984850668436
06/24/2022 14:41:25 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.4, bsz=8 ...
06/24/2022 14:41:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:41:26 - INFO - __main__ - Printing 3 examples
06/24/2022 14:41:26 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 14:41:26 - INFO - __main__ - ['1']
06/24/2022 14:41:26 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 14:41:26 - INFO - __main__ - ['1']
06/24/2022 14:41:26 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 14:41:26 - INFO - __main__ - ['1']
06/24/2022 14:41:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:41:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:41:26 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:41:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:41:26 - INFO - __main__ - Printing 3 examples
06/24/2022 14:41:26 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 14:41:26 - INFO - __main__ - ['1']
06/24/2022 14:41:26 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 14:41:26 - INFO - __main__ - ['1']
06/24/2022 14:41:26 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 14:41:26 - INFO - __main__ - ['1']
06/24/2022 14:41:26 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:41:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:41:26 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:41:32 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:41:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:41:32 - INFO - __main__ - Starting training!
06/24/2022 14:41:34 - INFO - __main__ - Step 10 Global step 10 Train loss 3.82 on epoch=4
06/24/2022 14:41:35 - INFO - __main__ - Step 20 Global step 20 Train loss 2.51 on epoch=9
06/24/2022 14:41:36 - INFO - __main__ - Step 30 Global step 30 Train loss 1.53 on epoch=14
06/24/2022 14:41:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.88 on epoch=19
06/24/2022 14:41:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.67 on epoch=24
06/24/2022 14:41:39 - INFO - __main__ - Global step 50 Train loss 1.88 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 14:41:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/24/2022 14:41:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=29
06/24/2022 14:41:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=34
06/24/2022 14:41:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=39
06/24/2022 14:41:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=44
06/24/2022 14:41:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
06/24/2022 14:41:45 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.3191489361702127 on epoch=49
06/24/2022 14:41:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=54
06/24/2022 14:41:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=59
06/24/2022 14:41:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=64
06/24/2022 14:41:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=69
06/24/2022 14:41:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=74
06/24/2022 14:41:51 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 14:41:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
06/24/2022 14:41:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.35 on epoch=84
06/24/2022 14:41:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
06/24/2022 14:41:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=94
06/24/2022 14:41:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=99
06/24/2022 14:41:58 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3191489361702127 on epoch=99
06/24/2022 14:41:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=104
06/24/2022 14:42:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=109
06/24/2022 14:42:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
06/24/2022 14:42:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
06/24/2022 14:42:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=124
06/24/2022 14:42:04 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.39756367663344405 on epoch=124
06/24/2022 14:42:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39756367663344405 on epoch=124, global_step=250
06/24/2022 14:42:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=129
06/24/2022 14:42:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
06/24/2022 14:42:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=139
06/24/2022 14:42:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=144
06/24/2022 14:42:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=149
06/24/2022 14:42:10 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.4181818181818182 on epoch=149
06/24/2022 14:42:10 - INFO - __main__ - Saving model with best Classification-F1: 0.39756367663344405 -> 0.4181818181818182 on epoch=149, global_step=300
06/24/2022 14:42:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=154
06/24/2022 14:42:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
06/24/2022 14:42:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=164
06/24/2022 14:42:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=169
06/24/2022 14:42:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=174
06/24/2022 14:42:17 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.46843853820598 on epoch=174
06/24/2022 14:42:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.46843853820598 on epoch=174, global_step=350
06/24/2022 14:42:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=179
06/24/2022 14:42:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=184
06/24/2022 14:42:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=189
06/24/2022 14:42:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=194
06/24/2022 14:42:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=199
06/24/2022 14:42:23 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.46843853820598 on epoch=199
06/24/2022 14:42:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=204
06/24/2022 14:42:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
06/24/2022 14:42:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=214
06/24/2022 14:42:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
06/24/2022 14:42:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/24/2022 14:42:29 - INFO - __main__ - Global step 450 Train loss 0.28 Classification-F1 0.49090909090909085 on epoch=224
06/24/2022 14:42:29 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.49090909090909085 on epoch=224, global_step=450
06/24/2022 14:42:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
06/24/2022 14:42:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=234
06/24/2022 14:42:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=239
06/24/2022 14:42:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=244
06/24/2022 14:42:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=249
06/24/2022 14:42:36 - INFO - __main__ - Global step 500 Train loss 0.29 Classification-F1 0.5151515151515151 on epoch=249
06/24/2022 14:42:36 - INFO - __main__ - Saving model with best Classification-F1: 0.49090909090909085 -> 0.5151515151515151 on epoch=249, global_step=500
06/24/2022 14:42:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=254
06/24/2022 14:42:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
06/24/2022 14:42:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
06/24/2022 14:42:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=269
06/24/2022 14:42:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
06/24/2022 14:42:42 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.4458874458874459 on epoch=274
06/24/2022 14:42:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
06/24/2022 14:42:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
06/24/2022 14:42:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=289
06/24/2022 14:42:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=294
06/24/2022 14:42:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=299
06/24/2022 14:42:48 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.5333333333333333 on epoch=299
06/24/2022 14:42:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5333333333333333 on epoch=299, global_step=600
06/24/2022 14:42:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=304
06/24/2022 14:42:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=309
06/24/2022 14:42:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=314
06/24/2022 14:42:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
06/24/2022 14:42:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=324
06/24/2022 14:42:55 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.5076923076923077 on epoch=324
06/24/2022 14:42:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=329
06/24/2022 14:42:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=334
06/24/2022 14:42:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=339
06/24/2022 14:42:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=344
06/24/2022 14:43:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=349
06/24/2022 14:43:01 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.4817813765182186 on epoch=349
06/24/2022 14:43:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=354
06/24/2022 14:43:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=359
06/24/2022 14:43:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=364
06/24/2022 14:43:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=369
06/24/2022 14:43:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=374
06/24/2022 14:43:07 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.5076923076923077 on epoch=374
06/24/2022 14:43:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
06/24/2022 14:43:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=384
06/24/2022 14:43:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=389
06/24/2022 14:43:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
06/24/2022 14:43:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=399
06/24/2022 14:43:14 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.5076923076923077 on epoch=399
06/24/2022 14:43:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
06/24/2022 14:43:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
06/24/2022 14:43:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=414
06/24/2022 14:43:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/24/2022 14:43:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/24/2022 14:43:20 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.4285714285714286 on epoch=424
06/24/2022 14:43:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/24/2022 14:43:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
06/24/2022 14:43:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
06/24/2022 14:43:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/24/2022 14:43:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/24/2022 14:43:26 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.4920634920634921 on epoch=449
06/24/2022 14:43:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
06/24/2022 14:43:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
06/24/2022 14:43:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
06/24/2022 14:43:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/24/2022 14:43:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
06/24/2022 14:43:33 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.464039408866995 on epoch=474
06/24/2022 14:43:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
06/24/2022 14:43:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/24/2022 14:43:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
06/24/2022 14:43:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
06/24/2022 14:43:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
06/24/2022 14:43:39 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.464039408866995 on epoch=499
06/24/2022 14:43:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
06/24/2022 14:43:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/24/2022 14:43:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
06/24/2022 14:43:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/24/2022 14:43:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/24/2022 14:43:46 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=524
06/24/2022 14:43:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/24/2022 14:43:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
06/24/2022 14:43:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 14:43:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/24/2022 14:43:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 14:43:52 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=549
06/24/2022 14:43:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/24/2022 14:43:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/24/2022 14:43:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 14:43:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
06/24/2022 14:43:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 14:43:59 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=574
06/24/2022 14:44:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
06/24/2022 14:44:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/24/2022 14:44:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/24/2022 14:44:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
06/24/2022 14:44:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 14:44:05 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=599
06/24/2022 14:44:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 14:44:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 14:44:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/24/2022 14:44:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/24/2022 14:44:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 14:44:11 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=624
06/24/2022 14:44:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 14:44:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 14:44:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/24/2022 14:44:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 14:44:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/24/2022 14:44:18 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=649
06/24/2022 14:44:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 14:44:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/24/2022 14:44:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 14:44:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 14:44:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 14:44:24 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=674
06/24/2022 14:44:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 14:44:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 14:44:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 14:44:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/24/2022 14:44:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 14:44:30 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.4285714285714286 on epoch=699
06/24/2022 14:44:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 14:44:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/24/2022 14:44:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 14:44:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 14:44:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 14:44:37 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=724
06/24/2022 14:44:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 14:44:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 14:44:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/24/2022 14:44:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 14:44:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 14:44:43 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=749
06/24/2022 14:44:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 14:44:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 14:44:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 14:44:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
06/24/2022 14:44:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 14:44:49 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=774
06/24/2022 14:44:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 14:44:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 14:44:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 14:44:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 14:44:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 14:44:56 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=799
06/24/2022 14:44:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 14:44:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
06/24/2022 14:44:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 14:45:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 14:45:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 14:45:02 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=824
06/24/2022 14:45:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 14:45:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 14:45:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 14:45:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 14:45:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 14:45:09 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=849
06/24/2022 14:45:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 14:45:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 14:45:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 14:45:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 14:45:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 14:45:15 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=874
06/24/2022 14:45:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 14:45:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 14:45:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 14:45:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 14:45:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 14:45:21 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=899
06/24/2022 14:45:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 14:45:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 14:45:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 14:45:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 14:45:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 14:45:28 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5465587044534412 on epoch=924
06/24/2022 14:45:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5465587044534412 on epoch=924, global_step=1850
06/24/2022 14:45:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/24/2022 14:45:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 14:45:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/24/2022 14:45:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 14:45:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 14:45:34 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5076923076923077 on epoch=949
06/24/2022 14:45:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 14:45:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 14:45:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 14:45:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 14:45:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 14:45:40 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4666666666666667 on epoch=974
06/24/2022 14:45:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 14:45:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 14:45:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 14:45:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 14:45:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 14:45:46 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=999
06/24/2022 14:45:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5465587044534412 -> 0.5555555555555556 on epoch=999, global_step=2000
06/24/2022 14:45:46 - INFO - __main__ - save last model!
06/24/2022 14:45:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 14:45:46 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 14:45:46 - INFO - __main__ - Printing 3 examples
06/24/2022 14:45:46 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 14:45:46 - INFO - __main__ - ['0']
06/24/2022 14:45:46 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 14:45:46 - INFO - __main__ - ['1']
06/24/2022 14:45:46 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 14:45:46 - INFO - __main__ - ['1']
06/24/2022 14:45:46 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:45:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:45:47 - INFO - __main__ - Printing 3 examples
06/24/2022 14:45:47 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 14:45:47 - INFO - __main__ - ['1']
06/24/2022 14:45:47 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 14:45:47 - INFO - __main__ - ['1']
06/24/2022 14:45:47 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 14:45:47 - INFO - __main__ - ['1']
06/24/2022 14:45:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:45:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:45:47 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:45:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:45:47 - INFO - __main__ - Printing 3 examples
06/24/2022 14:45:47 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 14:45:47 - INFO - __main__ - ['1']
06/24/2022 14:45:47 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 14:45:47 - INFO - __main__ - ['1']
06/24/2022 14:45:47 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 14:45:47 - INFO - __main__ - ['1']
06/24/2022 14:45:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:45:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:45:47 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:45:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:45:53 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:45:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:45:53 - INFO - __main__ - Starting training!
06/24/2022 14:45:58 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 14:47:30 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_13_0.4_8_predictions.txt
06/24/2022 14:47:30 - INFO - __main__ - Classification-F1 on test data: 0.0639
06/24/2022 14:47:30 - INFO - __main__ - prefix=paws_16_13, lr=0.4, bsz=8, dev_performance=0.5555555555555556, test_performance=0.06385326301734412
06/24/2022 14:47:30 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.3, bsz=8 ...
06/24/2022 14:47:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:47:31 - INFO - __main__ - Printing 3 examples
06/24/2022 14:47:31 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 14:47:31 - INFO - __main__ - ['1']
06/24/2022 14:47:31 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 14:47:31 - INFO - __main__ - ['1']
06/24/2022 14:47:31 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 14:47:31 - INFO - __main__ - ['1']
06/24/2022 14:47:31 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:47:31 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:47:31 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:47:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:47:31 - INFO - __main__ - Printing 3 examples
06/24/2022 14:47:31 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 14:47:31 - INFO - __main__ - ['1']
06/24/2022 14:47:31 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 14:47:31 - INFO - __main__ - ['1']
06/24/2022 14:47:31 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 14:47:31 - INFO - __main__ - ['1']
06/24/2022 14:47:31 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:47:31 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:47:31 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:47:37 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:47:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:47:37 - INFO - __main__ - Starting training!
06/24/2022 14:47:39 - INFO - __main__ - Step 10 Global step 10 Train loss 4.06 on epoch=4
06/24/2022 14:47:40 - INFO - __main__ - Step 20 Global step 20 Train loss 3.17 on epoch=9
06/24/2022 14:47:41 - INFO - __main__ - Step 30 Global step 30 Train loss 2.13 on epoch=14
06/24/2022 14:47:43 - INFO - __main__ - Step 40 Global step 40 Train loss 1.38 on epoch=19
06/24/2022 14:47:44 - INFO - __main__ - Step 50 Global step 50 Train loss 1.04 on epoch=24
06/24/2022 14:47:44 - INFO - __main__ - Global step 50 Train loss 2.36 Classification-F1 0.4285714285714286 on epoch=24
06/24/2022 14:47:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4285714285714286 on epoch=24, global_step=50
06/24/2022 14:47:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.74 on epoch=29
06/24/2022 14:47:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=34
06/24/2022 14:47:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=39
06/24/2022 14:47:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=44
06/24/2022 14:47:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=49
06/24/2022 14:47:51 - INFO - __main__ - Global step 100 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 14:47:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=54
06/24/2022 14:47:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=59
06/24/2022 14:47:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=64
06/24/2022 14:47:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=69
06/24/2022 14:47:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=74
06/24/2022 14:47:57 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 14:47:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=79
06/24/2022 14:48:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=84
06/24/2022 14:48:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
06/24/2022 14:48:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=94
06/24/2022 14:48:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=99
06/24/2022 14:48:04 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 14:48:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=104
06/24/2022 14:48:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=109
06/24/2022 14:48:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
06/24/2022 14:48:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=119
06/24/2022 14:48:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=124
06/24/2022 14:48:11 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 14:48:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
06/24/2022 14:48:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=134
06/24/2022 14:48:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=139
06/24/2022 14:48:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=144
06/24/2022 14:48:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=149
06/24/2022 14:48:17 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 14:48:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=154
06/24/2022 14:48:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
06/24/2022 14:48:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=164
06/24/2022 14:48:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=169
06/24/2022 14:48:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=174
06/24/2022 14:48:24 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.3191489361702127 on epoch=174
06/24/2022 14:48:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=179
06/24/2022 14:48:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=184
06/24/2022 14:48:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=189
06/24/2022 14:48:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=194
06/24/2022 14:48:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=199
06/24/2022 14:48:31 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 14:48:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.32 on epoch=204
06/24/2022 14:48:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=209
06/24/2022 14:48:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=214
06/24/2022 14:48:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=219
06/24/2022 14:48:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=224
06/24/2022 14:48:37 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.4554554554554554 on epoch=224
06/24/2022 14:48:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4285714285714286 -> 0.4554554554554554 on epoch=224, global_step=450
06/24/2022 14:48:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=229
06/24/2022 14:48:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=234
06/24/2022 14:48:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=239
06/24/2022 14:48:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=244
06/24/2022 14:48:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=249
06/24/2022 14:48:44 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.4458874458874459 on epoch=249
06/24/2022 14:48:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=254
06/24/2022 14:48:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=259
06/24/2022 14:48:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=264
06/24/2022 14:48:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
06/24/2022 14:48:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=274
06/24/2022 14:48:51 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.4458874458874459 on epoch=274
06/24/2022 14:48:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=279
06/24/2022 14:48:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
06/24/2022 14:48:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=289
06/24/2022 14:48:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=294
06/24/2022 14:48:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=299
06/24/2022 14:48:57 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.4231177094379639 on epoch=299
06/24/2022 14:48:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=304
06/24/2022 14:49:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=309
06/24/2022 14:49:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=314
06/24/2022 14:49:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=319
06/24/2022 14:49:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=324
06/24/2022 14:49:04 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.4231177094379639 on epoch=324
06/24/2022 14:49:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=329
06/24/2022 14:49:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=334
06/24/2022 14:49:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=339
06/24/2022 14:49:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
06/24/2022 14:49:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
06/24/2022 14:49:11 - INFO - __main__ - Global step 700 Train loss 0.26 Classification-F1 0.4817813765182186 on epoch=349
06/24/2022 14:49:11 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.4817813765182186 on epoch=349, global_step=700
06/24/2022 14:49:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=354
06/24/2022 14:49:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=359
06/24/2022 14:49:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=364
06/24/2022 14:49:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
06/24/2022 14:49:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
06/24/2022 14:49:17 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.5076923076923077 on epoch=374
06/24/2022 14:49:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4817813765182186 -> 0.5076923076923077 on epoch=374, global_step=750
06/24/2022 14:49:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=379
06/24/2022 14:49:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=384
06/24/2022 14:49:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=389
06/24/2022 14:49:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=394
06/24/2022 14:49:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=399
06/24/2022 14:49:24 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.5076923076923077 on epoch=399
06/24/2022 14:49:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=404
06/24/2022 14:49:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=409
06/24/2022 14:49:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=414
06/24/2022 14:49:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=419
06/24/2022 14:49:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=424
06/24/2022 14:49:31 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.4554554554554554 on epoch=424
06/24/2022 14:49:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=429
06/24/2022 14:49:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=434
06/24/2022 14:49:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=439
06/24/2022 14:49:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=444
06/24/2022 14:49:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=449
06/24/2022 14:49:37 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.4920634920634921 on epoch=449
06/24/2022 14:49:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=454
06/24/2022 14:49:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=459
06/24/2022 14:49:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
06/24/2022 14:49:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
06/24/2022 14:49:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=474
06/24/2022 14:49:44 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.5733333333333335 on epoch=474
06/24/2022 14:49:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5733333333333335 on epoch=474, global_step=950
06/24/2022 14:49:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/24/2022 14:49:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
06/24/2022 14:49:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
06/24/2022 14:49:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
06/24/2022 14:49:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=499
06/24/2022 14:49:51 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.4980392156862745 on epoch=499
06/24/2022 14:49:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
06/24/2022 14:49:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/24/2022 14:49:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/24/2022 14:49:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
06/24/2022 14:49:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
06/24/2022 14:49:58 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.5307917888563051 on epoch=524
06/24/2022 14:49:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
06/24/2022 14:50:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
06/24/2022 14:50:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
06/24/2022 14:50:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
06/24/2022 14:50:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
06/24/2022 14:50:04 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.5465587044534412 on epoch=549
06/24/2022 14:50:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/24/2022 14:50:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
06/24/2022 14:50:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
06/24/2022 14:50:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 14:50:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 14:50:11 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.5465587044534412 on epoch=574
06/24/2022 14:50:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/24/2022 14:50:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
06/24/2022 14:50:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
06/24/2022 14:50:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
06/24/2022 14:50:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
06/24/2022 14:50:18 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=599
06/24/2022 14:50:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 14:50:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 14:50:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/24/2022 14:50:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/24/2022 14:50:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/24/2022 14:50:24 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.5835835835835835 on epoch=624
06/24/2022 14:50:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5733333333333335 -> 0.5835835835835835 on epoch=624, global_step=1250
06/24/2022 14:50:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 14:50:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
06/24/2022 14:50:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 14:50:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
06/24/2022 14:50:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
06/24/2022 14:50:31 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4980392156862745 on epoch=649
06/24/2022 14:50:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
06/24/2022 14:50:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/24/2022 14:50:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/24/2022 14:50:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 14:50:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/24/2022 14:50:38 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.5 on epoch=674
06/24/2022 14:50:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/24/2022 14:50:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/24/2022 14:50:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/24/2022 14:50:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/24/2022 14:50:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 14:50:45 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=699
06/24/2022 14:50:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 14:50:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 14:50:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/24/2022 14:50:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/24/2022 14:50:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 14:50:51 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.5465587044534412 on epoch=724
06/24/2022 14:50:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/24/2022 14:50:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 14:50:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 14:50:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 14:50:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 14:50:58 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=749
06/24/2022 14:50:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 14:51:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/24/2022 14:51:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 14:51:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/24/2022 14:51:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 14:51:05 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=774
06/24/2022 14:51:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 14:51:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 14:51:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 14:51:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/24/2022 14:51:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 14:51:11 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=799
06/24/2022 14:51:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 14:51:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 14:51:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 14:51:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 14:51:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
06/24/2022 14:51:17 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=824
06/24/2022 14:51:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 14:51:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/24/2022 14:51:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
06/24/2022 14:51:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 14:51:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 14:51:24 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=849
06/24/2022 14:51:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 14:51:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/24/2022 14:51:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 14:51:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/24/2022 14:51:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 14:51:31 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5 on epoch=874
06/24/2022 14:51:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 14:51:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 14:51:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/24/2022 14:51:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 14:51:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 14:51:38 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=899
06/24/2022 14:51:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 14:51:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 14:51:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/24/2022 14:51:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 14:51:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/24/2022 14:51:44 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5 on epoch=924
06/24/2022 14:51:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 14:51:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 14:51:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 14:51:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 14:51:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/24/2022 14:51:51 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5 on epoch=949
06/24/2022 14:51:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/24/2022 14:51:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 14:51:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 14:51:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 14:51:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 14:51:58 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5835835835835835 on epoch=974
06/24/2022 14:51:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 14:52:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 14:52:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 14:52:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 14:52:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/24/2022 14:52:04 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5 on epoch=999
06/24/2022 14:52:04 - INFO - __main__ - save last model!
06/24/2022 14:52:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 14:52:04 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 14:52:04 - INFO - __main__ - Printing 3 examples
06/24/2022 14:52:04 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 14:52:04 - INFO - __main__ - ['0']
06/24/2022 14:52:04 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 14:52:04 - INFO - __main__ - ['1']
06/24/2022 14:52:04 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 14:52:04 - INFO - __main__ - ['1']
06/24/2022 14:52:04 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:52:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:52:05 - INFO - __main__ - Printing 3 examples
06/24/2022 14:52:05 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 14:52:05 - INFO - __main__ - ['1']
06/24/2022 14:52:05 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 14:52:05 - INFO - __main__ - ['1']
06/24/2022 14:52:05 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 14:52:05 - INFO - __main__ - ['1']
06/24/2022 14:52:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:52:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:52:05 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:52:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:52:05 - INFO - __main__ - Printing 3 examples
06/24/2022 14:52:05 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 14:52:05 - INFO - __main__ - ['1']
06/24/2022 14:52:05 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 14:52:05 - INFO - __main__ - ['1']
06/24/2022 14:52:05 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 14:52:05 - INFO - __main__ - ['1']
06/24/2022 14:52:05 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:52:05 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:52:05 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:52:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:52:11 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:52:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:52:11 - INFO - __main__ - Starting training!
06/24/2022 14:52:16 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 14:53:49 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_13_0.3_8_predictions.txt
06/24/2022 14:53:49 - INFO - __main__ - Classification-F1 on test data: 0.5041
06/24/2022 14:53:49 - INFO - __main__ - prefix=paws_16_13, lr=0.3, bsz=8, dev_performance=0.5835835835835835, test_performance=0.50411346210807
06/24/2022 14:53:49 - INFO - __main__ - Running ... prefix=paws_16_13, lr=0.2, bsz=8 ...
06/24/2022 14:53:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:53:50 - INFO - __main__ - Printing 3 examples
06/24/2022 14:53:50 - INFO - __main__ -  [paws] sentence 1: The film was written by AR Murugadoss and co-produced and produced by P. Madhan under the banner of Escape Artists Motion Pictures . [SEP] sentence 2: The film was written and co-produced by AR Murugadoss and produced by P. Madhan under banner of Escape Artists Motion Pictures .
06/24/2022 14:53:50 - INFO - __main__ - ['1']
06/24/2022 14:53:50 - INFO - __main__ -  [paws] sentence 1: Scraper was a hardcore punk band from the West Midlands of the United Kingdom . [SEP] sentence 2: Scraper was a hardcore punk band from the United Kingdom West Midlands .
06/24/2022 14:53:50 - INFO - __main__ - ['1']
06/24/2022 14:53:50 - INFO - __main__ -  [paws] sentence 1: The series was published by Dell ( NY ) for the US issues and Armada ( London ) for the UK editions . [SEP] sentence 2: The series was published by Dell ( NY ) for the US editions , and Armada ( London ) for the UK editions .
06/24/2022 14:53:50 - INFO - __main__ - ['1']
06/24/2022 14:53:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:53:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:53:50 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:53:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:53:50 - INFO - __main__ - Printing 3 examples
06/24/2022 14:53:50 - INFO - __main__ -  [paws] sentence 1: The website originally serves the market of Arizona , San Diego and Las Vegas and expanded to Atlanta in the autumn of 2006 . [SEP] sentence 2: The website originally served the market of Arizona , San Diego , and Las Vegas , and expanded to Atlanta in the fall of 2006 .
06/24/2022 14:53:50 - INFO - __main__ - ['1']
06/24/2022 14:53:50 - INFO - __main__ -  [paws] sentence 1: His mother and wife followed him and left Henry IV to Germany . [SEP] sentence 2: His mother and his wife followed him and left Henry IV to Germany .
06/24/2022 14:53:50 - INFO - __main__ - ['1']
06/24/2022 14:53:50 - INFO - __main__ -  [paws] sentence 1: With this high process , steganographic-quality copies of an original ( e.g . a bank note ) under blue light have become identifiable . [SEP] sentence 2: With this high process , steganographic copies of an original ( e.g . a note ) have become identifiable under blue light .
06/24/2022 14:53:50 - INFO - __main__ - ['1']
06/24/2022 14:53:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:53:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:53:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:53:55 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:53:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:53:56 - INFO - __main__ - Starting training!
06/24/2022 14:53:57 - INFO - __main__ - Step 10 Global step 10 Train loss 4.35 on epoch=4
06/24/2022 14:53:58 - INFO - __main__ - Step 20 Global step 20 Train loss 3.76 on epoch=9
06/24/2022 14:54:00 - INFO - __main__ - Step 30 Global step 30 Train loss 2.95 on epoch=14
06/24/2022 14:54:01 - INFO - __main__ - Step 40 Global step 40 Train loss 2.25 on epoch=19
06/24/2022 14:54:02 - INFO - __main__ - Step 50 Global step 50 Train loss 1.77 on epoch=24
06/24/2022 14:54:03 - INFO - __main__ - Global step 50 Train loss 3.02 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 14:54:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/24/2022 14:54:04 - INFO - __main__ - Step 60 Global step 60 Train loss 1.37 on epoch=29
06/24/2022 14:54:05 - INFO - __main__ - Step 70 Global step 70 Train loss 1.13 on epoch=34
06/24/2022 14:54:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=39
06/24/2022 14:54:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.76 on epoch=44
06/24/2022 14:54:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.73 on epoch=49
06/24/2022 14:54:09 - INFO - __main__ - Global step 100 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 14:54:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=54
06/24/2022 14:54:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.61 on epoch=59
06/24/2022 14:54:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=64
06/24/2022 14:54:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.56 on epoch=69
06/24/2022 14:54:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=74
06/24/2022 14:54:16 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.3191489361702127 on epoch=74
06/24/2022 14:54:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=79
06/24/2022 14:54:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=84
06/24/2022 14:54:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=89
06/24/2022 14:54:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=94
06/24/2022 14:54:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=99
06/24/2022 14:54:22 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 14:54:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=104
06/24/2022 14:54:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=109
06/24/2022 14:54:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=114
06/24/2022 14:54:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=119
06/24/2022 14:54:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=124
06/24/2022 14:54:29 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 14:54:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=129
06/24/2022 14:54:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=134
06/24/2022 14:54:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=139
06/24/2022 14:54:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=144
06/24/2022 14:54:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=149
06/24/2022 14:54:36 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 14:54:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=154
06/24/2022 14:54:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=159
06/24/2022 14:54:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=164
06/24/2022 14:54:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=169
06/24/2022 14:54:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=174
06/24/2022 14:54:42 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 14:54:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=179
06/24/2022 14:54:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=184
06/24/2022 14:54:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=189
06/24/2022 14:54:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
06/24/2022 14:54:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=199
06/24/2022 14:54:49 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 14:54:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=204
06/24/2022 14:54:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=209
06/24/2022 14:54:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=214
06/24/2022 14:54:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=219
06/24/2022 14:54:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=224
06/24/2022 14:54:55 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.36374269005847953 on epoch=224
06/24/2022 14:54:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36374269005847953 on epoch=224, global_step=450
06/24/2022 14:54:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=229
06/24/2022 14:54:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=234
06/24/2022 14:54:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=239
06/24/2022 14:55:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=244
06/24/2022 14:55:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=249
06/24/2022 14:55:02 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.46843853820598 on epoch=249
06/24/2022 14:55:02 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.46843853820598 on epoch=249, global_step=500
06/24/2022 14:55:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=254
06/24/2022 14:55:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=259
06/24/2022 14:55:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=264
06/24/2022 14:55:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
06/24/2022 14:55:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=274
06/24/2022 14:55:09 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.4181818181818182 on epoch=274
06/24/2022 14:55:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=279
06/24/2022 14:55:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=284
06/24/2022 14:55:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=289
06/24/2022 14:55:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=294
06/24/2022 14:55:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=299
06/24/2022 14:55:15 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.4458874458874459 on epoch=299
06/24/2022 14:55:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=304
06/24/2022 14:55:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.30 on epoch=309
06/24/2022 14:55:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=314
06/24/2022 14:55:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=319
06/24/2022 14:55:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=324
06/24/2022 14:55:22 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.4231177094379639 on epoch=324
06/24/2022 14:55:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=329
06/24/2022 14:55:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=334
06/24/2022 14:55:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=339
06/24/2022 14:55:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=344
06/24/2022 14:55:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=349
06/24/2022 14:55:28 - INFO - __main__ - Global step 700 Train loss 0.30 Classification-F1 0.4231177094379639 on epoch=349
06/24/2022 14:55:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=354
06/24/2022 14:55:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=359
06/24/2022 14:55:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=364
06/24/2022 14:55:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=369
06/24/2022 14:55:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=374
06/24/2022 14:55:35 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.4666666666666667 on epoch=374
06/24/2022 14:55:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=379
06/24/2022 14:55:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=384
06/24/2022 14:55:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=389
06/24/2022 14:55:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=394
06/24/2022 14:55:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
06/24/2022 14:55:41 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.4666666666666667 on epoch=399
06/24/2022 14:55:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=404
06/24/2022 14:55:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=409
06/24/2022 14:55:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=414
06/24/2022 14:55:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
06/24/2022 14:55:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=424
06/24/2022 14:55:48 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.5076923076923077 on epoch=424
06/24/2022 14:55:48 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5076923076923077 on epoch=424, global_step=850
06/24/2022 14:55:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=429
06/24/2022 14:55:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=434
06/24/2022 14:55:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=439
06/24/2022 14:55:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=444
06/24/2022 14:55:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=449
06/24/2022 14:55:55 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.5076923076923077 on epoch=449
06/24/2022 14:55:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=454
06/24/2022 14:55:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=459
06/24/2022 14:55:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
06/24/2022 14:56:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=469
06/24/2022 14:56:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=474
06/24/2022 14:56:01 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.4666666666666667 on epoch=474
06/24/2022 14:56:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=479
06/24/2022 14:56:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=484
06/24/2022 14:56:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=489
06/24/2022 14:56:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=494
06/24/2022 14:56:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=499
06/24/2022 14:56:08 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.5333333333333333 on epoch=499
06/24/2022 14:56:08 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5333333333333333 on epoch=499, global_step=1000
06/24/2022 14:56:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=504
06/24/2022 14:56:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=509
06/24/2022 14:56:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=514
06/24/2022 14:56:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=519
06/24/2022 14:56:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=524
06/24/2022 14:56:15 - INFO - __main__ - Global step 1050 Train loss 0.16 Classification-F1 0.5588547189819725 on epoch=524
06/24/2022 14:56:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5588547189819725 on epoch=524, global_step=1050
06/24/2022 14:56:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=529
06/24/2022 14:56:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=534
06/24/2022 14:56:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=539
06/24/2022 14:56:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=544
06/24/2022 14:56:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=549
06/24/2022 14:56:21 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.6190476190476191 on epoch=549
06/24/2022 14:56:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.6190476190476191 on epoch=549, global_step=1100
06/24/2022 14:56:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=554
06/24/2022 14:56:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=559
06/24/2022 14:56:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=564
06/24/2022 14:56:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=569
06/24/2022 14:56:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=574
06/24/2022 14:56:28 - INFO - __main__ - Global step 1150 Train loss 0.12 Classification-F1 0.6476476476476476 on epoch=574
06/24/2022 14:56:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6190476190476191 -> 0.6476476476476476 on epoch=574, global_step=1150
06/24/2022 14:56:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=579
06/24/2022 14:56:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
06/24/2022 14:56:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=589
06/24/2022 14:56:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=594
06/24/2022 14:56:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
06/24/2022 14:56:35 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.6190476190476191 on epoch=599
06/24/2022 14:56:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
06/24/2022 14:56:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=609
06/24/2022 14:56:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=614
06/24/2022 14:56:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
06/24/2022 14:56:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
06/24/2022 14:56:41 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.6000000000000001 on epoch=624
06/24/2022 14:56:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=629
06/24/2022 14:56:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=634
06/24/2022 14:56:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=639
06/24/2022 14:56:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
06/24/2022 14:56:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=649
06/24/2022 14:56:48 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.6113360323886641 on epoch=649
06/24/2022 14:56:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=654
06/24/2022 14:56:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=659
06/24/2022 14:56:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
06/24/2022 14:56:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
06/24/2022 14:56:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=674
06/24/2022 14:56:55 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.5835835835835835 on epoch=674
06/24/2022 14:56:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=679
06/24/2022 14:56:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
06/24/2022 14:56:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=689
06/24/2022 14:57:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
06/24/2022 14:57:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
06/24/2022 14:57:01 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6235294117647059 on epoch=699
06/24/2022 14:57:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=704
06/24/2022 14:57:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/24/2022 14:57:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
06/24/2022 14:57:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
06/24/2022 14:57:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/24/2022 14:57:08 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.6190476190476191 on epoch=724
06/24/2022 14:57:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
06/24/2022 14:57:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
06/24/2022 14:57:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=739
06/24/2022 14:57:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
06/24/2022 14:57:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
06/24/2022 14:57:15 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.5835835835835835 on epoch=749
06/24/2022 14:57:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
06/24/2022 14:57:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
06/24/2022 14:57:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
06/24/2022 14:57:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
06/24/2022 14:57:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/24/2022 14:57:21 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6113360323886641 on epoch=774
06/24/2022 14:57:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=779
06/24/2022 14:57:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
06/24/2022 14:57:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 14:57:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/24/2022 14:57:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/24/2022 14:57:28 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.5835835835835835 on epoch=799
06/24/2022 14:57:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
06/24/2022 14:57:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
06/24/2022 14:57:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 14:57:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
06/24/2022 14:57:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=824
06/24/2022 14:57:35 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.5270935960591133 on epoch=824
06/24/2022 14:57:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
06/24/2022 14:57:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
06/24/2022 14:57:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
06/24/2022 14:57:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/24/2022 14:57:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 14:57:41 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.5076923076923077 on epoch=849
06/24/2022 14:57:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/24/2022 14:57:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/24/2022 14:57:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/24/2022 14:57:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
06/24/2022 14:57:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/24/2022 14:57:48 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=874
06/24/2022 14:57:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 14:57:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=884
06/24/2022 14:57:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
06/24/2022 14:57:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/24/2022 14:57:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 14:57:55 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.4817813765182186 on epoch=899
06/24/2022 14:57:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/24/2022 14:57:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
06/24/2022 14:57:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/24/2022 14:58:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
06/24/2022 14:58:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/24/2022 14:58:01 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=924
06/24/2022 14:58:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/24/2022 14:58:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 14:58:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/24/2022 14:58:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 14:58:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/24/2022 14:58:08 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=949
06/24/2022 14:58:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=954
06/24/2022 14:58:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
06/24/2022 14:58:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 14:58:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/24/2022 14:58:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
06/24/2022 14:58:15 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.5607843137254902 on epoch=974
06/24/2022 14:58:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
06/24/2022 14:58:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/24/2022 14:58:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 14:58:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/24/2022 14:58:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/24/2022 14:58:22 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=999
06/24/2022 14:58:22 - INFO - __main__ - save last model!
06/24/2022 14:58:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 14:58:22 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 14:58:22 - INFO - __main__ - Printing 3 examples
06/24/2022 14:58:22 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 14:58:22 - INFO - __main__ - ['0']
06/24/2022 14:58:22 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 14:58:22 - INFO - __main__ - ['1']
06/24/2022 14:58:22 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 14:58:22 - INFO - __main__ - ['1']
06/24/2022 14:58:22 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:58:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:58:22 - INFO - __main__ - Printing 3 examples
06/24/2022 14:58:22 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 14:58:22 - INFO - __main__ - ['1']
06/24/2022 14:58:22 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 14:58:22 - INFO - __main__ - ['1']
06/24/2022 14:58:22 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 14:58:22 - INFO - __main__ - ['1']
06/24/2022 14:58:22 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:58:22 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:58:22 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 14:58:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 14:58:22 - INFO - __main__ - Printing 3 examples
06/24/2022 14:58:22 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 14:58:22 - INFO - __main__ - ['1']
06/24/2022 14:58:22 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 14:58:22 - INFO - __main__ - ['1']
06/24/2022 14:58:22 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 14:58:22 - INFO - __main__ - ['1']
06/24/2022 14:58:22 - INFO - __main__ - Tokenizing Input ...
06/24/2022 14:58:22 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:58:22 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 14:58:26 - INFO - __main__ - Tokenizing Output ...
06/24/2022 14:58:28 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 14:58:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 14:58:28 - INFO - __main__ - Starting training!
06/24/2022 14:58:34 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 15:00:13 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_13_0.2_8_predictions.txt
06/24/2022 15:00:13 - INFO - __main__ - Classification-F1 on test data: 0.1973
06/24/2022 15:00:13 - INFO - __main__ - prefix=paws_16_13, lr=0.2, bsz=8, dev_performance=0.6476476476476476, test_performance=0.19725137298362272
06/24/2022 15:00:13 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.5, bsz=8 ...
06/24/2022 15:00:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:00:14 - INFO - __main__ - Printing 3 examples
06/24/2022 15:00:14 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 15:00:14 - INFO - __main__ - ['1']
06/24/2022 15:00:14 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 15:00:14 - INFO - __main__ - ['1']
06/24/2022 15:00:14 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 15:00:14 - INFO - __main__ - ['1']
06/24/2022 15:00:14 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:00:14 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:00:14 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:00:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:00:14 - INFO - __main__ - Printing 3 examples
06/24/2022 15:00:14 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 15:00:14 - INFO - __main__ - ['1']
06/24/2022 15:00:14 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 15:00:14 - INFO - __main__ - ['1']
06/24/2022 15:00:14 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 15:00:14 - INFO - __main__ - ['1']
06/24/2022 15:00:14 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:00:14 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:00:14 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:00:20 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:00:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:00:20 - INFO - __main__ - Starting training!
06/24/2022 15:00:22 - INFO - __main__ - Step 10 Global step 10 Train loss 3.71 on epoch=4
06/24/2022 15:00:23 - INFO - __main__ - Step 20 Global step 20 Train loss 2.06 on epoch=9
06/24/2022 15:00:24 - INFO - __main__ - Step 30 Global step 30 Train loss 1.11 on epoch=14
06/24/2022 15:00:25 - INFO - __main__ - Step 40 Global step 40 Train loss 0.80 on epoch=19
06/24/2022 15:00:27 - INFO - __main__ - Step 50 Global step 50 Train loss 0.63 on epoch=24
06/24/2022 15:00:27 - INFO - __main__ - Global step 50 Train loss 1.66 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 15:00:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/24/2022 15:00:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=29
06/24/2022 15:00:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=34
06/24/2022 15:00:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=39
06/24/2022 15:00:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
06/24/2022 15:00:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=49
06/24/2022 15:00:34 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 15:00:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=54
06/24/2022 15:00:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=59
06/24/2022 15:00:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=64
06/24/2022 15:00:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=69
06/24/2022 15:00:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=74
06/24/2022 15:00:40 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 15:00:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=79
06/24/2022 15:00:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=84
06/24/2022 15:00:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
06/24/2022 15:00:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
06/24/2022 15:00:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=99
06/24/2022 15:00:47 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 15:00:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=104
06/24/2022 15:00:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
06/24/2022 15:00:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=114
06/24/2022 15:00:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=119
06/24/2022 15:00:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=124
06/24/2022 15:00:54 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.36374269005847953 on epoch=124
06/24/2022 15:00:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36374269005847953 on epoch=124, global_step=250
06/24/2022 15:00:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
06/24/2022 15:00:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
06/24/2022 15:00:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
06/24/2022 15:00:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.34 on epoch=144
06/24/2022 15:01:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=149
06/24/2022 15:01:00 - INFO - __main__ - Global step 300 Train loss 0.33 Classification-F1 0.3454545454545454 on epoch=149
06/24/2022 15:01:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/24/2022 15:01:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=159
06/24/2022 15:01:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=164
06/24/2022 15:01:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
06/24/2022 15:01:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
06/24/2022 15:01:07 - INFO - __main__ - Global step 350 Train loss 0.29 Classification-F1 0.39999999999999997 on epoch=174
06/24/2022 15:01:07 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.39999999999999997 on epoch=174, global_step=350
06/24/2022 15:01:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
06/24/2022 15:01:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
06/24/2022 15:01:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
06/24/2022 15:01:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
06/24/2022 15:01:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
06/24/2022 15:01:14 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.2748768472906404 on epoch=199
06/24/2022 15:01:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=204
06/24/2022 15:01:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=209
06/24/2022 15:01:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.17 on epoch=214
06/24/2022 15:01:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=219
06/24/2022 15:01:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.18 on epoch=224
06/24/2022 15:01:20 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.33793103448275863 on epoch=224
06/24/2022 15:01:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.16 on epoch=229
06/24/2022 15:01:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=234
06/24/2022 15:01:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.15 on epoch=239
06/24/2022 15:01:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
06/24/2022 15:01:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=249
06/24/2022 15:01:27 - INFO - __main__ - Global step 500 Train loss 0.15 Classification-F1 0.39999999999999997 on epoch=249
06/24/2022 15:01:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=254
06/24/2022 15:01:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
06/24/2022 15:01:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
06/24/2022 15:01:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
06/24/2022 15:01:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=274
06/24/2022 15:01:34 - INFO - __main__ - Global step 550 Train loss 0.11 Classification-F1 0.34310850439882695 on epoch=274
06/24/2022 15:01:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=279
06/24/2022 15:01:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
06/24/2022 15:01:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
06/24/2022 15:01:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
06/24/2022 15:01:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
06/24/2022 15:01:40 - INFO - __main__ - Global step 600 Train loss 0.06 Classification-F1 0.39139139139139134 on epoch=299
06/24/2022 15:01:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
06/24/2022 15:01:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
06/24/2022 15:01:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/24/2022 15:01:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
06/24/2022 15:01:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/24/2022 15:01:47 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.375 on epoch=324
06/24/2022 15:01:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
06/24/2022 15:01:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
06/24/2022 15:01:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
06/24/2022 15:01:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
06/24/2022 15:01:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
06/24/2022 15:01:54 - INFO - __main__ - Global step 700 Train loss 0.03 Classification-F1 0.41700404858299595 on epoch=349
06/24/2022 15:01:54 - INFO - __main__ - Saving model with best Classification-F1: 0.39999999999999997 -> 0.41700404858299595 on epoch=349, global_step=700
06/24/2022 15:01:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
06/24/2022 15:01:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
06/24/2022 15:01:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/24/2022 15:01:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
06/24/2022 15:02:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/24/2022 15:02:01 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.3650793650793651 on epoch=374
06/24/2022 15:02:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/24/2022 15:02:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/24/2022 15:02:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
06/24/2022 15:02:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
06/24/2022 15:02:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/24/2022 15:02:07 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.34310850439882695 on epoch=399
06/24/2022 15:02:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
06/24/2022 15:02:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/24/2022 15:02:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/24/2022 15:02:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/24/2022 15:02:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/24/2022 15:02:14 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.4009852216748768 on epoch=424
06/24/2022 15:02:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/24/2022 15:02:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/24/2022 15:02:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
06/24/2022 15:02:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
06/24/2022 15:02:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 15:02:20 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.33793103448275863 on epoch=449
06/24/2022 15:02:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/24/2022 15:02:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/24/2022 15:02:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=464
06/24/2022 15:02:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/24/2022 15:02:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/24/2022 15:02:27 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.39139139139139134 on epoch=474
06/24/2022 15:02:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
06/24/2022 15:02:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
06/24/2022 15:02:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/24/2022 15:02:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/24/2022 15:02:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 15:02:34 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=499
06/24/2022 15:02:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
06/24/2022 15:02:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
06/24/2022 15:02:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 15:02:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/24/2022 15:02:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/24/2022 15:02:40 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=524
06/24/2022 15:02:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/24/2022 15:02:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 15:02:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 15:02:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/24/2022 15:02:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 15:02:47 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.2904761904761905 on epoch=549
06/24/2022 15:02:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 15:02:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 15:02:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/24/2022 15:02:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 15:02:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/24/2022 15:02:54 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.375 on epoch=574
06/24/2022 15:02:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
06/24/2022 15:02:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 15:02:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 15:02:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 15:03:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 15:03:00 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=599
06/24/2022 15:03:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 15:03:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 15:03:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 15:03:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/24/2022 15:03:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 15:03:07 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=624
06/24/2022 15:03:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 15:03:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 15:03:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 15:03:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 15:03:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/24/2022 15:03:13 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.41700404858299595 on epoch=649
06/24/2022 15:03:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 15:03:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 15:03:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 15:03:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 15:03:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/24/2022 15:03:20 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=674
06/24/2022 15:03:20 - INFO - __main__ - Saving model with best Classification-F1: 0.41700404858299595 -> 0.4554554554554554 on epoch=674, global_step=1350
06/24/2022 15:03:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 15:03:23 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 15:03:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 15:03:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 15:03:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 15:03:27 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=699
06/24/2022 15:03:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 15:03:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 15:03:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 15:03:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 15:03:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 15:03:34 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=724
06/24/2022 15:03:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 15:03:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 15:03:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 15:03:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 15:03:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 15:03:40 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=749
06/24/2022 15:03:40 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.4682306940371457 on epoch=749, global_step=1500
06/24/2022 15:03:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 15:03:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 15:03:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 15:03:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 15:03:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 15:03:47 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=774
06/24/2022 15:03:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 15:03:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 15:03:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 15:03:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 15:03:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 15:03:54 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=799
06/24/2022 15:03:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 15:03:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 15:03:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 15:03:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 15:04:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 15:04:00 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=824
06/24/2022 15:04:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 15:04:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/24/2022 15:04:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 15:04:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 15:04:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 15:04:07 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5 on epoch=849
06/24/2022 15:04:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4682306940371457 -> 0.5 on epoch=849, global_step=1700
06/24/2022 15:04:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/24/2022 15:04:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 15:04:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 15:04:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/24/2022 15:04:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 15:04:14 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=874
06/24/2022 15:04:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 15:04:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 15:04:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 15:04:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 15:04:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 15:04:20 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=899
06/24/2022 15:04:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 15:04:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 15:04:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 15:04:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 15:04:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 15:04:27 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=924
06/24/2022 15:04:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 15:04:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 15:04:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 15:04:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 15:04:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/24/2022 15:04:34 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=949
06/24/2022 15:04:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 15:04:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 15:04:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 15:04:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 15:04:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 15:04:40 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=974
06/24/2022 15:04:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 15:04:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 15:04:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 15:04:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 15:04:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 15:04:47 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=999
06/24/2022 15:04:47 - INFO - __main__ - save last model!
06/24/2022 15:04:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 15:04:47 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 15:04:47 - INFO - __main__ - Printing 3 examples
06/24/2022 15:04:47 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 15:04:47 - INFO - __main__ - ['0']
06/24/2022 15:04:47 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 15:04:47 - INFO - __main__ - ['1']
06/24/2022 15:04:47 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 15:04:47 - INFO - __main__ - ['1']
06/24/2022 15:04:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:04:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:04:48 - INFO - __main__ - Printing 3 examples
06/24/2022 15:04:48 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 15:04:48 - INFO - __main__ - ['1']
06/24/2022 15:04:48 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 15:04:48 - INFO - __main__ - ['1']
06/24/2022 15:04:48 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 15:04:48 - INFO - __main__ - ['1']
06/24/2022 15:04:48 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:04:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:04:48 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:04:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:04:48 - INFO - __main__ - Printing 3 examples
06/24/2022 15:04:48 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 15:04:48 - INFO - __main__ - ['1']
06/24/2022 15:04:48 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 15:04:48 - INFO - __main__ - ['1']
06/24/2022 15:04:48 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 15:04:48 - INFO - __main__ - ['1']
06/24/2022 15:04:48 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:04:48 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:04:48 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:04:51 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:04:54 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:04:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:04:54 - INFO - __main__ - Starting training!
06/24/2022 15:04:59 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 15:06:26 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_21_0.5_8_predictions.txt
06/24/2022 15:06:26 - INFO - __main__ - Classification-F1 on test data: 0.1378
06/24/2022 15:06:26 - INFO - __main__ - prefix=paws_16_21, lr=0.5, bsz=8, dev_performance=0.5, test_performance=0.13777894654145909
06/24/2022 15:06:26 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.4, bsz=8 ...
06/24/2022 15:06:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:06:27 - INFO - __main__ - Printing 3 examples
06/24/2022 15:06:27 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 15:06:27 - INFO - __main__ - ['1']
06/24/2022 15:06:27 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 15:06:27 - INFO - __main__ - ['1']
06/24/2022 15:06:27 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 15:06:27 - INFO - __main__ - ['1']
06/24/2022 15:06:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:06:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:06:27 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:06:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:06:27 - INFO - __main__ - Printing 3 examples
06/24/2022 15:06:27 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 15:06:27 - INFO - __main__ - ['1']
06/24/2022 15:06:27 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 15:06:27 - INFO - __main__ - ['1']
06/24/2022 15:06:27 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 15:06:27 - INFO - __main__ - ['1']
06/24/2022 15:06:27 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:06:27 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:06:27 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:06:32 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:06:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:06:33 - INFO - __main__ - Starting training!
06/24/2022 15:06:34 - INFO - __main__ - Step 10 Global step 10 Train loss 4.00 on epoch=4
06/24/2022 15:06:35 - INFO - __main__ - Step 20 Global step 20 Train loss 2.71 on epoch=9
06/24/2022 15:06:37 - INFO - __main__ - Step 30 Global step 30 Train loss 1.60 on epoch=14
06/24/2022 15:06:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=19
06/24/2022 15:06:39 - INFO - __main__ - Step 50 Global step 50 Train loss 0.76 on epoch=24
06/24/2022 15:06:40 - INFO - __main__ - Global step 50 Train loss 2.01 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 15:06:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/24/2022 15:06:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.76 on epoch=29
06/24/2022 15:06:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=34
06/24/2022 15:06:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=39
06/24/2022 15:06:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=44
06/24/2022 15:06:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=49
06/24/2022 15:06:46 - INFO - __main__ - Global step 100 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 15:06:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=54
06/24/2022 15:06:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=59
06/24/2022 15:06:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
06/24/2022 15:06:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=69
06/24/2022 15:06:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=74
06/24/2022 15:06:53 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 15:06:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
06/24/2022 15:06:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
06/24/2022 15:06:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=89
06/24/2022 15:06:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=94
06/24/2022 15:06:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=99
06/24/2022 15:07:00 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 15:07:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=104
06/24/2022 15:07:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=109
06/24/2022 15:07:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/24/2022 15:07:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=119
06/24/2022 15:07:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=124
06/24/2022 15:07:06 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 15:07:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
06/24/2022 15:07:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=134
06/24/2022 15:07:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=139
06/24/2022 15:07:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=144
06/24/2022 15:07:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=149
06/24/2022 15:07:13 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 15:07:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=154
06/24/2022 15:07:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=159
06/24/2022 15:07:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=164
06/24/2022 15:07:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=169
06/24/2022 15:07:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=174
06/24/2022 15:07:20 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.4181818181818182 on epoch=174
06/24/2022 15:07:20 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4181818181818182 on epoch=174, global_step=350
06/24/2022 15:07:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=179
06/24/2022 15:07:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=184
06/24/2022 15:07:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=189
06/24/2022 15:07:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=194
06/24/2022 15:07:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=199
06/24/2022 15:07:26 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.4231177094379639 on epoch=199
06/24/2022 15:07:26 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.4231177094379639 on epoch=199, global_step=400
06/24/2022 15:07:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=204
06/24/2022 15:07:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=209
06/24/2022 15:07:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=214
06/24/2022 15:07:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=219
06/24/2022 15:07:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
06/24/2022 15:07:33 - INFO - __main__ - Global step 450 Train loss 0.30 Classification-F1 0.4181818181818182 on epoch=224
06/24/2022 15:07:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
06/24/2022 15:07:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=234
06/24/2022 15:07:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
06/24/2022 15:07:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=244
06/24/2022 15:07:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
06/24/2022 15:07:40 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.4420512820512821 on epoch=249
06/24/2022 15:07:40 - INFO - __main__ - Saving model with best Classification-F1: 0.4231177094379639 -> 0.4420512820512821 on epoch=249, global_step=500
06/24/2022 15:07:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=254
06/24/2022 15:07:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
06/24/2022 15:07:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
06/24/2022 15:07:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=269
06/24/2022 15:07:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=274
06/24/2022 15:07:46 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.39999999999999997 on epoch=274
06/24/2022 15:07:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
06/24/2022 15:07:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=284
06/24/2022 15:07:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=289
06/24/2022 15:07:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
06/24/2022 15:07:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.13 on epoch=299
06/24/2022 15:07:53 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.5270935960591133 on epoch=299
06/24/2022 15:07:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4420512820512821 -> 0.5270935960591133 on epoch=299, global_step=600
06/24/2022 15:07:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
06/24/2022 15:07:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=309
06/24/2022 15:07:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=314
06/24/2022 15:07:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=319
06/24/2022 15:07:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=324
06/24/2022 15:08:00 - INFO - __main__ - Global step 650 Train loss 0.11 Classification-F1 0.4980392156862745 on epoch=324
06/24/2022 15:08:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
06/24/2022 15:08:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=334
06/24/2022 15:08:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=339
06/24/2022 15:08:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=344
06/24/2022 15:08:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
06/24/2022 15:08:06 - INFO - __main__ - Global step 700 Train loss 0.08 Classification-F1 0.5270935960591133 on epoch=349
06/24/2022 15:08:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.09 on epoch=354
06/24/2022 15:08:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=359
06/24/2022 15:08:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=364
06/24/2022 15:08:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=369
06/24/2022 15:08:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
06/24/2022 15:08:13 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.4375 on epoch=374
06/24/2022 15:08:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
06/24/2022 15:08:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=384
06/24/2022 15:08:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
06/24/2022 15:08:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
06/24/2022 15:08:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
06/24/2022 15:08:20 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.4682306940371457 on epoch=399
06/24/2022 15:08:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/24/2022 15:08:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/24/2022 15:08:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
06/24/2022 15:08:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
06/24/2022 15:08:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
06/24/2022 15:08:26 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.4375 on epoch=424
06/24/2022 15:08:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
06/24/2022 15:08:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
06/24/2022 15:08:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/24/2022 15:08:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=444
06/24/2022 15:08:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=449
06/24/2022 15:08:33 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.5270935960591133 on epoch=449
06/24/2022 15:08:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/24/2022 15:08:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
06/24/2022 15:08:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
06/24/2022 15:08:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
06/24/2022 15:08:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
06/24/2022 15:08:40 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.4375 on epoch=474
06/24/2022 15:08:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/24/2022 15:08:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
06/24/2022 15:08:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/24/2022 15:08:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
06/24/2022 15:08:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/24/2022 15:08:47 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.4375 on epoch=499
06/24/2022 15:08:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 15:08:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/24/2022 15:08:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 15:08:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
06/24/2022 15:08:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=524
06/24/2022 15:08:53 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.4375 on epoch=524
06/24/2022 15:08:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/24/2022 15:08:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/24/2022 15:08:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
06/24/2022 15:08:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
06/24/2022 15:09:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 15:09:00 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.4682306940371457 on epoch=549
06/24/2022 15:09:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 15:09:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/24/2022 15:09:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
06/24/2022 15:09:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 15:09:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/24/2022 15:09:07 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.4375 on epoch=574
06/24/2022 15:09:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 15:09:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/24/2022 15:09:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
06/24/2022 15:09:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/24/2022 15:09:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 15:09:13 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=599
06/24/2022 15:09:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 15:09:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 15:09:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 15:09:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 15:09:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
06/24/2022 15:09:20 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=624
06/24/2022 15:09:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 15:09:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 15:09:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 15:09:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 15:09:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/24/2022 15:09:27 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=649
06/24/2022 15:09:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 15:09:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/24/2022 15:09:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 15:09:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 15:09:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 15:09:33 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4375 on epoch=674
06/24/2022 15:09:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 15:09:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
06/24/2022 15:09:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/24/2022 15:09:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/24/2022 15:09:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 15:09:40 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.4375 on epoch=699
06/24/2022 15:09:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 15:09:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 15:09:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
06/24/2022 15:09:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 15:09:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/24/2022 15:09:47 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=724
06/24/2022 15:09:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/24/2022 15:09:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 15:09:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/24/2022 15:09:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 15:09:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 15:09:53 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=749
06/24/2022 15:09:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 15:09:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 15:09:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 15:09:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 15:10:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 15:10:00 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=774
06/24/2022 15:10:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
06/24/2022 15:10:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 15:10:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 15:10:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 15:10:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
06/24/2022 15:10:07 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=799
06/24/2022 15:10:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 15:10:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/24/2022 15:10:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
06/24/2022 15:10:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/24/2022 15:10:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 15:10:13 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=824
06/24/2022 15:10:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 15:10:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/24/2022 15:10:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
06/24/2022 15:10:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/24/2022 15:10:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 15:10:20 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=849
06/24/2022 15:10:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 15:10:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 15:10:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 15:10:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 15:10:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 15:10:27 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=874
06/24/2022 15:10:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 15:10:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 15:10:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/24/2022 15:10:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 15:10:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 15:10:34 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=899
06/24/2022 15:10:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/24/2022 15:10:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 15:10:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 15:10:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 15:10:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 15:10:40 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=924
06/24/2022 15:10:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 15:10:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 15:10:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 15:10:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 15:10:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 15:10:47 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=949
06/24/2022 15:10:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 15:10:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 15:10:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 15:10:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/24/2022 15:10:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 15:10:54 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=974
06/24/2022 15:10:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 15:10:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=984
06/24/2022 15:10:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 15:10:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 15:11:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 15:11:00 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=999
06/24/2022 15:11:00 - INFO - __main__ - save last model!
06/24/2022 15:11:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 15:11:00 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 15:11:00 - INFO - __main__ - Printing 3 examples
06/24/2022 15:11:00 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 15:11:00 - INFO - __main__ - ['0']
06/24/2022 15:11:00 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 15:11:00 - INFO - __main__ - ['1']
06/24/2022 15:11:00 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 15:11:00 - INFO - __main__ - ['1']
06/24/2022 15:11:00 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:11:01 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:11:01 - INFO - __main__ - Printing 3 examples
06/24/2022 15:11:01 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 15:11:01 - INFO - __main__ - ['1']
06/24/2022 15:11:01 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 15:11:01 - INFO - __main__ - ['1']
06/24/2022 15:11:01 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 15:11:01 - INFO - __main__ - ['1']
06/24/2022 15:11:01 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:11:01 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:11:01 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:11:01 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:11:01 - INFO - __main__ - Printing 3 examples
06/24/2022 15:11:01 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 15:11:01 - INFO - __main__ - ['1']
06/24/2022 15:11:01 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 15:11:01 - INFO - __main__ - ['1']
06/24/2022 15:11:01 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 15:11:01 - INFO - __main__ - ['1']
06/24/2022 15:11:01 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:11:01 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:11:01 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:11:04 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:11:07 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:11:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:11:07 - INFO - __main__ - Starting training!
06/24/2022 15:11:12 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 15:12:39 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_21_0.4_8_predictions.txt
06/24/2022 15:12:39 - INFO - __main__ - Classification-F1 on test data: 0.3349
06/24/2022 15:12:39 - INFO - __main__ - prefix=paws_16_21, lr=0.4, bsz=8, dev_performance=0.5270935960591133, test_performance=0.3349194324053435
06/24/2022 15:12:39 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.3, bsz=8 ...
06/24/2022 15:12:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:12:40 - INFO - __main__ - Printing 3 examples
06/24/2022 15:12:40 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 15:12:40 - INFO - __main__ - ['1']
06/24/2022 15:12:40 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 15:12:40 - INFO - __main__ - ['1']
06/24/2022 15:12:40 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 15:12:40 - INFO - __main__ - ['1']
06/24/2022 15:12:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:12:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:12:40 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:12:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:12:40 - INFO - __main__ - Printing 3 examples
06/24/2022 15:12:40 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 15:12:40 - INFO - __main__ - ['1']
06/24/2022 15:12:40 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 15:12:40 - INFO - __main__ - ['1']
06/24/2022 15:12:40 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 15:12:40 - INFO - __main__ - ['1']
06/24/2022 15:12:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:12:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:12:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:12:46 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:12:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:12:46 - INFO - __main__ - Starting training!
06/24/2022 15:12:48 - INFO - __main__ - Step 10 Global step 10 Train loss 4.22 on epoch=4
06/24/2022 15:12:49 - INFO - __main__ - Step 20 Global step 20 Train loss 2.99 on epoch=9
06/24/2022 15:12:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.20 on epoch=14
06/24/2022 15:12:51 - INFO - __main__ - Step 40 Global step 40 Train loss 1.52 on epoch=19
06/24/2022 15:12:53 - INFO - __main__ - Step 50 Global step 50 Train loss 1.11 on epoch=24
06/24/2022 15:12:53 - INFO - __main__ - Global step 50 Train loss 2.41 Classification-F1 0.6532019704433498 on epoch=24
06/24/2022 15:12:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.6532019704433498 on epoch=24, global_step=50
06/24/2022 15:12:54 - INFO - __main__ - Step 60 Global step 60 Train loss 0.81 on epoch=29
06/24/2022 15:12:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=34
06/24/2022 15:12:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.74 on epoch=39
06/24/2022 15:12:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.70 on epoch=44
06/24/2022 15:12:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=49
06/24/2022 15:12:59 - INFO - __main__ - Global step 100 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 15:13:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.61 on epoch=54
06/24/2022 15:13:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=59
06/24/2022 15:13:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=64
06/24/2022 15:13:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=69
06/24/2022 15:13:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=74
06/24/2022 15:13:06 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 15:13:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=79
06/24/2022 15:13:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=84
06/24/2022 15:13:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
06/24/2022 15:13:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=94
06/24/2022 15:13:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=99
06/24/2022 15:13:12 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 15:13:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=104
06/24/2022 15:13:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=109
06/24/2022 15:13:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=114
06/24/2022 15:13:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
06/24/2022 15:13:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=124
06/24/2022 15:13:19 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 15:13:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=129
06/24/2022 15:13:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=134
06/24/2022 15:13:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
06/24/2022 15:13:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=144
06/24/2022 15:13:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=149
06/24/2022 15:13:25 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 15:13:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=154
06/24/2022 15:13:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=159
06/24/2022 15:13:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=164
06/24/2022 15:13:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=169
06/24/2022 15:13:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
06/24/2022 15:13:32 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 15:13:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=179
06/24/2022 15:13:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.29 on epoch=184
06/24/2022 15:13:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.33 on epoch=189
06/24/2022 15:13:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
06/24/2022 15:13:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=199
06/24/2022 15:13:38 - INFO - __main__ - Global step 400 Train loss 0.33 Classification-F1 0.4385964912280702 on epoch=199
06/24/2022 15:13:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=204
06/24/2022 15:13:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=209
06/24/2022 15:13:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=214
06/24/2022 15:13:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=219
06/24/2022 15:13:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=224
06/24/2022 15:13:45 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.4231177094379639 on epoch=224
06/24/2022 15:13:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=229
06/24/2022 15:13:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
06/24/2022 15:13:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=239
06/24/2022 15:13:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=244
06/24/2022 15:13:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=249
06/24/2022 15:13:51 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.39756367663344405 on epoch=249
06/24/2022 15:13:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=254
06/24/2022 15:13:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=259
06/24/2022 15:13:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=264
06/24/2022 15:13:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=269
06/24/2022 15:13:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=274
06/24/2022 15:13:58 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.3764102564102564 on epoch=274
06/24/2022 15:13:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
06/24/2022 15:14:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
06/24/2022 15:14:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=289
06/24/2022 15:14:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=294
06/24/2022 15:14:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
06/24/2022 15:14:04 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.39756367663344405 on epoch=299
06/24/2022 15:14:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=304
06/24/2022 15:14:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
06/24/2022 15:14:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=314
06/24/2022 15:14:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=319
06/24/2022 15:14:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=324
06/24/2022 15:14:11 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.37662337662337664 on epoch=324
06/24/2022 15:14:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=329
06/24/2022 15:14:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=334
06/24/2022 15:14:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=339
06/24/2022 15:14:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=344
06/24/2022 15:14:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=349
06/24/2022 15:14:17 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.4420512820512821 on epoch=349
06/24/2022 15:14:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=354
06/24/2022 15:14:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=359
06/24/2022 15:14:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
06/24/2022 15:14:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=369
06/24/2022 15:14:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=374
06/24/2022 15:14:24 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.41700404858299595 on epoch=374
06/24/2022 15:14:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
06/24/2022 15:14:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
06/24/2022 15:14:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=389
06/24/2022 15:14:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
06/24/2022 15:14:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=399
06/24/2022 15:14:30 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.43529411764705883 on epoch=399
06/24/2022 15:14:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=404
06/24/2022 15:14:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=409
06/24/2022 15:14:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=414
06/24/2022 15:14:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
06/24/2022 15:14:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
06/24/2022 15:14:37 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.40566959921798634 on epoch=424
06/24/2022 15:14:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
06/24/2022 15:14:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
06/24/2022 15:14:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
06/24/2022 15:14:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=444
06/24/2022 15:14:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
06/24/2022 15:14:43 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.4554554554554554 on epoch=449
06/24/2022 15:14:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=454
06/24/2022 15:14:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=459
06/24/2022 15:14:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
06/24/2022 15:14:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
06/24/2022 15:14:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/24/2022 15:14:50 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.4554554554554554 on epoch=474
06/24/2022 15:14:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
06/24/2022 15:14:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
06/24/2022 15:14:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
06/24/2022 15:14:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
06/24/2022 15:14:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
06/24/2022 15:14:56 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.4420512820512821 on epoch=499
06/24/2022 15:14:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
06/24/2022 15:14:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
06/24/2022 15:15:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
06/24/2022 15:15:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
06/24/2022 15:15:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
06/24/2022 15:15:03 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.40566959921798634 on epoch=524
06/24/2022 15:15:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
06/24/2022 15:15:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
06/24/2022 15:15:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 15:15:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=544
06/24/2022 15:15:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
06/24/2022 15:15:10 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.4285714285714286 on epoch=549
06/24/2022 15:15:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
06/24/2022 15:15:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
06/24/2022 15:15:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 15:15:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
06/24/2022 15:15:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 15:15:16 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.40566959921798634 on epoch=574
06/24/2022 15:15:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 15:15:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/24/2022 15:15:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/24/2022 15:15:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/24/2022 15:15:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
06/24/2022 15:15:23 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.4420512820512821 on epoch=599
06/24/2022 15:15:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
06/24/2022 15:15:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
06/24/2022 15:15:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/24/2022 15:15:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/24/2022 15:15:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
06/24/2022 15:15:29 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.4009852216748768 on epoch=624
06/24/2022 15:15:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 15:15:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 15:15:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
06/24/2022 15:15:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 15:15:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/24/2022 15:15:36 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=649
06/24/2022 15:15:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 15:15:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/24/2022 15:15:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/24/2022 15:15:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 15:15:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/24/2022 15:15:42 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.4817813765182186 on epoch=674
06/24/2022 15:15:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 15:15:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/24/2022 15:15:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/24/2022 15:15:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/24/2022 15:15:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/24/2022 15:15:49 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=699
06/24/2022 15:15:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 15:15:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
06/24/2022 15:15:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/24/2022 15:15:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/24/2022 15:15:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
06/24/2022 15:15:56 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=724
06/24/2022 15:15:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/24/2022 15:15:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/24/2022 15:15:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/24/2022 15:16:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
06/24/2022 15:16:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/24/2022 15:16:02 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=749
06/24/2022 15:16:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 15:16:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
06/24/2022 15:16:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 15:16:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/24/2022 15:16:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/24/2022 15:16:09 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.4420512820512821 on epoch=774
06/24/2022 15:16:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
06/24/2022 15:16:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 15:16:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 15:16:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/24/2022 15:16:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/24/2022 15:16:16 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.375 on epoch=799
06/24/2022 15:16:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 15:16:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/24/2022 15:16:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 15:16:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 15:16:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 15:16:22 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=824
06/24/2022 15:16:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 15:16:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 15:16:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/24/2022 15:16:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 15:16:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 15:16:29 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4375 on epoch=849
06/24/2022 15:16:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=854
06/24/2022 15:16:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
06/24/2022 15:16:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 15:16:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 15:16:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/24/2022 15:16:35 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=874
06/24/2022 15:16:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 15:16:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/24/2022 15:16:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/24/2022 15:16:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 15:16:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 15:16:42 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.375 on epoch=899
06/24/2022 15:16:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 15:16:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/24/2022 15:16:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/24/2022 15:16:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 15:16:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/24/2022 15:16:48 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=924
06/24/2022 15:16:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 15:16:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 15:16:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 15:16:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/24/2022 15:16:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 15:16:55 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.40566959921798634 on epoch=949
06/24/2022 15:16:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 15:16:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 15:16:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 15:16:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 15:17:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 15:17:01 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.37254901960784315 on epoch=974
06/24/2022 15:17:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/24/2022 15:17:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 15:17:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 15:17:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/24/2022 15:17:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
06/24/2022 15:17:07 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=999
06/24/2022 15:17:07 - INFO - __main__ - save last model!
06/24/2022 15:17:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 15:17:07 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 15:17:07 - INFO - __main__ - Printing 3 examples
06/24/2022 15:17:07 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 15:17:07 - INFO - __main__ - ['0']
06/24/2022 15:17:07 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 15:17:07 - INFO - __main__ - ['1']
06/24/2022 15:17:07 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 15:17:07 - INFO - __main__ - ['1']
06/24/2022 15:17:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:17:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:17:08 - INFO - __main__ - Printing 3 examples
06/24/2022 15:17:08 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 15:17:08 - INFO - __main__ - ['1']
06/24/2022 15:17:08 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 15:17:08 - INFO - __main__ - ['1']
06/24/2022 15:17:08 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 15:17:08 - INFO - __main__ - ['1']
06/24/2022 15:17:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:17:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:17:08 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:17:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:17:08 - INFO - __main__ - Printing 3 examples
06/24/2022 15:17:08 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 15:17:08 - INFO - __main__ - ['1']
06/24/2022 15:17:08 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 15:17:08 - INFO - __main__ - ['1']
06/24/2022 15:17:08 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 15:17:08 - INFO - __main__ - ['1']
06/24/2022 15:17:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:17:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:17:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:17:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:17:14 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:17:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:17:14 - INFO - __main__ - Starting training!
06/24/2022 15:17:19 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 15:18:46 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_21_0.3_8_predictions.txt
06/24/2022 15:18:46 - INFO - __main__ - Classification-F1 on test data: 0.4941
06/24/2022 15:18:46 - INFO - __main__ - prefix=paws_16_21, lr=0.3, bsz=8, dev_performance=0.6532019704433498, test_performance=0.4941380241398986
06/24/2022 15:18:46 - INFO - __main__ - Running ... prefix=paws_16_21, lr=0.2, bsz=8 ...
06/24/2022 15:18:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:18:47 - INFO - __main__ - Printing 3 examples
06/24/2022 15:18:47 - INFO - __main__ -  [paws] sentence 1: The incumbents were re-elected Church , Tremain , Vanderpoel , and Johnson , while the incumbent , Richmond , was defeated . [SEP] sentence 2: The incumbents Church , Tremain , Vanderpoel and Johnson were re-elected . The incumbent Richmond was defeated .
06/24/2022 15:18:47 - INFO - __main__ - ['1']
06/24/2022 15:18:47 - INFO - __main__ -  [paws] sentence 1: The main ferry ran to 42nd Street and for short time was a component of the transcontinental Lincoln Highway . [SEP] sentence 2: The main ferry ran to 42nd Street and was part of the transcontinental Lincoln Highway for a short time .
06/24/2022 15:18:47 - INFO - __main__ - ['1']
06/24/2022 15:18:47 - INFO - __main__ -  [paws] sentence 1: Custer County , Nebraska , United States is a village in Oconto . [SEP] sentence 2: Custer County , Nebraska , United States of America is a village in Oconto .
06/24/2022 15:18:47 - INFO - __main__ - ['1']
06/24/2022 15:18:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:18:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:18:47 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:18:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:18:47 - INFO - __main__ - Printing 3 examples
06/24/2022 15:18:47 - INFO - __main__ -  [paws] sentence 1: Elizabeth Wood 's first memorable encounter with Thomas Kane was at the age of six when he was twenty years old . [SEP] sentence 2: Elizabeth Wood 's first memorable encounter with Thomas Kane was at six years old when he was twenty .
06/24/2022 15:18:47 - INFO - __main__ - ['1']
06/24/2022 15:18:47 - INFO - __main__ -  [paws] sentence 1: Thomas Enqvist won the tournament , beating Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) . [SEP] sentence 2: The tournament won Thomas Thomas Enqvist and struck Brett Steven in the final , 4 -- 6 , 6 -- 3 , 7 -- 6 ( 0 ) .
06/24/2022 15:18:47 - INFO - __main__ - ['1']
06/24/2022 15:18:47 - INFO - __main__ -  [paws] sentence 1: His son was the scientist Francis Ratcliffe , and one of his two daughters was married to the neurophysiologist W. Grey Walter , whose grandson Nicholas Walter was . [SEP] sentence 2: Ratcliffe 's son was the scientist Francis Ratcliffe , and one of his two daughters married the neurophysiologist W. Grey Walter . Nicholas Walter was his grandson .
06/24/2022 15:18:47 - INFO - __main__ - ['1']
06/24/2022 15:18:47 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:18:47 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:18:47 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:18:53 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:18:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:18:53 - INFO - __main__ - Starting training!
06/24/2022 15:18:54 - INFO - __main__ - Step 10 Global step 10 Train loss 4.33 on epoch=4
06/24/2022 15:18:56 - INFO - __main__ - Step 20 Global step 20 Train loss 3.48 on epoch=9
06/24/2022 15:18:57 - INFO - __main__ - Step 30 Global step 30 Train loss 2.83 on epoch=14
06/24/2022 15:18:58 - INFO - __main__ - Step 40 Global step 40 Train loss 2.20 on epoch=19
06/24/2022 15:18:59 - INFO - __main__ - Step 50 Global step 50 Train loss 1.84 on epoch=24
06/24/2022 15:19:00 - INFO - __main__ - Global step 50 Train loss 2.94 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 15:19:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/24/2022 15:19:01 - INFO - __main__ - Step 60 Global step 60 Train loss 1.39 on epoch=29
06/24/2022 15:19:02 - INFO - __main__ - Step 70 Global step 70 Train loss 1.11 on epoch=34
06/24/2022 15:19:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=39
06/24/2022 15:19:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=44
06/24/2022 15:19:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.79 on epoch=49
06/24/2022 15:19:06 - INFO - __main__ - Global step 100 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 15:19:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.72 on epoch=54
06/24/2022 15:19:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.63 on epoch=59
06/24/2022 15:19:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=64
06/24/2022 15:19:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=69
06/24/2022 15:19:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=74
06/24/2022 15:19:12 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 15:19:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=79
06/24/2022 15:19:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=84
06/24/2022 15:19:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=89
06/24/2022 15:19:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.61 on epoch=94
06/24/2022 15:19:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=99
06/24/2022 15:19:19 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 15:19:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=104
06/24/2022 15:19:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=109
06/24/2022 15:19:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=114
06/24/2022 15:19:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=119
06/24/2022 15:19:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=124
06/24/2022 15:19:25 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 15:19:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
06/24/2022 15:19:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=134
06/24/2022 15:19:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=139
06/24/2022 15:19:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=144
06/24/2022 15:19:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=149
06/24/2022 15:19:31 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 15:19:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=154
06/24/2022 15:19:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=159
06/24/2022 15:19:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=164
06/24/2022 15:19:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=169
06/24/2022 15:19:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=174
06/24/2022 15:19:37 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 15:19:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=179
06/24/2022 15:19:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=184
06/24/2022 15:19:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=189
06/24/2022 15:19:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=194
06/24/2022 15:19:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=199
06/24/2022 15:19:44 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 15:19:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=204
06/24/2022 15:19:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=209
06/24/2022 15:19:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=214
06/24/2022 15:19:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=219
06/24/2022 15:19:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=224
06/24/2022 15:19:50 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 15:19:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=229
06/24/2022 15:19:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=234
06/24/2022 15:19:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=239
06/24/2022 15:19:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=244
06/24/2022 15:19:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=249
06/24/2022 15:19:57 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=249
06/24/2022 15:19:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=254
06/24/2022 15:19:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=259
06/24/2022 15:20:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=264
06/24/2022 15:20:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=269
06/24/2022 15:20:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=274
06/24/2022 15:20:03 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=274
06/24/2022 15:20:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=274, global_step=550
06/24/2022 15:20:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=279
06/24/2022 15:20:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=284
06/24/2022 15:20:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
06/24/2022 15:20:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=294
06/24/2022 15:20:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=299
06/24/2022 15:20:09 - INFO - __main__ - Global step 600 Train loss 0.32 Classification-F1 0.4589371980676329 on epoch=299
06/24/2022 15:20:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4589371980676329 on epoch=299, global_step=600
06/24/2022 15:20:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=304
06/24/2022 15:20:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=309
06/24/2022 15:20:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=314
06/24/2022 15:20:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=319
06/24/2022 15:20:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=324
06/24/2022 15:20:16 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.39756367663344405 on epoch=324
06/24/2022 15:20:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=329
06/24/2022 15:20:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=334
06/24/2022 15:20:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=339
06/24/2022 15:20:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=344
06/24/2022 15:20:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=349
06/24/2022 15:20:22 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.4181818181818182 on epoch=349
06/24/2022 15:20:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=354
06/24/2022 15:20:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=359
06/24/2022 15:20:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=364
06/24/2022 15:20:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
06/24/2022 15:20:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=374
06/24/2022 15:20:28 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.39756367663344405 on epoch=374
06/24/2022 15:20:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=379
06/24/2022 15:20:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=384
06/24/2022 15:20:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=389
06/24/2022 15:20:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=394
06/24/2022 15:20:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=399
06/24/2022 15:20:35 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.4231177094379639 on epoch=399
06/24/2022 15:20:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=404
06/24/2022 15:20:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=409
06/24/2022 15:20:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=414
06/24/2022 15:20:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=419
06/24/2022 15:20:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=424
06/24/2022 15:20:41 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.4420512820512821 on epoch=424
06/24/2022 15:20:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=429
06/24/2022 15:20:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=434
06/24/2022 15:20:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=439
06/24/2022 15:20:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=444
06/24/2022 15:20:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=449
06/24/2022 15:20:47 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.4920634920634921 on epoch=449
06/24/2022 15:20:47 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.4920634920634921 on epoch=449, global_step=900
06/24/2022 15:20:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=454
06/24/2022 15:20:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=459
06/24/2022 15:20:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=464
06/24/2022 15:20:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=469
06/24/2022 15:20:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=474
06/24/2022 15:20:54 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.4920634920634921 on epoch=474
06/24/2022 15:20:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=479
06/24/2022 15:20:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=484
06/24/2022 15:20:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=489
06/24/2022 15:20:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=494
06/24/2022 15:21:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=499
06/24/2022 15:21:00 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.4375 on epoch=499
06/24/2022 15:21:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=504
06/24/2022 15:21:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=509
06/24/2022 15:21:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.13 on epoch=514
06/24/2022 15:21:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=519
06/24/2022 15:21:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=524
06/24/2022 15:21:06 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.3522267206477733 on epoch=524
06/24/2022 15:21:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=529
06/24/2022 15:21:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=534
06/24/2022 15:21:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=539
06/24/2022 15:21:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=544
06/24/2022 15:21:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
06/24/2022 15:21:13 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.43529411764705883 on epoch=549
06/24/2022 15:21:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=554
06/24/2022 15:21:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
06/24/2022 15:21:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=564
06/24/2022 15:21:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=569
06/24/2022 15:21:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=574
06/24/2022 15:21:19 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.43529411764705883 on epoch=574
06/24/2022 15:21:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=579
06/24/2022 15:21:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=584
06/24/2022 15:21:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=589
06/24/2022 15:21:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=594
06/24/2022 15:21:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
06/24/2022 15:21:25 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.43529411764705883 on epoch=599
06/24/2022 15:21:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
06/24/2022 15:21:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
06/24/2022 15:21:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
06/24/2022 15:21:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=619
06/24/2022 15:21:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
06/24/2022 15:21:32 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.4009852216748768 on epoch=624
06/24/2022 15:21:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/24/2022 15:21:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=634
06/24/2022 15:21:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
06/24/2022 15:21:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=644
06/24/2022 15:21:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
06/24/2022 15:21:38 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.4682306940371457 on epoch=649
06/24/2022 15:21:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/24/2022 15:21:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
06/24/2022 15:21:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
06/24/2022 15:21:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
06/24/2022 15:21:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
06/24/2022 15:21:44 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.43529411764705883 on epoch=674
06/24/2022 15:21:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=679
06/24/2022 15:21:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
06/24/2022 15:21:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
06/24/2022 15:21:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
06/24/2022 15:21:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=699
06/24/2022 15:21:51 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.43529411764705883 on epoch=699
06/24/2022 15:21:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
06/24/2022 15:21:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
06/24/2022 15:21:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
06/24/2022 15:21:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
06/24/2022 15:21:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
06/24/2022 15:21:57 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.43529411764705883 on epoch=724
06/24/2022 15:21:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
06/24/2022 15:21:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
06/24/2022 15:22:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=739
06/24/2022 15:22:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 15:22:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
06/24/2022 15:22:03 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.4375 on epoch=749
06/24/2022 15:22:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
06/24/2022 15:22:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/24/2022 15:22:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
06/24/2022 15:22:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
06/24/2022 15:22:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/24/2022 15:22:10 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.43529411764705883 on epoch=774
06/24/2022 15:22:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
06/24/2022 15:22:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
06/24/2022 15:22:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=789
06/24/2022 15:22:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/24/2022 15:22:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
06/24/2022 15:22:16 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.43529411764705883 on epoch=799
06/24/2022 15:22:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
06/24/2022 15:22:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
06/24/2022 15:22:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
06/24/2022 15:22:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/24/2022 15:22:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
06/24/2022 15:22:22 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.4375 on epoch=824
06/24/2022 15:22:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
06/24/2022 15:22:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
06/24/2022 15:22:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
06/24/2022 15:22:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/24/2022 15:22:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
06/24/2022 15:22:29 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=849
06/24/2022 15:22:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
06/24/2022 15:22:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/24/2022 15:22:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/24/2022 15:22:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/24/2022 15:22:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/24/2022 15:22:35 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=874
06/24/2022 15:22:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=879
06/24/2022 15:22:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
06/24/2022 15:22:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
06/24/2022 15:22:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
06/24/2022 15:22:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 15:22:41 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.3650793650793651 on epoch=899
06/24/2022 15:22:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
06/24/2022 15:22:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/24/2022 15:22:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
06/24/2022 15:22:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/24/2022 15:22:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
06/24/2022 15:22:48 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=924
06/24/2022 15:22:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
06/24/2022 15:22:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 15:22:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=939
06/24/2022 15:22:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/24/2022 15:22:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 15:22:54 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.4682306940371457 on epoch=949
06/24/2022 15:22:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/24/2022 15:22:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
06/24/2022 15:22:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 15:22:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/24/2022 15:23:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
06/24/2022 15:23:01 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=974
06/24/2022 15:23:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
06/24/2022 15:23:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/24/2022 15:23:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 15:23:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/24/2022 15:23:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=999
06/24/2022 15:23:07 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=999
06/24/2022 15:23:07 - INFO - __main__ - save last model!
06/24/2022 15:23:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 15:23:07 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 15:23:07 - INFO - __main__ - Printing 3 examples
06/24/2022 15:23:07 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 15:23:07 - INFO - __main__ - ['0']
06/24/2022 15:23:07 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 15:23:07 - INFO - __main__ - ['1']
06/24/2022 15:23:07 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 15:23:07 - INFO - __main__ - ['1']
06/24/2022 15:23:07 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:23:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:23:08 - INFO - __main__ - Printing 3 examples
06/24/2022 15:23:08 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 15:23:08 - INFO - __main__ - ['1']
06/24/2022 15:23:08 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 15:23:08 - INFO - __main__ - ['1']
06/24/2022 15:23:08 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 15:23:08 - INFO - __main__ - ['1']
06/24/2022 15:23:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:23:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:23:08 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:23:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:23:08 - INFO - __main__ - Printing 3 examples
06/24/2022 15:23:08 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 15:23:08 - INFO - __main__ - ['1']
06/24/2022 15:23:08 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 15:23:08 - INFO - __main__ - ['1']
06/24/2022 15:23:08 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 15:23:08 - INFO - __main__ - ['1']
06/24/2022 15:23:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:23:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:23:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:23:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:23:13 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:23:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:23:13 - INFO - __main__ - Starting training!
06/24/2022 15:23:19 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 15:24:48 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_21_0.2_8_predictions.txt
06/24/2022 15:24:48 - INFO - __main__ - Classification-F1 on test data: 0.5040
06/24/2022 15:24:49 - INFO - __main__ - prefix=paws_16_21, lr=0.2, bsz=8, dev_performance=0.4920634920634921, test_performance=0.504031458576056
06/24/2022 15:24:49 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.5, bsz=8 ...
06/24/2022 15:24:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:24:50 - INFO - __main__ - Printing 3 examples
06/24/2022 15:24:50 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 15:24:50 - INFO - __main__ - ['1']
06/24/2022 15:24:50 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 15:24:50 - INFO - __main__ - ['1']
06/24/2022 15:24:50 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 15:24:50 - INFO - __main__ - ['1']
06/24/2022 15:24:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:24:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:24:50 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:24:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:24:50 - INFO - __main__ - Printing 3 examples
06/24/2022 15:24:50 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 15:24:50 - INFO - __main__ - ['1']
06/24/2022 15:24:50 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 15:24:50 - INFO - __main__ - ['1']
06/24/2022 15:24:50 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 15:24:50 - INFO - __main__ - ['1']
06/24/2022 15:24:50 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:24:50 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:24:50 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:24:55 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:24:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:24:56 - INFO - __main__ - Starting training!
06/24/2022 15:24:57 - INFO - __main__ - Step 10 Global step 10 Train loss 3.78 on epoch=4
06/24/2022 15:24:58 - INFO - __main__ - Step 20 Global step 20 Train loss 2.30 on epoch=9
06/24/2022 15:25:00 - INFO - __main__ - Step 30 Global step 30 Train loss 1.30 on epoch=14
06/24/2022 15:25:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.83 on epoch=19
06/24/2022 15:25:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.61 on epoch=24
06/24/2022 15:25:02 - INFO - __main__ - Global step 50 Train loss 1.76 Classification-F1 0.6825396825396826 on epoch=24
06/24/2022 15:25:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.6825396825396826 on epoch=24, global_step=50
06/24/2022 15:25:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.74 on epoch=29
06/24/2022 15:25:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=34
06/24/2022 15:25:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=39
06/24/2022 15:25:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=44
06/24/2022 15:25:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=49
06/24/2022 15:25:09 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.5134502923976608 on epoch=49
06/24/2022 15:25:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
06/24/2022 15:25:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=59
06/24/2022 15:25:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=64
06/24/2022 15:25:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
06/24/2022 15:25:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/24/2022 15:25:15 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 15:25:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
06/24/2022 15:25:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=84
06/24/2022 15:25:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
06/24/2022 15:25:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=94
06/24/2022 15:25:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
06/24/2022 15:25:22 - INFO - __main__ - Global step 200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 15:25:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=104
06/24/2022 15:25:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
06/24/2022 15:25:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=114
06/24/2022 15:25:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
06/24/2022 15:25:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=124
06/24/2022 15:25:28 - INFO - __main__ - Global step 250 Train loss 0.31 Classification-F1 0.3992490613266583 on epoch=124
06/24/2022 15:25:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
06/24/2022 15:25:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
06/24/2022 15:25:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
06/24/2022 15:25:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
06/24/2022 15:25:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
06/24/2022 15:25:35 - INFO - __main__ - Global step 300 Train loss 0.27 Classification-F1 0.4589371980676329 on epoch=149
06/24/2022 15:25:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=154
06/24/2022 15:25:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=159
06/24/2022 15:25:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
06/24/2022 15:25:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=169
06/24/2022 15:25:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=174
06/24/2022 15:25:41 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.4285714285714286 on epoch=174
06/24/2022 15:25:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.22 on epoch=179
06/24/2022 15:25:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=184
06/24/2022 15:25:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=189
06/24/2022 15:25:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=194
06/24/2022 15:25:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
06/24/2022 15:25:48 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.5270935960591133 on epoch=199
06/24/2022 15:25:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
06/24/2022 15:25:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=209
06/24/2022 15:25:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
06/24/2022 15:25:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=219
06/24/2022 15:25:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
06/24/2022 15:25:54 - INFO - __main__ - Global step 450 Train loss 0.11 Classification-F1 0.43529411764705883 on epoch=224
06/24/2022 15:25:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.11 on epoch=229
06/24/2022 15:25:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
06/24/2022 15:25:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.10 on epoch=239
06/24/2022 15:25:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
06/24/2022 15:26:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
06/24/2022 15:26:01 - INFO - __main__ - Global step 500 Train loss 0.08 Classification-F1 0.4285714285714286 on epoch=249
06/24/2022 15:26:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.06 on epoch=254
06/24/2022 15:26:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
06/24/2022 15:26:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
06/24/2022 15:26:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
06/24/2022 15:26:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
06/24/2022 15:26:08 - INFO - __main__ - Global step 550 Train loss 0.05 Classification-F1 0.4980392156862745 on epoch=274
06/24/2022 15:26:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
06/24/2022 15:26:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
06/24/2022 15:26:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
06/24/2022 15:26:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
06/24/2022 15:26:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
06/24/2022 15:26:14 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.5307917888563051 on epoch=299
06/24/2022 15:26:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
06/24/2022 15:26:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
06/24/2022 15:26:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
06/24/2022 15:26:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
06/24/2022 15:26:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
06/24/2022 15:26:21 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.5901477832512315 on epoch=324
06/24/2022 15:26:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
06/24/2022 15:26:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
06/24/2022 15:26:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
06/24/2022 15:26:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
06/24/2022 15:26:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
06/24/2022 15:26:27 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=349
06/24/2022 15:26:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
06/24/2022 15:26:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
06/24/2022 15:26:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/24/2022 15:26:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/24/2022 15:26:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
06/24/2022 15:26:34 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.6000000000000001 on epoch=374
06/24/2022 15:26:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
06/24/2022 15:26:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/24/2022 15:26:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
06/24/2022 15:26:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
06/24/2022 15:26:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
06/24/2022 15:26:41 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=399
06/24/2022 15:26:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
06/24/2022 15:26:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
06/24/2022 15:26:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/24/2022 15:26:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
06/24/2022 15:26:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/24/2022 15:26:47 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.5625 on epoch=424
06/24/2022 15:26:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/24/2022 15:26:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/24/2022 15:26:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
06/24/2022 15:26:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
06/24/2022 15:26:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
06/24/2022 15:26:54 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.5625 on epoch=449
06/24/2022 15:26:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
06/24/2022 15:26:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/24/2022 15:26:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
06/24/2022 15:26:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/24/2022 15:27:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/24/2022 15:27:00 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.5625 on epoch=474
06/24/2022 15:27:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/24/2022 15:27:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
06/24/2022 15:27:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
06/24/2022 15:27:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
06/24/2022 15:27:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 15:27:07 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=499
06/24/2022 15:27:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
06/24/2022 15:27:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/24/2022 15:27:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/24/2022 15:27:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/24/2022 15:27:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/24/2022 15:27:13 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=524
06/24/2022 15:27:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/24/2022 15:27:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 15:27:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/24/2022 15:27:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/24/2022 15:27:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 15:27:20 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=549
06/24/2022 15:27:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 15:27:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 15:27:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/24/2022 15:27:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 15:27:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/24/2022 15:27:26 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=574
06/24/2022 15:27:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
06/24/2022 15:27:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 15:27:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 15:27:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 15:27:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 15:27:33 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=599
06/24/2022 15:27:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 15:27:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 15:27:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 15:27:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 15:27:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 15:27:39 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=624
06/24/2022 15:27:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 15:27:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 15:27:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 15:27:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 15:27:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 15:27:46 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.5625 on epoch=649
06/24/2022 15:27:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
06/24/2022 15:27:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 15:27:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 15:27:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 15:27:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 15:27:52 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=674
06/24/2022 15:27:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 15:27:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 15:27:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 15:27:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 15:27:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 15:27:59 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=699
06/24/2022 15:28:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 15:28:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 15:28:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 15:28:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 15:28:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 15:28:05 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5 on epoch=724
06/24/2022 15:28:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 15:28:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 15:28:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 15:28:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 15:28:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 15:28:11 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=749
06/24/2022 15:28:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 15:28:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 15:28:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 15:28:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 15:28:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 15:28:18 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=774
06/24/2022 15:28:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 15:28:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 15:28:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 15:28:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 15:28:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 15:28:24 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=799
06/24/2022 15:28:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 15:28:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 15:28:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 15:28:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 15:28:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 15:28:31 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5 on epoch=824
06/24/2022 15:28:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 15:28:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 15:28:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 15:28:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 15:28:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 15:28:37 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=849
06/24/2022 15:28:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 15:28:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 15:28:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 15:28:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 15:28:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 15:28:43 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=874
06/24/2022 15:28:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 15:28:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 15:28:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 15:28:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 15:28:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 15:28:50 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=899
06/24/2022 15:28:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 15:28:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 15:28:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 15:28:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 15:28:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 15:28:56 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=924
06/24/2022 15:28:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 15:28:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 15:29:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 15:29:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 15:29:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/24/2022 15:29:03 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=949
06/24/2022 15:29:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 15:29:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=959
06/24/2022 15:29:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 15:29:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 15:29:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 15:29:09 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=974
06/24/2022 15:29:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 15:29:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 15:29:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 15:29:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 15:29:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 15:29:15 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=999
06/24/2022 15:29:15 - INFO - __main__ - save last model!
06/24/2022 15:29:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 15:29:15 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 15:29:15 - INFO - __main__ - Printing 3 examples
06/24/2022 15:29:15 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 15:29:15 - INFO - __main__ - ['0']
06/24/2022 15:29:15 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 15:29:15 - INFO - __main__ - ['1']
06/24/2022 15:29:15 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 15:29:15 - INFO - __main__ - ['1']
06/24/2022 15:29:16 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:29:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:29:16 - INFO - __main__ - Printing 3 examples
06/24/2022 15:29:16 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 15:29:16 - INFO - __main__ - ['1']
06/24/2022 15:29:16 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 15:29:16 - INFO - __main__ - ['1']
06/24/2022 15:29:16 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 15:29:16 - INFO - __main__ - ['1']
06/24/2022 15:29:16 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:29:16 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:29:16 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:29:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:29:16 - INFO - __main__ - Printing 3 examples
06/24/2022 15:29:16 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 15:29:16 - INFO - __main__ - ['1']
06/24/2022 15:29:16 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 15:29:16 - INFO - __main__ - ['1']
06/24/2022 15:29:16 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 15:29:16 - INFO - __main__ - ['1']
06/24/2022 15:29:16 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:29:16 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:29:16 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:29:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:29:22 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:29:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:29:22 - INFO - __main__ - Starting training!
06/24/2022 15:29:27 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 15:30:54 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_42_0.5_8_predictions.txt
06/24/2022 15:30:54 - INFO - __main__ - Classification-F1 on test data: 0.3332
06/24/2022 15:30:54 - INFO - __main__ - prefix=paws_16_42, lr=0.5, bsz=8, dev_performance=0.6825396825396826, test_performance=0.3332068229572284
06/24/2022 15:30:54 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.4, bsz=8 ...
06/24/2022 15:30:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:30:55 - INFO - __main__ - Printing 3 examples
06/24/2022 15:30:55 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 15:30:55 - INFO - __main__ - ['1']
06/24/2022 15:30:55 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 15:30:55 - INFO - __main__ - ['1']
06/24/2022 15:30:55 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 15:30:55 - INFO - __main__ - ['1']
06/24/2022 15:30:55 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:30:55 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:30:55 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:30:55 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:30:55 - INFO - __main__ - Printing 3 examples
06/24/2022 15:30:55 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 15:30:55 - INFO - __main__ - ['1']
06/24/2022 15:30:55 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 15:30:55 - INFO - __main__ - ['1']
06/24/2022 15:30:55 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 15:30:55 - INFO - __main__ - ['1']
06/24/2022 15:30:55 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:30:55 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:30:56 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:31:01 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:31:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:31:01 - INFO - __main__ - Starting training!
06/24/2022 15:31:03 - INFO - __main__ - Step 10 Global step 10 Train loss 4.00 on epoch=4
06/24/2022 15:31:04 - INFO - __main__ - Step 20 Global step 20 Train loss 2.50 on epoch=9
06/24/2022 15:31:05 - INFO - __main__ - Step 30 Global step 30 Train loss 1.58 on epoch=14
06/24/2022 15:31:06 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=19
06/24/2022 15:31:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.73 on epoch=24
06/24/2022 15:31:08 - INFO - __main__ - Global step 50 Train loss 1.95 Classification-F1 0.3816425120772947 on epoch=24
06/24/2022 15:31:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3816425120772947 on epoch=24, global_step=50
06/24/2022 15:31:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=29
06/24/2022 15:31:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.60 on epoch=34
06/24/2022 15:31:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=39
06/24/2022 15:31:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=44
06/24/2022 15:31:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
06/24/2022 15:31:14 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 15:31:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=54
06/24/2022 15:31:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
06/24/2022 15:31:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=64
06/24/2022 15:31:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=69
06/24/2022 15:31:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/24/2022 15:31:21 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 15:31:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=79
06/24/2022 15:31:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=84
06/24/2022 15:31:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=89
06/24/2022 15:31:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=94
06/24/2022 15:31:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
06/24/2022 15:31:27 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.4589371980676329 on epoch=99
06/24/2022 15:31:27 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.4589371980676329 on epoch=99, global_step=200
06/24/2022 15:31:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=104
06/24/2022 15:31:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=109
06/24/2022 15:31:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
06/24/2022 15:31:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=119
06/24/2022 15:31:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=124
06/24/2022 15:31:34 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.464039408866995 on epoch=124
06/24/2022 15:31:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.464039408866995 on epoch=124, global_step=250
06/24/2022 15:31:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=129
06/24/2022 15:31:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=134
06/24/2022 15:31:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=139
06/24/2022 15:31:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
06/24/2022 15:31:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
06/24/2022 15:31:40 - INFO - __main__ - Global step 300 Train loss 0.36 Classification-F1 0.4589371980676329 on epoch=149
06/24/2022 15:31:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=154
06/24/2022 15:31:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.30 on epoch=159
06/24/2022 15:31:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=164
06/24/2022 15:31:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=169
06/24/2022 15:31:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
06/24/2022 15:31:46 - INFO - __main__ - Global step 350 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 15:31:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
06/24/2022 15:31:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=184
06/24/2022 15:31:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=189
06/24/2022 15:31:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=194
06/24/2022 15:31:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=199
06/24/2022 15:31:53 - INFO - __main__ - Global step 400 Train loss 0.30 Classification-F1 0.3191489361702127 on epoch=199
06/24/2022 15:31:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
06/24/2022 15:31:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=209
06/24/2022 15:31:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.30 on epoch=214
06/24/2022 15:31:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
06/24/2022 15:31:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
06/24/2022 15:31:59 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.46843853820598 on epoch=224
06/24/2022 15:31:59 - INFO - __main__ - Saving model with best Classification-F1: 0.464039408866995 -> 0.46843853820598 on epoch=224, global_step=450
06/24/2022 15:32:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
06/24/2022 15:32:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
06/24/2022 15:32:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
06/24/2022 15:32:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
06/24/2022 15:32:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.17 on epoch=249
06/24/2022 15:32:06 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.36374269005847953 on epoch=249
06/24/2022 15:32:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
06/24/2022 15:32:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=259
06/24/2022 15:32:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=264
06/24/2022 15:32:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=269
06/24/2022 15:32:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=274
06/24/2022 15:32:12 - INFO - __main__ - Global step 550 Train loss 0.18 Classification-F1 0.43529411764705883 on epoch=274
06/24/2022 15:32:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=279
06/24/2022 15:32:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=284
06/24/2022 15:32:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=289
06/24/2022 15:32:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
06/24/2022 15:32:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=299
06/24/2022 15:32:19 - INFO - __main__ - Global step 600 Train loss 0.13 Classification-F1 0.5076923076923077 on epoch=299
06/24/2022 15:32:19 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5076923076923077 on epoch=299, global_step=600
06/24/2022 15:32:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=304
06/24/2022 15:32:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=309
06/24/2022 15:32:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=314
06/24/2022 15:32:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=319
06/24/2022 15:32:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
06/24/2022 15:32:25 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.43529411764705883 on epoch=324
06/24/2022 15:32:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=329
06/24/2022 15:32:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
06/24/2022 15:32:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=339
06/24/2022 15:32:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=344
06/24/2022 15:32:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
06/24/2022 15:32:32 - INFO - __main__ - Global step 700 Train loss 0.10 Classification-F1 0.4666666666666667 on epoch=349
06/24/2022 15:32:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
06/24/2022 15:32:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=359
06/24/2022 15:32:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
06/24/2022 15:32:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
06/24/2022 15:32:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
06/24/2022 15:32:38 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.5625 on epoch=374
06/24/2022 15:32:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5625 on epoch=374, global_step=750
06/24/2022 15:32:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
06/24/2022 15:32:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
06/24/2022 15:32:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
06/24/2022 15:32:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
06/24/2022 15:32:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
06/24/2022 15:32:45 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.5195195195195195 on epoch=399
06/24/2022 15:32:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
06/24/2022 15:32:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/24/2022 15:32:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
06/24/2022 15:32:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
06/24/2022 15:32:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/24/2022 15:32:51 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.5 on epoch=424
06/24/2022 15:32:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
06/24/2022 15:32:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
06/24/2022 15:32:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/24/2022 15:32:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/24/2022 15:32:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
06/24/2022 15:32:57 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.5733333333333335 on epoch=449
06/24/2022 15:32:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5625 -> 0.5733333333333335 on epoch=449, global_step=900
06/24/2022 15:32:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
06/24/2022 15:33:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
06/24/2022 15:33:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
06/24/2022 15:33:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
06/24/2022 15:33:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
06/24/2022 15:33:04 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.5465587044534412 on epoch=474
06/24/2022 15:33:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
06/24/2022 15:33:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
06/24/2022 15:33:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/24/2022 15:33:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
06/24/2022 15:33:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/24/2022 15:33:10 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.5465587044534412 on epoch=499
06/24/2022 15:33:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 15:33:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
06/24/2022 15:33:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 15:33:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 15:33:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/24/2022 15:33:17 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=524
06/24/2022 15:33:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5733333333333335 -> 0.5933528836754642 on epoch=524, global_step=1050
06/24/2022 15:33:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
06/24/2022 15:33:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
06/24/2022 15:33:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 15:33:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
06/24/2022 15:33:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 15:33:23 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=549
06/24/2022 15:33:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
06/24/2022 15:33:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 15:33:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 15:33:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
06/24/2022 15:33:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
06/24/2022 15:33:29 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.5607843137254902 on epoch=574
06/24/2022 15:33:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
06/24/2022 15:33:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/24/2022 15:33:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 15:33:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/24/2022 15:33:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
06/24/2022 15:33:36 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=599
06/24/2022 15:33:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 15:33:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/24/2022 15:33:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 15:33:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
06/24/2022 15:33:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
06/24/2022 15:33:42 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=624
06/24/2022 15:33:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 15:33:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 15:33:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 15:33:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 15:33:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/24/2022 15:33:49 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=649
06/24/2022 15:33:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 15:33:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 15:33:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 15:33:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 15:33:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 15:33:55 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=674
06/24/2022 15:33:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/24/2022 15:33:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/24/2022 15:33:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 15:34:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 15:34:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
06/24/2022 15:34:01 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=699
06/24/2022 15:34:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 15:34:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 15:34:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=714
06/24/2022 15:34:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 15:34:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 15:34:08 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=724
06/24/2022 15:34:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 15:34:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/24/2022 15:34:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=739
06/24/2022 15:34:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 15:34:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 15:34:14 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5901477832512315 on epoch=749
06/24/2022 15:34:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 15:34:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 15:34:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 15:34:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 15:34:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 15:34:20 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.6235294117647059 on epoch=774
06/24/2022 15:34:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6235294117647059 on epoch=774, global_step=1550
06/24/2022 15:34:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 15:34:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 15:34:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
06/24/2022 15:34:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 15:34:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 15:34:27 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5835835835835835 on epoch=799
06/24/2022 15:34:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 15:34:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 15:34:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 15:34:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 15:34:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 15:34:33 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=824
06/24/2022 15:34:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 15:34:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 15:34:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 15:34:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/24/2022 15:34:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 15:34:40 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=849
06/24/2022 15:34:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/24/2022 15:34:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 15:34:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 15:34:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 15:34:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 15:34:46 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=874
06/24/2022 15:34:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 15:34:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 15:34:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 15:34:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 15:34:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 15:34:52 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=899
06/24/2022 15:34:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
06/24/2022 15:34:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
06/24/2022 15:34:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 15:34:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 15:34:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 15:34:59 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=924
06/24/2022 15:35:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 15:35:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
06/24/2022 15:35:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 15:35:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 15:35:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 15:35:05 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=949
06/24/2022 15:35:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 15:35:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 15:35:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 15:35:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 15:35:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 15:35:12 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=974
06/24/2022 15:35:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 15:35:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 15:35:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 15:35:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 15:35:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 15:35:18 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=999
06/24/2022 15:35:18 - INFO - __main__ - save last model!
06/24/2022 15:35:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 15:35:18 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 15:35:18 - INFO - __main__ - Printing 3 examples
06/24/2022 15:35:18 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 15:35:18 - INFO - __main__ - ['0']
06/24/2022 15:35:18 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 15:35:18 - INFO - __main__ - ['1']
06/24/2022 15:35:18 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 15:35:18 - INFO - __main__ - ['1']
06/24/2022 15:35:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:35:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:35:19 - INFO - __main__ - Printing 3 examples
06/24/2022 15:35:19 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 15:35:19 - INFO - __main__ - ['1']
06/24/2022 15:35:19 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 15:35:19 - INFO - __main__ - ['1']
06/24/2022 15:35:19 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 15:35:19 - INFO - __main__ - ['1']
06/24/2022 15:35:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:35:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:35:19 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:35:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:35:19 - INFO - __main__ - Printing 3 examples
06/24/2022 15:35:19 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 15:35:19 - INFO - __main__ - ['1']
06/24/2022 15:35:19 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 15:35:19 - INFO - __main__ - ['1']
06/24/2022 15:35:19 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 15:35:19 - INFO - __main__ - ['1']
06/24/2022 15:35:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:35:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:35:19 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:35:22 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:35:24 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:35:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:35:24 - INFO - __main__ - Starting training!
06/24/2022 15:35:30 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 15:36:57 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_42_0.4_8_predictions.txt
06/24/2022 15:36:57 - INFO - __main__ - Classification-F1 on test data: 0.1982
06/24/2022 15:36:57 - INFO - __main__ - prefix=paws_16_42, lr=0.4, bsz=8, dev_performance=0.6235294117647059, test_performance=0.19823537589814302
06/24/2022 15:36:57 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.3, bsz=8 ...
06/24/2022 15:36:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:36:58 - INFO - __main__ - Printing 3 examples
06/24/2022 15:36:58 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 15:36:58 - INFO - __main__ - ['1']
06/24/2022 15:36:58 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 15:36:58 - INFO - __main__ - ['1']
06/24/2022 15:36:58 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 15:36:58 - INFO - __main__ - ['1']
06/24/2022 15:36:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:36:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:36:58 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:36:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:36:58 - INFO - __main__ - Printing 3 examples
06/24/2022 15:36:58 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 15:36:58 - INFO - __main__ - ['1']
06/24/2022 15:36:58 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 15:36:58 - INFO - __main__ - ['1']
06/24/2022 15:36:58 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 15:36:58 - INFO - __main__ - ['1']
06/24/2022 15:36:58 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:36:58 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:36:58 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:37:03 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:37:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:37:04 - INFO - __main__ - Starting training!
06/24/2022 15:37:05 - INFO - __main__ - Step 10 Global step 10 Train loss 4.35 on epoch=4
06/24/2022 15:37:07 - INFO - __main__ - Step 20 Global step 20 Train loss 3.01 on epoch=9
06/24/2022 15:37:08 - INFO - __main__ - Step 30 Global step 30 Train loss 2.17 on epoch=14
06/24/2022 15:37:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.57 on epoch=19
06/24/2022 15:37:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.09 on epoch=24
06/24/2022 15:37:11 - INFO - __main__ - Global step 50 Train loss 2.44 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 15:37:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/24/2022 15:37:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.82 on epoch=29
06/24/2022 15:37:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.79 on epoch=34
06/24/2022 15:37:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=39
06/24/2022 15:37:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=44
06/24/2022 15:37:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=49
06/24/2022 15:37:17 - INFO - __main__ - Global step 100 Train loss 0.67 Classification-F1 0.3191489361702127 on epoch=49
06/24/2022 15:37:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=54
06/24/2022 15:37:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=59
06/24/2022 15:37:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
06/24/2022 15:37:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=69
06/24/2022 15:37:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=74
06/24/2022 15:37:24 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 15:37:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
06/24/2022 15:37:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=84
06/24/2022 15:37:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=89
06/24/2022 15:37:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=94
06/24/2022 15:37:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
06/24/2022 15:37:30 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 15:37:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=104
06/24/2022 15:37:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=109
06/24/2022 15:37:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
06/24/2022 15:37:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=119
06/24/2022 15:37:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=124
06/24/2022 15:37:37 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 15:37:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=129
06/24/2022 15:37:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
06/24/2022 15:37:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
06/24/2022 15:37:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=144
06/24/2022 15:37:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=149
06/24/2022 15:37:43 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 15:37:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.33 on epoch=154
06/24/2022 15:37:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=159
06/24/2022 15:37:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=164
06/24/2022 15:37:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
06/24/2022 15:37:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=174
06/24/2022 15:37:49 - INFO - __main__ - Global step 350 Train loss 0.32 Classification-F1 0.4589371980676329 on epoch=174
06/24/2022 15:37:49 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=174, global_step=350
06/24/2022 15:37:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
06/24/2022 15:37:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.29 on epoch=184
06/24/2022 15:37:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=189
06/24/2022 15:37:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=194
06/24/2022 15:37:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=199
06/24/2022 15:37:55 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.4589371980676329 on epoch=199
06/24/2022 15:37:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=204
06/24/2022 15:37:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
06/24/2022 15:37:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=214
06/24/2022 15:38:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
06/24/2022 15:38:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
06/24/2022 15:38:02 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 15:38:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
06/24/2022 15:38:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=234
06/24/2022 15:38:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=239
06/24/2022 15:38:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
06/24/2022 15:38:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=249
06/24/2022 15:38:08 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.46843853820598 on epoch=249
06/24/2022 15:38:08 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.46843853820598 on epoch=249, global_step=500
06/24/2022 15:38:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=254
06/24/2022 15:38:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=259
06/24/2022 15:38:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=264
06/24/2022 15:38:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
06/24/2022 15:38:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=274
06/24/2022 15:38:14 - INFO - __main__ - Global step 550 Train loss 0.18 Classification-F1 0.5733333333333335 on epoch=274
06/24/2022 15:38:15 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5733333333333335 on epoch=274, global_step=550
06/24/2022 15:38:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=279
06/24/2022 15:38:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=284
06/24/2022 15:38:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
06/24/2022 15:38:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=294
06/24/2022 15:38:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
06/24/2022 15:38:21 - INFO - __main__ - Global step 600 Train loss 0.16 Classification-F1 0.4458874458874459 on epoch=299
06/24/2022 15:38:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=304
06/24/2022 15:38:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=309
06/24/2022 15:38:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=314
06/24/2022 15:38:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=319
06/24/2022 15:38:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=324
06/24/2022 15:38:27 - INFO - __main__ - Global step 650 Train loss 0.13 Classification-F1 0.4920634920634921 on epoch=324
06/24/2022 15:38:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=329
06/24/2022 15:38:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
06/24/2022 15:38:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
06/24/2022 15:38:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
06/24/2022 15:38:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=349
06/24/2022 15:38:33 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.5555555555555556 on epoch=349
06/24/2022 15:38:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
06/24/2022 15:38:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
06/24/2022 15:38:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=364
06/24/2022 15:38:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=369
06/24/2022 15:38:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
06/24/2022 15:38:40 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.5555555555555556 on epoch=374
06/24/2022 15:38:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=379
06/24/2022 15:38:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
06/24/2022 15:38:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=389
06/24/2022 15:38:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
06/24/2022 15:38:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
06/24/2022 15:38:46 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.5555555555555556 on epoch=399
06/24/2022 15:38:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
06/24/2022 15:38:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/24/2022 15:38:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
06/24/2022 15:38:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=419
06/24/2022 15:38:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
06/24/2022 15:38:52 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.5270935960591133 on epoch=424
06/24/2022 15:38:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
06/24/2022 15:38:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/24/2022 15:38:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/24/2022 15:38:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
06/24/2022 15:38:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
06/24/2022 15:38:59 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.5307917888563051 on epoch=449
06/24/2022 15:39:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/24/2022 15:39:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
06/24/2022 15:39:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 15:39:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/24/2022 15:39:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/24/2022 15:39:05 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=474
06/24/2022 15:39:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/24/2022 15:39:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/24/2022 15:39:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/24/2022 15:39:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
06/24/2022 15:39:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
06/24/2022 15:39:11 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=499
06/24/2022 15:39:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 15:39:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
06/24/2022 15:39:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 15:39:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 15:39:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
06/24/2022 15:39:18 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=524
06/24/2022 15:39:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/24/2022 15:39:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/24/2022 15:39:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/24/2022 15:39:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
06/24/2022 15:39:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
06/24/2022 15:39:24 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5625 on epoch=549
06/24/2022 15:39:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 15:39:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 15:39:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
06/24/2022 15:39:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 15:39:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/24/2022 15:39:30 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=574
06/24/2022 15:39:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/24/2022 15:39:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
06/24/2022 15:39:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/24/2022 15:39:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
06/24/2022 15:39:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 15:39:37 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=599
06/24/2022 15:39:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
06/24/2022 15:39:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 15:39:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
06/24/2022 15:39:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
06/24/2022 15:39:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 15:39:43 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=624
06/24/2022 15:39:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 15:39:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 15:39:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 15:39:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
06/24/2022 15:39:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/24/2022 15:39:49 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.5625 on epoch=649
06/24/2022 15:39:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
06/24/2022 15:39:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 15:39:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 15:39:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
06/24/2022 15:39:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 15:39:55 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=674
06/24/2022 15:39:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 15:39:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 15:39:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 15:40:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 15:40:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 15:40:02 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.5 on epoch=699
06/24/2022 15:40:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 15:40:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 15:40:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
06/24/2022 15:40:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 15:40:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 15:40:08 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5625 on epoch=724
06/24/2022 15:40:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 15:40:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=734
06/24/2022 15:40:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 15:40:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 15:40:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=749
06/24/2022 15:40:14 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=749
06/24/2022 15:40:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 15:40:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/24/2022 15:40:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 15:40:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 15:40:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/24/2022 15:40:21 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=774
06/24/2022 15:40:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5733333333333335 -> 0.5933528836754642 on epoch=774, global_step=1550
06/24/2022 15:40:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 15:40:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 15:40:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
06/24/2022 15:40:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 15:40:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
06/24/2022 15:40:27 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5835835835835835 on epoch=799
06/24/2022 15:40:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 15:40:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=809
06/24/2022 15:40:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 15:40:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 15:40:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 15:40:33 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=824
06/24/2022 15:40:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 15:40:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 15:40:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/24/2022 15:40:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/24/2022 15:40:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 15:40:40 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5933528836754642 on epoch=849
06/24/2022 15:40:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 15:40:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 15:40:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 15:40:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 15:40:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 15:40:46 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=874
06/24/2022 15:40:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 15:40:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
06/24/2022 15:40:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 15:40:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 15:40:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 15:40:52 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.5465587044534412 on epoch=899
06/24/2022 15:40:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
06/24/2022 15:40:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 15:40:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 15:40:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 15:40:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 15:40:59 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=924
06/24/2022 15:41:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 15:41:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
06/24/2022 15:41:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 15:41:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 15:41:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 15:41:05 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=949
06/24/2022 15:41:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 15:41:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 15:41:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 15:41:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 15:41:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/24/2022 15:41:12 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=974
06/24/2022 15:41:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 15:41:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 15:41:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 15:41:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 15:41:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 15:41:18 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=999
06/24/2022 15:41:18 - INFO - __main__ - save last model!
06/24/2022 15:41:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 15:41:18 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 15:41:18 - INFO - __main__ - Printing 3 examples
06/24/2022 15:41:18 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 15:41:18 - INFO - __main__ - ['0']
06/24/2022 15:41:18 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 15:41:18 - INFO - __main__ - ['1']
06/24/2022 15:41:18 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 15:41:18 - INFO - __main__ - ['1']
06/24/2022 15:41:18 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:41:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:41:19 - INFO - __main__ - Printing 3 examples
06/24/2022 15:41:19 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 15:41:19 - INFO - __main__ - ['1']
06/24/2022 15:41:19 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 15:41:19 - INFO - __main__ - ['1']
06/24/2022 15:41:19 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 15:41:19 - INFO - __main__ - ['1']
06/24/2022 15:41:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:41:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:41:19 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:41:19 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:41:19 - INFO - __main__ - Printing 3 examples
06/24/2022 15:41:19 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 15:41:19 - INFO - __main__ - ['1']
06/24/2022 15:41:19 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 15:41:19 - INFO - __main__ - ['1']
06/24/2022 15:41:19 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 15:41:19 - INFO - __main__ - ['1']
06/24/2022 15:41:19 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:41:19 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:41:19 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:41:22 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:41:25 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:41:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:41:25 - INFO - __main__ - Starting training!
06/24/2022 15:41:30 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 15:42:57 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_42_0.3_8_predictions.txt
06/24/2022 15:42:57 - INFO - __main__ - Classification-F1 on test data: 0.2502
06/24/2022 15:42:58 - INFO - __main__ - prefix=paws_16_42, lr=0.3, bsz=8, dev_performance=0.5933528836754642, test_performance=0.25018152440169544
06/24/2022 15:42:58 - INFO - __main__ - Running ... prefix=paws_16_42, lr=0.2, bsz=8 ...
06/24/2022 15:42:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:42:59 - INFO - __main__ - Printing 3 examples
06/24/2022 15:42:59 - INFO - __main__ -  [paws] sentence 1: The 500 Hispanic settlers who had lived near San Antonio had to relocate in Los Adaes in 1773 . [SEP] sentence 2: The 500 Hispanic settlers who had lived near San Antonio had to resettle in Los Adaes in 1773 .
06/24/2022 15:42:59 - INFO - __main__ - ['1']
06/24/2022 15:42:59 - INFO - __main__ -  [paws] sentence 1: Fuksas was born in Rome in 1944 ; his father was Lithuanian Jewish while his French mother was the daughter of a Catholic father and an Austrian mother . [SEP] sentence 2: He was born in 1944 in Rome , his father was Lithuanian - Jewish , his French mother was the daughter of a Catholic father and an Austrian mother .
06/24/2022 15:42:59 - INFO - __main__ - ['1']
06/24/2022 15:42:59 - INFO - __main__ -  [paws] sentence 1: Mukesh ( Venugopal ) is a lead singer of the music band `` Hits Orchestra '' . [SEP] sentence 2: Venugopal ( Mukesh ) is the lead singer of music band `` Hits Orchestra '' .
06/24/2022 15:42:59 - INFO - __main__ - ['1']
06/24/2022 15:42:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:42:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:42:59 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:42:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:42:59 - INFO - __main__ - Printing 3 examples
06/24/2022 15:42:59 - INFO - __main__ -  [paws] sentence 1: In November 2007 , Joseph Ransdell endorsed Jaime Nubiola 's view that `` It is simply a mystery at this point '' . [SEP] sentence 2: In November 2007 , Joseph Ransdell advocated Jaime Nubiola 's view that `` at this point it is simply a mystery '' .
06/24/2022 15:42:59 - INFO - __main__ - ['1']
06/24/2022 15:42:59 - INFO - __main__ -  [paws] sentence 1: Sara Sara Noxx is an award winning German musician and star of the alternative music scene . [SEP] sentence 2: Sara Noxx is an award winning German musician and star of the alternative music scene .
06/24/2022 15:42:59 - INFO - __main__ - ['1']
06/24/2022 15:42:59 - INFO - __main__ -  [paws] sentence 1: Darija Jurak and Anastasia Rodionova won the title , defeating Verónica Cepede Royg and Mariana Duque Mariño in the final , 6 -- 3 , 6 -- 2 . [SEP] sentence 2: Darija Jurak and Anastasia Rodionova won the title , Verónica Cepede Royg and Mariana Duque Mariño in the finals , 6 -- 3 , 6 -- 2 defeated .
06/24/2022 15:42:59 - INFO - __main__ - ['1']
06/24/2022 15:42:59 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:42:59 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:42:59 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:43:04 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:43:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:43:05 - INFO - __main__ - Starting training!
06/24/2022 15:43:06 - INFO - __main__ - Step 10 Global step 10 Train loss 4.49 on epoch=4
06/24/2022 15:43:07 - INFO - __main__ - Step 20 Global step 20 Train loss 3.71 on epoch=9
06/24/2022 15:43:08 - INFO - __main__ - Step 30 Global step 30 Train loss 2.83 on epoch=14
06/24/2022 15:43:10 - INFO - __main__ - Step 40 Global step 40 Train loss 2.26 on epoch=19
06/24/2022 15:43:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.89 on epoch=24
06/24/2022 15:43:11 - INFO - __main__ - Global step 50 Train loss 3.04 Classification-F1 0.21276595744680848 on epoch=24
06/24/2022 15:43:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21276595744680848 on epoch=24, global_step=50
06/24/2022 15:43:12 - INFO - __main__ - Step 60 Global step 60 Train loss 1.36 on epoch=29
06/24/2022 15:43:14 - INFO - __main__ - Step 70 Global step 70 Train loss 1.02 on epoch=34
06/24/2022 15:43:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=39
06/24/2022 15:43:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=44
06/24/2022 15:43:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.67 on epoch=49
06/24/2022 15:43:17 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.5933528836754642 on epoch=49
06/24/2022 15:43:17 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.5933528836754642 on epoch=49, global_step=100
06/24/2022 15:43:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.63 on epoch=54
06/24/2022 15:43:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.66 on epoch=59
06/24/2022 15:43:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=64
06/24/2022 15:43:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=69
06/24/2022 15:43:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=74
06/24/2022 15:43:24 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.5151515151515151 on epoch=74
06/24/2022 15:43:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=79
06/24/2022 15:43:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=84
06/24/2022 15:43:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=89
06/24/2022 15:43:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=94
06/24/2022 15:43:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=99
06/24/2022 15:43:31 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.3816425120772947 on epoch=99
06/24/2022 15:43:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=104
06/24/2022 15:43:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=109
06/24/2022 15:43:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=114
06/24/2022 15:43:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=119
06/24/2022 15:43:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=124
06/24/2022 15:43:37 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 15:43:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=129
06/24/2022 15:43:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=134
06/24/2022 15:43:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
06/24/2022 15:43:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=144
06/24/2022 15:43:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=149
06/24/2022 15:43:44 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 15:43:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=154
06/24/2022 15:43:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=159
06/24/2022 15:43:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=164
06/24/2022 15:43:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=169
06/24/2022 15:43:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=174
06/24/2022 15:43:50 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=174
06/24/2022 15:43:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=179
06/24/2022 15:43:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=184
06/24/2022 15:43:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=189
06/24/2022 15:43:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=194
06/24/2022 15:43:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.32 on epoch=199
06/24/2022 15:43:57 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=199
06/24/2022 15:43:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=204
06/24/2022 15:43:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=209
06/24/2022 15:44:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=214
06/24/2022 15:44:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=219
06/24/2022 15:44:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=224
06/24/2022 15:44:04 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=224
06/24/2022 15:44:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=229
06/24/2022 15:44:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
06/24/2022 15:44:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=239
06/24/2022 15:44:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=244
06/24/2022 15:44:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=249
06/24/2022 15:44:10 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.3992490613266583 on epoch=249
06/24/2022 15:44:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=254
06/24/2022 15:44:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=259
06/24/2022 15:44:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=264
06/24/2022 15:44:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
06/24/2022 15:44:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=274
06/24/2022 15:44:17 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=274
06/24/2022 15:44:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=279
06/24/2022 15:44:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=284
06/24/2022 15:44:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=289
06/24/2022 15:44:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=294
06/24/2022 15:44:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=299
06/24/2022 15:44:23 - INFO - __main__ - Global step 600 Train loss 0.30 Classification-F1 0.3816425120772947 on epoch=299
06/24/2022 15:44:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=304
06/24/2022 15:44:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=309
06/24/2022 15:44:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=314
06/24/2022 15:44:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=319
06/24/2022 15:44:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
06/24/2022 15:44:30 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.4385964912280702 on epoch=324
06/24/2022 15:44:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=329
06/24/2022 15:44:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=334
06/24/2022 15:44:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=339
06/24/2022 15:44:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=344
06/24/2022 15:44:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
06/24/2022 15:44:37 - INFO - __main__ - Global step 700 Train loss 0.24 Classification-F1 0.6000000000000001 on epoch=349
06/24/2022 15:44:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6000000000000001 on epoch=349, global_step=700
06/24/2022 15:44:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=354
06/24/2022 15:44:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=359
06/24/2022 15:44:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
06/24/2022 15:44:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=369
06/24/2022 15:44:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=374
06/24/2022 15:44:43 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.5076923076923077 on epoch=374
06/24/2022 15:44:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
06/24/2022 15:44:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.18 on epoch=384
06/24/2022 15:44:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
06/24/2022 15:44:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=394
06/24/2022 15:44:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=399
06/24/2022 15:44:50 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.5151515151515151 on epoch=399
06/24/2022 15:44:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=404
06/24/2022 15:44:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=409
06/24/2022 15:44:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=414
06/24/2022 15:44:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=419
06/24/2022 15:44:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=424
06/24/2022 15:44:56 - INFO - __main__ - Global step 850 Train loss 0.14 Classification-F1 0.5465587044534412 on epoch=424
06/24/2022 15:44:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=429
06/24/2022 15:44:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=434
06/24/2022 15:45:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=439
06/24/2022 15:45:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=444
06/24/2022 15:45:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=449
06/24/2022 15:45:03 - INFO - __main__ - Global step 900 Train loss 0.14 Classification-F1 0.4920634920634921 on epoch=449
06/24/2022 15:45:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=454
06/24/2022 15:45:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=459
06/24/2022 15:45:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=464
06/24/2022 15:45:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
06/24/2022 15:45:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=474
06/24/2022 15:45:09 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.5195195195195195 on epoch=474
06/24/2022 15:45:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=479
06/24/2022 15:45:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=484
06/24/2022 15:45:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=489
06/24/2022 15:45:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=494
06/24/2022 15:45:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=499
06/24/2022 15:45:16 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.5607843137254902 on epoch=499
06/24/2022 15:45:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
06/24/2022 15:45:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
06/24/2022 15:45:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
06/24/2022 15:45:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
06/24/2022 15:45:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=524
06/24/2022 15:45:23 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.5333333333333333 on epoch=524
06/24/2022 15:45:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
06/24/2022 15:45:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
06/24/2022 15:45:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=539
06/24/2022 15:45:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
06/24/2022 15:45:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
06/24/2022 15:45:29 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.5195195195195195 on epoch=549
06/24/2022 15:45:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/24/2022 15:45:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
06/24/2022 15:45:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
06/24/2022 15:45:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
06/24/2022 15:45:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
06/24/2022 15:45:36 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.5076923076923077 on epoch=574
06/24/2022 15:45:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
06/24/2022 15:45:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
06/24/2022 15:45:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
06/24/2022 15:45:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
06/24/2022 15:45:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
06/24/2022 15:45:42 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.5465587044534412 on epoch=599
06/24/2022 15:45:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 15:45:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
06/24/2022 15:45:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/24/2022 15:45:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=619
06/24/2022 15:45:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/24/2022 15:45:49 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.5333333333333333 on epoch=624
06/24/2022 15:45:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
06/24/2022 15:45:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
06/24/2022 15:45:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
06/24/2022 15:45:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
06/24/2022 15:45:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
06/24/2022 15:45:56 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.5333333333333333 on epoch=649
06/24/2022 15:45:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
06/24/2022 15:45:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/24/2022 15:45:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
06/24/2022 15:46:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 15:46:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
06/24/2022 15:46:02 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.5076923076923077 on epoch=674
06/24/2022 15:46:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
06/24/2022 15:46:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
06/24/2022 15:46:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/24/2022 15:46:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
06/24/2022 15:46:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/24/2022 15:46:09 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=699
06/24/2022 15:46:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
06/24/2022 15:46:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/24/2022 15:46:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
06/24/2022 15:46:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
06/24/2022 15:46:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
06/24/2022 15:46:16 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.5555555555555556 on epoch=724
06/24/2022 15:46:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/24/2022 15:46:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
06/24/2022 15:46:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/24/2022 15:46:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 15:46:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/24/2022 15:46:22 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=749
06/24/2022 15:46:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 15:46:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/24/2022 15:46:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
06/24/2022 15:46:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/24/2022 15:46:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/24/2022 15:46:29 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=774
06/24/2022 15:46:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/24/2022 15:46:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
06/24/2022 15:46:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 15:46:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
06/24/2022 15:46:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 15:46:35 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=799
06/24/2022 15:46:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 15:46:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/24/2022 15:46:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 15:46:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/24/2022 15:46:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 15:46:42 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=824
06/24/2022 15:46:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
06/24/2022 15:46:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 15:46:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 15:46:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=844
06/24/2022 15:46:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
06/24/2022 15:46:48 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=849
06/24/2022 15:46:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 15:46:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
06/24/2022 15:46:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/24/2022 15:46:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 15:46:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 15:46:55 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=874
06/24/2022 15:46:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
06/24/2022 15:46:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/24/2022 15:46:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 15:47:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 15:47:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 15:47:02 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=899
06/24/2022 15:47:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 15:47:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 15:47:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/24/2022 15:47:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/24/2022 15:47:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 15:47:08 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=924
06/24/2022 15:47:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 15:47:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 15:47:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 15:47:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/24/2022 15:47:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 15:47:15 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=949
06/24/2022 15:47:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 15:47:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 15:47:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 15:47:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
06/24/2022 15:47:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 15:47:21 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=974
06/24/2022 15:47:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 15:47:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
06/24/2022 15:47:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 15:47:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/24/2022 15:47:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 15:47:28 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=999
06/24/2022 15:47:28 - INFO - __main__ - save last model!
06/24/2022 15:47:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 15:47:28 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 15:47:28 - INFO - __main__ - Printing 3 examples
06/24/2022 15:47:28 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 15:47:28 - INFO - __main__ - ['0']
06/24/2022 15:47:28 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 15:47:28 - INFO - __main__ - ['1']
06/24/2022 15:47:28 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 15:47:28 - INFO - __main__ - ['1']
06/24/2022 15:47:28 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:47:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:47:29 - INFO - __main__ - Printing 3 examples
06/24/2022 15:47:29 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 15:47:29 - INFO - __main__ - ['0']
06/24/2022 15:47:29 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 15:47:29 - INFO - __main__ - ['0']
06/24/2022 15:47:29 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 15:47:29 - INFO - __main__ - ['0']
06/24/2022 15:47:29 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:47:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:47:29 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:47:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:47:29 - INFO - __main__ - Printing 3 examples
06/24/2022 15:47:29 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 15:47:29 - INFO - __main__ - ['0']
06/24/2022 15:47:29 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 15:47:29 - INFO - __main__ - ['0']
06/24/2022 15:47:29 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 15:47:29 - INFO - __main__ - ['0']
06/24/2022 15:47:29 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:47:29 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:47:29 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:47:32 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:47:35 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:47:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:47:35 - INFO - __main__ - Starting training!
06/24/2022 15:47:40 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 15:49:07 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_42_0.2_8_predictions.txt
06/24/2022 15:49:07 - INFO - __main__ - Classification-F1 on test data: 0.3155
06/24/2022 15:49:07 - INFO - __main__ - prefix=paws_16_42, lr=0.2, bsz=8, dev_performance=0.6000000000000001, test_performance=0.3155461803450166
06/24/2022 15:49:07 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.5, bsz=8 ...
06/24/2022 15:49:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:49:08 - INFO - __main__ - Printing 3 examples
06/24/2022 15:49:08 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 15:49:08 - INFO - __main__ - ['0']
06/24/2022 15:49:08 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 15:49:08 - INFO - __main__ - ['0']
06/24/2022 15:49:08 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 15:49:08 - INFO - __main__ - ['0']
06/24/2022 15:49:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:49:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:49:08 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:49:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:49:08 - INFO - __main__ - Printing 3 examples
06/24/2022 15:49:08 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 15:49:08 - INFO - __main__ - ['0']
06/24/2022 15:49:08 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 15:49:08 - INFO - __main__ - ['0']
06/24/2022 15:49:08 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 15:49:08 - INFO - __main__ - ['0']
06/24/2022 15:49:08 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:49:08 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:49:08 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:49:14 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:49:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:49:14 - INFO - __main__ - Starting training!
06/24/2022 15:49:15 - INFO - __main__ - Step 10 Global step 10 Train loss 3.69 on epoch=4
06/24/2022 15:49:17 - INFO - __main__ - Step 20 Global step 20 Train loss 2.26 on epoch=9
06/24/2022 15:49:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.15 on epoch=14
06/24/2022 15:49:19 - INFO - __main__ - Step 40 Global step 40 Train loss 0.68 on epoch=19
06/24/2022 15:49:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=24
06/24/2022 15:49:21 - INFO - __main__ - Global step 50 Train loss 1.67 Classification-F1 0.3992490613266583 on epoch=24
06/24/2022 15:49:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3992490613266583 on epoch=24, global_step=50
06/24/2022 15:49:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=29
06/24/2022 15:49:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=34
06/24/2022 15:49:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=39
06/24/2022 15:49:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=44
06/24/2022 15:49:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
06/24/2022 15:49:27 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.4385964912280702 on epoch=49
06/24/2022 15:49:27 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4385964912280702 on epoch=49, global_step=100
06/24/2022 15:49:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
06/24/2022 15:49:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
06/24/2022 15:49:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=64
06/24/2022 15:49:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=69
06/24/2022 15:49:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=74
06/24/2022 15:49:34 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=74
06/24/2022 15:49:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=79
06/24/2022 15:49:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=84
06/24/2022 15:49:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
06/24/2022 15:49:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.34 on epoch=94
06/24/2022 15:49:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.31 on epoch=99
06/24/2022 15:49:41 - INFO - __main__ - Global step 200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 15:49:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
06/24/2022 15:49:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=109
06/24/2022 15:49:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
06/24/2022 15:49:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=119
06/24/2022 15:49:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.32 on epoch=124
06/24/2022 15:49:47 - INFO - __main__ - Global step 250 Train loss 0.36 Classification-F1 0.39756367663344405 on epoch=124
06/24/2022 15:49:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
06/24/2022 15:49:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.32 on epoch=134
06/24/2022 15:49:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
06/24/2022 15:49:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
06/24/2022 15:49:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
06/24/2022 15:49:54 - INFO - __main__ - Global step 300 Train loss 0.30 Classification-F1 0.43529411764705883 on epoch=149
06/24/2022 15:49:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
06/24/2022 15:49:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
06/24/2022 15:49:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
06/24/2022 15:49:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
06/24/2022 15:50:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=174
06/24/2022 15:50:00 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.39139139139139134 on epoch=174
06/24/2022 15:50:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
06/24/2022 15:50:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=184
06/24/2022 15:50:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=189
06/24/2022 15:50:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=194
06/24/2022 15:50:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
06/24/2022 15:50:07 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.4285714285714286 on epoch=199
06/24/2022 15:50:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
06/24/2022 15:50:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=209
06/24/2022 15:50:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
06/24/2022 15:50:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.17 on epoch=219
06/24/2022 15:50:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
06/24/2022 15:50:14 - INFO - __main__ - Global step 450 Train loss 0.13 Classification-F1 0.464039408866995 on epoch=224
06/24/2022 15:50:14 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.464039408866995 on epoch=224, global_step=450
06/24/2022 15:50:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=229
06/24/2022 15:50:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
06/24/2022 15:50:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
06/24/2022 15:50:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
06/24/2022 15:50:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
06/24/2022 15:50:20 - INFO - __main__ - Global step 500 Train loss 0.10 Classification-F1 0.4420512820512821 on epoch=249
06/24/2022 15:50:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
06/24/2022 15:50:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
06/24/2022 15:50:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
06/24/2022 15:50:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
06/24/2022 15:50:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
06/24/2022 15:50:27 - INFO - __main__ - Global step 550 Train loss 0.06 Classification-F1 0.4285714285714286 on epoch=274
06/24/2022 15:50:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
06/24/2022 15:50:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
06/24/2022 15:50:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
06/24/2022 15:50:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
06/24/2022 15:50:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
06/24/2022 15:50:34 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.39139139139139134 on epoch=299
06/24/2022 15:50:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
06/24/2022 15:50:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
06/24/2022 15:50:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
06/24/2022 15:50:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
06/24/2022 15:50:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
06/24/2022 15:50:40 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.4285714285714286 on epoch=324
06/24/2022 15:50:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
06/24/2022 15:50:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
06/24/2022 15:50:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
06/24/2022 15:50:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
06/24/2022 15:50:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
06/24/2022 15:50:47 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.4285714285714286 on epoch=349
06/24/2022 15:50:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
06/24/2022 15:50:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
06/24/2022 15:50:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
06/24/2022 15:50:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
06/24/2022 15:50:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
06/24/2022 15:50:54 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.4285714285714286 on epoch=374
06/24/2022 15:50:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
06/24/2022 15:50:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
06/24/2022 15:50:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
06/24/2022 15:50:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
06/24/2022 15:51:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/24/2022 15:51:00 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=399
06/24/2022 15:51:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
06/24/2022 15:51:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
06/24/2022 15:51:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
06/24/2022 15:51:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
06/24/2022 15:51:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
06/24/2022 15:51:07 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=424
06/24/2022 15:51:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
06/24/2022 15:51:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/24/2022 15:51:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
06/24/2022 15:51:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
06/24/2022 15:51:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
06/24/2022 15:51:14 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=449
06/24/2022 15:51:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
06/24/2022 15:51:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
06/24/2022 15:51:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
06/24/2022 15:51:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
06/24/2022 15:51:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
06/24/2022 15:51:20 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=474
06/24/2022 15:51:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/24/2022 15:51:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
06/24/2022 15:51:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
06/24/2022 15:51:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/24/2022 15:51:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 15:51:27 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=499
06/24/2022 15:51:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
06/24/2022 15:51:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
06/24/2022 15:51:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
06/24/2022 15:51:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
06/24/2022 15:51:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
06/24/2022 15:51:33 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.4285714285714286 on epoch=524
06/24/2022 15:51:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
06/24/2022 15:51:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
06/24/2022 15:51:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
06/24/2022 15:51:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/24/2022 15:51:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 15:51:40 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=549
06/24/2022 15:51:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
06/24/2022 15:51:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 15:51:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 15:51:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 15:51:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/24/2022 15:51:47 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=574
06/24/2022 15:51:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
06/24/2022 15:51:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 15:51:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
06/24/2022 15:51:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 15:51:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 15:51:53 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=599
06/24/2022 15:51:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
06/24/2022 15:51:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 15:51:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 15:51:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 15:51:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 15:52:00 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=624
06/24/2022 15:52:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 15:52:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
06/24/2022 15:52:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
06/24/2022 15:52:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 15:52:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 15:52:06 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=649
06/24/2022 15:52:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
06/24/2022 15:52:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
06/24/2022 15:52:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 15:52:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
06/24/2022 15:52:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 15:52:13 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=674
06/24/2022 15:52:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
06/24/2022 15:52:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 15:52:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
06/24/2022 15:52:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 15:52:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 15:52:20 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=699
06/24/2022 15:52:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 15:52:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
06/24/2022 15:52:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/24/2022 15:52:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
06/24/2022 15:52:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
06/24/2022 15:52:26 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=724
06/24/2022 15:52:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/24/2022 15:52:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
06/24/2022 15:52:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 15:52:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 15:52:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 15:52:33 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=749
06/24/2022 15:52:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 15:52:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
06/24/2022 15:52:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 15:52:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 15:52:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 15:52:39 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=774
06/24/2022 15:52:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 15:52:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 15:52:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
06/24/2022 15:52:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 15:52:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 15:52:46 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=799
06/24/2022 15:52:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 15:52:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
06/24/2022 15:52:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 15:52:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 15:52:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 15:52:53 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=824
06/24/2022 15:52:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 15:52:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 15:52:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 15:52:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 15:52:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
06/24/2022 15:52:59 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=849
06/24/2022 15:53:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 15:53:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 15:53:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
06/24/2022 15:53:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 15:53:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 15:53:06 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=874
06/24/2022 15:53:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
06/24/2022 15:53:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 15:53:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 15:53:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 15:53:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 15:53:12 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=899
06/24/2022 15:53:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=904
06/24/2022 15:53:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 15:53:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 15:53:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 15:53:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 15:53:19 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=924
06/24/2022 15:53:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 15:53:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 15:53:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 15:53:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 15:53:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 15:53:26 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=949
06/24/2022 15:53:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 15:53:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 15:53:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 15:53:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 15:53:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/24/2022 15:53:32 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=974
06/24/2022 15:53:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 15:53:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 15:53:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 15:53:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 15:53:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 15:53:39 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=999
06/24/2022 15:53:39 - INFO - __main__ - save last model!
06/24/2022 15:53:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 15:53:39 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 15:53:39 - INFO - __main__ - Printing 3 examples
06/24/2022 15:53:39 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 15:53:39 - INFO - __main__ - ['0']
06/24/2022 15:53:39 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 15:53:39 - INFO - __main__ - ['1']
06/24/2022 15:53:39 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 15:53:39 - INFO - __main__ - ['1']
06/24/2022 15:53:39 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:53:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:53:40 - INFO - __main__ - Printing 3 examples
06/24/2022 15:53:40 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 15:53:40 - INFO - __main__ - ['0']
06/24/2022 15:53:40 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 15:53:40 - INFO - __main__ - ['0']
06/24/2022 15:53:40 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 15:53:40 - INFO - __main__ - ['0']
06/24/2022 15:53:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:53:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:53:40 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:53:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:53:40 - INFO - __main__ - Printing 3 examples
06/24/2022 15:53:40 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 15:53:40 - INFO - __main__ - ['0']
06/24/2022 15:53:40 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 15:53:40 - INFO - __main__ - ['0']
06/24/2022 15:53:40 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 15:53:40 - INFO - __main__ - ['0']
06/24/2022 15:53:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:53:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:53:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:53:43 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:53:46 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:53:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:53:46 - INFO - __main__ - Starting training!
06/24/2022 15:53:51 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 15:55:19 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_87_0.5_8_predictions.txt
06/24/2022 15:55:19 - INFO - __main__ - Classification-F1 on test data: 0.5042
06/24/2022 15:55:19 - INFO - __main__ - prefix=paws_16_87, lr=0.5, bsz=8, dev_performance=0.464039408866995, test_performance=0.50423453827715
06/24/2022 15:55:19 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.4, bsz=8 ...
06/24/2022 15:55:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:55:20 - INFO - __main__ - Printing 3 examples
06/24/2022 15:55:20 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 15:55:20 - INFO - __main__ - ['0']
06/24/2022 15:55:20 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 15:55:20 - INFO - __main__ - ['0']
06/24/2022 15:55:20 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 15:55:20 - INFO - __main__ - ['0']
06/24/2022 15:55:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:55:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:55:20 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:55:20 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:55:20 - INFO - __main__ - Printing 3 examples
06/24/2022 15:55:20 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 15:55:20 - INFO - __main__ - ['0']
06/24/2022 15:55:20 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 15:55:20 - INFO - __main__ - ['0']
06/24/2022 15:55:20 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 15:55:20 - INFO - __main__ - ['0']
06/24/2022 15:55:20 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:55:20 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:55:20 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 15:55:26 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 15:55:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 15:55:26 - INFO - __main__ - Starting training!
06/24/2022 15:55:28 - INFO - __main__ - Step 10 Global step 10 Train loss 3.91 on epoch=4
06/24/2022 15:55:29 - INFO - __main__ - Step 20 Global step 20 Train loss 2.41 on epoch=9
06/24/2022 15:55:30 - INFO - __main__ - Step 30 Global step 30 Train loss 1.51 on epoch=14
06/24/2022 15:55:31 - INFO - __main__ - Step 40 Global step 40 Train loss 0.88 on epoch=19
06/24/2022 15:55:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.69 on epoch=24
06/24/2022 15:55:33 - INFO - __main__ - Global step 50 Train loss 1.88 Classification-F1 0.4589371980676329 on epoch=24
06/24/2022 15:55:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4589371980676329 on epoch=24, global_step=50
06/24/2022 15:55:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=29
06/24/2022 15:55:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=34
06/24/2022 15:55:37 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=39
06/24/2022 15:55:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
06/24/2022 15:55:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
06/24/2022 15:55:39 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=49
06/24/2022 15:55:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=54
06/24/2022 15:55:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=59
06/24/2022 15:55:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
06/24/2022 15:55:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=69
06/24/2022 15:55:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
06/24/2022 15:55:46 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=74
06/24/2022 15:55:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=79
06/24/2022 15:55:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=84
06/24/2022 15:55:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=89
06/24/2022 15:55:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=94
06/24/2022 15:55:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=99
06/24/2022 15:55:52 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3992490613266583 on epoch=99
06/24/2022 15:55:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=104
06/24/2022 15:55:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
06/24/2022 15:55:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=114
06/24/2022 15:55:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.33 on epoch=119
06/24/2022 15:55:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=124
06/24/2022 15:55:58 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 15:56:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
06/24/2022 15:56:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
06/24/2022 15:56:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
06/24/2022 15:56:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
06/24/2022 15:56:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=149
06/24/2022 15:56:05 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.36374269005847953 on epoch=149
06/24/2022 15:56:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=154
06/24/2022 15:56:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=159
06/24/2022 15:56:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=164
06/24/2022 15:56:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=169
06/24/2022 15:56:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=174
06/24/2022 15:56:11 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.37254901960784315 on epoch=174
06/24/2022 15:56:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
06/24/2022 15:56:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=184
06/24/2022 15:56:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=189
06/24/2022 15:56:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=194
06/24/2022 15:56:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=199
06/24/2022 15:56:18 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.3454545454545454 on epoch=199
06/24/2022 15:56:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=204
06/24/2022 15:56:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=209
06/24/2022 15:56:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
06/24/2022 15:56:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
06/24/2022 15:56:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
06/24/2022 15:56:25 - INFO - __main__ - Global step 450 Train loss 0.26 Classification-F1 0.30980392156862746 on epoch=224
06/24/2022 15:56:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
06/24/2022 15:56:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=234
06/24/2022 15:56:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
06/24/2022 15:56:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
06/24/2022 15:56:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
06/24/2022 15:56:32 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3764102564102564 on epoch=249
06/24/2022 15:56:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
06/24/2022 15:56:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=259
06/24/2022 15:56:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=264
06/24/2022 15:56:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=269
06/24/2022 15:56:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.15 on epoch=274
06/24/2022 15:56:39 - INFO - __main__ - Global step 550 Train loss 0.19 Classification-F1 0.3522267206477733 on epoch=274
06/24/2022 15:56:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.18 on epoch=279
06/24/2022 15:56:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=284
06/24/2022 15:56:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
06/24/2022 15:56:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=294
06/24/2022 15:56:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=299
06/24/2022 15:56:45 - INFO - __main__ - Global step 600 Train loss 0.14 Classification-F1 0.3764102564102564 on epoch=299
06/24/2022 15:56:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=304
06/24/2022 15:56:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=309
06/24/2022 15:56:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=314
06/24/2022 15:56:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=319
06/24/2022 15:56:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=324
06/24/2022 15:56:52 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.4554554554554554 on epoch=324
06/24/2022 15:56:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
06/24/2022 15:56:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
06/24/2022 15:56:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
06/24/2022 15:56:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
06/24/2022 15:56:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=349
06/24/2022 15:56:59 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.4920634920634921 on epoch=349
06/24/2022 15:56:59 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.4920634920634921 on epoch=349, global_step=700
06/24/2022 15:57:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
06/24/2022 15:57:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
06/24/2022 15:57:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
06/24/2022 15:57:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
06/24/2022 15:57:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
06/24/2022 15:57:06 - INFO - __main__ - Global step 750 Train loss 0.04 Classification-F1 0.4920634920634921 on epoch=374
06/24/2022 15:57:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
06/24/2022 15:57:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
06/24/2022 15:57:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
06/24/2022 15:57:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
06/24/2022 15:57:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
06/24/2022 15:57:13 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.4554554554554554 on epoch=399
06/24/2022 15:57:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
06/24/2022 15:57:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
06/24/2022 15:57:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
06/24/2022 15:57:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
06/24/2022 15:57:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
06/24/2022 15:57:19 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=424
06/24/2022 15:57:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
06/24/2022 15:57:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
06/24/2022 15:57:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
06/24/2022 15:57:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
06/24/2022 15:57:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
06/24/2022 15:57:26 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=449
06/24/2022 15:57:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
06/24/2022 15:57:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
06/24/2022 15:57:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
06/24/2022 15:57:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
06/24/2022 15:57:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
06/24/2022 15:57:33 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=474
06/24/2022 15:57:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
06/24/2022 15:57:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
06/24/2022 15:57:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
06/24/2022 15:57:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
06/24/2022 15:57:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
06/24/2022 15:57:40 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=499
06/24/2022 15:57:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
06/24/2022 15:57:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
06/24/2022 15:57:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
06/24/2022 15:57:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
06/24/2022 15:57:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
06/24/2022 15:57:46 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=524
06/24/2022 15:57:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
06/24/2022 15:57:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
06/24/2022 15:57:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
06/24/2022 15:57:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
06/24/2022 15:57:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
06/24/2022 15:57:53 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=549
06/24/2022 15:57:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5270935960591133 on epoch=549, global_step=1100
06/24/2022 15:57:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
06/24/2022 15:57:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
06/24/2022 15:57:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
06/24/2022 15:57:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
06/24/2022 15:58:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
06/24/2022 15:58:00 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=574
06/24/2022 15:58:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
06/24/2022 15:58:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
06/24/2022 15:58:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
06/24/2022 15:58:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
06/24/2022 15:58:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
06/24/2022 15:58:07 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=599
06/24/2022 15:58:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
06/24/2022 15:58:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
06/24/2022 15:58:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
06/24/2022 15:58:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
06/24/2022 15:58:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
06/24/2022 15:58:14 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=624
06/24/2022 15:58:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
06/24/2022 15:58:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
06/24/2022 15:58:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
06/24/2022 15:58:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
06/24/2022 15:58:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
06/24/2022 15:58:20 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=649
06/24/2022 15:58:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
06/24/2022 15:58:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
06/24/2022 15:58:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
06/24/2022 15:58:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
06/24/2022 15:58:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
06/24/2022 15:58:27 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=674
06/24/2022 15:58:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 15:58:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
06/24/2022 15:58:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/24/2022 15:58:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
06/24/2022 15:58:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
06/24/2022 15:58:34 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=699
06/24/2022 15:58:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
06/24/2022 15:58:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
06/24/2022 15:58:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
06/24/2022 15:58:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/24/2022 15:58:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
06/24/2022 15:58:41 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=724
06/24/2022 15:58:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
06/24/2022 15:58:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
06/24/2022 15:58:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
06/24/2022 15:58:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
06/24/2022 15:58:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
06/24/2022 15:58:48 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=749
06/24/2022 15:58:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
06/24/2022 15:58:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
06/24/2022 15:58:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
06/24/2022 15:58:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
06/24/2022 15:58:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
06/24/2022 15:58:55 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=774
06/24/2022 15:58:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
06/24/2022 15:58:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
06/24/2022 15:58:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 15:59:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
06/24/2022 15:59:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
06/24/2022 15:59:01 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=799
06/24/2022 15:59:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
06/24/2022 15:59:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/24/2022 15:59:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
06/24/2022 15:59:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
06/24/2022 15:59:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
06/24/2022 15:59:08 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=824
06/24/2022 15:59:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 15:59:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
06/24/2022 15:59:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
06/24/2022 15:59:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
06/24/2022 15:59:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 15:59:15 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=849
06/24/2022 15:59:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
06/24/2022 15:59:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
06/24/2022 15:59:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/24/2022 15:59:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
06/24/2022 15:59:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 15:59:22 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=874
06/24/2022 15:59:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 15:59:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
06/24/2022 15:59:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 15:59:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 15:59:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
06/24/2022 15:59:29 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=899
06/24/2022 15:59:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
06/24/2022 15:59:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
06/24/2022 15:59:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
06/24/2022 15:59:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
06/24/2022 15:59:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 15:59:36 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=924
06/24/2022 15:59:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
06/24/2022 15:59:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 15:59:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
06/24/2022 15:59:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
06/24/2022 15:59:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
06/24/2022 15:59:42 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=949
06/24/2022 15:59:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
06/24/2022 15:59:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 15:59:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 15:59:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 15:59:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 15:59:49 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.2540322580645162 on epoch=974
06/24/2022 15:59:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 15:59:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 15:59:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 15:59:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/24/2022 15:59:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 15:59:56 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.22844827586206895 on epoch=999
06/24/2022 15:59:56 - INFO - __main__ - save last model!
06/24/2022 15:59:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 15:59:56 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 15:59:56 - INFO - __main__ - Printing 3 examples
06/24/2022 15:59:56 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 15:59:56 - INFO - __main__ - ['0']
06/24/2022 15:59:56 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 15:59:56 - INFO - __main__ - ['1']
06/24/2022 15:59:56 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 15:59:56 - INFO - __main__ - ['1']
06/24/2022 15:59:56 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:59:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:59:57 - INFO - __main__ - Printing 3 examples
06/24/2022 15:59:57 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 15:59:57 - INFO - __main__ - ['0']
06/24/2022 15:59:57 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 15:59:57 - INFO - __main__ - ['0']
06/24/2022 15:59:57 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 15:59:57 - INFO - __main__ - ['0']
06/24/2022 15:59:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:59:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:59:57 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 15:59:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 15:59:57 - INFO - __main__ - Printing 3 examples
06/24/2022 15:59:57 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 15:59:57 - INFO - __main__ - ['0']
06/24/2022 15:59:57 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 15:59:57 - INFO - __main__ - ['0']
06/24/2022 15:59:57 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 15:59:57 - INFO - __main__ - ['0']
06/24/2022 15:59:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 15:59:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 15:59:57 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:00:00 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:00:02 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 16:00:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 16:00:03 - INFO - __main__ - Starting training!
06/24/2022 16:00:08 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 16:01:39 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_87_0.4_8_predictions.txt
06/24/2022 16:01:39 - INFO - __main__ - Classification-F1 on test data: 0.1203
06/24/2022 16:01:39 - INFO - __main__ - prefix=paws_16_87, lr=0.4, bsz=8, dev_performance=0.5270935960591133, test_performance=0.12033816415693749
06/24/2022 16:01:39 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.3, bsz=8 ...
06/24/2022 16:01:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:01:40 - INFO - __main__ - Printing 3 examples
06/24/2022 16:01:40 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 16:01:40 - INFO - __main__ - ['0']
06/24/2022 16:01:40 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 16:01:40 - INFO - __main__ - ['0']
06/24/2022 16:01:40 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 16:01:40 - INFO - __main__ - ['0']
06/24/2022 16:01:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:01:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:01:40 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 16:01:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:01:40 - INFO - __main__ - Printing 3 examples
06/24/2022 16:01:40 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 16:01:40 - INFO - __main__ - ['0']
06/24/2022 16:01:40 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 16:01:40 - INFO - __main__ - ['0']
06/24/2022 16:01:40 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 16:01:40 - INFO - __main__ - ['0']
06/24/2022 16:01:40 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:01:40 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:01:40 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:01:46 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 16:01:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 16:01:46 - INFO - __main__ - Starting training!
06/24/2022 16:01:48 - INFO - __main__ - Step 10 Global step 10 Train loss 4.05 on epoch=4
06/24/2022 16:01:49 - INFO - __main__ - Step 20 Global step 20 Train loss 2.99 on epoch=9
06/24/2022 16:01:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.02 on epoch=14
06/24/2022 16:01:51 - INFO - __main__ - Step 40 Global step 40 Train loss 1.25 on epoch=19
06/24/2022 16:01:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.90 on epoch=24
06/24/2022 16:01:53 - INFO - __main__ - Global step 50 Train loss 2.24 Classification-F1 0.35819819819819826 on epoch=24
06/24/2022 16:01:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.35819819819819826 on epoch=24, global_step=50
06/24/2022 16:01:54 - INFO - __main__ - Step 60 Global step 60 Train loss 0.80 on epoch=29
06/24/2022 16:01:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.69 on epoch=34
06/24/2022 16:01:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=39
06/24/2022 16:01:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=44
06/24/2022 16:01:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
06/24/2022 16:02:00 - INFO - __main__ - Global step 100 Train loss 0.63 Classification-F1 0.4817813765182186 on epoch=49
06/24/2022 16:02:00 - INFO - __main__ - Saving model with best Classification-F1: 0.35819819819819826 -> 0.4817813765182186 on epoch=49, global_step=100
06/24/2022 16:02:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=54
06/24/2022 16:02:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=59
06/24/2022 16:02:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
06/24/2022 16:02:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
06/24/2022 16:02:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=74
06/24/2022 16:02:06 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.3552492046659597 on epoch=74
06/24/2022 16:02:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=79
06/24/2022 16:02:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=84
06/24/2022 16:02:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
06/24/2022 16:02:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=94
06/24/2022 16:02:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=99
06/24/2022 16:02:13 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.3992490613266583 on epoch=99
06/24/2022 16:02:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=104
06/24/2022 16:02:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=109
06/24/2022 16:02:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=114
06/24/2022 16:02:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=119
06/24/2022 16:02:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=124
06/24/2022 16:02:19 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=124
06/24/2022 16:02:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=129
06/24/2022 16:02:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=134
06/24/2022 16:02:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
06/24/2022 16:02:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=144
06/24/2022 16:02:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=149
06/24/2022 16:02:26 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.4231177094379639 on epoch=149
06/24/2022 16:02:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
06/24/2022 16:02:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=159
06/24/2022 16:02:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=164
06/24/2022 16:02:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=169
06/24/2022 16:02:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=174
06/24/2022 16:02:33 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.5835835835835835 on epoch=174
06/24/2022 16:02:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4817813765182186 -> 0.5835835835835835 on epoch=174, global_step=350
06/24/2022 16:02:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=179
06/24/2022 16:02:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=184
06/24/2022 16:02:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=189
06/24/2022 16:02:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
06/24/2022 16:02:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=199
06/24/2022 16:02:39 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.40566959921798634 on epoch=199
06/24/2022 16:02:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=204
06/24/2022 16:02:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=209
06/24/2022 16:02:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.30 on epoch=214
06/24/2022 16:02:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=219
06/24/2022 16:02:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=224
06/24/2022 16:02:46 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.464039408866995 on epoch=224
06/24/2022 16:02:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.31 on epoch=229
06/24/2022 16:02:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
06/24/2022 16:02:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=239
06/24/2022 16:02:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=244
06/24/2022 16:02:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=249
06/24/2022 16:02:52 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.464039408866995 on epoch=249
06/24/2022 16:02:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=254
06/24/2022 16:02:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=259
06/24/2022 16:02:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
06/24/2022 16:02:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
06/24/2022 16:02:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
06/24/2022 16:02:59 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.4920634920634921 on epoch=274
06/24/2022 16:03:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=279
06/24/2022 16:03:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
06/24/2022 16:03:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
06/24/2022 16:03:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=294
06/24/2022 16:03:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
06/24/2022 16:03:06 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.39999999999999997 on epoch=299
06/24/2022 16:03:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=304
06/24/2022 16:03:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=309
06/24/2022 16:03:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=314
06/24/2022 16:03:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
06/24/2022 16:03:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=324
06/24/2022 16:03:12 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.5465587044534412 on epoch=324
06/24/2022 16:03:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=329
06/24/2022 16:03:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=334
06/24/2022 16:03:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
06/24/2022 16:03:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=344
06/24/2022 16:03:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=349
06/24/2022 16:03:19 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.40566959921798634 on epoch=349
06/24/2022 16:03:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=354
06/24/2022 16:03:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.15 on epoch=359
06/24/2022 16:03:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=364
06/24/2022 16:03:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=369
06/24/2022 16:03:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=374
06/24/2022 16:03:25 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.5076923076923077 on epoch=374
06/24/2022 16:03:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=379
06/24/2022 16:03:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=384
06/24/2022 16:03:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=389
06/24/2022 16:03:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=394
06/24/2022 16:03:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=399
06/24/2022 16:03:32 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.5465587044534412 on epoch=399
06/24/2022 16:03:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=404
06/24/2022 16:03:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=409
06/24/2022 16:03:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=414
06/24/2022 16:03:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=419
06/24/2022 16:03:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=424
06/24/2022 16:03:39 - INFO - __main__ - Global step 850 Train loss 0.13 Classification-F1 0.4909862142099682 on epoch=424
06/24/2022 16:03:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=429
06/24/2022 16:03:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=434
06/24/2022 16:03:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=439
06/24/2022 16:03:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=444
06/24/2022 16:03:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=449
06/24/2022 16:03:45 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.5733333333333335 on epoch=449
06/24/2022 16:03:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=454
06/24/2022 16:03:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=459
06/24/2022 16:03:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=464
06/24/2022 16:03:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
06/24/2022 16:03:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=474
06/24/2022 16:03:52 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.5733333333333335 on epoch=474
06/24/2022 16:03:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=479
06/24/2022 16:03:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
06/24/2022 16:03:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=489
06/24/2022 16:03:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=494
06/24/2022 16:03:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=499
06/24/2022 16:03:59 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.5465587044534412 on epoch=499
06/24/2022 16:04:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=504
06/24/2022 16:04:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
06/24/2022 16:04:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
06/24/2022 16:04:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
06/24/2022 16:04:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
06/24/2022 16:04:05 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.5465587044534412 on epoch=524
06/24/2022 16:04:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=529
06/24/2022 16:04:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=534
06/24/2022 16:04:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
06/24/2022 16:04:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=544
06/24/2022 16:04:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=549
06/24/2022 16:04:12 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.4909862142099682 on epoch=549
06/24/2022 16:04:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=554
06/24/2022 16:04:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
06/24/2022 16:04:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
06/24/2022 16:04:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
06/24/2022 16:04:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=574
06/24/2022 16:04:18 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.5465587044534412 on epoch=574
06/24/2022 16:04:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
06/24/2022 16:04:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
06/24/2022 16:04:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=589
06/24/2022 16:04:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=594
06/24/2022 16:04:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=599
06/24/2022 16:04:25 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.4920634920634921 on epoch=599
06/24/2022 16:04:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
06/24/2022 16:04:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
06/24/2022 16:04:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
06/24/2022 16:04:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/24/2022 16:04:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/24/2022 16:04:31 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.4920634920634921 on epoch=624
06/24/2022 16:04:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
06/24/2022 16:04:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
06/24/2022 16:04:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
06/24/2022 16:04:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
06/24/2022 16:04:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
06/24/2022 16:04:38 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.5195195195195195 on epoch=649
06/24/2022 16:04:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=654
06/24/2022 16:04:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
06/24/2022 16:04:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
06/24/2022 16:04:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
06/24/2022 16:04:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
06/24/2022 16:04:45 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.4920634920634921 on epoch=674
06/24/2022 16:04:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
06/24/2022 16:04:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
06/24/2022 16:04:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
06/24/2022 16:04:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
06/24/2022 16:04:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
06/24/2022 16:04:51 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.4980392156862745 on epoch=699
06/24/2022 16:04:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
06/24/2022 16:04:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
06/24/2022 16:04:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
06/24/2022 16:04:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
06/24/2022 16:04:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/24/2022 16:04:58 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=724
06/24/2022 16:04:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
06/24/2022 16:05:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
06/24/2022 16:05:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
06/24/2022 16:05:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
06/24/2022 16:05:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/24/2022 16:05:04 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=749
06/24/2022 16:05:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
06/24/2022 16:05:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
06/24/2022 16:05:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 16:05:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
06/24/2022 16:05:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
06/24/2022 16:05:11 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=774
06/24/2022 16:05:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/24/2022 16:05:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
06/24/2022 16:05:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=789
06/24/2022 16:05:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/24/2022 16:05:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=799
06/24/2022 16:05:18 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.4920634920634921 on epoch=799
06/24/2022 16:05:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=804
06/24/2022 16:05:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/24/2022 16:05:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 16:05:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/24/2022 16:05:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
06/24/2022 16:05:24 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.464039408866995 on epoch=824
06/24/2022 16:05:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
06/24/2022 16:05:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
06/24/2022 16:05:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/24/2022 16:05:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
06/24/2022 16:05:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 16:05:31 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=849
06/24/2022 16:05:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
06/24/2022 16:05:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=859
06/24/2022 16:05:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
06/24/2022 16:05:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/24/2022 16:05:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
06/24/2022 16:05:37 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.4420512820512821 on epoch=874
06/24/2022 16:05:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
06/24/2022 16:05:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
06/24/2022 16:05:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
06/24/2022 16:05:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
06/24/2022 16:05:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
06/24/2022 16:05:44 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.41700404858299595 on epoch=899
06/24/2022 16:05:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/24/2022 16:05:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/24/2022 16:05:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/24/2022 16:05:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/24/2022 16:05:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
06/24/2022 16:05:50 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=924
06/24/2022 16:05:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/24/2022 16:05:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 16:05:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
06/24/2022 16:05:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/24/2022 16:05:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=949
06/24/2022 16:05:57 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=949
06/24/2022 16:05:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
06/24/2022 16:05:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 16:06:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
06/24/2022 16:06:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 16:06:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
06/24/2022 16:06:04 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=974
06/24/2022 16:06:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 16:06:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 16:06:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
06/24/2022 16:06:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
06/24/2022 16:06:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 16:06:10 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=999
06/24/2022 16:06:10 - INFO - __main__ - save last model!
06/24/2022 16:06:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 16:06:10 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 16:06:10 - INFO - __main__ - Printing 3 examples
06/24/2022 16:06:10 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 16:06:10 - INFO - __main__ - ['0']
06/24/2022 16:06:10 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 16:06:10 - INFO - __main__ - ['1']
06/24/2022 16:06:10 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 16:06:10 - INFO - __main__ - ['1']
06/24/2022 16:06:10 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:06:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:06:11 - INFO - __main__ - Printing 3 examples
06/24/2022 16:06:11 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 16:06:11 - INFO - __main__ - ['0']
06/24/2022 16:06:11 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 16:06:11 - INFO - __main__ - ['0']
06/24/2022 16:06:11 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 16:06:11 - INFO - __main__ - ['0']
06/24/2022 16:06:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:06:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:06:11 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 16:06:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:06:11 - INFO - __main__ - Printing 3 examples
06/24/2022 16:06:11 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 16:06:11 - INFO - __main__ - ['0']
06/24/2022 16:06:11 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 16:06:11 - INFO - __main__ - ['0']
06/24/2022 16:06:11 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 16:06:11 - INFO - __main__ - ['0']
06/24/2022 16:06:11 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:06:11 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:06:11 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:06:14 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:06:17 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 16:06:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 16:06:17 - INFO - __main__ - Starting training!
06/24/2022 16:06:22 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 16:07:56 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_87_0.3_8_predictions.txt
06/24/2022 16:07:56 - INFO - __main__ - Classification-F1 on test data: 0.4932
06/24/2022 16:07:56 - INFO - __main__ - prefix=paws_16_87, lr=0.3, bsz=8, dev_performance=0.5835835835835835, test_performance=0.49317820853627264
06/24/2022 16:07:56 - INFO - __main__ - Running ... prefix=paws_16_87, lr=0.2, bsz=8 ...
06/24/2022 16:07:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:07:57 - INFO - __main__ - Printing 3 examples
06/24/2022 16:07:57 - INFO - __main__ -  [paws] sentence 1: Pike , Wyoming County , New York is the name of two villages in New York : [SEP] sentence 2: Pike , New York is the name of two locations in Wyoming County , New York :
06/24/2022 16:07:57 - INFO - __main__ - ['0']
06/24/2022 16:07:57 - INFO - __main__ -  [paws] sentence 1: Howes married three times in his life : in 1923 with Lillian Pechin , with Catherine Tabor in 1932 and in 1937 with Mary Donovan Howard . [SEP] sentence 2: Howes married three times in his life : to Mary Donovan Howard in 1923 , Catherine Tabor in 1932 , and Lillian Pechin in 1937 .
06/24/2022 16:07:57 - INFO - __main__ - ['0']
06/24/2022 16:07:57 - INFO - __main__ -  [paws] sentence 1: The 1953 Labour Party deputy election took place on 29 October 1953 , after the current deputy leader Aneurin Bevan , was challenged by Herbert Morrison . [SEP] sentence 2: The 1953 Labour Party deputy leadership election took place on 29 October 1953 , after the current deputy leader Herbert Morrison was challenged by Aneurin Bevan .
06/24/2022 16:07:57 - INFO - __main__ - ['0']
06/24/2022 16:07:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:07:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:07:57 - INFO - __main__ - Loaded 32 examples from train data
06/24/2022 16:07:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/24/2022 16:07:57 - INFO - __main__ - Printing 3 examples
06/24/2022 16:07:57 - INFO - __main__ -  [paws] sentence 1: Through the 2017 season , the Cornell Big Red have won 649 games , tied 529 games , and lost 33 regular season games . [SEP] sentence 2: The Cornell Big Red have won 649 games during the 2017 season , 529 games lost and 33 regular season games bound .
06/24/2022 16:07:57 - INFO - __main__ - ['0']
06/24/2022 16:07:57 - INFO - __main__ -  [paws] sentence 1: ( The EL - succeeded Conrail in 1982 sold the old main route Utica , Chenango , and Susquehanna Valley through Cassville to New York , Susquehanna , and Western Railway . ) [SEP] sentence 2: ( EL - Successor Conrail sold the old New York main route to Cassville , Susquehanna , and Western Railway in 1982 through Utica , Chenango , and Susquehanna Valley . )
06/24/2022 16:07:57 - INFO - __main__ - ['0']
06/24/2022 16:07:57 - INFO - __main__ -  [paws] sentence 1: There was once a Nottingham railway station on the line between Market Harborough and Hallaton . [SEP] sentence 2: On the line between Market Harborough and Nottingham , there was once a train station of Hallaton .
06/24/2022 16:07:57 - INFO - __main__ - ['0']
06/24/2022 16:07:57 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:07:57 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:07:57 - INFO - __main__ - Loaded 32 examples from dev data
06/24/2022 16:08:03 - INFO - __main__ - load prompt embedding from ckpt
06/24/2022 16:08:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/24/2022 16:08:03 - INFO - __main__ - Starting training!
06/24/2022 16:08:05 - INFO - __main__ - Step 10 Global step 10 Train loss 4.18 on epoch=4
06/24/2022 16:08:06 - INFO - __main__ - Step 20 Global step 20 Train loss 3.58 on epoch=9
06/24/2022 16:08:07 - INFO - __main__ - Step 30 Global step 30 Train loss 2.94 on epoch=14
06/24/2022 16:08:08 - INFO - __main__ - Step 40 Global step 40 Train loss 2.17 on epoch=19
06/24/2022 16:08:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.64 on epoch=24
06/24/2022 16:08:10 - INFO - __main__ - Global step 50 Train loss 2.90 Classification-F1 0.3333333333333333 on epoch=24
06/24/2022 16:08:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
06/24/2022 16:08:11 - INFO - __main__ - Step 60 Global step 60 Train loss 1.20 on epoch=29
06/24/2022 16:08:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=34
06/24/2022 16:08:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.80 on epoch=39
06/24/2022 16:08:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=44
06/24/2022 16:08:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=49
06/24/2022 16:08:17 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.4589371980676329 on epoch=49
06/24/2022 16:08:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=49, global_step=100
06/24/2022 16:08:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=54
06/24/2022 16:08:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.64 on epoch=59
06/24/2022 16:08:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=64
06/24/2022 16:08:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=69
06/24/2022 16:08:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=74
06/24/2022 16:08:23 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.4181818181818182 on epoch=74
06/24/2022 16:08:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=79
06/24/2022 16:08:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
06/24/2022 16:08:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=89
06/24/2022 16:08:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=94
06/24/2022 16:08:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=99
06/24/2022 16:08:30 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=99
06/24/2022 16:08:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=104
06/24/2022 16:08:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=109
06/24/2022 16:08:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=114
06/24/2022 16:08:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=119
06/24/2022 16:08:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=124
06/24/2022 16:08:36 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=124
06/24/2022 16:08:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=129
06/24/2022 16:08:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
06/24/2022 16:08:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=139
06/24/2022 16:08:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=144
06/24/2022 16:08:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=149
06/24/2022 16:08:42 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=149
06/24/2022 16:08:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=154
06/24/2022 16:08:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=159
06/24/2022 16:08:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=164
06/24/2022 16:08:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=169
06/24/2022 16:08:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
06/24/2022 16:08:49 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3992490613266583 on epoch=174
06/24/2022 16:08:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=179
06/24/2022 16:08:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=184
06/24/2022 16:08:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=189
06/24/2022 16:08:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
06/24/2022 16:08:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=199
06/24/2022 16:08:55 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.5151515151515151 on epoch=199
06/24/2022 16:08:55 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5151515151515151 on epoch=199, global_step=400
06/24/2022 16:08:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=204
06/24/2022 16:08:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=209
06/24/2022 16:08:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=214
06/24/2022 16:09:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=219
06/24/2022 16:09:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=224
06/24/2022 16:09:02 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.5134502923976608 on epoch=224
06/24/2022 16:09:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=229
06/24/2022 16:09:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=234
06/24/2022 16:09:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=239
06/24/2022 16:09:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=244
06/24/2022 16:09:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=249
06/24/2022 16:09:08 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.46843853820598 on epoch=249
06/24/2022 16:09:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=254
06/24/2022 16:09:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=259
06/24/2022 16:09:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=264
06/24/2022 16:09:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=269
06/24/2022 16:09:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=274
06/24/2022 16:09:15 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.5195195195195195 on epoch=274
06/24/2022 16:09:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5195195195195195 on epoch=274, global_step=550
06/24/2022 16:09:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=279
06/24/2022 16:09:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=284
06/24/2022 16:09:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=289
06/24/2022 16:09:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=294
06/24/2022 16:09:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=299
06/24/2022 16:09:21 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.5901477832512315 on epoch=299
06/24/2022 16:09:22 - INFO - __main__ - Saving model with best Classification-F1: 0.5195195195195195 -> 0.5901477832512315 on epoch=299, global_step=600
06/24/2022 16:09:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=304
06/24/2022 16:09:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.29 on epoch=309
06/24/2022 16:09:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=314
06/24/2022 16:09:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=319
06/24/2022 16:09:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=324
06/24/2022 16:09:28 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.4817813765182186 on epoch=324
06/24/2022 16:09:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=329
06/24/2022 16:09:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=334
06/24/2022 16:09:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=339
06/24/2022 16:09:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=344
06/24/2022 16:09:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=349
06/24/2022 16:09:34 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.5607843137254902 on epoch=349
06/24/2022 16:09:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=354
06/24/2022 16:09:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=359
06/24/2022 16:09:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=364
06/24/2022 16:09:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=369
06/24/2022 16:09:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=374
06/24/2022 16:09:41 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.4231177094379639 on epoch=374
06/24/2022 16:09:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=379
06/24/2022 16:09:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=384
06/24/2022 16:09:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
06/24/2022 16:09:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=394
06/24/2022 16:09:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=399
06/24/2022 16:09:47 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.3552492046659597 on epoch=399
06/24/2022 16:09:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=404
06/24/2022 16:09:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=409
06/24/2022 16:09:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=414
06/24/2022 16:09:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=419
06/24/2022 16:09:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=424
06/24/2022 16:09:54 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.4666666666666667 on epoch=424
06/24/2022 16:09:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=429
06/24/2022 16:09:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=434
06/24/2022 16:09:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=439
06/24/2022 16:10:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=444
06/24/2022 16:10:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=449
06/24/2022 16:10:01 - INFO - __main__ - Global step 900 Train loss 0.16 Classification-F1 0.4817813765182186 on epoch=449
06/24/2022 16:10:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=454
06/24/2022 16:10:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=459
06/24/2022 16:10:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=464
06/24/2022 16:10:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=469
06/24/2022 16:10:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=474
06/24/2022 16:10:08 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.4666666666666667 on epoch=474
06/24/2022 16:10:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=479
06/24/2022 16:10:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=484
06/24/2022 16:10:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=489
06/24/2022 16:10:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=494
06/24/2022 16:10:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=499
06/24/2022 16:10:16 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.5076923076923077 on epoch=499
06/24/2022 16:10:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=504
06/24/2022 16:10:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
06/24/2022 16:10:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=514
06/24/2022 16:10:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=519
06/24/2022 16:10:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
06/24/2022 16:10:22 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.5076923076923077 on epoch=524
06/24/2022 16:10:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=529
06/24/2022 16:10:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=534
06/24/2022 16:10:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=539
06/24/2022 16:10:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
06/24/2022 16:10:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
06/24/2022 16:10:29 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.4554554554554554 on epoch=549
06/24/2022 16:10:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
06/24/2022 16:10:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=559
06/24/2022 16:10:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=564
06/24/2022 16:10:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
06/24/2022 16:10:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
06/24/2022 16:10:36 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.34310850439882695 on epoch=574
06/24/2022 16:10:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=579
06/24/2022 16:10:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
06/24/2022 16:10:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
06/24/2022 16:10:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=594
06/24/2022 16:10:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=599
06/24/2022 16:10:43 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.4285714285714286 on epoch=599
06/24/2022 16:10:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=604
06/24/2022 16:10:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
06/24/2022 16:10:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
06/24/2022 16:10:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
06/24/2022 16:10:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
06/24/2022 16:10:50 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.4009852216748768 on epoch=624
06/24/2022 16:10:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
06/24/2022 16:10:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=634
06/24/2022 16:10:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
06/24/2022 16:10:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
06/24/2022 16:10:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
06/24/2022 16:10:57 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.3650793650793651 on epoch=649
06/24/2022 16:10:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
06/24/2022 16:10:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
06/24/2022 16:11:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
06/24/2022 16:11:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
06/24/2022 16:11:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
06/24/2022 16:11:04 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.3650793650793651 on epoch=674
06/24/2022 16:11:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
06/24/2022 16:11:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
06/24/2022 16:11:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
06/24/2022 16:11:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
06/24/2022 16:11:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=699
06/24/2022 16:11:10 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.3650793650793651 on epoch=699
06/24/2022 16:11:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
06/24/2022 16:11:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
06/24/2022 16:11:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
06/24/2022 16:11:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
06/24/2022 16:11:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
06/24/2022 16:11:17 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.39139139139139134 on epoch=724
06/24/2022 16:11:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
06/24/2022 16:11:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
06/24/2022 16:11:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
06/24/2022 16:11:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
06/24/2022 16:11:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
06/24/2022 16:11:24 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=749
06/24/2022 16:11:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
06/24/2022 16:11:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
06/24/2022 16:11:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
06/24/2022 16:11:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
06/24/2022 16:11:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
06/24/2022 16:11:30 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.39139139139139134 on epoch=774
06/24/2022 16:11:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
06/24/2022 16:11:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
06/24/2022 16:11:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
06/24/2022 16:11:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
06/24/2022 16:11:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
06/24/2022 16:11:37 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=799
06/24/2022 16:11:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
06/24/2022 16:11:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
06/24/2022 16:11:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
06/24/2022 16:11:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
06/24/2022 16:11:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
06/24/2022 16:11:43 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=824
06/24/2022 16:11:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
06/24/2022 16:11:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
06/24/2022 16:11:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
06/24/2022 16:11:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
06/24/2022 16:11:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
06/24/2022 16:11:50 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=849
06/24/2022 16:11:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=854
06/24/2022 16:11:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
06/24/2022 16:11:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
06/24/2022 16:11:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
06/24/2022 16:11:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
06/24/2022 16:11:56 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.3650793650793651 on epoch=874
06/24/2022 16:11:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
06/24/2022 16:11:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
06/24/2022 16:12:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
06/24/2022 16:12:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
06/24/2022 16:12:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
06/24/2022 16:12:03 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.39139139139139134 on epoch=899
06/24/2022 16:12:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
06/24/2022 16:12:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
06/24/2022 16:12:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
06/24/2022 16:12:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
06/24/2022 16:12:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
06/24/2022 16:12:10 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.4285714285714286 on epoch=924
06/24/2022 16:12:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
06/24/2022 16:12:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
06/24/2022 16:12:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
06/24/2022 16:12:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
06/24/2022 16:12:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
06/24/2022 16:12:16 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.3650793650793651 on epoch=949
06/24/2022 16:12:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
06/24/2022 16:12:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
06/24/2022 16:12:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
06/24/2022 16:12:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
06/24/2022 16:12:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
06/24/2022 16:12:23 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.3650793650793651 on epoch=974
06/24/2022 16:12:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
06/24/2022 16:12:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
06/24/2022 16:12:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
06/24/2022 16:12:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
06/24/2022 16:12:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
06/24/2022 16:12:29 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.3650793650793651 on epoch=999
06/24/2022 16:12:29 - INFO - __main__ - save last model!
06/24/2022 16:12:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/24/2022 16:12:29 - INFO - __main__ - Start tokenizing ... 8000 instances
06/24/2022 16:12:29 - INFO - __main__ - Printing 3 examples
06/24/2022 16:12:29 - INFO - __main__ -  [paws] sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . [SEP] sentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .
06/24/2022 16:12:29 - INFO - __main__ - ['0']
06/24/2022 16:12:29 - INFO - __main__ -  [paws] sentence 1: They were there to enjoy us and they were there to pray for us . [SEP] sentence 2: They were there for us to enjoy and they were there for us to pray .
06/24/2022 16:12:29 - INFO - __main__ - ['1']
06/24/2022 16:12:29 - INFO - __main__ -  [paws] sentence 1: After the end of the war in June 1902 , Higgins left Southampton in the `` SSBavarian '' in August , returning to Cape Town the following month . [SEP] sentence 2: In August , after the end of the war in June 1902 , Higgins Southampton left the `` SSBavarian '' and returned to Cape Town the following month .
06/24/2022 16:12:29 - INFO - __main__ - ['1']
06/24/2022 16:12:29 - INFO - __main__ - Tokenizing Input ...
06/24/2022 16:12:33 - INFO - __main__ - Tokenizing Output ...
06/24/2022 16:12:41 - INFO - __main__ - Loaded 8000 examples from test data
06/24/2022 16:14:09 - INFO - __main__ - Saved prediction in models/T5-base-multitask-nopara2para-5e-1-4-20/singletask-paws/paws_16_87_0.2_8_predictions.txt
06/24/2022 16:14:09 - INFO - __main__ - Classification-F1 on test data: 0.5094
06/24/2022 16:14:09 - INFO - __main__ - prefix=paws_16_87, lr=0.2, bsz=8, dev_performance=0.5901477832512315, test_performance=0.5093858615233618
