nohup: ignoring input
t5base para reptile upstream
06/19/2022 14:35:10 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', identifier='base', output_dir='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base', do_train=True, do_predict=False, inner_bsz=4, inner_lr=3e-05, reptile_step=10, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=2, num_train_epochs=120.0, warmup_steps=360, total_steps=5000, wait_step=10000000000, verbose=False, eval_period=10, prefix='', debug=False, seed=42, custom_tasks_splits='./dataloader/custom_tasks_splits/train_nonparaphrase_classification_test_paraphrase.json', cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=-1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6')
06/19/2022 14:35:10 - INFO - __main__ - models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base
06/19/2022 14:35:10 - INFO - __main__ - args.device: cuda
06/19/2022 14:35:10 - INFO - __main__ - Using 1 gpus
06/19/2022 14:35:11 - INFO - __main__ - Training on the following tasks: ['ade_corpus_v2-classification', 'ag_news', 'amazon_polarity', 'anli', 'circa', 'climate_fever', 'dbpedia_14', 'discovery', 'emo', 'emotion', 'ethos-directed_vs_generalized', 'ethos-disability', 'ethos-gender', 'ethos-national_origin', 'ethos-race', 'ethos-religion', 'ethos-sexual_orientation', 'financial_phrasebank', 'glue-cola', 'glue-mnli', 'glue-qnli', 'glue-rte', 'glue-sst2', 'glue-wnli', 'google_wellformed_query', 'hate_speech18', 'hate_speech_offensive', 'hatexplain', 'health_fact', 'imdb', 'kilt_fever', 'liar', 'onestop_english', 'poem_sentiment', 'rotten_tomatoes', 'scicite', 'scitail', 'sick', 'sms_spam', 'superglue-cb', 'superglue-rte', 'superglue-wic', 'superglue-wsc', 'tab_fact', 'trec', 'trec-finegrained', 'tweet_eval-emoji', 'tweet_eval-emotion', 'tweet_eval-hate', 'tweet_eval-irony', 'tweet_eval-offensive', 'tweet_eval-sentiment', 'tweet_eval-stance_abortion', 'tweet_eval-stance_atheism', 'tweet_eval-stance_climate', 'tweet_eval-stance_feminist', 'tweet_eval-stance_hillary', 'wiki_qa', 'yahoo_answers_topics', 'yelp_polarity']
06/19/2022 14:35:14 - INFO - __main__ - Start tokenizing ... 300 instances
06/19/2022 14:35:15 - INFO - __main__ - Printing 3 examples
06/19/2022 14:35:15 - INFO - __main__ -  [ade_corpus_v2-classification] The treatment of FMF attacks in patients who cannot use colchicine is an important problem.
06/19/2022 14:35:15 - INFO - __main__ -  Not Related
06/19/2022 14:35:15 - INFO - __main__ -  [ade_corpus_v2-classification] The origin of NIS is outlined briefly and some fundamental clinical and experimental facts are presented, all of which stress the importance of the acute blockade of postsynaptic DA-ergic receptors.
06/19/2022 14:35:15 - INFO - __main__ -  Not Related
06/19/2022 14:35:15 - INFO - __main__ -  [ade_corpus_v2-classification] His AFP was initially 9828 microg/L and rapidly dropped to 5597 microg/L in ten days after oral sorafenib treatment.
06/19/2022 14:35:15 - INFO - __main__ -  Not Related
06/19/2022 14:35:15 - INFO - __main__ - Tokenizing Train Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 14:35:32 - INFO - __main__ - Tokenizing Train Output ...
06/19/2022 14:35:43 - INFO - __main__ - Tokenizing Dev Input ...
06/19/2022 14:36:00 - INFO - __main__ - Tokenizing Dev Output ...
06/19/2022 14:36:52 - INFO - __main__ - Loaded 300 examples from train data
06/19/2022 14:36:59 - INFO - __main__ - try to initialize prompt embeddings
06/19/2022 14:37:03 - INFO - __main__ - Starting training!
use RandomSampler
initialize from c4
[(1219, 784566), (205, 3175490), (3178, 338316), (2767, 377111), (2421, 410967), (1481, 640748), (924, 843157), (5916, 171503), (566, 946306), (5534, 158824), (4682, 222111), (360, 2496612), (333, 2670525), (958, 890939), (2537, 417981), (2569, 396692), (5644, 185115), (6480, 156027), (326, 2947559), (6313, 166539), (2123, 459189), (3488, 171592), (4602, 223532), (1752, 415773), (3177, 209086), (6847, 159087), (3213, 334916), (68, 12242894), (1729, 565725), (3341, 221693), (3798, 276026), (3041, 335480), (986, 581568), (2389, 425302), (3832, 279541), (1023, 843875), (725, 898187), (4260, 246836), (1139, 870475), (4205, 261446)]
Epoch 0:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 0:   0%|          | 1/300 [00:01<09:52,  1.98s/it]Epoch 0:   1%|          | 2/300 [00:03<08:43,  1.76s/it]Epoch 0:   1%|          | 3/300 [00:05<08:12,  1.66s/it]Epoch 0:   1%|▏         | 4/300 [00:07<08:39,  1.76s/it]Epoch 0:   2%|▏         | 5/300 [00:08<08:57,  1.82s/it]Epoch 0:   2%|▏         | 6/300 [00:10<08:43,  1.78s/it]Epoch 0:   2%|▏         | 7/300 [00:12<08:57,  1.84s/it]Epoch 0:   3%|▎         | 8/300 [00:14<08:57,  1.84s/it]Epoch 0:   3%|▎         | 9/300 [00:16<08:58,  1.85s/it]Epoch 0:   3%|▎         | 10/300 [00:18<08:44,  1.81s/it]Epoch 0:   4%|▎         | 11/300 [00:19<08:32,  1.77s/it]Epoch 0:   4%|▍         | 12/300 [00:21<08:38,  1.80s/it]Epoch 0:   4%|▍         | 13/300 [00:23<08:27,  1.77s/it]Epoch 0:   5%|▍         | 14/300 [00:25<08:27,  1.77s/it]Epoch 0:   5%|▌         | 15/300 [00:26<08:15,  1.74s/it]Epoch 0:   5%|▌         | 16/300 [00:28<08:25,  1.78s/it]Epoch 0:   6%|▌         | 17/300 [00:30<08:35,  1.82s/it]Epoch 0:   6%|▌         | 18/300 [00:32<08:43,  1.86s/it]Epoch 0:   6%|▋         | 19/300 [00:34<08:24,  1.79s/it]06/19/2022 14:37:39 - INFO - __main__ - global step: 10; train loss: 8.377994537353516; dev loss
Epoch 0:   7%|▋         | 20/300 [00:35<08:10,  1.75s/it]Epoch 0:   7%|▋         | 21/300 [00:37<07:58,  1.72s/it]Epoch 0:   7%|▋         | 22/300 [00:39<07:52,  1.70s/it]Epoch 0:   8%|▊         | 23/300 [00:41<08:17,  1.80s/it]Epoch 0:   8%|▊         | 24/300 [00:42<07:53,  1.71s/it]Epoch 0:   8%|▊         | 25/300 [00:44<07:48,  1.70s/it]Epoch 0:   9%|▊         | 26/300 [00:46<08:09,  1.78s/it]Epoch 0:   9%|▉         | 27/300 [00:48<08:31,  1.87s/it]Epoch 0:   9%|▉         | 28/300 [00:49<08:08,  1.80s/it]Epoch 0:  10%|▉         | 29/300 [00:51<07:48,  1.73s/it]Epoch 0:  10%|█         | 30/300 [00:53<07:34,  1.68s/it]Epoch 0:  10%|█         | 31/300 [00:55<07:58,  1.78s/it]Epoch 0:  11%|█         | 32/300 [00:56<08:02,  1.80s/it]Epoch 0:  11%|█         | 33/300 [00:58<07:41,  1.73s/it]Epoch 0:  11%|█▏        | 34/300 [01:00<07:31,  1.70s/it]Epoch 0:  12%|█▏        | 35/300 [01:02<07:57,  1.80s/it]Epoch 0:  12%|█▏        | 36/300 [01:04<08:02,  1.83s/it]Epoch 0:  12%|█▏        | 37/300 [01:05<07:31,  1.72s/it]Epoch 0:  13%|█▎        | 38/300 [01:07<07:14,  1.66s/it]Epoch 0:  13%|█▎        | 39/300 [01:09<07:44,  1.78s/it]06/19/2022 14:38:15 - INFO - __main__ - global step: 20; train loss: 8.000921249389648; dev loss
Epoch 0:  13%|█▎        | 40/300 [01:11<08:03,  1.86s/it]Epoch 0:  14%|█▎        | 41/300 [01:13<08:12,  1.90s/it]Epoch 0:  14%|█▍        | 42/300 [01:14<07:40,  1.79s/it]Epoch 0:  14%|█▍        | 43/300 [01:16<08:02,  1.88s/it]Epoch 0:  15%|█▍        | 44/300 [01:18<07:58,  1.87s/it]Epoch 0:  15%|█▌        | 45/300 [01:20<08:00,  1.88s/it]Epoch 0:  15%|█▌        | 46/300 [01:22<07:58,  1.88s/it]Epoch 0:  16%|█▌        | 47/300 [01:24<07:37,  1.81s/it]Epoch 0:  16%|█▌        | 48/300 [01:25<07:21,  1.75s/it]Epoch 0:  16%|█▋        | 49/300 [01:27<06:59,  1.67s/it]Epoch 0:  17%|█▋        | 50/300 [01:29<07:11,  1.73s/it]Epoch 0:  17%|█▋        | 51/300 [01:30<06:54,  1.67s/it]Epoch 0:  17%|█▋        | 52/300 [01:32<07:07,  1.73s/it]Epoch 0:  18%|█▊        | 53/300 [01:34<07:01,  1.71s/it]Epoch 0:  18%|█▊        | 54/300 [01:35<06:56,  1.69s/it]Epoch 0:  18%|█▊        | 55/300 [01:37<07:09,  1.75s/it]Epoch 0:  19%|█▊        | 56/300 [01:39<07:16,  1.79s/it]Epoch 0:  19%|█▉        | 57/300 [01:41<07:12,  1.78s/it]Epoch 0:  19%|█▉        | 58/300 [01:42<06:57,  1.73s/it]Epoch 0:  20%|█▉        | 59/300 [01:44<06:56,  1.73s/it]06/19/2022 14:38:50 - INFO - __main__ - global step: 30; train loss: 8.287264823913574; dev loss
Epoch 0:  20%|██        | 60/300 [01:46<06:57,  1.74s/it]Epoch 0:  20%|██        | 61/300 [01:47<06:42,  1.68s/it]Epoch 0:  21%|██        | 62/300 [01:49<06:54,  1.74s/it]Epoch 0:  21%|██        | 63/300 [01:51<06:54,  1.75s/it]Epoch 0:  21%|██▏       | 64/300 [01:53<07:12,  1.83s/it]Epoch 0:  22%|██▏       | 65/300 [01:55<07:13,  1.84s/it]Epoch 0:  22%|██▏       | 66/300 [01:57<07:12,  1.85s/it]Epoch 0:  22%|██▏       | 67/300 [01:59<07:10,  1.85s/it]Epoch 0:  23%|██▎       | 68/300 [02:00<07:02,  1.82s/it]Epoch 0:  23%|██▎       | 69/300 [02:02<07:12,  1.87s/it]Epoch 0:  23%|██▎       | 70/300 [02:04<06:47,  1.77s/it]Epoch 0:  24%|██▎       | 71/300 [02:06<06:57,  1.82s/it]Epoch 0:  24%|██▍       | 72/300 [02:08<07:16,  1.92s/it]Epoch 0:  24%|██▍       | 73/300 [02:10<07:01,  1.86s/it]Epoch 0:  25%|██▍       | 74/300 [02:12<07:03,  1.87s/it]Epoch 0:  25%|██▌       | 75/300 [02:13<06:36,  1.76s/it]Epoch 0:  25%|██▌       | 76/300 [02:15<06:36,  1.77s/it]Epoch 0:  26%|██▌       | 77/300 [02:17<06:40,  1.79s/it]Epoch 0:  26%|██▌       | 78/300 [02:19<06:51,  1.86s/it]Epoch 0:  26%|██▋       | 79/300 [02:21<06:55,  1.88s/it]06/19/2022 14:39:26 - INFO - __main__ - global step: 40; train loss: 7.621449947357178; dev loss
Epoch 0:  27%|██▋       | 80/300 [02:22<06:41,  1.83s/it]Epoch 0:  27%|██▋       | 81/300 [02:24<06:48,  1.87s/it]Epoch 0:  27%|██▋       | 82/300 [02:26<06:23,  1.76s/it]Epoch 0:  28%|██▊       | 83/300 [02:28<06:27,  1.79s/it]Epoch 0:  28%|██▊       | 84/300 [02:30<06:31,  1.81s/it]Epoch 0:  28%|██▊       | 85/300 [02:32<06:43,  1.88s/it]Epoch 0:  29%|██▊       | 86/300 [02:34<06:51,  1.93s/it]Epoch 0:  29%|██▉       | 87/300 [02:35<06:30,  1.83s/it]Epoch 0:  29%|██▉       | 88/300 [02:37<06:43,  1.90s/it]Epoch 0:  30%|██▉       | 89/300 [02:39<06:51,  1.95s/it]Epoch 0:  30%|███       | 90/300 [02:41<06:47,  1.94s/it]Epoch 0:  30%|███       | 91/300 [02:43<06:43,  1.93s/it]Epoch 0:  31%|███       | 92/300 [02:45<06:17,  1.82s/it]Epoch 0:  31%|███       | 93/300 [02:46<06:05,  1.77s/it]Epoch 0:  31%|███▏      | 94/300 [02:48<05:50,  1.70s/it]Epoch 0:  32%|███▏      | 95/300 [02:50<06:11,  1.81s/it]Epoch 0:  32%|███▏      | 96/300 [02:52<06:16,  1.85s/it]Epoch 0:  32%|███▏      | 97/300 [02:54<06:09,  1.82s/it]Epoch 0:  33%|███▎      | 98/300 [02:55<06:02,  1.79s/it]Epoch 0:  33%|███▎      | 99/300 [02:57<05:59,  1.79s/it]06/19/2022 14:40:03 - INFO - __main__ - global step: 50; train loss: 8.398483276367188; dev loss
Epoch 0:  33%|███▎      | 100/300 [02:59<06:07,  1.84s/it]Epoch 0:  34%|███▎      | 101/300 [03:01<05:57,  1.80s/it]Epoch 0:  34%|███▍      | 102/300 [03:03<06:07,  1.86s/it]Epoch 0:  34%|███▍      | 103/300 [03:05<06:03,  1.84s/it]Epoch 0:  35%|███▍      | 104/300 [03:07<05:58,  1.83s/it]Epoch 0:  35%|███▌      | 105/300 [03:08<06:00,  1.85s/it]Epoch 0:  35%|███▌      | 106/300 [03:10<05:52,  1.82s/it]Epoch 0:  36%|███▌      | 107/300 [03:12<05:49,  1.81s/it]Epoch 0:  36%|███▌      | 108/300 [03:14<06:03,  1.89s/it]Epoch 0:  36%|███▋      | 109/300 [03:16<05:52,  1.85s/it]Epoch 0:  37%|███▋      | 110/300 [03:18<06:02,  1.91s/it]Epoch 0:  37%|███▋      | 111/300 [03:20<06:00,  1.91s/it]Epoch 0:  37%|███▋      | 112/300 [03:21<05:40,  1.81s/it]Epoch 0:  38%|███▊      | 113/300 [03:23<05:28,  1.75s/it]Epoch 0:  38%|███▊      | 114/300 [03:25<05:43,  1.84s/it]Epoch 0:  38%|███▊      | 115/300 [03:27<05:46,  1.87s/it]Epoch 0:  39%|███▊      | 116/300 [03:29<05:49,  1.90s/it]Epoch 0:  39%|███▉      | 117/300 [03:31<05:31,  1.81s/it]Epoch 0:  39%|███▉      | 118/300 [03:32<05:18,  1.75s/it]Epoch 0:  40%|███▉      | 119/300 [03:34<05:04,  1.68s/it]06/19/2022 14:40:39 - INFO - __main__ - global step: 60; train loss: 7.834259986877441; dev loss
Epoch 0:  40%|████      | 120/300 [03:36<05:15,  1.75s/it]Epoch 0:  40%|████      | 121/300 [03:37<05:20,  1.79s/it]Epoch 0:  41%|████      | 122/300 [03:39<05:09,  1.74s/it]Epoch 0:  41%|████      | 123/300 [03:41<04:56,  1.67s/it]Epoch 0:  41%|████▏     | 124/300 [03:42<05:06,  1.74s/it]Epoch 0:  42%|████▏     | 125/300 [03:44<05:11,  1.78s/it]Epoch 0:  42%|████▏     | 126/300 [03:46<04:57,  1.71s/it]Epoch 0:  42%|████▏     | 127/300 [03:48<04:52,  1.69s/it]Epoch 0:  43%|████▎     | 128/300 [03:49<04:59,  1.74s/it]Epoch 0:  43%|████▎     | 129/300 [03:51<05:05,  1.78s/it]Epoch 0:  43%|████▎     | 130/300 [03:53<04:53,  1.73s/it]Epoch 0:  44%|████▎     | 131/300 [03:54<04:43,  1.68s/it]Epoch 0:  44%|████▍     | 132/300 [03:56<04:39,  1.66s/it]Epoch 0:  44%|████▍     | 133/300 [03:58<04:27,  1.60s/it]Epoch 0:  45%|████▍     | 134/300 [03:59<04:27,  1.61s/it]Epoch 0:  45%|████▌     | 135/300 [04:01<04:41,  1.71s/it]Epoch 0:  45%|████▌     | 136/300 [04:03<04:42,  1.72s/it]Epoch 0:  46%|████▌     | 137/300 [04:05<04:41,  1.73s/it]Epoch 0:  46%|████▌     | 138/300 [04:06<04:30,  1.67s/it]Epoch 0:  46%|████▋     | 139/300 [04:08<04:27,  1.66s/it]06/19/2022 14:41:13 - INFO - __main__ - global step: 70; train loss: 8.510248184204102; dev loss
Epoch 0:  47%|████▋     | 140/300 [04:09<04:19,  1.62s/it]Epoch 0:  47%|████▋     | 141/300 [04:11<04:27,  1.69s/it]Epoch 0:  47%|████▋     | 142/300 [04:13<04:36,  1.75s/it]Epoch 0:  48%|████▊     | 143/300 [04:15<04:33,  1.74s/it]Epoch 0:  48%|████▊     | 144/300 [04:16<04:30,  1.74s/it]Epoch 0:  48%|████▊     | 145/300 [04:18<04:35,  1.78s/it]Epoch 0:  49%|████▊     | 146/300 [04:20<04:37,  1.80s/it]Epoch 0:  49%|████▉     | 147/300 [04:22<04:25,  1.74s/it]Epoch 0:  49%|████▉     | 148/300 [04:23<04:11,  1.65s/it]Epoch 0:  50%|████▉     | 149/300 [04:25<04:19,  1.72s/it]Epoch 0:  50%|█████     | 150/300 [04:27<04:19,  1.73s/it]Epoch 0:  50%|█████     | 151/300 [04:29<04:24,  1.77s/it]Epoch 0:  51%|█████     | 152/300 [04:30<04:16,  1.73s/it]Epoch 0:  51%|█████     | 153/300 [04:32<04:09,  1.69s/it]Epoch 0:  51%|█████▏    | 154/300 [04:33<03:56,  1.62s/it]Epoch 0:  52%|█████▏    | 155/300 [04:35<03:56,  1.63s/it]Epoch 0:  52%|█████▏    | 156/300 [04:37<04:05,  1.71s/it]Epoch 0:  52%|█████▏    | 157/300 [04:39<04:08,  1.74s/it]Epoch 0:  53%|█████▎    | 158/300 [04:41<04:17,  1.81s/it]Epoch 0:  53%|█████▎    | 159/300 [04:43<04:16,  1.82s/it]06/19/2022 14:41:48 - INFO - __main__ - global step: 80; train loss: 7.6774797439575195; dev loss
Epoch 0:  53%|█████▎    | 160/300 [04:44<04:10,  1.79s/it]Epoch 0:  54%|█████▎    | 161/300 [04:46<04:20,  1.87s/it]Epoch 0:  54%|█████▍    | 162/300 [04:48<04:11,  1.82s/it]Epoch 0:  54%|█████▍    | 163/300 [04:50<03:56,  1.73s/it]Epoch 0:  55%|█████▍    | 164/300 [04:51<03:47,  1.67s/it]Epoch 0:  55%|█████▌    | 165/300 [04:53<03:37,  1.61s/it]Epoch 0:  55%|█████▌    | 166/300 [04:55<03:49,  1.71s/it]Epoch 0:  56%|█████▌    | 167/300 [04:56<03:48,  1.72s/it]Epoch 0:  56%|█████▌    | 168/300 [04:58<03:57,  1.80s/it]Epoch 0:  56%|█████▋    | 169/300 [05:00<03:59,  1.83s/it]Epoch 0:  57%|█████▋    | 170/300 [05:02<03:44,  1.73s/it]Epoch 0:  57%|█████▋    | 171/300 [05:03<03:36,  1.68s/it]Epoch 0:  57%|█████▋    | 172/300 [05:05<03:35,  1.69s/it]Epoch 0:  58%|█████▊    | 173/300 [05:07<03:31,  1.66s/it]Epoch 0:  58%|█████▊    | 174/300 [05:08<03:21,  1.60s/it]Epoch 0:  58%|█████▊    | 175/300 [05:10<03:17,  1.58s/it]Epoch 0:  59%|█████▊    | 176/300 [05:11<03:19,  1.61s/it]Epoch 0:  59%|█████▉    | 177/300 [05:13<03:15,  1.59s/it]Epoch 0:  59%|█████▉    | 178/300 [05:15<03:21,  1.66s/it]Epoch 0:  60%|█████▉    | 179/300 [05:16<03:29,  1.73s/it]06/19/2022 14:42:22 - INFO - __main__ - global step: 90; train loss: 8.221892356872559; dev loss
Epoch 0:  60%|██████    | 180/300 [05:18<03:21,  1.68s/it]Epoch 0:  60%|██████    | 181/300 [05:20<03:18,  1.67s/it]Epoch 0:  61%|██████    | 182/300 [05:22<03:22,  1.71s/it]Epoch 0:  61%|██████    | 183/300 [05:23<03:12,  1.65s/it]Epoch 0:  61%|██████▏   | 184/300 [05:24<03:05,  1.60s/it]Epoch 0:  62%|██████▏   | 185/300 [05:26<03:07,  1.63s/it]Epoch 0:  62%|██████▏   | 186/300 [05:28<02:59,  1.57s/it]Epoch 0:  62%|██████▏   | 187/300 [05:29<02:56,  1.57s/it]Epoch 0:  63%|██████▎   | 188/300 [05:31<02:53,  1.54s/it]Epoch 0:  63%|██████▎   | 189/300 [05:32<02:59,  1.61s/it]Epoch 0:  63%|██████▎   | 190/300 [05:34<02:58,  1.62s/it]Epoch 0:  64%|██████▎   | 191/300 [05:36<03:05,  1.70s/it]Epoch 0:  64%|██████▍   | 192/300 [05:38<03:09,  1.75s/it]Epoch 0:  64%|██████▍   | 193/300 [05:40<03:16,  1.84s/it]Epoch 0:  65%|██████▍   | 194/300 [05:42<03:15,  1.85s/it]Epoch 0:  65%|██████▌   | 195/300 [05:43<03:03,  1.75s/it]Epoch 0:  65%|██████▌   | 196/300 [05:45<03:04,  1.78s/it]Epoch 0:  66%|██████▌   | 197/300 [05:47<03:10,  1.85s/it]Epoch 0:  66%|██████▌   | 198/300 [05:49<03:05,  1.81s/it]Epoch 0:  66%|██████▋   | 199/300 [05:50<02:55,  1.74s/it]06/19/2022 14:42:56 - INFO - __main__ - global step: 100; train loss: 7.481749057769775; dev loss
Epoch 0:  67%|██████▋   | 200/300 [05:52<02:59,  1.79s/it]Epoch 0:  67%|██████▋   | 201/300 [05:54<02:58,  1.81s/it]Epoch 0:  67%|██████▋   | 202/300 [05:56<02:56,  1.80s/it]Epoch 0:  68%|██████▊   | 203/300 [05:58<02:53,  1.79s/it]Epoch 0:  68%|██████▊   | 204/300 [06:00<02:57,  1.85s/it]Epoch 0:  68%|██████▊   | 205/300 [06:01<02:53,  1.82s/it]Epoch 0:  69%|██████▊   | 206/300 [06:03<02:55,  1.86s/it]Epoch 0:  69%|██████▉   | 207/300 [06:05<02:43,  1.76s/it]Epoch 0:  69%|██████▉   | 208/300 [06:06<02:33,  1.67s/it]Epoch 0:  70%|██████▉   | 209/300 [06:08<02:30,  1.66s/it]Epoch 0:  70%|███████   | 210/300 [06:10<02:37,  1.75s/it]Epoch 0:  70%|███████   | 211/300 [06:12<02:32,  1.71s/it]Epoch 0:  71%|███████   | 212/300 [06:14<02:36,  1.78s/it]Epoch 0:  71%|███████   | 213/300 [06:15<02:38,  1.82s/it]Epoch 0:  71%|███████▏  | 214/300 [06:17<02:41,  1.88s/it]Epoch 0:  72%|███████▏  | 215/300 [06:19<02:33,  1.80s/it]Epoch 0:  72%|███████▏  | 216/300 [06:21<02:33,  1.83s/it]Epoch 0:  72%|███████▏  | 217/300 [06:23<02:26,  1.77s/it]Epoch 0:  73%|███████▎  | 218/300 [06:24<02:22,  1.74s/it]Epoch 0:  73%|███████▎  | 219/300 [06:26<02:19,  1.72s/it]06/19/2022 14:43:31 - INFO - __main__ - global step: 110; train loss: 7.856598854064941; dev loss
Epoch 0:  73%|███████▎  | 220/300 [06:28<02:12,  1.66s/it]Epoch 0:  74%|███████▎  | 221/300 [06:29<02:12,  1.67s/it]Epoch 0:  74%|███████▍  | 222/300 [06:31<02:18,  1.78s/it]Epoch 0:  74%|███████▍  | 223/300 [06:33<02:18,  1.80s/it]Epoch 0:  75%|███████▍  | 224/300 [06:35<02:09,  1.70s/it]Epoch 0:  75%|███████▌  | 225/300 [06:37<02:14,  1.79s/it]Epoch 0:  75%|███████▌  | 226/300 [06:38<02:09,  1.74s/it]Epoch 0:  76%|███████▌  | 227/300 [06:40<02:03,  1.70s/it]Epoch 0:  76%|███████▌  | 228/300 [06:42<02:08,  1.78s/it]Epoch 0:  76%|███████▋  | 229/300 [06:44<02:08,  1.81s/it]Epoch 0:  77%|███████▋  | 230/300 [06:46<02:12,  1.89s/it]Epoch 0:  77%|███████▋  | 231/300 [06:48<02:08,  1.86s/it]Epoch 0:  77%|███████▋  | 232/300 [06:49<02:01,  1.78s/it]Epoch 0:  78%|███████▊  | 233/300 [06:51<01:52,  1.68s/it]Epoch 0:  78%|███████▊  | 234/300 [06:52<01:51,  1.69s/it]Epoch 0:  78%|███████▊  | 235/300 [06:54<01:46,  1.64s/it]Epoch 0:  79%|███████▊  | 236/300 [06:56<01:49,  1.71s/it]Epoch 0:  79%|███████▉  | 237/300 [06:57<01:43,  1.65s/it]Epoch 0:  79%|███████▉  | 238/300 [06:59<01:37,  1.58s/it]Epoch 0:  80%|███████▉  | 239/300 [07:01<01:44,  1.71s/it]06/19/2022 14:44:06 - INFO - __main__ - global step: 120; train loss: 7.222466468811035; dev loss
Epoch 0:  80%|████████  | 240/300 [07:02<01:45,  1.75s/it]Epoch 0:  80%|████████  | 241/300 [07:04<01:40,  1.70s/it]Epoch 0:  81%|████████  | 242/300 [07:06<01:41,  1.75s/it]Epoch 0:  81%|████████  | 243/300 [07:08<01:37,  1.72s/it]Epoch 0:  81%|████████▏ | 244/300 [07:09<01:33,  1.67s/it]Epoch 0:  82%|████████▏ | 245/300 [07:11<01:33,  1.70s/it]Epoch 0:  82%|████████▏ | 246/300 [07:13<01:32,  1.72s/it]Epoch 0:  82%|████████▏ | 247/300 [07:14<01:30,  1.70s/it]Epoch 0:  83%|████████▎ | 248/300 [07:16<01:26,  1.66s/it]Epoch 0:  83%|████████▎ | 249/300 [07:17<01:22,  1.62s/it]Epoch 0:  83%|████████▎ | 250/300 [07:19<01:19,  1.59s/it]Epoch 0:  84%|████████▎ | 251/300 [07:21<01:18,  1.61s/it]Epoch 0:  84%|████████▍ | 252/300 [07:22<01:15,  1.58s/it]Epoch 0:  84%|████████▍ | 253/300 [07:24<01:19,  1.68s/it]Epoch 0:  85%|████████▍ | 254/300 [07:26<01:16,  1.65s/it]Epoch 0:  85%|████████▌ | 255/300 [07:28<01:21,  1.81s/it]Epoch 0:  85%|████████▌ | 256/300 [07:30<01:21,  1.86s/it]Epoch 0:  86%|████████▌ | 257/300 [07:32<01:19,  1.85s/it]Epoch 0:  86%|████████▌ | 258/300 [07:34<01:19,  1.90s/it]Epoch 0:  86%|████████▋ | 259/300 [07:35<01:14,  1.81s/it]06/19/2022 14:44:41 - INFO - __main__ - global step: 130; train loss: 7.575237274169922; dev loss
Epoch 0:  87%|████████▋ | 260/300 [07:37<01:13,  1.83s/it]Epoch 0:  87%|████████▋ | 261/300 [07:39<01:09,  1.78s/it]Epoch 0:  87%|████████▋ | 262/300 [07:40<01:06,  1.74s/it]Epoch 0:  88%|████████▊ | 263/300 [07:42<01:07,  1.83s/it]Epoch 0:  88%|████████▊ | 264/300 [07:44<01:08,  1.90s/it]Epoch 0:  88%|████████▊ | 265/300 [07:47<01:08,  1.96s/it]Epoch 0:  89%|████████▊ | 266/300 [07:48<01:03,  1.87s/it]Epoch 0:  89%|████████▉ | 267/300 [07:50<01:00,  1.82s/it]Epoch 0:  89%|████████▉ | 268/300 [07:52<00:57,  1.81s/it]Epoch 0:  90%|████████▉ | 269/300 [07:53<00:53,  1.73s/it]Epoch 0:  90%|█████████ | 270/300 [07:55<00:53,  1.78s/it]Epoch 0:  90%|█████████ | 271/300 [07:57<00:53,  1.83s/it]Epoch 0:  91%|█████████ | 272/300 [07:59<00:49,  1.77s/it]Epoch 0:  91%|█████████ | 273/300 [08:00<00:47,  1.76s/it]Epoch 0:  91%|█████████▏| 274/300 [08:02<00:43,  1.69s/it]Epoch 0:  92%|█████████▏| 275/300 [08:04<00:43,  1.74s/it]Epoch 0:  92%|█████████▏| 276/300 [08:06<00:43,  1.81s/it]Epoch 0:  92%|█████████▏| 277/300 [08:08<00:41,  1.82s/it]Epoch 0:  93%|█████████▎| 278/300 [08:10<00:40,  1.86s/it]Epoch 0:  93%|█████████▎| 279/300 [08:11<00:38,  1.85s/it]06/19/2022 14:45:17 - INFO - __main__ - global step: 140; train loss: 7.913110256195068; dev loss
Epoch 0:  93%|█████████▎| 280/300 [08:13<00:35,  1.76s/it]Epoch 0:  94%|█████████▎| 281/300 [08:15<00:34,  1.79s/it]Epoch 0:  94%|█████████▍| 282/300 [08:17<00:32,  1.81s/it]Epoch 0:  94%|█████████▍| 283/300 [08:18<00:30,  1.80s/it]Epoch 0:  95%|█████████▍| 284/300 [08:20<00:27,  1.71s/it]Epoch 0:  95%|█████████▌| 285/300 [08:22<00:26,  1.74s/it]Epoch 0:  95%|█████████▌| 286/300 [08:24<00:24,  1.77s/it]Epoch 0:  96%|█████████▌| 287/300 [08:25<00:22,  1.71s/it]Epoch 0:  96%|█████████▌| 288/300 [08:27<00:21,  1.77s/it]Epoch 0:  96%|█████████▋| 289/300 [08:29<00:20,  1.83s/it]Epoch 0:  97%|█████████▋| 290/300 [08:31<00:18,  1.84s/it]Epoch 0:  97%|█████████▋| 291/300 [08:33<00:16,  1.80s/it]Epoch 0:  97%|█████████▋| 292/300 [08:34<00:14,  1.77s/it]Epoch 0:  98%|█████████▊| 293/300 [08:36<00:12,  1.73s/it]Epoch 0:  98%|█████████▊| 294/300 [08:38<00:10,  1.76s/it]Epoch 0:  98%|█████████▊| 295/300 [08:39<00:08,  1.68s/it]Epoch 0:  99%|█████████▊| 296/300 [08:41<00:06,  1.63s/it]Epoch 0:  99%|█████████▉| 297/300 [08:42<00:04,  1.64s/it]Epoch 0:  99%|█████████▉| 298/300 [08:44<00:03,  1.61s/it]Epoch 0: 100%|█████████▉| 299/300 [08:45<00:01,  1.56s/it]06/19/2022 14:45:51 - INFO - __main__ - global step: 150; train loss: 7.645398139953613; dev loss
Epoch 0: 100%|██████████| 300/300 [08:47<00:00,  1.54s/it]Epoch 0: 100%|██████████| 300/300 [08:47<00:00,  1.76s/it]
Epoch 1:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 1:   0%|          | 1/300 [00:01<07:44,  1.56s/it]Epoch 1:   1%|          | 2/300 [00:03<08:46,  1.77s/it]Epoch 1:   1%|          | 3/300 [00:05<08:57,  1.81s/it]Epoch 1:   1%|▏         | 4/300 [00:06<08:16,  1.68s/it]Epoch 1:   2%|▏         | 5/300 [00:08<08:43,  1.77s/it]Epoch 1:   2%|▏         | 6/300 [00:10<08:12,  1.67s/it]Epoch 1:   2%|▏         | 7/300 [00:12<08:27,  1.73s/it]Epoch 1:   3%|▎         | 8/300 [00:13<08:06,  1.67s/it]Epoch 1:   3%|▎         | 9/300 [00:15<08:39,  1.79s/it]Epoch 1:   3%|▎         | 10/300 [00:17<08:23,  1.74s/it]Epoch 1:   4%|▎         | 11/300 [00:19<08:48,  1.83s/it]Epoch 1:   4%|▍         | 12/300 [00:21<08:52,  1.85s/it]Epoch 1:   4%|▍         | 13/300 [00:23<09:05,  1.90s/it]Epoch 1:   5%|▍         | 14/300 [00:24<08:32,  1.79s/it]Epoch 1:   5%|▌         | 15/300 [00:26<08:07,  1.71s/it]Epoch 1:   5%|▌         | 16/300 [00:28<08:23,  1.77s/it]Epoch 1:   6%|▌         | 17/300 [00:29<08:12,  1.74s/it]Epoch 1:   6%|▌         | 18/300 [00:31<08:23,  1.79s/it]Epoch 1:   6%|▋         | 19/300 [00:33<08:24,  1.79s/it]06/19/2022 14:46:26 - INFO - __main__ - global step: 160; train loss: 8.227466583251953; dev loss
Epoch 1:   7%|▋         | 20/300 [00:35<07:54,  1.69s/it]Epoch 1:   7%|▋         | 21/300 [00:36<07:35,  1.63s/it]Epoch 1:   7%|▋         | 22/300 [00:38<07:38,  1.65s/it]Epoch 1:   8%|▊         | 23/300 [00:39<07:36,  1.65s/it]Epoch 1:   8%|▊         | 24/300 [00:41<07:49,  1.70s/it]Epoch 1:   8%|▊         | 25/300 [00:43<08:08,  1.78s/it]Epoch 1:   9%|▊         | 26/300 [00:45<07:52,  1.72s/it]Epoch 1:   9%|▉         | 27/300 [00:47<07:59,  1.76s/it]Epoch 1:   9%|▉         | 28/300 [00:48<07:39,  1.69s/it]Epoch 1:  10%|▉         | 29/300 [00:50<07:25,  1.64s/it]Epoch 1:  10%|█         | 30/300 [00:51<07:22,  1.64s/it]Epoch 1:  10%|█         | 31/300 [00:53<07:38,  1.70s/it]Epoch 1:  11%|█         | 32/300 [00:55<07:29,  1.68s/it]Epoch 1:  11%|█         | 33/300 [00:57<07:36,  1.71s/it]Epoch 1:  11%|█▏        | 34/300 [00:59<08:14,  1.86s/it]Epoch 1:  12%|█▏        | 35/300 [01:00<07:56,  1.80s/it]Epoch 1:  12%|█▏        | 36/300 [01:02<08:05,  1.84s/it]Epoch 1:  12%|█▏        | 37/300 [01:04<08:00,  1.83s/it]Epoch 1:  13%|█▎        | 38/300 [01:06<08:09,  1.87s/it]Epoch 1:  13%|█▎        | 39/300 [01:08<07:40,  1.77s/it]06/19/2022 14:47:01 - INFO - __main__ - global step: 170; train loss: 7.920413017272949; dev loss
Epoch 1:  13%|█▎        | 40/300 [01:09<07:23,  1.71s/it]Epoch 1:  14%|█▎        | 41/300 [01:11<07:51,  1.82s/it]Epoch 1:  14%|█▍        | 42/300 [01:13<07:29,  1.74s/it]Epoch 1:  14%|█▍        | 43/300 [01:15<08:04,  1.88s/it]Epoch 1:  15%|█▍        | 44/300 [01:17<08:05,  1.90s/it]Epoch 1:  15%|█▌        | 45/300 [01:19<08:05,  1.91s/it]Epoch 1:  15%|█▌        | 46/300 [01:21<07:52,  1.86s/it]Epoch 1:  16%|█▌        | 47/300 [01:22<07:47,  1.85s/it]Epoch 1:  16%|█▌        | 48/300 [01:24<07:57,  1.89s/it]Epoch 1:  16%|█▋        | 49/300 [01:27<08:07,  1.94s/it]Epoch 1:  17%|█▋        | 50/300 [01:28<07:42,  1.85s/it]Epoch 1:  17%|█▋        | 51/300 [01:30<07:52,  1.90s/it]Epoch 1:  17%|█▋        | 52/300 [01:32<07:32,  1.82s/it]Epoch 1:  18%|█▊        | 53/300 [01:34<07:31,  1.83s/it]Epoch 1:  18%|█▊        | 54/300 [01:36<07:35,  1.85s/it]Epoch 1:  18%|█▊        | 55/300 [01:37<07:22,  1.80s/it]Epoch 1:  19%|█▊        | 56/300 [01:39<06:57,  1.71s/it]Epoch 1:  19%|█▉        | 57/300 [01:41<07:24,  1.83s/it]Epoch 1:  19%|█▉        | 58/300 [01:42<06:58,  1.73s/it]Epoch 1:  20%|█▉        | 59/300 [01:44<06:47,  1.69s/it]06/19/2022 14:47:37 - INFO - __main__ - global step: 180; train loss: 8.02047061920166; dev loss
Epoch 1:  20%|██        | 60/300 [01:45<06:32,  1.64s/it]Epoch 1:  20%|██        | 61/300 [01:47<06:20,  1.59s/it]Epoch 1:  21%|██        | 62/300 [01:49<06:19,  1.59s/it]Epoch 1:  21%|██        | 63/300 [01:51<06:43,  1.70s/it]Epoch 1:  21%|██▏       | 64/300 [01:52<06:50,  1.74s/it]Epoch 1:  22%|██▏       | 65/300 [01:54<06:46,  1.73s/it]Epoch 1:  22%|██▏       | 66/300 [01:56<06:48,  1.75s/it]Epoch 1:  22%|██▏       | 67/300 [01:58<06:49,  1.76s/it]Epoch 1:  23%|██▎       | 68/300 [01:59<06:33,  1.69s/it]Epoch 1:  23%|██▎       | 69/300 [02:01<06:48,  1.77s/it]Epoch 1:  23%|██▎       | 70/300 [02:03<06:48,  1.77s/it]Epoch 1:  24%|██▎       | 71/300 [02:05<06:43,  1.76s/it]Epoch 1:  24%|██▍       | 72/300 [02:06<06:40,  1.76s/it]Epoch 1:  24%|██▍       | 73/300 [02:08<06:37,  1.75s/it]Epoch 1:  25%|██▍       | 74/300 [02:10<06:32,  1.74s/it]Epoch 1:  25%|██▌       | 75/300 [02:11<06:20,  1.69s/it]Epoch 1:  25%|██▌       | 76/300 [02:13<06:15,  1.67s/it]Epoch 1:  26%|██▌       | 77/300 [02:15<06:24,  1.73s/it]Epoch 1:  26%|██▌       | 78/300 [02:17<06:37,  1.79s/it]Epoch 1:  26%|██▋       | 79/300 [02:19<06:42,  1.82s/it]06/19/2022 14:48:12 - INFO - __main__ - global step: 190; train loss: 7.635000705718994; dev loss
Epoch 1:  27%|██▋       | 80/300 [02:21<06:43,  1.83s/it]Epoch 1:  27%|██▋       | 81/300 [02:22<06:25,  1.76s/it]Epoch 1:  27%|██▋       | 82/300 [02:24<06:34,  1.81s/it]Epoch 1:  28%|██▊       | 83/300 [02:26<06:21,  1.76s/it]Epoch 1:  28%|██▊       | 84/300 [02:28<06:34,  1.83s/it]Epoch 1:  28%|██▊       | 85/300 [02:29<06:29,  1.81s/it]Epoch 1:  29%|██▊       | 86/300 [02:31<06:40,  1.87s/it]Epoch 1:  29%|██▉       | 87/300 [02:33<06:30,  1.83s/it]Epoch 1:  29%|██▉       | 88/300 [02:35<06:11,  1.75s/it]Epoch 1:  30%|██▉       | 89/300 [02:37<06:25,  1.83s/it]Epoch 1:  30%|███       | 90/300 [02:38<05:59,  1.71s/it]Epoch 1:  30%|███       | 91/300 [02:40<06:08,  1.76s/it]Epoch 1:  31%|███       | 92/300 [02:42<05:59,  1.73s/it]Epoch 1:  31%|███       | 93/300 [02:43<05:51,  1.70s/it]Epoch 1:  31%|███▏      | 94/300 [02:45<05:41,  1.66s/it]Epoch 1:  32%|███▏      | 95/300 [02:46<05:25,  1.59s/it]Epoch 1:  32%|███▏      | 96/300 [02:48<05:31,  1.62s/it]Epoch 1:  32%|███▏      | 97/300 [02:50<05:49,  1.72s/it]Epoch 1:  33%|███▎      | 98/300 [02:52<05:37,  1.67s/it]Epoch 1:  33%|███▎      | 99/300 [02:53<05:49,  1.74s/it]06/19/2022 14:48:47 - INFO - __main__ - global step: 200; train loss: 7.890843391418457; dev loss
Epoch 1:  33%|███▎      | 100/300 [02:55<05:57,  1.79s/it]Epoch 1:  34%|███▎      | 101/300 [02:58<06:18,  1.90s/it]Epoch 1:  34%|███▍      | 102/300 [03:00<06:27,  1.96s/it]Epoch 1:  34%|███▍      | 103/300 [03:02<06:22,  1.94s/it]Epoch 1:  35%|███▍      | 104/300 [03:04<06:21,  1.95s/it]Epoch 1:  35%|███▌      | 105/300 [03:05<06:12,  1.91s/it]Epoch 1:  35%|███▌      | 106/300 [03:07<05:53,  1.82s/it]Epoch 1:  36%|███▌      | 107/300 [03:08<05:26,  1.69s/it]Epoch 1:  36%|███▌      | 108/300 [03:10<05:08,  1.61s/it]Epoch 1:  36%|███▋      | 109/300 [03:12<05:26,  1.71s/it]Epoch 1:  37%|███▋      | 110/300 [03:13<05:16,  1.67s/it]Epoch 1:  37%|███▋      | 111/300 [03:15<05:19,  1.69s/it]Epoch 1:  37%|███▋      | 112/300 [03:16<05:06,  1.63s/it]Epoch 1:  38%|███▊      | 113/300 [03:18<05:08,  1.65s/it]Epoch 1:  38%|███▊      | 114/300 [03:20<05:15,  1.70s/it]Epoch 1:  38%|███▊      | 115/300 [03:22<05:10,  1.68s/it]Epoch 1:  39%|███▊      | 116/300 [03:23<04:59,  1.63s/it]Epoch 1:  39%|███▉      | 117/300 [03:25<05:07,  1.68s/it]Epoch 1:  39%|███▉      | 118/300 [03:26<04:52,  1.61s/it]Epoch 1:  40%|███▉      | 119/300 [03:28<05:04,  1.68s/it]06/19/2022 14:49:21 - INFO - __main__ - global step: 210; train loss: 7.490105628967285; dev loss
Epoch 1:  40%|████      | 120/300 [03:30<05:08,  1.71s/it]Epoch 1:  40%|████      | 121/300 [03:32<05:27,  1.83s/it]Epoch 1:  41%|████      | 122/300 [03:34<05:04,  1.71s/it]Epoch 1:  41%|████      | 123/300 [03:35<04:46,  1.62s/it]Epoch 1:  41%|████▏     | 124/300 [03:36<04:35,  1.57s/it]Epoch 1:  42%|████▏     | 125/300 [03:38<04:25,  1.51s/it]Epoch 1:  42%|████▏     | 126/300 [03:40<04:42,  1.62s/it]Epoch 1:  42%|████▏     | 127/300 [03:42<04:52,  1.69s/it]Epoch 1:  43%|████▎     | 128/300 [03:43<04:36,  1.61s/it]Epoch 1:  43%|████▎     | 129/300 [03:44<04:25,  1.55s/it]Epoch 1:  43%|████▎     | 130/300 [03:46<04:20,  1.53s/it]Epoch 1:  44%|████▎     | 131/300 [03:47<04:11,  1.49s/it]Epoch 1:  44%|████▍     | 132/300 [03:49<04:05,  1.46s/it]Epoch 1:  44%|████▍     | 133/300 [03:50<04:01,  1.45s/it]Epoch 1:  45%|████▍     | 134/300 [03:52<04:01,  1.46s/it]Epoch 1:  45%|████▌     | 135/300 [03:53<04:02,  1.47s/it]Epoch 1:  45%|████▌     | 136/300 [03:54<03:58,  1.45s/it]Epoch 1:  46%|████▌     | 137/300 [03:56<03:58,  1.46s/it]Epoch 1:  46%|████▌     | 138/300 [03:58<04:06,  1.52s/it]Epoch 1:  46%|████▋     | 139/300 [03:59<04:07,  1.53s/it]06/19/2022 14:49:52 - INFO - __main__ - global step: 220; train loss: 7.644092559814453; dev loss
Epoch 1:  47%|████▋     | 140/300 [04:01<04:04,  1.53s/it]Epoch 1:  47%|████▋     | 141/300 [04:02<04:06,  1.55s/it]Epoch 1:  47%|████▋     | 142/300 [04:04<04:03,  1.54s/it]Epoch 1:  48%|████▊     | 143/300 [04:05<03:56,  1.51s/it]Epoch 1:  48%|████▊     | 144/300 [04:07<04:15,  1.64s/it]Epoch 1:  48%|████▊     | 145/300 [04:09<04:17,  1.66s/it]Epoch 1:  49%|████▊     | 146/300 [04:11<04:38,  1.81s/it]Epoch 1:  49%|████▉     | 147/300 [04:13<04:45,  1.86s/it]Epoch 1:  49%|████▉     | 148/300 [04:15<04:55,  1.95s/it]Epoch 1:  50%|████▉     | 149/300 [04:17<04:52,  1.94s/it]Epoch 1:  50%|█████     | 150/300 [04:19<04:48,  1.92s/it]Epoch 1:  50%|█████     | 151/300 [04:21<04:47,  1.93s/it]Epoch 1:  51%|█████     | 152/300 [04:23<04:48,  1.95s/it]Epoch 1:  51%|█████     | 153/300 [04:24<04:28,  1.83s/it]Epoch 1:  51%|█████▏    | 154/300 [04:26<04:35,  1.89s/it]Epoch 1:  52%|█████▏    | 155/300 [04:28<04:22,  1.81s/it]Epoch 1:  52%|█████▏    | 156/300 [04:30<04:27,  1.86s/it]Epoch 1:  52%|█████▏    | 157/300 [04:32<04:12,  1.77s/it]Epoch 1:  53%|█████▎    | 158/300 [04:33<04:04,  1.72s/it]Epoch 1:  53%|█████▎    | 159/300 [04:35<04:05,  1.74s/it]06/19/2022 14:50:28 - INFO - __main__ - global step: 230; train loss: 7.44420862197876; dev loss
Epoch 1:  53%|█████▎    | 160/300 [04:37<04:10,  1.79s/it]Epoch 1:  54%|█████▎    | 161/300 [04:38<03:57,  1.71s/it]Epoch 1:  54%|█████▍    | 162/300 [04:40<03:45,  1.63s/it]Epoch 1:  54%|█████▍    | 163/300 [04:42<03:58,  1.74s/it]Epoch 1:  55%|█████▍    | 164/300 [04:44<04:01,  1.77s/it]Epoch 1:  55%|█████▌    | 165/300 [04:45<03:52,  1.72s/it]Epoch 1:  55%|█████▌    | 166/300 [04:47<03:44,  1.68s/it]Epoch 1:  56%|█████▌    | 167/300 [04:49<03:57,  1.78s/it]Epoch 1:  56%|█████▌    | 168/300 [04:51<03:58,  1.80s/it]Epoch 1:  56%|█████▋    | 169/300 [04:53<04:00,  1.84s/it]Epoch 1:  57%|█████▋    | 170/300 [04:55<04:00,  1.85s/it]Epoch 1:  57%|█████▋    | 171/300 [04:57<04:01,  1.87s/it]Epoch 1:  57%|█████▋    | 172/300 [04:58<03:45,  1.76s/it]Epoch 1:  58%|█████▊    | 173/300 [05:00<03:33,  1.68s/it]Epoch 1:  58%|█████▊    | 174/300 [05:01<03:23,  1.61s/it]Epoch 1:  58%|█████▊    | 175/300 [05:03<03:39,  1.75s/it]Epoch 1:  59%|█████▊    | 176/300 [05:05<03:46,  1.83s/it]Epoch 1:  59%|█████▉    | 177/300 [05:07<03:44,  1.83s/it]Epoch 1:  59%|█████▉    | 178/300 [05:08<03:30,  1.73s/it]Epoch 1:  60%|█████▉    | 179/300 [05:10<03:22,  1.67s/it]06/19/2022 14:51:03 - INFO - __main__ - global step: 240; train loss: 7.857883453369141; dev loss
Epoch 1:  60%|██████    | 180/300 [05:12<03:20,  1.67s/it]Epoch 1:  60%|██████    | 181/300 [05:13<03:15,  1.64s/it]Epoch 1:  61%|██████    | 182/300 [05:15<03:11,  1.63s/it]Epoch 1:  61%|██████    | 183/300 [05:17<03:23,  1.74s/it]Epoch 1:  61%|██████▏   | 184/300 [05:19<03:27,  1.79s/it]Epoch 1:  62%|██████▏   | 185/300 [05:20<03:16,  1.71s/it]Epoch 1:  62%|██████▏   | 186/300 [05:22<03:18,  1.74s/it]Epoch 1:  62%|██████▏   | 187/300 [05:24<03:10,  1.68s/it]Epoch 1:  63%|██████▎   | 188/300 [05:25<03:11,  1.71s/it]Epoch 1:  63%|██████▎   | 189/300 [05:27<03:18,  1.79s/it]Epoch 1:  63%|██████▎   | 190/300 [05:29<03:19,  1.82s/it]Epoch 1:  64%|██████▎   | 191/300 [05:31<03:12,  1.77s/it]Epoch 1:  64%|██████▍   | 192/300 [05:33<03:17,  1.83s/it]Epoch 1:  64%|██████▍   | 193/300 [05:34<03:04,  1.73s/it]Epoch 1:  65%|██████▍   | 194/300 [05:36<03:08,  1.78s/it]Epoch 1:  65%|██████▌   | 195/300 [05:38<02:58,  1.70s/it]Epoch 1:  65%|██████▌   | 196/300 [05:40<03:07,  1.80s/it]Epoch 1:  66%|██████▌   | 197/300 [05:42<03:10,  1.85s/it]Epoch 1:  66%|██████▌   | 198/300 [05:43<03:03,  1.80s/it]Epoch 1:  66%|██████▋   | 199/300 [05:45<02:54,  1.73s/it]06/19/2022 14:51:38 - INFO - __main__ - global step: 250; train loss: 8.099969863891602; dev loss
Epoch 1:  67%|██████▋   | 200/300 [05:47<02:51,  1.71s/it]Epoch 1:  67%|██████▋   | 201/300 [05:49<02:54,  1.76s/it]Epoch 1:  67%|██████▋   | 202/300 [05:50<02:45,  1.69s/it]Epoch 1:  68%|██████▊   | 203/300 [05:52<02:45,  1.70s/it]Epoch 1:  68%|██████▊   | 204/300 [05:54<02:45,  1.72s/it]Epoch 1:  68%|██████▊   | 205/300 [05:56<02:51,  1.81s/it]Epoch 1:  69%|██████▊   | 206/300 [05:57<02:41,  1.71s/it]Epoch 1:  69%|██████▉   | 207/300 [05:59<02:45,  1.78s/it]Epoch 1:  69%|██████▉   | 208/300 [06:01<02:49,  1.84s/it]Epoch 1:  70%|██████▉   | 209/300 [06:03<02:48,  1.85s/it]Epoch 1:  70%|███████   | 210/300 [06:05<02:42,  1.80s/it]Epoch 1:  70%|███████   | 211/300 [06:06<02:42,  1.82s/it]Epoch 1:  71%|███████   | 212/300 [06:08<02:42,  1.85s/it]Epoch 1:  71%|███████   | 213/300 [06:10<02:37,  1.81s/it]Epoch 1:  71%|███████▏  | 214/300 [06:12<02:29,  1.74s/it]Epoch 1:  72%|███████▏  | 215/300 [06:13<02:30,  1.78s/it]Epoch 1:  72%|███████▏  | 216/300 [06:15<02:34,  1.84s/it]Epoch 1:  72%|███████▏  | 217/300 [06:17<02:26,  1.77s/it]Epoch 1:  73%|███████▎  | 218/300 [06:19<02:17,  1.68s/it]Epoch 1:  73%|███████▎  | 219/300 [06:21<02:22,  1.76s/it]06/19/2022 14:52:14 - INFO - __main__ - global step: 260; train loss: 8.287989616394043; dev loss
Epoch 1:  73%|███████▎  | 220/300 [06:22<02:18,  1.73s/it]Epoch 1:  74%|███████▎  | 221/300 [06:24<02:15,  1.72s/it]Epoch 1:  74%|███████▍  | 222/300 [06:26<02:15,  1.74s/it]Epoch 1:  74%|███████▍  | 223/300 [06:27<02:16,  1.78s/it]Epoch 1:  75%|███████▍  | 224/300 [06:29<02:16,  1.80s/it]Epoch 1:  75%|███████▌  | 225/300 [06:31<02:18,  1.85s/it]Epoch 1:  75%|███████▌  | 226/300 [06:33<02:17,  1.85s/it]Epoch 1:  76%|███████▌  | 227/300 [06:35<02:09,  1.77s/it]Epoch 1:  76%|███████▌  | 228/300 [06:36<02:04,  1.73s/it]Epoch 1:  76%|███████▋  | 229/300 [06:39<02:11,  1.85s/it]Epoch 1:  77%|███████▋  | 230/300 [06:40<02:10,  1.87s/it]Epoch 1:  77%|███████▋  | 231/300 [06:42<02:02,  1.77s/it]Epoch 1:  77%|███████▋  | 232/300 [06:43<01:54,  1.68s/it]Epoch 1:  78%|███████▊  | 233/300 [06:45<01:50,  1.64s/it]Epoch 1:  78%|███████▊  | 234/300 [06:47<01:51,  1.69s/it]Epoch 1:  78%|███████▊  | 235/300 [06:48<01:44,  1.61s/it]Epoch 1:  79%|███████▊  | 236/300 [06:50<01:42,  1.60s/it]Epoch 1:  79%|███████▉  | 237/300 [06:52<01:48,  1.72s/it]Epoch 1:  79%|███████▉  | 238/300 [06:54<01:51,  1.80s/it]Epoch 1:  80%|███████▉  | 239/300 [06:56<01:49,  1.79s/it]06/19/2022 14:52:48 - INFO - __main__ - global step: 270; train loss: 8.061104774475098; dev loss
Epoch 1:  80%|████████  | 240/300 [06:57<01:43,  1.72s/it]Epoch 1:  80%|████████  | 241/300 [06:59<01:41,  1.72s/it]Epoch 1:  81%|████████  | 242/300 [07:01<01:39,  1.72s/it]Epoch 1:  81%|████████  | 243/300 [07:02<01:40,  1.77s/it]Epoch 1:  81%|████████▏ | 244/300 [07:04<01:43,  1.84s/it]Epoch 1:  82%|████████▏ | 245/300 [07:06<01:41,  1.84s/it]Epoch 1:  82%|████████▏ | 246/300 [07:08<01:42,  1.91s/it]Epoch 1:  82%|████████▏ | 247/300 [07:10<01:34,  1.78s/it]Epoch 1:  83%|████████▎ | 248/300 [07:11<01:28,  1.71s/it]Epoch 1:  83%|████████▎ | 249/300 [07:13<01:29,  1.76s/it]Epoch 1:  83%|████████▎ | 250/300 [07:15<01:31,  1.82s/it]Epoch 1:  84%|████████▎ | 251/300 [07:17<01:26,  1.77s/it]Epoch 1:  84%|████████▍ | 252/300 [07:19<01:25,  1.77s/it]Epoch 1:  84%|████████▍ | 253/300 [07:21<01:24,  1.80s/it]Epoch 1:  85%|████████▍ | 254/300 [07:22<01:20,  1.75s/it]Epoch 1:  85%|████████▌ | 255/300 [07:24<01:16,  1.69s/it]Epoch 1:  85%|████████▌ | 256/300 [07:25<01:12,  1.66s/it]Epoch 1:  86%|████████▌ | 257/300 [07:27<01:13,  1.71s/it]Epoch 1:  86%|████████▌ | 258/300 [07:29<01:09,  1.66s/it]Epoch 1:  86%|████████▋ | 259/300 [07:30<01:09,  1.70s/it]06/19/2022 14:53:23 - INFO - __main__ - global step: 280; train loss: 8.084623336791992; dev loss
Epoch 1:  87%|████████▋ | 260/300 [07:32<01:05,  1.65s/it]Epoch 1:  87%|████████▋ | 261/300 [07:34<01:04,  1.66s/it]Epoch 1:  87%|████████▋ | 262/300 [07:36<01:08,  1.79s/it]Epoch 1:  88%|████████▊ | 263/300 [07:38<01:09,  1.87s/it]Epoch 1:  88%|████████▊ | 264/300 [07:39<01:03,  1.76s/it]Epoch 1:  88%|████████▊ | 265/300 [07:41<00:59,  1.71s/it]Epoch 1:  89%|████████▊ | 266/300 [07:43<00:57,  1.68s/it]Epoch 1:  89%|████████▉ | 267/300 [07:44<00:55,  1.67s/it]Epoch 1:  89%|████████▉ | 268/300 [07:46<00:55,  1.75s/it]Epoch 1:  90%|████████▉ | 269/300 [07:48<00:55,  1.81s/it]Epoch 1:  90%|█████████ | 270/300 [07:50<00:53,  1.79s/it]Epoch 1:  90%|█████████ | 271/300 [07:52<00:54,  1.88s/it]Epoch 1:  91%|█████████ | 272/300 [07:54<00:53,  1.91s/it]Epoch 1:  91%|█████████ | 273/300 [07:56<00:50,  1.87s/it]Epoch 1:  91%|█████████▏| 274/300 [07:58<00:49,  1.90s/it]Epoch 1:  92%|█████████▏| 275/300 [07:59<00:47,  1.90s/it]Epoch 1:  92%|█████████▏| 276/300 [08:02<00:47,  1.97s/it]Epoch 1:  92%|█████████▏| 277/300 [08:03<00:42,  1.84s/it]Epoch 1:  93%|█████████▎| 278/300 [08:05<00:41,  1.88s/it]Epoch 1:  93%|█████████▎| 279/300 [08:07<00:37,  1.79s/it]06/19/2022 14:54:00 - INFO - __main__ - global step: 290; train loss: 8.03775691986084; dev loss
Epoch 1:  93%|█████████▎| 280/300 [08:09<00:36,  1.81s/it]Epoch 1:  94%|█████████▎| 281/300 [08:10<00:32,  1.72s/it]Epoch 1:  94%|█████████▍| 282/300 [08:12<00:32,  1.78s/it]Epoch 1:  94%|█████████▍| 283/300 [08:14<00:31,  1.86s/it]Epoch 1:  95%|█████████▍| 284/300 [08:16<00:27,  1.73s/it]Epoch 1:  95%|█████████▌| 285/300 [08:17<00:25,  1.69s/it]Epoch 1:  95%|█████████▌| 286/300 [08:19<00:22,  1.62s/it]Epoch 1:  96%|█████████▌| 287/300 [08:20<00:20,  1.59s/it]Epoch 1:  96%|█████████▌| 288/300 [08:22<00:19,  1.66s/it]Epoch 1:  96%|█████████▋| 289/300 [08:24<00:18,  1.65s/it]Epoch 1:  97%|█████████▋| 290/300 [08:25<00:16,  1.60s/it]Epoch 1:  97%|█████████▋| 291/300 [08:26<00:13,  1.55s/it]Epoch 1:  97%|█████████▋| 292/300 [08:28<00:12,  1.59s/it]Epoch 1:  98%|█████████▊| 293/300 [08:30<00:11,  1.67s/it]Epoch 1:  98%|█████████▊| 294/300 [08:32<00:10,  1.71s/it]Epoch 1:  98%|█████████▊| 295/300 [08:33<00:08,  1.64s/it]Epoch 1:  99%|█████████▊| 296/300 [08:35<00:06,  1.62s/it]Epoch 1:  99%|█████████▉| 297/300 [08:37<00:05,  1.70s/it]Epoch 1:  99%|█████████▉| 298/300 [08:38<00:03,  1.69s/it]Epoch 1: 100%|█████████▉| 299/300 [08:40<00:01,  1.67s/it]06/19/2022 14:54:33 - INFO - __main__ - global step: 300; train loss: 7.828869819641113; dev loss
Epoch 1: 100%|██████████| 300/300 [08:42<00:00,  1.75s/it]Epoch 1: 100%|██████████| 300/300 [08:42<00:00,  1.74s/it]
Epoch 2:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 2:   0%|          | 1/300 [00:01<07:48,  1.57s/it]Epoch 2:   1%|          | 2/300 [00:03<08:28,  1.71s/it]Epoch 2:   1%|          | 3/300 [00:04<07:55,  1.60s/it]Epoch 2:   1%|▏         | 4/300 [00:06<07:56,  1.61s/it]Epoch 2:   2%|▏         | 5/300 [00:08<08:33,  1.74s/it]Epoch 2:   2%|▏         | 6/300 [00:10<08:22,  1.71s/it]Epoch 2:   2%|▏         | 7/300 [00:11<08:02,  1.65s/it]Epoch 2:   3%|▎         | 8/300 [00:13<08:35,  1.76s/it]Epoch 2:   3%|▎         | 9/300 [00:15<08:49,  1.82s/it]Epoch 2:   3%|▎         | 10/300 [00:17<08:37,  1.79s/it]Epoch 2:   4%|▎         | 11/300 [00:19<08:47,  1.83s/it]Epoch 2:   4%|▍         | 12/300 [00:21<09:04,  1.89s/it]Epoch 2:   4%|▍         | 13/300 [00:23<09:16,  1.94s/it]Epoch 2:   5%|▍         | 14/300 [00:24<08:53,  1.87s/it]Epoch 2:   5%|▌         | 15/300 [00:26<09:02,  1.90s/it]Epoch 2:   5%|▌         | 16/300 [00:28<08:40,  1.83s/it]Epoch 2:   6%|▌         | 17/300 [00:30<08:45,  1.86s/it]Epoch 2:   6%|▌         | 18/300 [00:32<08:39,  1.84s/it]Epoch 2:   6%|▋         | 19/300 [00:34<08:54,  1.90s/it]06/19/2022 14:55:09 - INFO - __main__ - global step: 310; train loss: 7.73529052734375; dev loss
Epoch 2:   7%|▋         | 20/300 [00:35<08:21,  1.79s/it]Epoch 2:   7%|▋         | 21/300 [00:37<08:06,  1.74s/it]Epoch 2:   7%|▋         | 22/300 [00:39<08:24,  1.81s/it]Epoch 2:   8%|▊         | 23/300 [00:41<08:30,  1.84s/it]Epoch 2:   8%|▊         | 24/300 [00:43<08:38,  1.88s/it]Epoch 2:   8%|▊         | 25/300 [00:45<08:13,  1.79s/it]Epoch 2:   9%|▊         | 26/300 [00:46<07:43,  1.69s/it]Epoch 2:   9%|▉         | 27/300 [00:47<07:22,  1.62s/it]Epoch 2:   9%|▉         | 28/300 [00:49<07:15,  1.60s/it]Epoch 2:  10%|▉         | 29/300 [00:51<07:20,  1.62s/it]Epoch 2:  10%|█         | 30/300 [00:52<07:11,  1.60s/it]Epoch 2:  10%|█         | 31/300 [00:54<07:34,  1.69s/it]Epoch 2:  11%|█         | 32/300 [00:56<07:55,  1.78s/it]Epoch 2:  11%|█         | 33/300 [00:58<08:19,  1.87s/it]Epoch 2:  11%|█▏        | 34/300 [01:00<08:23,  1.89s/it]Epoch 2:  12%|█▏        | 35/300 [01:02<08:02,  1.82s/it]Epoch 2:  12%|█▏        | 36/300 [01:04<08:04,  1.84s/it]Epoch 2:  12%|█▏        | 37/300 [01:06<08:18,  1.90s/it]Epoch 2:  13%|█▎        | 38/300 [01:07<07:59,  1.83s/it]Epoch 2:  13%|█▎        | 39/300 [01:09<08:04,  1.86s/it]06/19/2022 14:55:45 - INFO - __main__ - global step: 320; train loss: 7.5901079177856445; dev loss
Epoch 2:  13%|█▎        | 40/300 [01:11<08:07,  1.87s/it]Epoch 2:  14%|█▎        | 41/300 [01:13<08:12,  1.90s/it]Epoch 2:  14%|█▍        | 42/300 [01:15<08:18,  1.93s/it]Epoch 2:  14%|█▍        | 43/300 [01:17<07:44,  1.81s/it]Epoch 2:  15%|█▍        | 44/300 [01:18<07:34,  1.77s/it]Epoch 2:  15%|█▌        | 45/300 [01:20<07:26,  1.75s/it]Epoch 2:  15%|█▌        | 46/300 [01:22<07:41,  1.82s/it]Epoch 2:  16%|█▌        | 47/300 [01:24<07:42,  1.83s/it]Epoch 2:  16%|█▌        | 48/300 [01:25<07:21,  1.75s/it]Epoch 2:  16%|█▋        | 49/300 [01:27<07:31,  1.80s/it]Epoch 2:  17%|█▋        | 50/300 [01:29<07:40,  1.84s/it]Epoch 2:  17%|█▋        | 51/300 [01:31<07:13,  1.74s/it]Epoch 2:  17%|█▋        | 52/300 [01:32<06:56,  1.68s/it]Epoch 2:  18%|█▊        | 53/300 [01:34<06:44,  1.64s/it]Epoch 2:  18%|█▊        | 54/300 [01:36<06:41,  1.63s/it]Epoch 2:  18%|█▊        | 55/300 [01:37<06:31,  1.60s/it]Epoch 2:  19%|█▊        | 56/300 [01:39<06:25,  1.58s/it]Epoch 2:  19%|█▉        | 57/300 [01:40<06:48,  1.68s/it]Epoch 2:  19%|█▉        | 58/300 [01:42<06:50,  1.70s/it]Epoch 2:  20%|█▉        | 59/300 [01:44<06:32,  1.63s/it]06/19/2022 14:56:19 - INFO - __main__ - global step: 330; train loss: 7.871311187744141; dev loss
Epoch 2:  20%|██        | 60/300 [01:45<06:21,  1.59s/it]Epoch 2:  20%|██        | 61/300 [01:47<06:36,  1.66s/it]Epoch 2:  21%|██        | 62/300 [01:49<07:02,  1.77s/it]Epoch 2:  21%|██        | 63/300 [01:51<07:16,  1.84s/it]Epoch 2:  21%|██▏       | 64/300 [01:53<07:26,  1.89s/it]Epoch 2:  22%|██▏       | 65/300 [01:55<07:03,  1.80s/it]Epoch 2:  22%|██▏       | 66/300 [01:56<06:50,  1.75s/it]Epoch 2:  22%|██▏       | 67/300 [01:58<06:42,  1.73s/it]Epoch 2:  23%|██▎       | 68/300 [02:00<06:33,  1.70s/it]Epoch 2:  23%|██▎       | 69/300 [02:01<06:25,  1.67s/it]Epoch 2:  23%|██▎       | 70/300 [02:03<06:19,  1.65s/it]Epoch 2:  24%|██▎       | 71/300 [02:05<07:01,  1.84s/it]Epoch 2:  24%|██▍       | 72/300 [02:07<06:33,  1.73s/it]Epoch 2:  24%|██▍       | 73/300 [02:09<06:50,  1.81s/it]Epoch 2:  25%|██▍       | 74/300 [02:11<07:01,  1.86s/it]Epoch 2:  25%|██▌       | 75/300 [02:12<06:56,  1.85s/it]Epoch 2:  25%|██▌       | 76/300 [02:14<07:05,  1.90s/it]Epoch 2:  26%|██▌       | 77/300 [02:16<06:45,  1.82s/it]Epoch 2:  26%|██▌       | 78/300 [02:18<06:47,  1.84s/it]Epoch 2:  26%|██▋       | 79/300 [02:20<06:58,  1.90s/it]06/19/2022 14:56:55 - INFO - __main__ - global step: 340; train loss: 8.0453519821167; dev loss
Epoch 2:  27%|██▋       | 80/300 [02:22<06:36,  1.80s/it]Epoch 2:  27%|██▋       | 81/300 [02:23<06:16,  1.72s/it]Epoch 2:  27%|██▋       | 82/300 [02:25<06:04,  1.67s/it]Epoch 2:  28%|██▊       | 83/300 [02:27<06:31,  1.80s/it]Epoch 2:  28%|██▊       | 84/300 [02:28<06:15,  1.74s/it]Epoch 2:  28%|██▊       | 85/300 [02:30<06:32,  1.82s/it]Epoch 2:  29%|██▊       | 86/300 [02:32<06:24,  1.80s/it]Epoch 2:  29%|██▉       | 87/300 [02:34<06:33,  1.85s/it]Epoch 2:  29%|██▉       | 88/300 [02:36<06:30,  1.84s/it]Epoch 2:  30%|██▉       | 89/300 [02:37<06:09,  1.75s/it]Epoch 2:  30%|███       | 90/300 [02:39<05:49,  1.66s/it]Epoch 2:  30%|███       | 91/300 [02:41<06:02,  1.73s/it]Epoch 2:  31%|███       | 92/300 [02:43<06:14,  1.80s/it]Epoch 2:  31%|███       | 93/300 [02:44<05:51,  1.70s/it]Epoch 2:  31%|███▏      | 94/300 [02:46<05:55,  1.73s/it]Epoch 2:  32%|███▏      | 95/300 [02:47<05:43,  1.67s/it]Epoch 2:  32%|███▏      | 96/300 [02:49<05:39,  1.66s/it]Epoch 2:  32%|███▏      | 97/300 [02:51<05:47,  1.71s/it]Epoch 2:  33%|███▎      | 98/300 [02:53<05:38,  1.68s/it]Epoch 2:  33%|███▎      | 99/300 [02:55<06:00,  1.79s/it]06/19/2022 14:57:30 - INFO - __main__ - global step: 350; train loss: 7.625683784484863; dev loss
Epoch 2:  33%|███▎      | 100/300 [02:56<05:50,  1.75s/it]Epoch 2:  34%|███▎      | 101/300 [02:58<05:32,  1.67s/it]Epoch 2:  34%|███▍      | 102/300 [02:59<05:21,  1.62s/it]Epoch 2:  34%|███▍      | 103/300 [03:01<05:13,  1.59s/it]Epoch 2:  35%|███▍      | 104/300 [03:02<05:13,  1.60s/it]Epoch 2:  35%|███▌      | 105/300 [03:04<05:29,  1.69s/it]Epoch 2:  35%|███▌      | 106/300 [03:06<05:42,  1.77s/it]Epoch 2:  36%|███▌      | 107/300 [03:08<05:35,  1.74s/it]Epoch 2:  36%|███▌      | 108/300 [03:09<05:22,  1.68s/it]Epoch 2:  36%|███▋      | 109/300 [03:11<05:11,  1.63s/it]Epoch 2:  37%|███▋      | 110/300 [03:13<05:26,  1.72s/it]Epoch 2:  37%|███▋      | 111/300 [03:15<05:21,  1.70s/it]Epoch 2:  37%|███▋      | 112/300 [03:16<05:19,  1.70s/it]Epoch 2:  38%|███▊      | 113/300 [03:18<05:05,  1.64s/it]Epoch 2:  38%|███▊      | 114/300 [03:20<05:13,  1.68s/it]Epoch 2:  38%|███▊      | 115/300 [03:22<05:27,  1.77s/it]Epoch 2:  39%|███▊      | 116/300 [03:23<05:34,  1.82s/it]Epoch 2:  39%|███▉      | 117/300 [03:25<05:41,  1.87s/it]Epoch 2:  39%|███▉      | 118/300 [03:27<05:20,  1.76s/it]Epoch 2:  40%|███▉      | 119/300 [03:29<05:26,  1.81s/it]06/19/2022 14:58:04 - INFO - __main__ - global step: 360; train loss: 8.167433738708496; dev loss
Epoch 2:  40%|████      | 120/300 [03:30<05:09,  1.72s/it]Epoch 2:  40%|████      | 121/300 [03:32<05:23,  1.81s/it]Epoch 2:  41%|████      | 122/300 [03:34<05:05,  1.72s/it]Epoch 2:  41%|████      | 123/300 [03:35<04:52,  1.65s/it]Epoch 2:  41%|████▏     | 124/300 [03:37<05:01,  1.71s/it]Epoch 2:  42%|████▏     | 125/300 [03:39<04:53,  1.68s/it]Epoch 2:  42%|████▏     | 126/300 [03:40<04:42,  1.62s/it]Epoch 2:  42%|████▏     | 127/300 [03:42<04:59,  1.73s/it]Epoch 2:  43%|████▎     | 128/300 [03:44<05:10,  1.80s/it]Epoch 2:  43%|████▎     | 129/300 [03:46<05:03,  1.77s/it]Epoch 2:  43%|████▎     | 130/300 [03:48<04:54,  1.73s/it]Epoch 2:  44%|████▎     | 131/300 [03:50<05:03,  1.80s/it]Epoch 2:  44%|████▍     | 132/300 [03:51<05:02,  1.80s/it]Epoch 2:  44%|████▍     | 133/300 [03:53<05:07,  1.84s/it]Epoch 2:  45%|████▍     | 134/300 [03:55<04:49,  1.74s/it]Epoch 2:  45%|████▌     | 135/300 [03:56<04:34,  1.66s/it]Epoch 2:  45%|████▌     | 136/300 [03:58<04:42,  1.72s/it]Epoch 2:  46%|████▌     | 137/300 [04:00<04:36,  1.70s/it]Epoch 2:  46%|████▌     | 138/300 [04:01<04:22,  1.62s/it]Epoch 2:  46%|████▋     | 139/300 [04:03<04:15,  1.59s/it]06/19/2022 14:58:38 - INFO - __main__ - global step: 370; train loss: 7.824731349945068; dev loss
Epoch 2:  47%|████▋     | 140/300 [04:05<04:25,  1.66s/it]Epoch 2:  47%|████▋     | 141/300 [04:06<04:21,  1.65s/it]Epoch 2:  47%|████▋     | 142/300 [04:08<04:29,  1.70s/it]Epoch 2:  48%|████▊     | 143/300 [04:10<04:19,  1.66s/it]Epoch 2:  48%|████▊     | 144/300 [04:12<04:31,  1.74s/it]Epoch 2:  48%|████▊     | 145/300 [04:13<04:25,  1.72s/it]Epoch 2:  49%|████▊     | 146/300 [04:15<04:32,  1.77s/it]Epoch 2:  49%|████▉     | 147/300 [04:17<04:34,  1.80s/it]Epoch 2:  49%|████▉     | 148/300 [04:18<04:18,  1.70s/it]Epoch 2:  50%|████▉     | 149/300 [04:20<04:08,  1.65s/it]Epoch 2:  50%|█████     | 150/300 [04:22<04:05,  1.64s/it]Epoch 2:  50%|█████     | 151/300 [04:23<03:56,  1.59s/it]Epoch 2:  51%|█████     | 152/300 [04:25<04:10,  1.69s/it]Epoch 2:  51%|█████     | 153/300 [04:26<03:59,  1.63s/it]Epoch 2:  51%|█████▏    | 154/300 [04:28<04:12,  1.73s/it]Epoch 2:  52%|█████▏    | 155/300 [04:31<04:28,  1.85s/it]Epoch 2:  52%|█████▏    | 156/300 [04:32<04:13,  1.76s/it]Epoch 2:  52%|█████▏    | 157/300 [04:34<04:00,  1.68s/it]Epoch 2:  53%|█████▎    | 158/300 [04:35<03:57,  1.68s/it]Epoch 2:  53%|█████▎    | 159/300 [04:37<04:08,  1.76s/it]06/19/2022 14:59:13 - INFO - __main__ - global step: 380; train loss: 8.353010177612305; dev loss
Epoch 2:  53%|█████▎    | 160/300 [04:39<04:12,  1.80s/it]Epoch 2:  54%|█████▎    | 161/300 [04:41<04:18,  1.86s/it]Epoch 2:  54%|█████▍    | 162/300 [04:43<04:19,  1.88s/it]Epoch 2:  54%|█████▍    | 163/300 [04:45<04:02,  1.77s/it]Epoch 2:  55%|█████▍    | 164/300 [04:46<04:08,  1.83s/it]Epoch 2:  55%|█████▌    | 165/300 [04:48<04:11,  1.86s/it]Epoch 2:  55%|█████▌    | 166/300 [04:50<03:59,  1.79s/it]Epoch 2:  56%|█████▌    | 167/300 [04:52<03:54,  1.76s/it]Epoch 2:  56%|█████▌    | 168/300 [04:53<03:47,  1.72s/it]Epoch 2:  56%|█████▋    | 169/300 [04:55<03:55,  1.80s/it]Epoch 2:  57%|█████▋    | 170/300 [04:57<03:46,  1.75s/it]Epoch 2:  57%|█████▋    | 171/300 [04:59<03:39,  1.70s/it]Epoch 2:  57%|█████▋    | 172/300 [05:00<03:31,  1.65s/it]Epoch 2:  58%|█████▊    | 173/300 [05:02<03:28,  1.64s/it]Epoch 2:  58%|█████▊    | 174/300 [05:04<03:32,  1.68s/it]Epoch 2:  58%|█████▊    | 175/300 [05:05<03:32,  1.70s/it]Epoch 2:  59%|█████▊    | 176/300 [05:07<03:22,  1.63s/it]Epoch 2:  59%|█████▉    | 177/300 [05:08<03:21,  1.64s/it]Epoch 2:  59%|█████▉    | 178/300 [05:10<03:19,  1.63s/it]Epoch 2:  60%|█████▉    | 179/300 [05:12<03:19,  1.65s/it]06/19/2022 14:59:47 - INFO - __main__ - global step: 390; train loss: 8.401211738586426; dev loss
Epoch 2:  60%|██████    | 180/300 [05:14<03:25,  1.71s/it]Epoch 2:  60%|██████    | 181/300 [05:16<03:33,  1.80s/it]Epoch 2:  61%|██████    | 182/300 [05:17<03:20,  1.70s/it]Epoch 2:  61%|██████    | 183/300 [05:19<03:32,  1.81s/it]Epoch 2:  61%|██████▏   | 184/300 [05:21<03:19,  1.72s/it]Epoch 2:  62%|██████▏   | 185/300 [05:22<03:23,  1.77s/it]Epoch 2:  62%|██████▏   | 186/300 [05:24<03:19,  1.75s/it]Epoch 2:  62%|██████▏   | 187/300 [05:26<03:15,  1.73s/it]Epoch 2:  63%|██████▎   | 188/300 [05:28<03:13,  1.73s/it]Epoch 2:  63%|██████▎   | 189/300 [05:29<03:16,  1.77s/it]Epoch 2:  63%|██████▎   | 190/300 [05:31<03:12,  1.75s/it]Epoch 2:  64%|██████▎   | 191/300 [05:33<03:14,  1.78s/it]Epoch 2:  64%|██████▍   | 192/300 [05:35<03:15,  1.81s/it]Epoch 2:  64%|██████▍   | 193/300 [05:36<03:04,  1.73s/it]Epoch 2:  65%|██████▍   | 194/300 [05:38<02:59,  1.69s/it]Epoch 2:  65%|██████▌   | 195/300 [05:40<02:57,  1.69s/it]Epoch 2:  65%|██████▌   | 196/300 [05:42<03:03,  1.76s/it]Epoch 2:  66%|██████▌   | 197/300 [05:43<02:54,  1.70s/it]Epoch 2:  66%|██████▌   | 198/300 [05:45<02:54,  1.71s/it]Epoch 2:  66%|██████▋   | 199/300 [05:47<03:00,  1.78s/it]06/19/2022 15:00:23 - INFO - __main__ - global step: 400; train loss: 7.6624908447265625; dev loss
Epoch 2:  67%|██████▋   | 200/300 [05:49<03:01,  1.82s/it]Epoch 2:  67%|██████▋   | 201/300 [05:51<02:56,  1.79s/it]Epoch 2:  67%|██████▋   | 202/300 [05:52<02:51,  1.75s/it]Epoch 2:  68%|██████▊   | 203/300 [05:54<02:41,  1.66s/it]Epoch 2:  68%|██████▊   | 204/300 [05:56<02:47,  1.74s/it]Epoch 2:  68%|██████▊   | 205/300 [05:57<02:44,  1.73s/it]Epoch 2:  69%|██████▊   | 206/300 [05:59<02:38,  1.69s/it]Epoch 2:  69%|██████▉   | 207/300 [06:01<02:40,  1.73s/it]Epoch 2:  69%|██████▉   | 208/300 [06:03<02:46,  1.81s/it]Epoch 2:  70%|██████▉   | 209/300 [06:05<02:46,  1.83s/it]Epoch 2:  70%|███████   | 210/300 [06:06<02:39,  1.77s/it]Epoch 2:  70%|███████   | 211/300 [06:08<02:30,  1.69s/it]Epoch 2:  71%|███████   | 212/300 [06:10<02:37,  1.79s/it]Epoch 2:  71%|███████   | 213/300 [06:12<02:37,  1.81s/it]Epoch 2:  71%|███████▏  | 214/300 [06:13<02:38,  1.84s/it]Epoch 2:  72%|███████▏  | 215/300 [06:15<02:38,  1.87s/it]Epoch 2:  72%|███████▏  | 216/300 [06:17<02:35,  1.85s/it]Epoch 2:  72%|███████▏  | 217/300 [06:19<02:41,  1.94s/it]Epoch 2:  73%|███████▎  | 218/300 [06:21<02:37,  1.93s/it]Epoch 2:  73%|███████▎  | 219/300 [06:23<02:33,  1.89s/it]06/19/2022 15:00:59 - INFO - __main__ - global step: 410; train loss: 7.931025505065918; dev loss
Epoch 2:  73%|███████▎  | 220/300 [06:25<02:26,  1.83s/it]Epoch 2:  74%|███████▎  | 221/300 [06:26<02:13,  1.70s/it]Epoch 2:  74%|███████▍  | 222/300 [06:28<02:07,  1.63s/it]Epoch 2:  74%|███████▍  | 223/300 [06:30<02:11,  1.71s/it]Epoch 2:  75%|███████▍  | 224/300 [06:32<02:17,  1.81s/it]Epoch 2:  75%|███████▌  | 225/300 [06:33<02:10,  1.74s/it]Epoch 2:  75%|███████▌  | 226/300 [06:35<02:02,  1.66s/it]Epoch 2:  76%|███████▌  | 227/300 [06:36<01:56,  1.60s/it]Epoch 2:  76%|███████▌  | 228/300 [06:38<01:59,  1.66s/it]Epoch 2:  76%|███████▋  | 229/300 [06:39<01:56,  1.64s/it]Epoch 2:  77%|███████▋  | 230/300 [06:41<01:59,  1.71s/it]Epoch 2:  77%|███████▋  | 231/300 [06:43<01:54,  1.66s/it]Epoch 2:  77%|███████▋  | 232/300 [06:45<01:56,  1.71s/it]Epoch 2:  78%|███████▊  | 233/300 [06:46<01:51,  1.66s/it]Epoch 2:  78%|███████▊  | 234/300 [06:48<01:48,  1.65s/it]Epoch 2:  78%|███████▊  | 235/300 [06:50<01:47,  1.65s/it]Epoch 2:  79%|███████▊  | 236/300 [06:51<01:44,  1.64s/it]Epoch 2:  79%|███████▉  | 237/300 [06:53<01:42,  1.63s/it]Epoch 2:  79%|███████▉  | 238/300 [06:54<01:41,  1.63s/it]Epoch 2:  80%|███████▉  | 239/300 [06:56<01:41,  1.67s/it]06/19/2022 15:01:32 - INFO - __main__ - global step: 420; train loss: 7.810005187988281; dev loss
Epoch 2:  80%|████████  | 240/300 [06:58<01:45,  1.75s/it]Epoch 2:  80%|████████  | 241/300 [07:00<01:47,  1.83s/it]Epoch 2:  81%|████████  | 242/300 [07:02<01:47,  1.85s/it]Epoch 2:  81%|████████  | 243/300 [07:04<01:47,  1.89s/it]Epoch 2:  81%|████████▏ | 244/300 [07:06<01:45,  1.88s/it]Epoch 2:  82%|████████▏ | 245/300 [07:08<01:46,  1.94s/it]Epoch 2:  82%|████████▏ | 246/300 [07:10<01:40,  1.87s/it]Epoch 2:  82%|████████▏ | 247/300 [07:11<01:33,  1.76s/it]Epoch 2:  83%|████████▎ | 248/300 [07:13<01:27,  1.69s/it]Epoch 2:  83%|████████▎ | 249/300 [07:14<01:24,  1.67s/it]Epoch 2:  83%|████████▎ | 250/300 [07:16<01:28,  1.76s/it]Epoch 2:  84%|████████▎ | 251/300 [07:18<01:27,  1.78s/it]Epoch 2:  84%|████████▍ | 252/300 [07:20<01:21,  1.70s/it]Epoch 2:  84%|████████▍ | 253/300 [07:22<01:26,  1.84s/it]Epoch 2:  85%|████████▍ | 254/300 [07:24<01:26,  1.87s/it]Epoch 2:  85%|████████▌ | 255/300 [07:25<01:20,  1.80s/it]Epoch 2:  85%|████████▌ | 256/300 [07:27<01:21,  1.85s/it]Epoch 2:  86%|████████▌ | 257/300 [07:29<01:20,  1.87s/it]Epoch 2:  86%|████████▌ | 258/300 [07:31<01:20,  1.91s/it]Epoch 2:  86%|████████▋ | 259/300 [07:33<01:20,  1.96s/it]06/19/2022 15:02:09 - INFO - __main__ - global step: 430; train loss: 7.479113578796387; dev loss
Epoch 2:  87%|████████▋ | 260/300 [07:35<01:16,  1.91s/it]Epoch 2:  87%|████████▋ | 261/300 [07:37<01:14,  1.92s/it]Epoch 2:  87%|████████▋ | 262/300 [07:39<01:10,  1.86s/it]Epoch 2:  88%|████████▊ | 263/300 [07:40<01:05,  1.78s/it]Epoch 2:  88%|████████▊ | 264/300 [07:42<01:01,  1.72s/it]Epoch 2:  88%|████████▊ | 265/300 [07:44<01:01,  1.77s/it]Epoch 2:  89%|████████▊ | 266/300 [07:46<00:59,  1.76s/it]Epoch 2:  89%|████████▉ | 267/300 [07:47<00:59,  1.80s/it]Epoch 2:  89%|████████▉ | 268/300 [07:49<00:54,  1.71s/it]Epoch 2:  90%|████████▉ | 269/300 [07:50<00:51,  1.65s/it]Epoch 2:  90%|█████████ | 270/300 [07:52<00:49,  1.64s/it]Epoch 2:  90%|█████████ | 271/300 [07:54<00:46,  1.60s/it]Epoch 2:  91%|█████████ | 272/300 [07:55<00:47,  1.70s/it]Epoch 2:  91%|█████████ | 273/300 [07:57<00:44,  1.65s/it]Epoch 2:  91%|█████████▏| 274/300 [07:59<00:42,  1.64s/it]Epoch 2:  92%|█████████▏| 275/300 [08:00<00:40,  1.62s/it]Epoch 2:  92%|█████████▏| 276/300 [08:02<00:38,  1.60s/it]Epoch 2:  92%|█████████▏| 277/300 [08:03<00:35,  1.56s/it]Epoch 2:  93%|█████████▎| 278/300 [08:05<00:34,  1.59s/it]Epoch 2:  93%|█████████▎| 279/300 [08:06<00:32,  1.55s/it]06/19/2022 15:02:42 - INFO - __main__ - global step: 440; train loss: 7.828091621398926; dev loss
Epoch 2:  93%|█████████▎| 280/300 [08:08<00:30,  1.54s/it]Epoch 2:  94%|█████████▎| 281/300 [08:09<00:29,  1.57s/it]Epoch 2:  94%|█████████▍| 282/300 [08:11<00:27,  1.54s/it]Epoch 2:  94%|█████████▍| 283/300 [08:13<00:28,  1.69s/it]Epoch 2:  95%|█████████▍| 284/300 [08:15<00:27,  1.73s/it]Epoch 2:  95%|█████████▌| 285/300 [08:16<00:25,  1.71s/it]Epoch 2:  95%|█████████▌| 286/300 [08:18<00:23,  1.65s/it]Epoch 2:  96%|█████████▌| 287/300 [08:20<00:21,  1.68s/it]Epoch 2:  96%|█████████▌| 288/300 [08:21<00:19,  1.61s/it]Epoch 2:  96%|█████████▋| 289/300 [08:23<00:17,  1.61s/it]Epoch 2:  97%|█████████▋| 290/300 [08:25<00:16,  1.66s/it]Epoch 2:  97%|█████████▋| 291/300 [08:26<00:15,  1.73s/it]Epoch 2:  97%|█████████▋| 292/300 [08:28<00:14,  1.79s/it]Epoch 2:  98%|█████████▊| 293/300 [08:30<00:13,  1.88s/it]Epoch 2:  98%|█████████▊| 294/300 [08:32<00:11,  1.91s/it]Epoch 2:  98%|█████████▊| 295/300 [08:34<00:09,  1.87s/it]Epoch 2:  99%|█████████▊| 296/300 [08:36<00:07,  1.75s/it]Epoch 2:  99%|█████████▉| 297/300 [08:37<00:05,  1.74s/it]Epoch 2:  99%|█████████▉| 298/300 [08:39<00:03,  1.69s/it]Epoch 2: 100%|█████████▉| 299/300 [08:41<00:01,  1.68s/it]06/19/2022 15:03:16 - INFO - __main__ - global step: 450; train loss: 7.600025177001953; dev loss
Epoch 2: 100%|██████████| 300/300 [08:42<00:00,  1.69s/it]Epoch 2: 100%|██████████| 300/300 [08:42<00:00,  1.74s/it]
Epoch 3:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 3:   0%|          | 1/300 [00:01<08:59,  1.81s/it]Epoch 3:   1%|          | 2/300 [00:03<08:15,  1.66s/it]Epoch 3:   1%|          | 3/300 [00:05<09:10,  1.85s/it]Epoch 3:   1%|▏         | 4/300 [00:06<08:29,  1.72s/it]Epoch 3:   2%|▏         | 5/300 [00:08<08:10,  1.66s/it]Epoch 3:   2%|▏         | 6/300 [00:10<08:40,  1.77s/it]Epoch 3:   2%|▏         | 7/300 [00:12<08:21,  1.71s/it]Epoch 3:   3%|▎         | 8/300 [00:13<07:59,  1.64s/it]Epoch 3:   3%|▎         | 9/300 [00:15<08:31,  1.76s/it]Epoch 3:   3%|▎         | 10/300 [00:17<09:05,  1.88s/it]Epoch 3:   4%|▎         | 11/300 [00:19<08:49,  1.83s/it]Epoch 3:   4%|▍         | 12/300 [00:21<09:00,  1.88s/it]Epoch 3:   4%|▍         | 13/300 [00:23<08:34,  1.79s/it]Epoch 3:   5%|▍         | 14/300 [00:24<08:14,  1.73s/it]Epoch 3:   5%|▌         | 15/300 [00:26<08:05,  1.70s/it]Epoch 3:   5%|▌         | 16/300 [00:28<08:08,  1.72s/it]Epoch 3:   6%|▌         | 17/300 [00:29<07:51,  1.67s/it]Epoch 3:   6%|▌         | 18/300 [00:31<08:08,  1.73s/it]Epoch 3:   6%|▋         | 19/300 [00:32<07:46,  1.66s/it]06/19/2022 15:03:51 - INFO - __main__ - global step: 460; train loss: 8.102995872497559; dev loss
Epoch 3:   7%|▋         | 20/300 [00:34<07:38,  1.64s/it]Epoch 3:   7%|▋         | 21/300 [00:36<07:27,  1.60s/it]Epoch 3:   7%|▋         | 22/300 [00:37<07:32,  1.63s/it]Epoch 3:   8%|▊         | 23/300 [00:39<07:30,  1.63s/it]Epoch 3:   8%|▊         | 24/300 [00:41<07:48,  1.70s/it]Epoch 3:   8%|▊         | 25/300 [00:42<07:38,  1.67s/it]Epoch 3:   9%|▊         | 26/300 [00:44<08:04,  1.77s/it]Epoch 3:   9%|▉         | 27/300 [00:46<08:25,  1.85s/it]Epoch 3:   9%|▉         | 28/300 [00:48<08:43,  1.93s/it]Epoch 3:  10%|▉         | 29/300 [00:50<08:42,  1.93s/it]Epoch 3:  10%|█         | 30/300 [00:52<08:07,  1.80s/it]Epoch 3:  10%|█         | 31/300 [00:53<07:42,  1.72s/it]Epoch 3:  11%|█         | 32/300 [00:55<08:04,  1.81s/it]Epoch 3:  11%|█         | 33/300 [00:57<08:12,  1.85s/it]Epoch 3:  11%|█▏        | 34/300 [00:59<07:55,  1.79s/it]Epoch 3:  12%|█▏        | 35/300 [01:00<07:25,  1.68s/it]Epoch 3:  12%|█▏        | 36/300 [01:02<07:11,  1.63s/it]Epoch 3:  12%|█▏        | 37/300 [01:04<07:03,  1.61s/it]Epoch 3:  13%|█▎        | 38/300 [01:05<07:17,  1.67s/it]Epoch 3:  13%|█▎        | 39/300 [01:07<07:01,  1.61s/it]06/19/2022 15:04:26 - INFO - __main__ - global step: 470; train loss: 7.686532020568848; dev loss
Epoch 3:  13%|█▎        | 40/300 [01:09<07:29,  1.73s/it]Epoch 3:  14%|█▎        | 41/300 [01:11<07:52,  1.83s/it]Epoch 3:  14%|█▍        | 42/300 [01:13<07:56,  1.85s/it]Epoch 3:  14%|█▍        | 43/300 [01:15<07:58,  1.86s/it]Epoch 3:  15%|█▍        | 44/300 [01:16<07:36,  1.78s/it]Epoch 3:  15%|█▌        | 45/300 [01:18<07:46,  1.83s/it]Epoch 3:  15%|█▌        | 46/300 [01:20<07:38,  1.80s/it]Epoch 3:  16%|█▌        | 47/300 [01:22<07:22,  1.75s/it]Epoch 3:  16%|█▌        | 48/300 [01:23<07:13,  1.72s/it]Epoch 3:  16%|█▋        | 49/300 [01:25<07:14,  1.73s/it]Epoch 3:  17%|█▋        | 50/300 [01:27<07:20,  1.76s/it]Epoch 3:  17%|█▋        | 51/300 [01:29<07:18,  1.76s/it]Epoch 3:  17%|█▋        | 52/300 [01:30<07:21,  1.78s/it]Epoch 3:  18%|█▊        | 53/300 [01:32<07:14,  1.76s/it]Epoch 3:  18%|█▊        | 54/300 [01:34<07:21,  1.79s/it]Epoch 3:  18%|█▊        | 55/300 [01:36<07:35,  1.86s/it]Epoch 3:  19%|█▊        | 56/300 [01:38<07:16,  1.79s/it]Epoch 3:  19%|█▉        | 57/300 [01:39<07:05,  1.75s/it]Epoch 3:  19%|█▉        | 58/300 [01:41<06:46,  1.68s/it]Epoch 3:  20%|█▉        | 59/300 [01:43<07:15,  1.81s/it]06/19/2022 15:05:01 - INFO - __main__ - global step: 480; train loss: 7.6919732093811035; dev loss
Epoch 3:  20%|██        | 60/300 [01:45<06:55,  1.73s/it]Epoch 3:  20%|██        | 61/300 [01:46<06:52,  1.73s/it]Epoch 3:  21%|██        | 62/300 [01:48<07:05,  1.79s/it]Epoch 3:  21%|██        | 63/300 [01:50<07:06,  1.80s/it]Epoch 3:  21%|██▏       | 64/300 [01:52<07:16,  1.85s/it]Epoch 3:  22%|██▏       | 65/300 [01:54<07:19,  1.87s/it]Epoch 3:  22%|██▏       | 66/300 [01:56<07:08,  1.83s/it]Epoch 3:  22%|██▏       | 67/300 [01:58<07:15,  1.87s/it]Epoch 3:  23%|██▎       | 68/300 [02:00<07:23,  1.91s/it]Epoch 3:  23%|██▎       | 69/300 [02:01<06:59,  1.81s/it]Epoch 3:  23%|██▎       | 70/300 [02:03<06:53,  1.80s/it]Epoch 3:  24%|██▎       | 71/300 [02:05<07:05,  1.86s/it]Epoch 3:  24%|██▍       | 72/300 [02:07<06:48,  1.79s/it]Epoch 3:  24%|██▍       | 73/300 [02:08<06:56,  1.84s/it]Epoch 3:  25%|██▍       | 74/300 [02:11<07:10,  1.90s/it]Epoch 3:  25%|██▌       | 75/300 [02:13<07:14,  1.93s/it]Epoch 3:  25%|██▌       | 76/300 [02:14<06:44,  1.81s/it]Epoch 3:  26%|██▌       | 77/300 [02:16<06:23,  1.72s/it]Epoch 3:  26%|██▌       | 78/300 [02:18<06:44,  1.82s/it]Epoch 3:  26%|██▋       | 79/300 [02:20<06:49,  1.85s/it]06/19/2022 15:05:38 - INFO - __main__ - global step: 490; train loss: 7.444011688232422; dev loss
Epoch 3:  27%|██▋       | 80/300 [02:21<06:37,  1.81s/it]Epoch 3:  27%|██▋       | 81/300 [02:23<06:34,  1.80s/it]Epoch 3:  27%|██▋       | 82/300 [02:25<06:53,  1.90s/it]Epoch 3:  28%|██▊       | 83/300 [02:27<06:46,  1.88s/it]Epoch 3:  28%|██▊       | 84/300 [02:29<06:53,  1.91s/it]Epoch 3:  28%|██▊       | 85/300 [02:31<06:32,  1.82s/it]Epoch 3:  29%|██▊       | 86/300 [02:32<06:10,  1.73s/it]Epoch 3:  29%|██▉       | 87/300 [02:34<05:54,  1.67s/it]Epoch 3:  29%|██▉       | 88/300 [02:35<05:39,  1.60s/it]Epoch 3:  30%|██▉       | 89/300 [02:37<05:50,  1.66s/it]Epoch 3:  30%|███       | 90/300 [02:38<05:41,  1.63s/it]Epoch 3:  30%|███       | 91/300 [02:40<05:38,  1.62s/it]Epoch 3:  31%|███       | 92/300 [02:42<05:33,  1.61s/it]Epoch 3:  31%|███       | 93/300 [02:44<05:58,  1.73s/it]Epoch 3:  31%|███▏      | 94/300 [02:45<05:44,  1.67s/it]Epoch 3:  32%|███▏      | 95/300 [02:47<05:37,  1.65s/it]Epoch 3:  32%|███▏      | 96/300 [02:49<05:44,  1.69s/it]Epoch 3:  32%|███▏      | 97/300 [02:51<06:00,  1.78s/it]Epoch 3:  33%|███▎      | 98/300 [02:52<06:06,  1.82s/it]Epoch 3:  33%|███▎      | 99/300 [02:54<05:48,  1.74s/it]06/19/2022 15:06:12 - INFO - __main__ - global step: 500; train loss: 7.7503461837768555; dev loss
Epoch 3:  33%|███▎      | 100/300 [02:55<05:27,  1.64s/it]Epoch 3:  34%|███▎      | 101/300 [02:57<05:12,  1.57s/it]Epoch 3:  34%|███▍      | 102/300 [02:59<05:33,  1.68s/it]Epoch 3:  34%|███▍      | 103/300 [03:01<05:44,  1.75s/it]Epoch 3:  35%|███▍      | 104/300 [03:02<05:36,  1.72s/it]Epoch 3:  35%|███▌      | 105/300 [03:04<05:22,  1.65s/it]Epoch 3:  35%|███▌      | 106/300 [03:05<05:07,  1.58s/it]Epoch 3:  36%|███▌      | 107/300 [03:07<05:06,  1.59s/it]Epoch 3:  36%|███▌      | 108/300 [03:09<05:25,  1.70s/it]Epoch 3:  36%|███▋      | 109/300 [03:10<05:12,  1.64s/it]Epoch 3:  37%|███▋      | 110/300 [03:12<05:00,  1.58s/it]Epoch 3:  37%|███▋      | 111/300 [03:14<05:19,  1.69s/it]Epoch 3:  37%|███▋      | 112/300 [03:15<05:02,  1.61s/it]Epoch 3:  38%|███▊      | 113/300 [03:17<04:51,  1.56s/it]Epoch 3:  38%|███▊      | 114/300 [03:18<04:42,  1.52s/it]Epoch 3:  38%|███▊      | 115/300 [03:19<04:40,  1.52s/it]Epoch 3:  39%|███▊      | 116/300 [03:21<04:33,  1.49s/it]Epoch 3:  39%|███▉      | 117/300 [03:22<04:27,  1.46s/it]Epoch 3:  39%|███▉      | 118/300 [03:24<04:49,  1.59s/it]Epoch 3:  40%|███▉      | 119/300 [03:26<04:46,  1.58s/it]06/19/2022 15:06:44 - INFO - __main__ - global step: 510; train loss: 7.889486789703369; dev loss
Epoch 3:  40%|████      | 120/300 [03:27<04:44,  1.58s/it]Epoch 3:  40%|████      | 121/300 [03:29<04:38,  1.55s/it]Epoch 3:  41%|████      | 122/300 [03:31<04:50,  1.63s/it]Epoch 3:  41%|████      | 123/300 [03:32<04:45,  1.61s/it]Epoch 3:  41%|████▏     | 124/300 [03:34<05:00,  1.71s/it]Epoch 3:  42%|████▏     | 125/300 [03:36<05:06,  1.75s/it]Epoch 3:  42%|████▏     | 126/300 [03:38<05:11,  1.79s/it]Epoch 3:  42%|████▏     | 127/300 [03:40<05:15,  1.83s/it]Epoch 3:  43%|████▎     | 128/300 [03:41<05:00,  1.75s/it]Epoch 3:  43%|████▎     | 129/300 [03:43<04:46,  1.68s/it]Epoch 3:  43%|████▎     | 130/300 [03:44<04:33,  1.61s/it]Epoch 3:  44%|████▎     | 131/300 [03:46<04:26,  1.58s/it]Epoch 3:  44%|████▍     | 132/300 [03:48<04:48,  1.72s/it]Epoch 3:  44%|████▍     | 133/300 [03:50<04:59,  1.79s/it]Epoch 3:  45%|████▍     | 134/300 [03:51<04:45,  1.72s/it]Epoch 3:  45%|████▌     | 135/300 [03:53<04:51,  1.77s/it]Epoch 3:  45%|████▌     | 136/300 [03:55<05:04,  1.86s/it]Epoch 3:  46%|████▌     | 137/300 [03:57<05:05,  1.87s/it]Epoch 3:  46%|████▌     | 138/300 [03:59<04:51,  1.80s/it]Epoch 3:  46%|████▋     | 139/300 [04:00<04:35,  1.71s/it]06/19/2022 15:07:19 - INFO - __main__ - global step: 520; train loss: 8.14991569519043; dev loss
Epoch 3:  47%|████▋     | 140/300 [04:02<04:34,  1.72s/it]Epoch 3:  47%|████▋     | 141/300 [04:04<04:23,  1.65s/it]Epoch 3:  47%|████▋     | 142/300 [04:06<04:33,  1.73s/it]Epoch 3:  48%|████▊     | 143/300 [04:07<04:24,  1.68s/it]Epoch 3:  48%|████▊     | 144/300 [04:09<04:32,  1.75s/it]Epoch 3:  48%|████▊     | 145/300 [04:11<04:34,  1.77s/it]Epoch 3:  49%|████▊     | 146/300 [04:13<04:38,  1.81s/it]Epoch 3:  49%|████▉     | 147/300 [04:14<04:29,  1.76s/it]Epoch 3:  49%|████▉     | 148/300 [04:16<04:22,  1.73s/it]Epoch 3:  50%|████▉     | 149/300 [04:18<04:22,  1.74s/it]Epoch 3:  50%|█████     | 150/300 [04:20<04:31,  1.81s/it]Epoch 3:  50%|█████     | 151/300 [04:21<04:17,  1.73s/it]Epoch 3:  51%|█████     | 152/300 [04:23<04:21,  1.77s/it]Epoch 3:  51%|█████     | 153/300 [04:25<04:32,  1.86s/it]Epoch 3:  51%|█████▏    | 154/300 [04:27<04:28,  1.84s/it]Epoch 3:  52%|█████▏    | 155/300 [04:29<04:17,  1.78s/it]Epoch 3:  52%|█████▏    | 156/300 [04:30<04:10,  1.74s/it]Epoch 3:  52%|█████▏    | 157/300 [04:32<04:24,  1.85s/it]Epoch 3:  53%|█████▎    | 158/300 [04:34<04:07,  1.75s/it]Epoch 3:  53%|█████▎    | 159/300 [04:36<04:07,  1.75s/it]06/19/2022 15:07:54 - INFO - __main__ - global step: 530; train loss: 7.590920925140381; dev loss
Epoch 3:  53%|█████▎    | 160/300 [04:38<04:11,  1.80s/it]Epoch 3:  54%|█████▎    | 161/300 [04:39<04:01,  1.74s/it]Epoch 3:  54%|█████▍    | 162/300 [04:41<04:05,  1.78s/it]Epoch 3:  54%|█████▍    | 163/300 [04:43<03:58,  1.74s/it]Epoch 3:  55%|█████▍    | 164/300 [04:45<04:02,  1.79s/it]Epoch 3:  55%|█████▌    | 165/300 [04:46<04:02,  1.79s/it]Epoch 3:  55%|█████▌    | 166/300 [04:48<03:48,  1.71s/it]Epoch 3:  56%|█████▌    | 167/300 [04:50<03:49,  1.72s/it]Epoch 3:  56%|█████▌    | 168/300 [04:51<03:47,  1.72s/it]Epoch 3:  56%|█████▋    | 169/300 [04:53<03:54,  1.79s/it]Epoch 3:  57%|█████▋    | 170/300 [04:55<03:53,  1.80s/it]Epoch 3:  57%|█████▋    | 171/300 [04:57<03:54,  1.82s/it]Epoch 3:  57%|█████▋    | 172/300 [04:59<03:45,  1.76s/it]Epoch 3:  58%|█████▊    | 173/300 [05:01<03:53,  1.84s/it]Epoch 3:  58%|█████▊    | 174/300 [05:03<03:57,  1.88s/it]Epoch 3:  58%|█████▊    | 175/300 [05:04<03:41,  1.77s/it]Epoch 3:  59%|█████▊    | 176/300 [05:06<03:30,  1.70s/it]Epoch 3:  59%|█████▉    | 177/300 [05:07<03:21,  1.64s/it]Epoch 3:  59%|█████▉    | 178/300 [05:09<03:33,  1.75s/it]Epoch 3:  60%|█████▉    | 179/300 [05:11<03:37,  1.80s/it]06/19/2022 15:08:30 - INFO - __main__ - global step: 540; train loss: 7.317327976226807; dev loss
Epoch 3:  60%|██████    | 180/300 [05:13<03:38,  1.82s/it]Epoch 3:  60%|██████    | 181/300 [05:15<03:38,  1.84s/it]Epoch 3:  61%|██████    | 182/300 [05:17<03:42,  1.89s/it]Epoch 3:  61%|██████    | 183/300 [05:19<03:40,  1.88s/it]Epoch 3:  61%|██████▏   | 184/300 [05:21<03:39,  1.89s/it]Epoch 3:  62%|██████▏   | 185/300 [05:22<03:31,  1.84s/it]Epoch 3:  62%|██████▏   | 186/300 [05:24<03:30,  1.85s/it]Epoch 3:  62%|██████▏   | 187/300 [05:26<03:32,  1.88s/it]Epoch 3:  63%|██████▎   | 188/300 [05:28<03:38,  1.95s/it]Epoch 3:  63%|██████▎   | 189/300 [05:30<03:22,  1.83s/it]Epoch 3:  63%|██████▎   | 190/300 [05:32<03:29,  1.91s/it]Epoch 3:  64%|██████▎   | 191/300 [05:34<03:25,  1.89s/it]Epoch 3:  64%|██████▍   | 192/300 [05:35<03:13,  1.79s/it]Epoch 3:  64%|██████▍   | 193/300 [05:37<03:05,  1.73s/it]Epoch 3:  65%|██████▍   | 194/300 [05:39<03:03,  1.73s/it]Epoch 3:  65%|██████▌   | 195/300 [05:41<03:07,  1.78s/it]Epoch 3:  65%|██████▌   | 196/300 [05:43<03:11,  1.84s/it]Epoch 3:  66%|██████▌   | 197/300 [05:44<03:10,  1.85s/it]Epoch 3:  66%|██████▌   | 198/300 [05:46<03:09,  1.85s/it]Epoch 3:  66%|██████▋   | 199/300 [05:48<03:11,  1.89s/it]06/19/2022 15:09:07 - INFO - __main__ - global step: 550; train loss: 8.149862289428711; dev loss
Epoch 3:  67%|██████▋   | 200/300 [05:50<03:09,  1.89s/it]Epoch 3:  67%|██████▋   | 201/300 [05:52<02:56,  1.79s/it]Epoch 3:  67%|██████▋   | 202/300 [05:54<03:00,  1.84s/it]Epoch 3:  68%|██████▊   | 203/300 [05:56<03:04,  1.90s/it]Epoch 3:  68%|██████▊   | 204/300 [05:58<03:04,  1.92s/it]Epoch 3:  68%|██████▊   | 205/300 [05:59<02:53,  1.82s/it]Epoch 3:  69%|██████▊   | 206/300 [06:01<02:45,  1.76s/it]Epoch 3:  69%|██████▉   | 207/300 [06:03<02:41,  1.73s/it]Epoch 3:  69%|██████▉   | 208/300 [06:04<02:42,  1.76s/it]Epoch 3:  70%|██████▉   | 209/300 [06:06<02:44,  1.81s/it]Epoch 3:  70%|███████   | 210/300 [06:08<02:34,  1.72s/it]Epoch 3:  70%|███████   | 211/300 [06:10<02:41,  1.82s/it]Epoch 3:  71%|███████   | 212/300 [06:12<02:41,  1.84s/it]Epoch 3:  71%|███████   | 213/300 [06:14<02:38,  1.82s/it]Epoch 3:  71%|███████▏  | 214/300 [06:15<02:38,  1.84s/it]Epoch 3:  72%|███████▏  | 215/300 [06:17<02:30,  1.77s/it]Epoch 3:  72%|███████▏  | 216/300 [06:19<02:28,  1.76s/it]Epoch 3:  72%|███████▏  | 217/300 [06:20<02:23,  1.73s/it]Epoch 3:  73%|███████▎  | 218/300 [06:22<02:22,  1.73s/it]Epoch 3:  73%|███████▎  | 219/300 [06:24<02:20,  1.73s/it]06/19/2022 15:09:42 - INFO - __main__ - global step: 560; train loss: 7.906826019287109; dev loss
Epoch 3:  73%|███████▎  | 220/300 [06:26<02:21,  1.77s/it]Epoch 3:  74%|███████▎  | 221/300 [06:28<02:23,  1.81s/it]Epoch 3:  74%|███████▍  | 222/300 [06:30<02:24,  1.85s/it]Epoch 3:  74%|███████▍  | 223/300 [06:31<02:21,  1.83s/it]Epoch 3:  75%|███████▍  | 224/300 [06:33<02:23,  1.89s/it]Epoch 3:  75%|███████▌  | 225/300 [06:35<02:20,  1.87s/it]Epoch 3:  75%|███████▌  | 226/300 [06:37<02:14,  1.82s/it]Epoch 3:  76%|███████▌  | 227/300 [06:39<02:14,  1.84s/it]Epoch 3:  76%|███████▌  | 228/300 [06:41<02:14,  1.87s/it]Epoch 3:  76%|███████▋  | 229/300 [06:42<02:06,  1.77s/it]Epoch 3:  77%|███████▋  | 230/300 [06:44<02:04,  1.78s/it]Epoch 3:  77%|███████▋  | 231/300 [06:46<02:03,  1.79s/it]Epoch 3:  77%|███████▋  | 232/300 [06:48<02:05,  1.85s/it]Epoch 3:  78%|███████▊  | 233/300 [06:50<02:02,  1.83s/it]Epoch 3:  78%|███████▊  | 234/300 [06:52<02:04,  1.89s/it]Epoch 3:  78%|███████▊  | 235/300 [06:54<02:02,  1.89s/it]Epoch 3:  79%|███████▊  | 236/300 [06:56<02:03,  1.93s/it]Epoch 3:  79%|███████▉  | 237/300 [06:57<01:54,  1.83s/it]Epoch 3:  79%|███████▉  | 238/300 [06:59<01:57,  1.90s/it]Epoch 3:  80%|███████▉  | 239/300 [07:01<01:57,  1.93s/it]06/19/2022 15:10:20 - INFO - __main__ - global step: 570; train loss: 8.295252799987793; dev loss
Epoch 3:  80%|████████  | 240/300 [07:03<01:52,  1.87s/it]Epoch 3:  80%|████████  | 241/300 [07:05<01:48,  1.83s/it]Epoch 3:  81%|████████  | 242/300 [07:06<01:42,  1.76s/it]Epoch 3:  81%|████████  | 243/300 [07:08<01:40,  1.77s/it]Epoch 3:  81%|████████▏ | 244/300 [07:10<01:40,  1.79s/it]Epoch 3:  82%|████████▏ | 245/300 [07:12<01:39,  1.81s/it]Epoch 3:  82%|████████▏ | 246/300 [07:14<01:38,  1.83s/it]Epoch 3:  82%|████████▏ | 247/300 [07:15<01:32,  1.75s/it]Epoch 3:  83%|████████▎ | 248/300 [07:17<01:29,  1.72s/it]Epoch 3:  83%|████████▎ | 249/300 [07:19<01:29,  1.76s/it]Epoch 3:  83%|████████▎ | 250/300 [07:21<01:28,  1.78s/it]Epoch 3:  84%|████████▎ | 251/300 [07:22<01:25,  1.74s/it]Epoch 3:  84%|████████▍ | 252/300 [07:24<01:21,  1.70s/it]Epoch 3:  84%|████████▍ | 253/300 [07:26<01:20,  1.71s/it]Epoch 3:  85%|████████▍ | 254/300 [07:27<01:19,  1.73s/it]Epoch 3:  85%|████████▌ | 255/300 [07:29<01:16,  1.70s/it]Epoch 3:  85%|████████▌ | 256/300 [07:31<01:16,  1.73s/it]Epoch 3:  86%|████████▌ | 257/300 [07:33<01:17,  1.80s/it]Epoch 3:  86%|████████▌ | 258/300 [07:35<01:15,  1.80s/it]Epoch 3:  86%|████████▋ | 259/300 [07:36<01:13,  1.80s/it]06/19/2022 15:10:55 - INFO - __main__ - global step: 580; train loss: 7.860937595367432; dev loss
Epoch 3:  87%|████████▋ | 260/300 [07:38<01:11,  1.79s/it]Epoch 3:  87%|████████▋ | 261/300 [07:40<01:10,  1.81s/it]Epoch 3:  87%|████████▋ | 262/300 [07:42<01:10,  1.87s/it]Epoch 3:  88%|████████▊ | 263/300 [07:44<01:09,  1.87s/it]Epoch 3:  88%|████████▊ | 264/300 [07:46<01:08,  1.90s/it]Epoch 3:  88%|████████▊ | 265/300 [07:48<01:06,  1.91s/it]Epoch 3:  89%|████████▊ | 266/300 [07:49<01:01,  1.80s/it]Epoch 3:  89%|████████▉ | 267/300 [07:51<01:02,  1.90s/it]Epoch 3:  89%|████████▉ | 268/300 [07:53<00:56,  1.78s/it]Epoch 3:  90%|████████▉ | 269/300 [07:55<00:56,  1.83s/it]Epoch 3:  90%|█████████ | 270/300 [07:57<00:55,  1.87s/it]Epoch 3:  90%|█████████ | 271/300 [07:58<00:51,  1.78s/it]Epoch 3:  91%|█████████ | 272/300 [08:00<00:47,  1.70s/it]Epoch 3:  91%|█████████ | 273/300 [08:02<00:46,  1.71s/it]Epoch 3:  91%|█████████▏| 274/300 [08:03<00:43,  1.67s/it]Epoch 3:  92%|█████████▏| 275/300 [08:05<00:42,  1.71s/it]Epoch 3:  92%|█████████▏| 276/300 [08:07<00:42,  1.79s/it]Epoch 3:  92%|█████████▏| 277/300 [08:09<00:43,  1.87s/it]Epoch 3:  93%|█████████▎| 278/300 [08:11<00:39,  1.79s/it]Epoch 3:  93%|█████████▎| 279/300 [08:12<00:35,  1.71s/it]06/19/2022 15:11:30 - INFO - __main__ - global step: 590; train loss: 7.774659156799316; dev loss
Epoch 3:  93%|█████████▎| 280/300 [08:14<00:33,  1.65s/it]Epoch 3:  94%|█████████▎| 281/300 [08:16<00:32,  1.70s/it]Epoch 3:  94%|█████████▍| 282/300 [08:17<00:31,  1.73s/it]Epoch 3:  94%|█████████▍| 283/300 [08:19<00:29,  1.75s/it]Epoch 3:  95%|█████████▍| 284/300 [08:21<00:27,  1.73s/it]Epoch 3:  95%|█████████▌| 285/300 [08:22<00:25,  1.70s/it]Epoch 3:  95%|█████████▌| 286/300 [08:24<00:23,  1.68s/it]Epoch 3:  96%|█████████▌| 287/300 [08:26<00:20,  1.61s/it]Epoch 3:  96%|█████████▌| 288/300 [08:27<00:19,  1.60s/it]Epoch 3:  96%|█████████▋| 289/300 [08:29<00:19,  1.73s/it]Epoch 3:  97%|█████████▋| 290/300 [08:31<00:18,  1.80s/it]Epoch 3:  97%|█████████▋| 291/300 [08:33<00:15,  1.70s/it]Epoch 3:  97%|█████████▋| 292/300 [08:34<00:13,  1.63s/it]Epoch 3:  98%|█████████▊| 293/300 [08:36<00:11,  1.60s/it]Epoch 3:  98%|█████████▊| 294/300 [08:37<00:09,  1.60s/it]Epoch 3:  98%|█████████▊| 295/300 [08:39<00:08,  1.67s/it]Epoch 3:  99%|█████████▊| 296/300 [08:41<00:06,  1.75s/it]Epoch 3:  99%|█████████▉| 297/300 [08:43<00:05,  1.70s/it]Epoch 3:  99%|█████████▉| 298/300 [08:44<00:03,  1.71s/it]Epoch 3: 100%|█████████▉| 299/300 [08:46<00:01,  1.82s/it]06/19/2022 15:12:05 - INFO - __main__ - global step: 600; train loss: 7.69240665435791; dev loss
Epoch 3: 100%|██████████| 300/300 [08:48<00:00,  1.85s/it]Epoch 3: 100%|██████████| 300/300 [08:48<00:00,  1.76s/it]
Epoch 4:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 4:   0%|          | 1/300 [00:01<08:52,  1.78s/it]Epoch 4:   1%|          | 2/300 [00:03<09:04,  1.83s/it]Epoch 4:   1%|          | 3/300 [00:05<09:25,  1.90s/it]Epoch 4:   1%|▏         | 4/300 [00:07<08:50,  1.79s/it]Epoch 4:   2%|▏         | 5/300 [00:08<08:40,  1.76s/it]Epoch 4:   2%|▏         | 6/300 [00:10<08:25,  1.72s/it]Epoch 4:   2%|▏         | 7/300 [00:12<08:24,  1.72s/it]Epoch 4:   3%|▎         | 8/300 [00:14<08:50,  1.82s/it]Epoch 4:   3%|▎         | 9/300 [00:16<08:35,  1.77s/it]Epoch 4:   3%|▎         | 10/300 [00:17<08:31,  1.77s/it]Epoch 4:   4%|▎         | 11/300 [00:19<08:28,  1.76s/it]Epoch 4:   4%|▍         | 12/300 [00:21<08:19,  1.73s/it]Epoch 4:   4%|▍         | 13/300 [00:22<07:55,  1.66s/it]Epoch 4:   5%|▍         | 14/300 [00:24<08:06,  1.70s/it]Epoch 4:   5%|▌         | 15/300 [00:26<08:30,  1.79s/it]Epoch 4:   5%|▌         | 16/300 [00:28<08:48,  1.86s/it]Epoch 4:   6%|▌         | 17/300 [00:30<09:01,  1.91s/it]Epoch 4:   6%|▌         | 18/300 [00:32<08:30,  1.81s/it]Epoch 4:   6%|▋         | 19/300 [00:34<08:59,  1.92s/it]06/19/2022 15:12:41 - INFO - __main__ - global step: 610; train loss: 7.3009209632873535; dev loss
Epoch 4:   7%|▋         | 20/300 [00:35<08:35,  1.84s/it]Epoch 4:   7%|▋         | 21/300 [00:37<08:10,  1.76s/it]Epoch 4:   7%|▋         | 22/300 [00:39<07:50,  1.69s/it]Epoch 4:   8%|▊         | 23/300 [00:41<08:18,  1.80s/it]Epoch 4:   8%|▊         | 24/300 [00:42<08:23,  1.83s/it]Epoch 4:   8%|▊         | 25/300 [00:44<07:53,  1.72s/it]Epoch 4:   9%|▊         | 26/300 [00:46<07:57,  1.74s/it]Epoch 4:   9%|▉         | 27/300 [00:48<08:18,  1.83s/it]Epoch 4:   9%|▉         | 28/300 [00:49<07:54,  1.74s/it]Epoch 4:  10%|▉         | 29/300 [00:51<07:36,  1.68s/it]Epoch 4:  10%|█         | 30/300 [00:52<07:21,  1.63s/it]Epoch 4:  10%|█         | 31/300 [00:54<07:25,  1.66s/it]Epoch 4:  11%|█         | 32/300 [00:56<07:16,  1.63s/it]Epoch 4:  11%|█         | 33/300 [00:58<07:43,  1.73s/it]Epoch 4:  11%|█▏        | 34/300 [00:59<07:46,  1.75s/it]Epoch 4:  12%|█▏        | 35/300 [01:01<07:25,  1.68s/it]Epoch 4:  12%|█▏        | 36/300 [01:03<07:46,  1.77s/it]Epoch 4:  12%|█▏        | 37/300 [01:05<08:05,  1.85s/it]Epoch 4:  13%|█▎        | 38/300 [01:07<07:46,  1.78s/it]Epoch 4:  13%|█▎        | 39/300 [01:08<07:55,  1.82s/it]06/19/2022 15:13:16 - INFO - __main__ - global step: 620; train loss: 7.8008246421813965; dev loss
Epoch 4:  13%|█▎        | 40/300 [01:10<07:50,  1.81s/it]Epoch 4:  14%|█▎        | 41/300 [01:12<07:21,  1.71s/it]Epoch 4:  14%|█▍        | 42/300 [01:13<07:05,  1.65s/it]Epoch 4:  14%|█▍        | 43/300 [01:15<06:57,  1.62s/it]Epoch 4:  15%|█▍        | 44/300 [01:17<07:18,  1.71s/it]Epoch 4:  15%|█▌        | 45/300 [01:18<07:20,  1.73s/it]Epoch 4:  15%|█▌        | 46/300 [01:20<07:24,  1.75s/it]Epoch 4:  16%|█▌        | 47/300 [01:22<07:30,  1.78s/it]Epoch 4:  16%|█▌        | 48/300 [01:24<08:05,  1.93s/it]Epoch 4:  16%|█▋        | 49/300 [01:26<07:36,  1.82s/it]Epoch 4:  17%|█▋        | 50/300 [01:28<07:34,  1.82s/it]Epoch 4:  17%|█▋        | 51/300 [01:29<07:18,  1.76s/it]Epoch 4:  17%|█▋        | 52/300 [01:31<07:14,  1.75s/it]Epoch 4:  18%|█▊        | 53/300 [01:33<07:27,  1.81s/it]Epoch 4:  18%|█▊        | 54/300 [01:35<07:28,  1.82s/it]Epoch 4:  18%|█▊        | 55/300 [01:37<07:33,  1.85s/it]Epoch 4:  19%|█▊        | 56/300 [01:38<07:13,  1.78s/it]Epoch 4:  19%|█▉        | 57/300 [01:40<06:50,  1.69s/it]Epoch 4:  19%|█▉        | 58/300 [01:42<07:01,  1.74s/it]Epoch 4:  20%|█▉        | 59/300 [01:43<06:39,  1.66s/it]06/19/2022 15:13:51 - INFO - __main__ - global step: 630; train loss: 7.514742851257324; dev loss
Epoch 4:  20%|██        | 60/300 [01:45<06:49,  1.70s/it]Epoch 4:  20%|██        | 61/300 [01:47<06:48,  1.71s/it]Epoch 4:  21%|██        | 62/300 [01:49<06:51,  1.73s/it]Epoch 4:  21%|██        | 63/300 [01:50<06:57,  1.76s/it]Epoch 4:  21%|██▏       | 64/300 [01:52<06:38,  1.69s/it]Epoch 4:  22%|██▏       | 65/300 [01:54<06:55,  1.77s/it]Epoch 4:  22%|██▏       | 66/300 [01:55<06:36,  1.69s/it]Epoch 4:  22%|██▏       | 67/300 [01:57<06:45,  1.74s/it]Epoch 4:  23%|██▎       | 68/300 [01:59<06:28,  1.67s/it]Epoch 4:  23%|██▎       | 69/300 [02:00<06:27,  1.68s/it]Epoch 4:  23%|██▎       | 70/300 [02:02<06:12,  1.62s/it]Epoch 4:  24%|██▎       | 71/300 [02:04<06:34,  1.72s/it]Epoch 4:  24%|██▍       | 72/300 [02:05<06:15,  1.65s/it]Epoch 4:  24%|██▍       | 73/300 [02:07<06:42,  1.77s/it]Epoch 4:  25%|██▍       | 74/300 [02:10<06:59,  1.85s/it]Epoch 4:  25%|██▌       | 75/300 [02:11<06:31,  1.74s/it]Epoch 4:  25%|██▌       | 76/300 [02:13<06:33,  1.75s/it]Epoch 4:  26%|██▌       | 77/300 [02:14<06:19,  1.70s/it]Epoch 4:  26%|██▌       | 78/300 [02:16<06:06,  1.65s/it]Epoch 4:  26%|██▋       | 79/300 [02:17<05:58,  1.62s/it]06/19/2022 15:14:24 - INFO - __main__ - global step: 640; train loss: 7.542835235595703; dev loss
Epoch 4:  27%|██▋       | 80/300 [02:19<05:46,  1.57s/it]Epoch 4:  27%|██▋       | 81/300 [02:21<06:08,  1.68s/it]Epoch 4:  27%|██▋       | 82/300 [02:23<06:17,  1.73s/it]Epoch 4:  28%|██▊       | 83/300 [02:25<06:23,  1.77s/it]Epoch 4:  28%|██▊       | 84/300 [02:26<06:09,  1.71s/it]Epoch 4:  28%|██▊       | 85/300 [02:28<06:05,  1.70s/it]Epoch 4:  29%|██▊       | 86/300 [02:29<06:03,  1.70s/it]Epoch 4:  29%|██▉       | 87/300 [02:31<05:59,  1.69s/it]Epoch 4:  29%|██▉       | 88/300 [02:33<06:07,  1.73s/it]Epoch 4:  30%|██▉       | 89/300 [02:35<06:15,  1.78s/it]Epoch 4:  30%|███       | 90/300 [02:36<06:01,  1.72s/it]Epoch 4:  30%|███       | 91/300 [02:38<06:08,  1.76s/it]Epoch 4:  31%|███       | 92/300 [02:40<05:55,  1.71s/it]Epoch 4:  31%|███       | 93/300 [02:41<05:44,  1.66s/it]Epoch 4:  31%|███▏      | 94/300 [02:43<06:03,  1.76s/it]Epoch 4:  32%|███▏      | 95/300 [02:45<05:50,  1.71s/it]Epoch 4:  32%|███▏      | 96/300 [02:47<05:58,  1.76s/it]Epoch 4:  32%|███▏      | 97/300 [02:48<05:40,  1.68s/it]Epoch 4:  33%|███▎      | 98/300 [02:50<05:32,  1.65s/it]Epoch 4:  33%|███▎      | 99/300 [02:52<05:54,  1.76s/it]06/19/2022 15:14:59 - INFO - __main__ - global step: 650; train loss: 7.650483131408691; dev loss
Epoch 4:  33%|███▎      | 100/300 [02:54<05:38,  1.69s/it]Epoch 4:  34%|███▎      | 101/300 [02:55<05:26,  1.64s/it]Epoch 4:  34%|███▍      | 102/300 [02:57<05:42,  1.73s/it]Epoch 4:  34%|███▍      | 103/300 [02:59<05:27,  1.66s/it]Epoch 4:  35%|███▍      | 104/300 [03:00<05:30,  1.68s/it]Epoch 4:  35%|███▌      | 105/300 [03:02<05:24,  1.66s/it]Epoch 4:  35%|███▌      | 106/300 [03:04<05:27,  1.69s/it]Epoch 4:  36%|███▌      | 107/300 [03:05<05:21,  1.67s/it]Epoch 4:  36%|███▌      | 108/300 [03:07<05:13,  1.63s/it]Epoch 4:  36%|███▋      | 109/300 [03:08<05:16,  1.66s/it]Epoch 4:  37%|███▋      | 110/300 [03:10<05:28,  1.73s/it]Epoch 4:  37%|███▋      | 111/300 [03:12<05:13,  1.66s/it]Epoch 4:  37%|███▋      | 112/300 [03:13<05:01,  1.60s/it]Epoch 4:  38%|███▊      | 113/300 [03:15<04:52,  1.57s/it]Epoch 4:  38%|███▊      | 114/300 [03:16<04:44,  1.53s/it]Epoch 4:  38%|███▊      | 115/300 [03:18<04:47,  1.56s/it]Epoch 4:  39%|███▊      | 116/300 [03:19<04:41,  1.53s/it]Epoch 4:  39%|███▉      | 117/300 [03:21<04:39,  1.53s/it]Epoch 4:  39%|███▉      | 118/300 [03:22<04:38,  1.53s/it]Epoch 4:  40%|███▉      | 119/300 [03:24<04:42,  1.56s/it]06/19/2022 15:15:31 - INFO - __main__ - global step: 660; train loss: 8.09609317779541; dev loss
Epoch 4:  40%|████      | 120/300 [03:26<04:40,  1.56s/it]Epoch 4:  40%|████      | 121/300 [03:27<04:39,  1.56s/it]Epoch 4:  41%|████      | 122/300 [03:29<04:36,  1.55s/it]Epoch 4:  41%|████      | 123/300 [03:31<05:00,  1.70s/it]Epoch 4:  41%|████▏     | 124/300 [03:32<04:53,  1.67s/it]Epoch 4:  42%|████▏     | 125/300 [03:34<05:01,  1.72s/it]Epoch 4:  42%|████▏     | 126/300 [03:36<04:49,  1.66s/it]Epoch 4:  42%|████▏     | 127/300 [03:38<05:02,  1.75s/it]Epoch 4:  43%|████▎     | 128/300 [03:40<05:09,  1.80s/it]Epoch 4:  43%|████▎     | 129/300 [03:41<05:10,  1.81s/it]Epoch 4:  43%|████▎     | 130/300 [03:43<05:13,  1.84s/it]Epoch 4:  44%|████▎     | 131/300 [03:45<05:17,  1.88s/it]Epoch 4:  44%|████▍     | 132/300 [03:47<05:25,  1.94s/it]Epoch 4:  44%|████▍     | 133/300 [03:49<05:08,  1.85s/it]Epoch 4:  45%|████▍     | 134/300 [03:51<05:03,  1.83s/it]Epoch 4:  45%|████▌     | 135/300 [03:52<04:54,  1.78s/it]Epoch 4:  45%|████▌     | 136/300 [03:54<04:35,  1.68s/it]Epoch 4:  46%|████▌     | 137/300 [03:55<04:25,  1.63s/it]Epoch 4:  46%|████▌     | 138/300 [03:57<04:31,  1.68s/it]Epoch 4:  46%|████▋     | 139/300 [03:59<04:43,  1.76s/it]06/19/2022 15:16:07 - INFO - __main__ - global step: 670; train loss: 7.944150447845459; dev loss
Epoch 4:  47%|████▋     | 140/300 [04:01<04:47,  1.80s/it]Epoch 4:  47%|████▋     | 141/300 [04:03<04:30,  1.70s/it]Epoch 4:  47%|████▋     | 142/300 [04:04<04:22,  1.66s/it]Epoch 4:  48%|████▊     | 143/300 [04:06<04:27,  1.71s/it]Epoch 4:  48%|████▊     | 144/300 [04:08<04:42,  1.81s/it]Epoch 4:  48%|████▊     | 145/300 [04:10<04:38,  1.80s/it]Epoch 4:  49%|████▊     | 146/300 [04:11<04:25,  1.72s/it]Epoch 4:  49%|████▉     | 147/300 [04:13<04:35,  1.80s/it]Epoch 4:  49%|████▉     | 148/300 [04:15<04:21,  1.72s/it]Epoch 4:  50%|████▉     | 149/300 [04:17<04:33,  1.81s/it]Epoch 4:  50%|█████     | 150/300 [04:18<04:24,  1.76s/it]Epoch 4:  50%|█████     | 151/300 [04:20<04:11,  1.69s/it]Epoch 4:  51%|█████     | 152/300 [04:22<04:22,  1.77s/it]Epoch 4:  51%|█████     | 153/300 [04:24<04:26,  1.81s/it]Epoch 4:  51%|█████▏    | 154/300 [04:26<04:23,  1.80s/it]Epoch 4:  52%|█████▏    | 155/300 [04:27<04:15,  1.76s/it]Epoch 4:  52%|█████▏    | 156/300 [04:29<04:27,  1.85s/it]Epoch 4:  52%|█████▏    | 157/300 [04:31<04:14,  1.78s/it]Epoch 4:  53%|█████▎    | 158/300 [04:33<04:07,  1.75s/it]Epoch 4:  53%|█████▎    | 159/300 [04:34<04:04,  1.73s/it]06/19/2022 15:16:42 - INFO - __main__ - global step: 680; train loss: 8.300430297851562; dev loss
Epoch 4:  53%|█████▎    | 160/300 [04:36<04:06,  1.76s/it]Epoch 4:  54%|█████▎    | 161/300 [04:38<04:01,  1.73s/it]Epoch 4:  54%|█████▍    | 162/300 [04:40<03:59,  1.74s/it]Epoch 4:  54%|█████▍    | 163/300 [04:41<04:03,  1.78s/it]Epoch 4:  55%|█████▍    | 164/300 [04:43<04:09,  1.83s/it]Epoch 4:  55%|█████▌    | 165/300 [04:45<04:05,  1.82s/it]Epoch 4:  55%|█████▌    | 166/300 [04:47<04:07,  1.85s/it]Epoch 4:  56%|█████▌    | 167/300 [04:49<04:07,  1.86s/it]Epoch 4:  56%|█████▌    | 168/300 [04:51<03:51,  1.76s/it]Epoch 4:  56%|█████▋    | 169/300 [04:53<04:03,  1.86s/it]Epoch 4:  57%|█████▋    | 170/300 [04:55<04:04,  1.88s/it]Epoch 4:  57%|█████▋    | 171/300 [04:56<03:49,  1.78s/it]Epoch 4:  57%|█████▋    | 172/300 [04:58<03:48,  1.79s/it]Epoch 4:  58%|█████▊    | 173/300 [05:00<03:41,  1.74s/it]Epoch 4:  58%|█████▊    | 174/300 [05:01<03:33,  1.70s/it]Epoch 4:  58%|█████▊    | 175/300 [05:03<03:27,  1.66s/it]Epoch 4:  59%|█████▊    | 176/300 [05:04<03:19,  1.61s/it]Epoch 4:  59%|█████▉    | 177/300 [05:06<03:17,  1.60s/it]Epoch 4:  59%|█████▉    | 178/300 [05:08<03:23,  1.67s/it]Epoch 4:  60%|█████▉    | 179/300 [05:09<03:14,  1.61s/it]06/19/2022 15:17:16 - INFO - __main__ - global step: 690; train loss: 7.84765625; dev loss
Epoch 4:  60%|██████    | 180/300 [05:11<03:08,  1.57s/it]Epoch 4:  60%|██████    | 181/300 [05:13<03:21,  1.69s/it]Epoch 4:  61%|██████    | 182/300 [05:14<03:25,  1.74s/it]Epoch 4:  61%|██████    | 183/300 [05:16<03:18,  1.70s/it]Epoch 4:  61%|██████▏   | 184/300 [05:18<03:18,  1.71s/it]Epoch 4:  62%|██████▏   | 185/300 [05:20<03:26,  1.80s/it]Epoch 4:  62%|██████▏   | 186/300 [05:22<03:25,  1.80s/it]Epoch 4:  62%|██████▏   | 187/300 [05:23<03:28,  1.84s/it]Epoch 4:  63%|██████▎   | 188/300 [05:25<03:26,  1.85s/it]Epoch 4:  63%|██████▎   | 189/300 [05:27<03:19,  1.80s/it]Epoch 4:  63%|██████▎   | 190/300 [05:29<03:13,  1.76s/it]Epoch 4:  64%|██████▎   | 191/300 [05:31<03:15,  1.80s/it]Epoch 4:  64%|██████▍   | 192/300 [05:33<03:18,  1.83s/it]Epoch 4:  64%|██████▍   | 193/300 [05:34<03:19,  1.86s/it]Epoch 4:  65%|██████▍   | 194/300 [05:36<03:05,  1.75s/it]Epoch 4:  65%|██████▌   | 195/300 [05:38<03:10,  1.81s/it]Epoch 4:  65%|██████▌   | 196/300 [05:40<03:11,  1.84s/it]Epoch 4:  66%|██████▌   | 197/300 [05:42<03:09,  1.84s/it]Epoch 4:  66%|██████▌   | 198/300 [05:44<03:12,  1.89s/it]Epoch 4:  66%|██████▋   | 199/300 [05:45<02:59,  1.78s/it]06/19/2022 15:17:52 - INFO - __main__ - global step: 700; train loss: 7.649356842041016; dev loss
Epoch 4:  67%|██████▋   | 200/300 [05:47<02:50,  1.70s/it]Epoch 4:  67%|██████▋   | 201/300 [05:48<02:46,  1.68s/it]Epoch 4:  67%|██████▋   | 202/300 [05:50<02:49,  1.73s/it]Epoch 4:  68%|██████▊   | 203/300 [05:52<02:45,  1.71s/it]Epoch 4:  68%|██████▊   | 204/300 [05:54<02:45,  1.73s/it]Epoch 4:  68%|██████▊   | 205/300 [05:55<02:37,  1.66s/it]Epoch 4:  69%|██████▊   | 206/300 [05:57<02:34,  1.65s/it]Epoch 4:  69%|██████▉   | 207/300 [05:58<02:34,  1.67s/it]Epoch 4:  69%|██████▉   | 208/300 [06:00<02:37,  1.71s/it]Epoch 4:  70%|██████▉   | 209/300 [06:02<02:31,  1.67s/it]Epoch 4:  70%|███████   | 210/300 [06:04<02:42,  1.80s/it]Epoch 4:  70%|███████   | 211/300 [06:05<02:32,  1.72s/it]Epoch 4:  71%|███████   | 212/300 [06:07<02:38,  1.80s/it]Epoch 4:  71%|███████   | 213/300 [06:09<02:29,  1.72s/it]Epoch 4:  71%|███████▏  | 214/300 [06:11<02:24,  1.68s/it]Epoch 4:  72%|███████▏  | 215/300 [06:12<02:23,  1.69s/it]Epoch 4:  72%|███████▏  | 216/300 [06:14<02:17,  1.63s/it]Epoch 4:  72%|███████▏  | 217/300 [06:16<02:22,  1.71s/it]Epoch 4:  73%|███████▎  | 218/300 [06:18<02:28,  1.81s/it]Epoch 4:  73%|███████▎  | 219/300 [06:20<02:29,  1.85s/it]06/19/2022 15:18:27 - INFO - __main__ - global step: 710; train loss: 7.617549896240234; dev loss
Epoch 4:  73%|███████▎  | 220/300 [06:21<02:20,  1.75s/it]Epoch 4:  74%|███████▎  | 221/300 [06:23<02:11,  1.67s/it]Epoch 4:  74%|███████▍  | 222/300 [06:25<02:15,  1.74s/it]Epoch 4:  74%|███████▍  | 223/300 [06:26<02:16,  1.78s/it]Epoch 4:  75%|███████▍  | 224/300 [06:28<02:17,  1.81s/it]Epoch 4:  75%|███████▌  | 225/300 [06:30<02:10,  1.75s/it]Epoch 4:  75%|███████▌  | 226/300 [06:31<02:03,  1.67s/it]Epoch 4:  76%|███████▌  | 227/300 [06:33<02:11,  1.81s/it]Epoch 4:  76%|███████▌  | 228/300 [06:35<02:06,  1.75s/it]Epoch 4:  76%|███████▋  | 229/300 [06:37<02:00,  1.69s/it]Epoch 4:  77%|███████▋  | 230/300 [06:38<01:55,  1.66s/it]Epoch 4:  77%|███████▋  | 231/300 [06:40<01:54,  1.66s/it]Epoch 4:  77%|███████▋  | 232/300 [06:41<01:50,  1.63s/it]Epoch 4:  78%|███████▊  | 233/300 [06:43<01:55,  1.73s/it]Epoch 4:  78%|███████▊  | 234/300 [06:45<01:58,  1.80s/it]Epoch 4:  78%|███████▊  | 235/300 [06:47<02:01,  1.87s/it]Epoch 4:  79%|███████▊  | 236/300 [06:49<02:00,  1.89s/it]Epoch 4:  79%|███████▉  | 237/300 [06:51<01:58,  1.87s/it]Epoch 4:  79%|███████▉  | 238/300 [06:53<01:55,  1.87s/it]Epoch 4:  80%|███████▉  | 239/300 [06:55<01:48,  1.79s/it]06/19/2022 15:19:02 - INFO - __main__ - global step: 720; train loss: 8.056711196899414; dev loss
Epoch 4:  80%|████████  | 240/300 [06:57<01:49,  1.82s/it]Epoch 4:  80%|████████  | 241/300 [06:58<01:42,  1.75s/it]Epoch 4:  81%|████████  | 242/300 [07:00<01:38,  1.70s/it]Epoch 4:  81%|████████  | 243/300 [07:02<01:39,  1.75s/it]Epoch 4:  81%|████████▏ | 244/300 [07:03<01:37,  1.75s/it]Epoch 4:  82%|████████▏ | 245/300 [07:05<01:31,  1.66s/it]Epoch 4:  82%|████████▏ | 246/300 [07:06<01:25,  1.59s/it]Epoch 4:  82%|████████▏ | 247/300 [07:08<01:22,  1.56s/it]Epoch 4:  83%|████████▎ | 248/300 [07:09<01:19,  1.53s/it]Epoch 4:  83%|████████▎ | 249/300 [07:11<01:24,  1.66s/it]Epoch 4:  83%|████████▎ | 250/300 [07:13<01:20,  1.60s/it]Epoch 4:  84%|████████▎ | 251/300 [07:14<01:21,  1.66s/it]Epoch 4:  84%|████████▍ | 252/300 [07:16<01:20,  1.67s/it]Epoch 4:  84%|████████▍ | 253/300 [07:18<01:21,  1.73s/it]Epoch 4:  85%|████████▍ | 254/300 [07:20<01:20,  1.76s/it]Epoch 4:  85%|████████▌ | 255/300 [07:22<01:20,  1.79s/it]Epoch 4:  85%|████████▌ | 256/300 [07:24<01:20,  1.83s/it]Epoch 4:  86%|████████▌ | 257/300 [07:25<01:19,  1.85s/it]Epoch 4:  86%|████████▌ | 258/300 [07:27<01:13,  1.76s/it]Epoch 4:  86%|████████▋ | 259/300 [07:29<01:13,  1.79s/it]06/19/2022 15:19:36 - INFO - __main__ - global step: 730; train loss: 7.770208835601807; dev loss
Epoch 4:  87%|████████▋ | 260/300 [07:31<01:14,  1.86s/it]Epoch 4:  87%|████████▋ | 261/300 [07:33<01:12,  1.86s/it]Epoch 4:  87%|████████▋ | 262/300 [07:35<01:12,  1.90s/it]Epoch 4:  88%|████████▊ | 263/300 [07:36<01:07,  1.82s/it]Epoch 4:  88%|████████▊ | 264/300 [07:38<01:03,  1.76s/it]Epoch 4:  88%|████████▊ | 265/300 [07:40<01:02,  1.78s/it]Epoch 4:  89%|████████▊ | 266/300 [07:41<00:59,  1.74s/it]Epoch 4:  89%|████████▉ | 267/300 [07:43<00:55,  1.69s/it]Epoch 4:  89%|████████▉ | 268/300 [07:45<00:52,  1.65s/it]Epoch 4:  90%|████████▉ | 269/300 [07:46<00:52,  1.70s/it]Epoch 4:  90%|█████████ | 270/300 [07:48<00:49,  1.64s/it]Epoch 4:  90%|█████████ | 271/300 [07:49<00:46,  1.59s/it]Epoch 4:  91%|█████████ | 272/300 [07:51<00:47,  1.70s/it]Epoch 4:  91%|█████████ | 273/300 [07:53<00:43,  1.60s/it]Epoch 4:  91%|█████████▏| 274/300 [07:54<00:41,  1.59s/it]Epoch 4:  92%|█████████▏| 275/300 [07:56<00:40,  1.62s/it]Epoch 4:  92%|█████████▏| 276/300 [07:57<00:38,  1.59s/it]Epoch 4:  92%|█████████▏| 277/300 [07:59<00:39,  1.70s/it]Epoch 4:  93%|█████████▎| 278/300 [08:01<00:35,  1.62s/it]Epoch 4:  93%|█████████▎| 279/300 [08:03<00:35,  1.70s/it]06/19/2022 15:20:10 - INFO - __main__ - global step: 740; train loss: 8.064140319824219; dev loss
Epoch 4:  93%|█████████▎| 280/300 [08:05<00:35,  1.76s/it]Epoch 4:  94%|█████████▎| 281/300 [08:07<00:34,  1.84s/it]Epoch 4:  94%|█████████▍| 282/300 [08:09<00:33,  1.83s/it]Epoch 4:  94%|█████████▍| 283/300 [08:10<00:31,  1.83s/it]Epoch 4:  95%|█████████▍| 284/300 [08:12<00:29,  1.87s/it]Epoch 4:  95%|█████████▌| 285/300 [08:14<00:27,  1.86s/it]Epoch 4:  95%|█████████▌| 286/300 [08:16<00:26,  1.86s/it]Epoch 4:  96%|█████████▌| 287/300 [08:18<00:24,  1.87s/it]Epoch 4:  96%|█████████▌| 288/300 [08:19<00:21,  1.75s/it]Epoch 4:  96%|█████████▋| 289/300 [08:21<00:20,  1.83s/it]Epoch 4:  97%|█████████▋| 290/300 [08:23<00:17,  1.79s/it]Epoch 4:  97%|█████████▋| 291/300 [08:25<00:15,  1.68s/it]Epoch 4:  97%|█████████▋| 292/300 [08:26<00:13,  1.71s/it]Epoch 4:  98%|█████████▊| 293/300 [08:28<00:12,  1.78s/it]Epoch 4:  98%|█████████▊| 294/300 [08:30<00:10,  1.82s/it]Epoch 4:  98%|█████████▊| 295/300 [08:32<00:09,  1.84s/it]Epoch 4:  99%|█████████▊| 296/300 [08:34<00:07,  1.91s/it]Epoch 4:  99%|█████████▉| 297/300 [08:36<00:05,  1.85s/it]Epoch 4:  99%|█████████▉| 298/300 [08:37<00:03,  1.73s/it]Epoch 4: 100%|█████████▉| 299/300 [08:39<00:01,  1.67s/it]06/19/2022 15:20:46 - INFO - __main__ - global step: 750; train loss: 8.11451530456543; dev loss
Epoch 4: 100%|██████████| 300/300 [08:41<00:00,  1.70s/it]Epoch 4: 100%|██████████| 300/300 [08:41<00:00,  1.74s/it]
Epoch 5:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 5:   0%|          | 1/300 [00:01<08:22,  1.68s/it]Epoch 5:   1%|          | 2/300 [00:03<08:23,  1.69s/it]Epoch 5:   1%|          | 3/300 [00:05<08:33,  1.73s/it]Epoch 5:   1%|▏         | 4/300 [00:06<08:40,  1.76s/it]Epoch 5:   2%|▏         | 5/300 [00:08<09:06,  1.85s/it]Epoch 5:   2%|▏         | 6/300 [00:11<09:33,  1.95s/it]Epoch 5:   2%|▏         | 7/300 [00:12<09:08,  1.87s/it]Epoch 5:   3%|▎         | 8/300 [00:14<09:20,  1.92s/it]Epoch 5:   3%|▎         | 9/300 [00:16<09:25,  1.94s/it]Epoch 5:   3%|▎         | 10/300 [00:18<09:25,  1.95s/it]Epoch 5:   4%|▎         | 11/300 [00:20<09:19,  1.94s/it]Epoch 5:   4%|▍         | 12/300 [00:22<09:14,  1.93s/it]Epoch 5:   4%|▍         | 13/300 [00:24<08:35,  1.80s/it]Epoch 5:   5%|▍         | 14/300 [00:26<08:45,  1.84s/it]Epoch 5:   5%|▌         | 15/300 [00:27<08:15,  1.74s/it]Epoch 5:   5%|▌         | 16/300 [00:29<07:48,  1.65s/it]Epoch 5:   6%|▌         | 17/300 [00:30<08:09,  1.73s/it]Epoch 5:   6%|▌         | 18/300 [00:32<08:29,  1.81s/it]Epoch 5:   6%|▋         | 19/300 [00:34<08:27,  1.81s/it]06/19/2022 15:21:23 - INFO - __main__ - global step: 760; train loss: 7.412230014801025; dev loss
Epoch 5:   7%|▋         | 20/300 [00:36<08:32,  1.83s/it]Epoch 5:   7%|▋         | 21/300 [00:38<08:28,  1.82s/it]Epoch 5:   7%|▋         | 22/300 [00:40<08:12,  1.77s/it]Epoch 5:   8%|▊         | 23/300 [00:41<07:56,  1.72s/it]Epoch 5:   8%|▊         | 24/300 [00:43<07:42,  1.67s/it]Epoch 5:   8%|▊         | 25/300 [00:44<07:44,  1.69s/it]Epoch 5:   9%|▊         | 26/300 [00:46<07:52,  1.72s/it]Epoch 5:   9%|▉         | 27/300 [00:48<07:56,  1.74s/it]Epoch 5:   9%|▉         | 28/300 [00:50<08:00,  1.77s/it]Epoch 5:  10%|▉         | 29/300 [00:52<07:59,  1.77s/it]Epoch 5:  10%|█         | 30/300 [00:53<07:38,  1.70s/it]Epoch 5:  10%|█         | 31/300 [00:55<07:50,  1.75s/it]Epoch 5:  11%|█         | 32/300 [00:56<07:25,  1.66s/it]Epoch 5:  11%|█         | 33/300 [00:58<07:44,  1.74s/it]Epoch 5:  11%|█▏        | 34/300 [01:00<07:27,  1.68s/it]Epoch 5:  12%|█▏        | 35/300 [01:02<07:55,  1.80s/it]Epoch 5:  12%|█▏        | 36/300 [01:04<08:08,  1.85s/it]Epoch 5:  12%|█▏        | 37/300 [01:06<08:20,  1.90s/it]Epoch 5:  13%|█▎        | 38/300 [01:08<08:26,  1.93s/it]Epoch 5:  13%|█▎        | 39/300 [01:10<08:04,  1.86s/it]06/19/2022 15:21:58 - INFO - __main__ - global step: 770; train loss: 7.6262993812561035; dev loss
Epoch 5:  13%|█▎        | 40/300 [01:12<07:58,  1.84s/it]Epoch 5:  14%|█▎        | 41/300 [01:13<07:57,  1.84s/it]Epoch 5:  14%|█▍        | 42/300 [01:15<07:31,  1.75s/it]Epoch 5:  14%|█▍        | 43/300 [01:17<07:49,  1.83s/it]Epoch 5:  15%|█▍        | 44/300 [01:18<07:19,  1.72s/it]Epoch 5:  15%|█▌        | 45/300 [01:20<07:07,  1.68s/it]Epoch 5:  15%|█▌        | 46/300 [01:22<07:18,  1.73s/it]Epoch 5:  16%|█▌        | 47/300 [01:23<07:11,  1.71s/it]Epoch 5:  16%|█▌        | 48/300 [01:25<06:53,  1.64s/it]Epoch 5:  16%|█▋        | 49/300 [01:26<06:41,  1.60s/it]Epoch 5:  17%|█▋        | 50/300 [01:28<06:31,  1.56s/it]Epoch 5:  17%|█▋        | 51/300 [01:30<07:01,  1.69s/it]Epoch 5:  17%|█▋        | 52/300 [01:31<06:44,  1.63s/it]Epoch 5:  18%|█▊        | 53/300 [01:33<06:50,  1.66s/it]Epoch 5:  18%|█▊        | 54/300 [01:35<06:48,  1.66s/it]Epoch 5:  18%|█▊        | 55/300 [01:37<07:12,  1.77s/it]Epoch 5:  19%|█▊        | 56/300 [01:39<07:21,  1.81s/it]Epoch 5:  19%|█▉        | 57/300 [01:40<06:57,  1.72s/it]Epoch 5:  19%|█▉        | 58/300 [01:42<06:51,  1.70s/it]Epoch 5:  20%|█▉        | 59/300 [01:43<06:41,  1.67s/it]06/19/2022 15:22:32 - INFO - __main__ - global step: 780; train loss: 8.226781845092773; dev loss
Epoch 5:  20%|██        | 60/300 [01:45<06:55,  1.73s/it]Epoch 5:  20%|██        | 61/300 [01:47<07:07,  1.79s/it]Epoch 5:  21%|██        | 62/300 [01:49<06:49,  1.72s/it]Epoch 5:  21%|██        | 63/300 [01:50<06:29,  1.64s/it]Epoch 5:  21%|██▏       | 64/300 [01:53<07:15,  1.85s/it]Epoch 5:  22%|██▏       | 65/300 [01:54<07:11,  1.84s/it]Epoch 5:  22%|██▏       | 66/300 [01:56<07:22,  1.89s/it]Epoch 5:  22%|██▏       | 67/300 [01:58<07:16,  1.87s/it]Epoch 5:  23%|██▎       | 68/300 [02:00<07:26,  1.92s/it]Epoch 5:  23%|██▎       | 69/300 [02:02<07:18,  1.90s/it]Epoch 5:  23%|██▎       | 70/300 [02:04<07:29,  1.95s/it]Epoch 5:  24%|██▎       | 71/300 [02:06<07:39,  2.01s/it]Epoch 5:  24%|██▍       | 72/300 [02:08<07:35,  2.00s/it]Epoch 5:  24%|██▍       | 73/300 [02:10<07:09,  1.89s/it]Epoch 5:  25%|██▍       | 74/300 [02:12<07:13,  1.92s/it]Epoch 5:  25%|██▌       | 75/300 [02:14<06:51,  1.83s/it]Epoch 5:  25%|██▌       | 76/300 [02:15<06:39,  1.78s/it]Epoch 5:  26%|██▌       | 77/300 [02:17<06:26,  1.73s/it]Epoch 5:  26%|██▌       | 78/300 [02:19<06:29,  1.75s/it]Epoch 5:  26%|██▋       | 79/300 [02:20<06:12,  1.68s/it]06/19/2022 15:23:08 - INFO - __main__ - global step: 790; train loss: 7.933718681335449; dev loss
Epoch 5:  27%|██▋       | 80/300 [02:22<06:09,  1.68s/it]Epoch 5:  27%|██▋       | 81/300 [02:24<06:27,  1.77s/it]Epoch 5:  27%|██▋       | 82/300 [02:26<06:45,  1.86s/it]Epoch 5:  28%|██▊       | 83/300 [02:27<06:20,  1.75s/it]Epoch 5:  28%|██▊       | 84/300 [02:29<06:10,  1.71s/it]Epoch 5:  28%|██▊       | 85/300 [02:31<06:17,  1.76s/it]Epoch 5:  29%|██▊       | 86/300 [02:33<06:24,  1.80s/it]Epoch 5:  29%|██▉       | 87/300 [02:35<06:31,  1.84s/it]Epoch 5:  29%|██▉       | 88/300 [02:37<06:40,  1.89s/it]Epoch 5:  30%|██▉       | 89/300 [02:39<06:39,  1.90s/it]Epoch 5:  30%|███       | 90/300 [02:40<06:20,  1.81s/it]Epoch 5:  30%|███       | 91/300 [02:42<06:04,  1.74s/it]Epoch 5:  31%|███       | 92/300 [02:44<06:19,  1.82s/it]Epoch 5:  31%|███       | 93/300 [02:46<06:32,  1.90s/it]Epoch 5:  31%|███▏      | 94/300 [02:47<06:08,  1.79s/it]Epoch 5:  32%|███▏      | 95/300 [02:49<05:51,  1.72s/it]Epoch 5:  32%|███▏      | 96/300 [02:51<05:57,  1.75s/it]Epoch 5:  32%|███▏      | 97/300 [02:52<05:48,  1.71s/it]Epoch 5:  33%|███▎      | 98/300 [02:54<05:39,  1.68s/it]Epoch 5:  33%|███▎      | 99/300 [02:56<05:34,  1.66s/it]06/19/2022 15:23:44 - INFO - __main__ - global step: 800; train loss: 7.550823211669922; dev loss
Epoch 5:  33%|███▎      | 100/300 [02:58<06:00,  1.80s/it]Epoch 5:  34%|███▎      | 101/300 [03:00<06:18,  1.90s/it]Epoch 5:  34%|███▍      | 102/300 [03:02<06:02,  1.83s/it]Epoch 5:  34%|███▍      | 103/300 [03:03<05:42,  1.74s/it]Epoch 5:  35%|███▍      | 104/300 [03:05<05:49,  1.78s/it]Epoch 5:  35%|███▌      | 105/300 [03:07<05:59,  1.84s/it]Epoch 5:  35%|███▌      | 106/300 [03:09<06:17,  1.94s/it]Epoch 5:  36%|███▌      | 107/300 [03:11<05:50,  1.82s/it]Epoch 5:  36%|███▌      | 108/300 [03:13<05:47,  1.81s/it]Epoch 5:  36%|███▋      | 109/300 [03:14<05:30,  1.73s/it]Epoch 5:  37%|███▋      | 110/300 [03:15<05:11,  1.64s/it]Epoch 5:  37%|███▋      | 111/300 [03:17<05:29,  1.74s/it]Epoch 5:  37%|███▋      | 112/300 [03:19<05:41,  1.82s/it]Epoch 5:  38%|███▊      | 113/300 [03:21<05:44,  1.84s/it]Epoch 5:  38%|███▊      | 114/300 [03:24<06:00,  1.94s/it]Epoch 5:  38%|███▊      | 115/300 [03:26<06:01,  1.95s/it]Epoch 5:  39%|███▊      | 116/300 [03:27<05:42,  1.86s/it]Epoch 5:  39%|███▉      | 117/300 [03:29<05:43,  1.88s/it]Epoch 5:  39%|███▉      | 118/300 [03:31<05:37,  1.86s/it]Epoch 5:  40%|███▉      | 119/300 [03:32<05:20,  1.77s/it]06/19/2022 15:24:21 - INFO - __main__ - global step: 810; train loss: 7.7957658767700195; dev loss
Epoch 5:  40%|████      | 120/300 [03:34<05:15,  1.75s/it]Epoch 5:  40%|████      | 121/300 [03:36<05:08,  1.72s/it]Epoch 5:  41%|████      | 122/300 [03:38<05:21,  1.81s/it]Epoch 5:  41%|████      | 123/300 [03:40<05:29,  1.86s/it]Epoch 5:  41%|████▏     | 124/300 [03:41<05:10,  1.76s/it]Epoch 5:  42%|████▏     | 125/300 [03:43<04:55,  1.69s/it]Epoch 5:  42%|████▏     | 126/300 [03:45<05:10,  1.79s/it]Epoch 5:  42%|████▏     | 127/300 [03:46<04:55,  1.71s/it]Epoch 5:  43%|████▎     | 128/300 [03:48<04:42,  1.64s/it]Epoch 5:  43%|████▎     | 129/300 [03:49<04:37,  1.62s/it]Epoch 5:  43%|████▎     | 130/300 [03:51<04:40,  1.65s/it]Epoch 5:  44%|████▎     | 131/300 [03:53<04:41,  1.66s/it]Epoch 5:  44%|████▍     | 132/300 [03:55<04:45,  1.70s/it]Epoch 5:  44%|████▍     | 133/300 [03:56<04:42,  1.69s/it]Epoch 5:  45%|████▍     | 134/300 [03:58<04:45,  1.72s/it]Epoch 5:  45%|████▌     | 135/300 [04:00<04:51,  1.77s/it]Epoch 5:  45%|████▌     | 136/300 [04:02<04:50,  1.77s/it]Epoch 5:  46%|████▌     | 137/300 [04:04<04:52,  1.80s/it]Epoch 5:  46%|████▌     | 138/300 [04:06<04:59,  1.85s/it]Epoch 5:  46%|████▋     | 139/300 [04:08<05:06,  1.90s/it]06/19/2022 15:24:56 - INFO - __main__ - global step: 820; train loss: 7.993821620941162; dev loss
Epoch 5:  47%|████▋     | 140/300 [04:10<05:10,  1.94s/it]Epoch 5:  47%|████▋     | 141/300 [04:12<05:07,  1.93s/it]Epoch 5:  47%|████▋     | 142/300 [04:14<05:06,  1.94s/it]Epoch 5:  48%|████▊     | 143/300 [04:15<05:03,  1.93s/it]Epoch 5:  48%|████▊     | 144/300 [04:17<04:46,  1.83s/it]Epoch 5:  48%|████▊     | 145/300 [04:19<04:39,  1.80s/it]Epoch 5:  49%|████▊     | 146/300 [04:21<04:35,  1.79s/it]Epoch 5:  49%|████▉     | 147/300 [04:22<04:31,  1.77s/it]Epoch 5:  49%|████▉     | 148/300 [04:24<04:14,  1.68s/it]Epoch 5:  50%|████▉     | 149/300 [04:25<04:00,  1.59s/it]Epoch 5:  50%|█████     | 150/300 [04:27<03:50,  1.54s/it]Epoch 5:  50%|█████     | 151/300 [04:28<03:48,  1.53s/it]Epoch 5:  51%|█████     | 152/300 [04:29<03:40,  1.49s/it]Epoch 5:  51%|█████     | 153/300 [04:31<03:35,  1.47s/it]Epoch 5:  51%|█████▏    | 154/300 [04:32<03:31,  1.45s/it]Epoch 5:  52%|█████▏    | 155/300 [04:34<03:32,  1.47s/it]Epoch 5:  52%|█████▏    | 156/300 [04:35<03:29,  1.45s/it]Epoch 5:  52%|█████▏    | 157/300 [04:37<03:31,  1.48s/it]Epoch 5:  53%|█████▎    | 158/300 [04:38<03:28,  1.47s/it]Epoch 5:  53%|█████▎    | 159/300 [04:40<03:35,  1.53s/it]06/19/2022 15:25:28 - INFO - __main__ - global step: 830; train loss: 7.659390926361084; dev loss
Epoch 5:  53%|█████▎    | 160/300 [04:41<03:31,  1.51s/it]Epoch 5:  54%|█████▎    | 161/300 [04:43<03:40,  1.59s/it]Epoch 5:  54%|█████▍    | 162/300 [04:45<03:33,  1.55s/it]Epoch 5:  54%|█████▍    | 163/300 [04:46<03:47,  1.66s/it]Epoch 5:  55%|█████▍    | 164/300 [04:48<03:56,  1.74s/it]Epoch 5:  55%|█████▌    | 165/300 [04:50<03:42,  1.65s/it]Epoch 5:  55%|█████▌    | 166/300 [04:51<03:33,  1.59s/it]Epoch 5:  56%|█████▌    | 167/300 [04:53<03:41,  1.66s/it]Epoch 5:  56%|█████▌    | 168/300 [04:55<03:49,  1.74s/it]Epoch 5:  56%|█████▋    | 169/300 [04:57<03:38,  1.67s/it]Epoch 5:  57%|█████▋    | 170/300 [04:58<03:45,  1.74s/it]Epoch 5:  57%|█████▋    | 171/300 [05:00<03:33,  1.65s/it]Epoch 5:  57%|█████▋    | 172/300 [05:02<03:41,  1.73s/it]Epoch 5:  58%|█████▊    | 173/300 [05:03<03:28,  1.64s/it]Epoch 5:  58%|█████▊    | 174/300 [05:05<03:35,  1.71s/it]Epoch 5:  58%|█████▊    | 175/300 [05:07<03:40,  1.77s/it]Epoch 5:  59%|█████▊    | 176/300 [05:09<03:44,  1.81s/it]Epoch 5:  59%|█████▉    | 177/300 [05:11<03:49,  1.86s/it]Epoch 5:  59%|█████▉    | 178/300 [05:12<03:34,  1.76s/it]Epoch 5:  60%|█████▉    | 179/300 [05:14<03:36,  1.79s/it]06/19/2022 15:26:02 - INFO - __main__ - global step: 840; train loss: 7.7545671463012695; dev loss
Epoch 5:  60%|██████    | 180/300 [05:16<03:29,  1.75s/it]Epoch 5:  60%|██████    | 181/300 [05:18<03:34,  1.80s/it]Epoch 5:  61%|██████    | 182/300 [05:19<03:25,  1.74s/it]Epoch 5:  61%|██████    | 183/300 [05:21<03:34,  1.83s/it]Epoch 5:  61%|██████▏   | 184/300 [05:24<03:43,  1.93s/it]Epoch 5:  62%|██████▏   | 185/300 [05:25<03:27,  1.80s/it]Epoch 5:  62%|██████▏   | 186/300 [05:27<03:26,  1.81s/it]Epoch 5:  62%|██████▏   | 187/300 [05:29<03:24,  1.81s/it]Epoch 5:  63%|██████▎   | 188/300 [05:31<03:35,  1.92s/it]Epoch 5:  63%|██████▎   | 189/300 [05:33<03:44,  2.03s/it]Epoch 5:  63%|██████▎   | 190/300 [05:35<03:29,  1.91s/it]Epoch 5:  64%|██████▎   | 191/300 [05:36<03:18,  1.82s/it]Epoch 5:  64%|██████▍   | 192/300 [05:38<03:21,  1.86s/it]Epoch 5:  64%|██████▍   | 193/300 [05:40<03:15,  1.83s/it]Epoch 5:  65%|██████▍   | 194/300 [05:42<03:08,  1.78s/it]Epoch 5:  65%|██████▌   | 195/300 [05:44<03:10,  1.82s/it]Epoch 5:  65%|██████▌   | 196/300 [05:46<03:08,  1.81s/it]Epoch 5:  66%|██████▌   | 197/300 [05:47<03:05,  1.80s/it]Epoch 5:  66%|██████▌   | 198/300 [05:49<02:53,  1.70s/it]Epoch 5:  66%|██████▋   | 199/300 [05:51<02:58,  1.77s/it]06/19/2022 15:26:39 - INFO - __main__ - global step: 850; train loss: 7.815877437591553; dev loss
Epoch 5:  67%|██████▋   | 200/300 [05:53<03:04,  1.85s/it]Epoch 5:  67%|██████▋   | 201/300 [05:54<02:58,  1.80s/it]Epoch 5:  67%|██████▋   | 202/300 [05:56<02:58,  1.82s/it]Epoch 5:  68%|██████▊   | 203/300 [05:58<02:58,  1.84s/it]Epoch 5:  68%|██████▊   | 204/300 [06:00<02:49,  1.76s/it]Epoch 5:  68%|██████▊   | 205/300 [06:02<02:52,  1.82s/it]Epoch 5:  69%|██████▊   | 206/300 [06:03<02:42,  1.73s/it]Epoch 5:  69%|██████▉   | 207/300 [06:05<02:50,  1.83s/it]Epoch 5:  69%|██████▉   | 208/300 [06:07<02:44,  1.79s/it]Epoch 5:  70%|██████▉   | 209/300 [06:09<02:39,  1.75s/it]Epoch 5:  70%|███████   | 210/300 [06:10<02:30,  1.68s/it]Epoch 5:  70%|███████   | 211/300 [06:12<02:28,  1.66s/it]Epoch 5:  71%|███████   | 212/300 [06:14<02:30,  1.71s/it]Epoch 5:  71%|███████   | 213/300 [06:16<02:34,  1.78s/it]Epoch 5:  71%|███████▏  | 214/300 [06:17<02:23,  1.67s/it]Epoch 5:  72%|███████▏  | 215/300 [06:19<02:19,  1.64s/it]Epoch 5:  72%|███████▏  | 216/300 [06:20<02:12,  1.58s/it]Epoch 5:  72%|███████▏  | 217/300 [06:22<02:20,  1.70s/it]Epoch 5:  73%|███████▎  | 218/300 [06:24<02:22,  1.74s/it]Epoch 5:  73%|███████▎  | 219/300 [06:25<02:13,  1.65s/it]06/19/2022 15:27:13 - INFO - __main__ - global step: 860; train loss: 7.816531181335449; dev loss
Epoch 5:  73%|███████▎  | 220/300 [06:27<02:07,  1.59s/it]Epoch 5:  74%|███████▎  | 221/300 [06:28<02:04,  1.58s/it]Epoch 5:  74%|███████▍  | 222/300 [06:30<02:03,  1.58s/it]Epoch 5:  74%|███████▍  | 223/300 [06:32<02:07,  1.66s/it]Epoch 5:  75%|███████▍  | 224/300 [06:34<02:11,  1.73s/it]Epoch 5:  75%|███████▌  | 225/300 [06:35<02:03,  1.65s/it]Epoch 5:  75%|███████▌  | 226/300 [06:37<01:59,  1.62s/it]Epoch 5:  76%|███████▌  | 227/300 [06:38<01:57,  1.62s/it]Epoch 5:  76%|███████▌  | 228/300 [06:40<02:02,  1.70s/it]Epoch 5:  76%|███████▋  | 229/300 [06:42<01:59,  1.69s/it]Epoch 5:  77%|███████▋  | 230/300 [06:44<02:01,  1.74s/it]Epoch 5:  77%|███████▋  | 231/300 [06:46<02:04,  1.80s/it]Epoch 5:  77%|███████▋  | 232/300 [06:47<02:01,  1.79s/it]Epoch 5:  78%|███████▊  | 233/300 [06:49<01:53,  1.70s/it]Epoch 5:  78%|███████▊  | 234/300 [06:51<01:57,  1.78s/it]Epoch 5:  78%|███████▊  | 235/300 [06:53<01:57,  1.81s/it]Epoch 5:  79%|███████▊  | 236/300 [06:54<01:50,  1.73s/it]Epoch 5:  79%|███████▉  | 237/300 [06:56<01:46,  1.68s/it]Epoch 5:  79%|███████▉  | 238/300 [06:58<01:52,  1.82s/it]Epoch 5:  80%|███████▉  | 239/300 [07:00<01:51,  1.82s/it]06/19/2022 15:27:48 - INFO - __main__ - global step: 870; train loss: 7.930661201477051; dev loss
Epoch 5:  80%|████████  | 240/300 [07:02<01:50,  1.85s/it]Epoch 5:  80%|████████  | 241/300 [07:03<01:43,  1.75s/it]Epoch 5:  81%|████████  | 242/300 [07:05<01:40,  1.73s/it]Epoch 5:  81%|████████  | 243/300 [07:07<01:42,  1.80s/it]Epoch 5:  81%|████████▏ | 244/300 [07:08<01:36,  1.72s/it]Epoch 5:  82%|████████▏ | 245/300 [07:10<01:36,  1.75s/it]Epoch 5:  82%|████████▏ | 246/300 [07:12<01:34,  1.75s/it]Epoch 5:  82%|████████▏ | 247/300 [07:14<01:31,  1.73s/it]Epoch 5:  83%|████████▎ | 248/300 [07:15<01:26,  1.66s/it]Epoch 5:  83%|████████▎ | 249/300 [07:17<01:30,  1.77s/it]Epoch 5:  83%|████████▎ | 250/300 [07:19<01:32,  1.86s/it]Epoch 5:  84%|████████▎ | 251/300 [07:21<01:32,  1.89s/it]Epoch 5:  84%|████████▍ | 252/300 [07:23<01:31,  1.90s/it]Epoch 5:  84%|████████▍ | 253/300 [07:25<01:24,  1.79s/it]Epoch 5:  85%|████████▍ | 254/300 [07:26<01:18,  1.70s/it]Epoch 5:  85%|████████▌ | 255/300 [07:28<01:16,  1.69s/it]Epoch 5:  85%|████████▌ | 256/300 [07:29<01:13,  1.66s/it]Epoch 5:  86%|████████▌ | 257/300 [07:31<01:15,  1.75s/it]Epoch 5:  86%|████████▌ | 258/300 [07:33<01:09,  1.67s/it]Epoch 5:  86%|████████▋ | 259/300 [07:35<01:11,  1.74s/it]06/19/2022 15:28:23 - INFO - __main__ - global step: 880; train loss: 8.131505966186523; dev loss
Epoch 5:  87%|████████▋ | 260/300 [07:37<01:11,  1.79s/it]Epoch 5:  87%|████████▋ | 261/300 [07:39<01:13,  1.88s/it]Epoch 5:  87%|████████▋ | 262/300 [07:41<01:11,  1.89s/it]Epoch 5:  88%|████████▊ | 263/300 [07:43<01:11,  1.94s/it]Epoch 5:  88%|████████▊ | 264/300 [07:45<01:09,  1.93s/it]Epoch 5:  88%|████████▊ | 265/300 [07:46<01:03,  1.80s/it]Epoch 5:  89%|████████▊ | 266/300 [07:48<00:59,  1.75s/it]Epoch 5:  89%|████████▉ | 267/300 [07:49<00:56,  1.71s/it]Epoch 5:  89%|████████▉ | 268/300 [07:51<00:57,  1.80s/it]Epoch 5:  90%|████████▉ | 269/300 [07:53<00:57,  1.86s/it]Epoch 5:  90%|█████████ | 270/300 [07:55<00:52,  1.76s/it]Epoch 5:  90%|█████████ | 271/300 [07:57<00:53,  1.83s/it]Epoch 5:  91%|█████████ | 272/300 [07:59<00:51,  1.85s/it]Epoch 5:  91%|█████████ | 273/300 [08:00<00:47,  1.74s/it]Epoch 5:  91%|█████████▏| 274/300 [08:02<00:46,  1.78s/it]Epoch 5:  92%|█████████▏| 275/300 [08:04<00:43,  1.72s/it]Epoch 5:  92%|█████████▏| 276/300 [08:06<00:43,  1.81s/it]Epoch 5:  92%|█████████▏| 277/300 [08:08<00:41,  1.82s/it]Epoch 5:  93%|█████████▎| 278/300 [08:09<00:38,  1.77s/it]Epoch 5:  93%|█████████▎| 279/300 [08:11<00:39,  1.86s/it]06/19/2022 15:28:59 - INFO - __main__ - global step: 890; train loss: 7.845594882965088; dev loss
Epoch 5:  93%|█████████▎| 280/300 [08:13<00:35,  1.78s/it]Epoch 5:  94%|█████████▎| 281/300 [08:14<00:32,  1.69s/it]Epoch 5:  94%|█████████▍| 282/300 [08:16<00:30,  1.68s/it]Epoch 5:  94%|█████████▍| 283/300 [08:18<00:29,  1.73s/it]Epoch 5:  95%|█████████▍| 284/300 [08:20<00:27,  1.70s/it]Epoch 5:  95%|█████████▌| 285/300 [08:21<00:26,  1.75s/it]Epoch 5:  95%|█████████▌| 286/300 [08:23<00:25,  1.80s/it]Epoch 5:  96%|█████████▌| 287/300 [08:25<00:23,  1.84s/it]Epoch 5:  96%|█████████▌| 288/300 [08:27<00:21,  1.79s/it]Epoch 5:  96%|█████████▋| 289/300 [08:29<00:19,  1.74s/it]Epoch 5:  97%|█████████▋| 290/300 [08:30<00:16,  1.66s/it]Epoch 5:  97%|█████████▋| 291/300 [08:32<00:15,  1.70s/it]Epoch 5:  97%|█████████▋| 292/300 [08:34<00:14,  1.79s/it]Epoch 5:  98%|█████████▊| 293/300 [08:35<00:12,  1.74s/it]Epoch 5:  98%|█████████▊| 294/300 [08:37<00:10,  1.81s/it]Epoch 5:  98%|█████████▊| 295/300 [08:39<00:09,  1.82s/it]Epoch 5:  99%|█████████▊| 296/300 [08:41<00:07,  1.91s/it]Epoch 5:  99%|█████████▉| 297/300 [08:43<00:05,  1.94s/it]Epoch 5:  99%|█████████▉| 298/300 [08:45<00:03,  1.92s/it]Epoch 5: 100%|█████████▉| 299/300 [08:47<00:01,  1.91s/it]06/19/2022 15:29:35 - INFO - __main__ - global step: 900; train loss: 8.13010311126709; dev loss
Epoch 5: 100%|██████████| 300/300 [08:49<00:00,  1.84s/it]Epoch 5: 100%|██████████| 300/300 [08:49<00:00,  1.76s/it]
Epoch 6:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 6:   0%|          | 1/300 [00:01<08:11,  1.65s/it]Epoch 6:   1%|          | 2/300 [00:03<08:43,  1.76s/it]Epoch 6:   1%|          | 3/300 [00:05<08:23,  1.70s/it]Epoch 6:   1%|▏         | 4/300 [00:07<09:04,  1.84s/it]Epoch 6:   2%|▏         | 5/300 [00:08<08:48,  1.79s/it]Epoch 6:   2%|▏         | 6/300 [00:10<09:04,  1.85s/it]Epoch 6:   2%|▏         | 7/300 [00:12<08:44,  1.79s/it]Epoch 6:   3%|▎         | 8/300 [00:14<09:04,  1.86s/it]Epoch 6:   3%|▎         | 9/300 [00:16<09:12,  1.90s/it]Epoch 6:   3%|▎         | 10/300 [00:18<09:07,  1.89s/it]Epoch 6:   4%|▎         | 11/300 [00:19<08:32,  1.77s/it]Epoch 6:   4%|▍         | 12/300 [00:21<08:13,  1.71s/it]Epoch 6:   4%|▍         | 13/300 [00:23<08:07,  1.70s/it]Epoch 6:   5%|▍         | 14/300 [00:24<08:04,  1.69s/it]Epoch 6:   5%|▌         | 15/300 [00:26<08:23,  1.77s/it]Epoch 6:   5%|▌         | 16/300 [00:28<08:21,  1.77s/it]Epoch 6:   6%|▌         | 17/300 [00:30<08:53,  1.89s/it]Epoch 6:   6%|▌         | 18/300 [00:32<08:54,  1.90s/it]Epoch 6:   6%|▋         | 19/300 [00:34<08:56,  1.91s/it]06/19/2022 15:30:12 - INFO - __main__ - global step: 910; train loss: 7.647329807281494; dev loss
Epoch 6:   7%|▋         | 20/300 [00:36<08:37,  1.85s/it]Epoch 6:   7%|▋         | 21/300 [00:38<08:29,  1.83s/it]Epoch 6:   7%|▋         | 22/300 [00:39<08:05,  1.75s/it]Epoch 6:   8%|▊         | 23/300 [00:41<08:14,  1.78s/it]Epoch 6:   8%|▊         | 24/300 [00:43<08:02,  1.75s/it]Epoch 6:   8%|▊         | 25/300 [00:44<07:51,  1.71s/it]Epoch 6:   9%|▊         | 26/300 [00:46<08:04,  1.77s/it]Epoch 6:   9%|▉         | 27/300 [00:48<07:44,  1.70s/it]Epoch 6:   9%|▉         | 28/300 [00:49<07:46,  1.71s/it]Epoch 6:  10%|▉         | 29/300 [00:51<07:35,  1.68s/it]Epoch 6:  10%|█         | 30/300 [00:53<07:35,  1.69s/it]Epoch 6:  10%|█         | 31/300 [00:54<07:20,  1.64s/it]Epoch 6:  11%|█         | 32/300 [00:56<07:10,  1.61s/it]Epoch 6:  11%|█         | 33/300 [00:57<07:09,  1.61s/it]Epoch 6:  11%|█▏        | 34/300 [00:59<07:03,  1.59s/it]Epoch 6:  12%|█▏        | 35/300 [01:01<07:38,  1.73s/it]Epoch 6:  12%|█▏        | 36/300 [01:03<07:25,  1.69s/it]Epoch 6:  12%|█▏        | 37/300 [01:04<07:13,  1.65s/it]Epoch 6:  13%|█▎        | 38/300 [01:06<07:37,  1.75s/it]Epoch 6:  13%|█▎        | 39/300 [01:08<07:48,  1.79s/it]06/19/2022 15:30:45 - INFO - __main__ - global step: 920; train loss: 7.918756008148193; dev loss
Epoch 6:  13%|█▎        | 40/300 [01:10<07:22,  1.70s/it]Epoch 6:  14%|█▎        | 41/300 [01:12<07:54,  1.83s/it]Epoch 6:  14%|█▍        | 42/300 [01:13<07:47,  1.81s/it]Epoch 6:  14%|█▍        | 43/300 [01:16<08:11,  1.91s/it]Epoch 6:  15%|█▍        | 44/300 [01:17<07:42,  1.81s/it]Epoch 6:  15%|█▌        | 45/300 [01:19<07:53,  1.86s/it]Epoch 6:  15%|█▌        | 46/300 [01:21<07:58,  1.88s/it]Epoch 6:  16%|█▌        | 47/300 [01:23<07:59,  1.90s/it]Epoch 6:  16%|█▌        | 48/300 [01:25<07:30,  1.79s/it]Epoch 6:  16%|█▋        | 49/300 [01:26<07:09,  1.71s/it]Epoch 6:  17%|█▋        | 50/300 [01:28<07:34,  1.82s/it]Epoch 6:  17%|█▋        | 51/300 [01:30<07:08,  1.72s/it]Epoch 6:  17%|█▋        | 52/300 [01:32<07:26,  1.80s/it]Epoch 6:  18%|█▊        | 53/300 [01:33<07:06,  1.73s/it]Epoch 6:  18%|█▊        | 54/300 [01:35<07:06,  1.73s/it]Epoch 6:  18%|█▊        | 55/300 [01:37<07:11,  1.76s/it]Epoch 6:  19%|█▊        | 56/300 [01:39<07:13,  1.78s/it]Epoch 6:  19%|█▉        | 57/300 [01:40<06:58,  1.72s/it]Epoch 6:  19%|█▉        | 58/300 [01:42<06:43,  1.67s/it]Epoch 6:  20%|█▉        | 59/300 [01:43<06:39,  1.66s/it]06/19/2022 15:31:21 - INFO - __main__ - global step: 930; train loss: 7.565264701843262; dev loss
Epoch 6:  20%|██        | 60/300 [01:45<07:00,  1.75s/it]Epoch 6:  20%|██        | 61/300 [01:47<06:44,  1.69s/it]Epoch 6:  21%|██        | 62/300 [01:48<06:31,  1.64s/it]Epoch 6:  21%|██        | 63/300 [01:50<06:26,  1.63s/it]Epoch 6:  21%|██▏       | 64/300 [01:52<06:47,  1.73s/it]Epoch 6:  22%|██▏       | 65/300 [01:54<06:49,  1.74s/it]Epoch 6:  22%|██▏       | 66/300 [01:55<06:43,  1.73s/it]Epoch 6:  22%|██▏       | 67/300 [01:57<06:37,  1.70s/it]Epoch 6:  23%|██▎       | 68/300 [01:59<06:22,  1.65s/it]Epoch 6:  23%|██▎       | 69/300 [02:00<06:29,  1.69s/it]Epoch 6:  23%|██▎       | 70/300 [02:02<06:47,  1.77s/it]Epoch 6:  24%|██▎       | 71/300 [02:04<06:55,  1.81s/it]Epoch 6:  24%|██▍       | 72/300 [02:06<06:33,  1.73s/it]Epoch 6:  24%|██▍       | 73/300 [02:08<06:41,  1.77s/it]Epoch 6:  25%|██▍       | 74/300 [02:10<06:50,  1.82s/it]Epoch 6:  25%|██▌       | 75/300 [02:12<07:05,  1.89s/it]Epoch 6:  25%|██▌       | 76/300 [02:13<07:00,  1.88s/it]Epoch 6:  26%|██▌       | 77/300 [02:15<07:05,  1.91s/it]Epoch 6:  26%|██▌       | 78/300 [02:17<07:00,  1.89s/it]Epoch 6:  26%|██▋       | 79/300 [02:19<07:03,  1.91s/it]06/19/2022 15:31:56 - INFO - __main__ - global step: 940; train loss: 8.00893497467041; dev loss
Epoch 6:  27%|██▋       | 80/300 [02:21<06:30,  1.77s/it]Epoch 6:  27%|██▋       | 81/300 [02:23<06:32,  1.79s/it]Epoch 6:  27%|██▋       | 82/300 [02:24<06:10,  1.70s/it]Epoch 6:  28%|██▊       | 83/300 [02:26<06:17,  1.74s/it]Epoch 6:  28%|██▊       | 84/300 [02:27<06:01,  1.67s/it]Epoch 6:  28%|██▊       | 85/300 [02:29<06:10,  1.72s/it]Epoch 6:  29%|██▊       | 86/300 [02:31<06:27,  1.81s/it]Epoch 6:  29%|██▉       | 87/300 [02:33<06:04,  1.71s/it]Epoch 6:  29%|██▉       | 88/300 [02:35<06:23,  1.81s/it]Epoch 6:  30%|██▉       | 89/300 [02:37<06:22,  1.81s/it]Epoch 6:  30%|███       | 90/300 [02:38<06:05,  1.74s/it]Epoch 6:  30%|███       | 91/300 [02:40<06:17,  1.81s/it]Epoch 6:  31%|███       | 92/300 [02:42<06:13,  1.80s/it]Epoch 6:  31%|███       | 93/300 [02:44<06:19,  1.83s/it]Epoch 6:  31%|███▏      | 94/300 [02:46<06:31,  1.90s/it]Epoch 6:  32%|███▏      | 95/300 [02:48<06:18,  1.84s/it]Epoch 6:  32%|███▏      | 96/300 [02:49<06:03,  1.78s/it]Epoch 6:  32%|███▏      | 97/300 [02:51<06:05,  1.80s/it]Epoch 6:  33%|███▎      | 98/300 [02:53<05:52,  1.74s/it]Epoch 6:  33%|███▎      | 99/300 [02:54<05:44,  1.72s/it]06/19/2022 15:32:32 - INFO - __main__ - global step: 950; train loss: 7.601418972015381; dev loss
Epoch 6:  33%|███▎      | 100/300 [02:56<06:07,  1.84s/it]Epoch 6:  34%|███▎      | 101/300 [02:58<06:08,  1.85s/it]Epoch 6:  34%|███▍      | 102/300 [03:00<05:50,  1.77s/it]Epoch 6:  34%|███▍      | 103/300 [03:01<05:33,  1.69s/it]Epoch 6:  35%|███▍      | 104/300 [03:03<05:34,  1.71s/it]Epoch 6:  35%|███▌      | 105/300 [03:05<05:45,  1.77s/it]Epoch 6:  35%|███▌      | 106/300 [03:07<05:33,  1.72s/it]Epoch 6:  36%|███▌      | 107/300 [03:08<05:25,  1.69s/it]Epoch 6:  36%|███▌      | 108/300 [03:10<05:34,  1.74s/it]Epoch 6:  36%|███▋      | 109/300 [03:12<05:30,  1.73s/it]Epoch 6:  37%|███▋      | 110/300 [03:13<05:13,  1.65s/it]Epoch 6:  37%|███▋      | 111/300 [03:15<05:31,  1.75s/it]Epoch 6:  37%|███▋      | 112/300 [03:17<05:22,  1.72s/it]Epoch 6:  38%|███▊      | 113/300 [03:19<05:39,  1.82s/it]Epoch 6:  38%|███▊      | 114/300 [03:21<05:49,  1.88s/it]Epoch 6:  38%|███▊      | 115/300 [03:22<05:24,  1.75s/it]Epoch 6:  39%|███▊      | 116/300 [03:24<05:35,  1.82s/it]Epoch 6:  39%|███▉      | 117/300 [03:26<05:25,  1.78s/it]Epoch 6:  39%|███▉      | 118/300 [03:28<05:25,  1.79s/it]Epoch 6:  40%|███▉      | 119/300 [03:29<05:08,  1.70s/it]06/19/2022 15:33:07 - INFO - __main__ - global step: 960; train loss: 8.19001579284668; dev loss
Epoch 6:  40%|████      | 120/300 [03:31<05:13,  1.74s/it]Epoch 6:  40%|████      | 121/300 [03:33<05:32,  1.86s/it]Epoch 6:  41%|████      | 122/300 [03:35<05:20,  1.80s/it]Epoch 6:  41%|████      | 123/300 [03:37<05:23,  1.83s/it]Epoch 6:  41%|████▏     | 124/300 [03:39<05:35,  1.90s/it]Epoch 6:  42%|████▏     | 125/300 [03:41<05:44,  1.97s/it]Epoch 6:  42%|████▏     | 126/300 [03:43<05:37,  1.94s/it]Epoch 6:  42%|████▏     | 127/300 [03:45<05:23,  1.87s/it]Epoch 6:  43%|████▎     | 128/300 [03:47<05:19,  1.86s/it]Epoch 6:  43%|████▎     | 129/300 [03:49<05:30,  1.93s/it]Epoch 6:  43%|████▎     | 130/300 [03:51<05:26,  1.92s/it]Epoch 6:  44%|████▎     | 131/300 [03:52<05:22,  1.91s/it]Epoch 6:  44%|████▍     | 132/300 [03:54<05:20,  1.91s/it]Epoch 6:  44%|████▍     | 133/300 [03:56<05:06,  1.84s/it]Epoch 6:  45%|████▍     | 134/300 [03:58<05:19,  1.92s/it]Epoch 6:  45%|████▌     | 135/300 [04:00<05:14,  1.90s/it]Epoch 6:  45%|████▌     | 136/300 [04:02<04:53,  1.79s/it]Epoch 6:  46%|████▌     | 137/300 [04:04<05:02,  1.85s/it]Epoch 6:  46%|████▌     | 138/300 [04:05<04:54,  1.82s/it]Epoch 6:  46%|████▋     | 139/300 [04:07<04:36,  1.72s/it]06/19/2022 15:33:44 - INFO - __main__ - global step: 970; train loss: 7.845965385437012; dev loss
Epoch 6:  47%|████▋     | 140/300 [04:08<04:28,  1.68s/it]Epoch 6:  47%|████▋     | 141/300 [04:10<04:23,  1.66s/it]Epoch 6:  47%|████▋     | 142/300 [04:12<04:26,  1.69s/it]Epoch 6:  48%|████▊     | 143/300 [04:14<04:31,  1.73s/it]Epoch 6:  48%|████▊     | 144/300 [04:15<04:31,  1.74s/it]Epoch 6:  48%|████▊     | 145/300 [04:17<04:25,  1.71s/it]Epoch 6:  49%|████▊     | 146/300 [04:19<04:24,  1.72s/it]Epoch 6:  49%|████▉     | 147/300 [04:20<04:15,  1.67s/it]Epoch 6:  49%|████▉     | 148/300 [04:22<04:09,  1.64s/it]Epoch 6:  50%|████▉     | 149/300 [04:24<04:25,  1.76s/it]Epoch 6:  50%|█████     | 150/300 [04:26<04:41,  1.88s/it]Epoch 6:  50%|█████     | 151/300 [04:28<04:41,  1.89s/it]Epoch 6:  51%|█████     | 152/300 [04:30<04:42,  1.91s/it]Epoch 6:  51%|█████     | 153/300 [04:32<04:33,  1.86s/it]Epoch 6:  51%|█████▏    | 154/300 [04:33<04:23,  1.80s/it]Epoch 6:  52%|█████▏    | 155/300 [04:35<04:15,  1.76s/it]Epoch 6:  52%|█████▏    | 156/300 [04:37<04:11,  1.75s/it]Epoch 6:  52%|█████▏    | 157/300 [04:39<04:19,  1.82s/it]Epoch 6:  53%|█████▎    | 158/300 [04:41<04:25,  1.87s/it]Epoch 6:  53%|█████▎    | 159/300 [04:42<04:11,  1.78s/it]06/19/2022 15:34:20 - INFO - __main__ - global step: 980; train loss: 8.00606918334961; dev loss
Epoch 6:  53%|█████▎    | 160/300 [04:44<04:11,  1.80s/it]Epoch 6:  54%|█████▎    | 161/300 [04:46<04:12,  1.82s/it]Epoch 6:  54%|█████▍    | 162/300 [04:48<04:09,  1.81s/it]Epoch 6:  54%|█████▍    | 163/300 [04:50<04:18,  1.89s/it]Epoch 6:  55%|█████▍    | 164/300 [04:51<04:08,  1.83s/it]Epoch 6:  55%|█████▌    | 165/300 [04:53<04:01,  1.79s/it]Epoch 6:  55%|█████▌    | 166/300 [04:55<03:54,  1.75s/it]Epoch 6:  56%|█████▌    | 167/300 [04:57<03:58,  1.79s/it]Epoch 6:  56%|█████▌    | 168/300 [04:59<03:59,  1.82s/it]Epoch 6:  56%|█████▋    | 169/300 [05:00<03:55,  1.80s/it]Epoch 6:  57%|█████▋    | 170/300 [05:02<03:57,  1.82s/it]Epoch 6:  57%|█████▋    | 171/300 [05:04<03:47,  1.76s/it]Epoch 6:  57%|█████▋    | 172/300 [05:05<03:41,  1.73s/it]Epoch 6:  58%|█████▊    | 173/300 [05:07<03:30,  1.66s/it]Epoch 6:  58%|█████▊    | 174/300 [05:08<03:19,  1.59s/it]Epoch 6:  58%|█████▊    | 175/300 [05:10<03:18,  1.59s/it]Epoch 6:  59%|█████▊    | 176/300 [05:11<03:13,  1.56s/it]Epoch 6:  59%|█████▉    | 177/300 [05:13<03:19,  1.62s/it]Epoch 6:  59%|█████▉    | 178/300 [05:15<03:26,  1.70s/it]Epoch 6:  60%|█████▉    | 179/300 [05:17<03:37,  1.80s/it]06/19/2022 15:34:55 - INFO - __main__ - global step: 990; train loss: 8.021347999572754; dev loss
Epoch 6:  60%|██████    | 180/300 [05:19<03:36,  1.81s/it]Epoch 6:  60%|██████    | 181/300 [05:20<03:23,  1.71s/it]Epoch 6:  61%|██████    | 182/300 [05:22<03:13,  1.64s/it]Epoch 6:  61%|██████    | 183/300 [05:24<03:22,  1.73s/it]Epoch 6:  61%|██████▏   | 184/300 [05:26<03:25,  1.77s/it]Epoch 6:  62%|██████▏   | 185/300 [05:27<03:13,  1.68s/it]Epoch 6:  62%|██████▏   | 186/300 [05:29<03:09,  1.66s/it]Epoch 6:  62%|██████▏   | 187/300 [05:31<03:12,  1.71s/it]Epoch 6:  63%|██████▎   | 188/300 [05:32<03:10,  1.70s/it]Epoch 6:  63%|██████▎   | 189/300 [05:34<03:12,  1.73s/it]Epoch 6:  63%|██████▎   | 190/300 [05:36<03:16,  1.78s/it]Epoch 6:  64%|██████▎   | 191/300 [05:38<03:22,  1.86s/it]Epoch 6:  64%|██████▍   | 192/300 [05:40<03:24,  1.89s/it]Epoch 6:  64%|██████▍   | 193/300 [05:42<03:22,  1.89s/it]Epoch 6:  65%|██████▍   | 194/300 [05:44<03:22,  1.91s/it]Epoch 6:  65%|██████▌   | 195/300 [05:46<03:23,  1.94s/it]Epoch 6:  65%|██████▌   | 196/300 [05:48<03:22,  1.94s/it]Epoch 6:  66%|██████▌   | 197/300 [05:49<03:05,  1.80s/it]Epoch 6:  66%|██████▌   | 198/300 [05:51<02:56,  1.73s/it]Epoch 6:  66%|██████▋   | 199/300 [05:52<02:50,  1.69s/it]06/19/2022 15:35:30 - INFO - __main__ - global step: 1000; train loss: 8.023429870605469; dev loss
Epoch 6:  67%|██████▋   | 200/300 [05:54<02:47,  1.67s/it]Epoch 6:  67%|██████▋   | 201/300 [05:56<02:41,  1.63s/it]Epoch 6:  67%|██████▋   | 202/300 [05:57<02:38,  1.62s/it]Epoch 6:  68%|██████▊   | 203/300 [05:59<02:35,  1.60s/it]Epoch 6:  68%|██████▊   | 204/300 [06:00<02:32,  1.58s/it]Epoch 6:  68%|██████▊   | 205/300 [06:02<02:25,  1.54s/it]Epoch 6:  69%|██████▊   | 206/300 [06:03<02:21,  1.50s/it]Epoch 6:  69%|██████▉   | 207/300 [06:05<02:31,  1.63s/it]Epoch 6:  69%|██████▉   | 208/300 [06:07<02:28,  1.62s/it]Epoch 6:  70%|██████▉   | 209/300 [06:09<02:39,  1.75s/it]Epoch 6:  70%|███████   | 210/300 [06:11<02:39,  1.78s/it]Epoch 6:  70%|███████   | 211/300 [06:12<02:36,  1.76s/it]Epoch 6:  71%|███████   | 212/300 [06:14<02:31,  1.72s/it]Epoch 6:  71%|███████   | 213/300 [06:15<02:25,  1.67s/it]Epoch 6:  71%|███████▏  | 214/300 [06:17<02:20,  1.64s/it]Epoch 6:  72%|███████▏  | 215/300 [06:19<02:25,  1.72s/it]Epoch 6:  72%|███████▏  | 216/300 [06:21<02:21,  1.69s/it]Epoch 6:  72%|███████▏  | 217/300 [06:22<02:16,  1.65s/it]Epoch 6:  73%|███████▎  | 218/300 [06:24<02:11,  1.61s/it]Epoch 6:  73%|███████▎  | 219/300 [06:25<02:16,  1.68s/it]06/19/2022 15:36:03 - INFO - __main__ - global step: 1010; train loss: 7.521821022033691; dev loss
Epoch 6:  73%|███████▎  | 220/300 [06:27<02:12,  1.65s/it]Epoch 6:  74%|███████▎  | 221/300 [06:29<02:18,  1.76s/it]Epoch 6:  74%|███████▍  | 222/300 [06:31<02:18,  1.78s/it]Epoch 6:  74%|███████▍  | 223/300 [06:32<02:12,  1.72s/it]Epoch 6:  75%|███████▍  | 224/300 [06:34<02:12,  1.74s/it]Epoch 6:  75%|███████▌  | 225/300 [06:36<02:14,  1.80s/it]Epoch 6:  75%|███████▌  | 226/300 [06:38<02:05,  1.69s/it]Epoch 6:  76%|███████▌  | 227/300 [06:39<02:06,  1.73s/it]Epoch 6:  76%|███████▌  | 228/300 [06:41<01:58,  1.64s/it]Epoch 6:  76%|███████▋  | 229/300 [06:43<02:02,  1.73s/it]Epoch 6:  77%|███████▋  | 230/300 [06:45<02:03,  1.76s/it]Epoch 6:  77%|███████▋  | 231/300 [06:46<02:02,  1.78s/it]Epoch 6:  77%|███████▋  | 232/300 [06:48<02:04,  1.83s/it]Epoch 6:  78%|███████▊  | 233/300 [06:50<02:05,  1.88s/it]Epoch 6:  78%|███████▊  | 234/300 [06:52<01:57,  1.78s/it]Epoch 6:  78%|███████▊  | 235/300 [06:54<01:51,  1.72s/it]Epoch 6:  79%|███████▊  | 236/300 [06:55<01:43,  1.62s/it]Epoch 6:  79%|███████▉  | 237/300 [06:57<01:42,  1.62s/it]Epoch 6:  79%|███████▉  | 238/300 [06:58<01:37,  1.58s/it]Epoch 6:  80%|███████▉  | 239/300 [06:59<01:33,  1.54s/it]06/19/2022 15:36:37 - INFO - __main__ - global step: 1020; train loss: 7.831107139587402; dev loss
Epoch 6:  80%|████████  | 240/300 [07:01<01:36,  1.60s/it]Epoch 6:  80%|████████  | 241/300 [07:03<01:41,  1.72s/it]Epoch 6:  81%|████████  | 242/300 [07:05<01:37,  1.68s/it]Epoch 6:  81%|████████  | 243/300 [07:06<01:33,  1.65s/it]Epoch 6:  81%|████████▏ | 244/300 [07:08<01:30,  1.62s/it]Epoch 6:  82%|████████▏ | 245/300 [07:09<01:26,  1.58s/it]Epoch 6:  82%|████████▏ | 246/300 [07:11<01:24,  1.57s/it]Epoch 6:  82%|████████▏ | 247/300 [07:13<01:23,  1.57s/it]Epoch 6:  83%|████████▎ | 248/300 [07:14<01:24,  1.62s/it]Epoch 6:  83%|████████▎ | 249/300 [07:16<01:23,  1.64s/it]Epoch 6:  83%|████████▎ | 250/300 [07:18<01:24,  1.69s/it]Epoch 6:  84%|████████▎ | 251/300 [07:19<01:20,  1.64s/it]Epoch 6:  84%|████████▍ | 252/300 [07:21<01:17,  1.62s/it]Epoch 6:  84%|████████▍ | 253/300 [07:22<01:13,  1.57s/it]Epoch 6:  85%|████████▍ | 254/300 [07:24<01:17,  1.68s/it]Epoch 6:  85%|████████▌ | 255/300 [07:26<01:20,  1.79s/it]Epoch 6:  85%|████████▌ | 256/300 [07:28<01:21,  1.85s/it]Epoch 6:  86%|████████▌ | 257/300 [07:30<01:16,  1.77s/it]Epoch 6:  86%|████████▌ | 258/300 [07:32<01:12,  1.74s/it]Epoch 6:  86%|████████▋ | 259/300 [07:33<01:12,  1.76s/it]06/19/2022 15:37:11 - INFO - __main__ - global step: 1030; train loss: 7.837424278259277; dev loss
Epoch 6:  87%|████████▋ | 260/300 [07:35<01:08,  1.71s/it]Epoch 6:  87%|████████▋ | 261/300 [07:37<01:05,  1.69s/it]Epoch 6:  87%|████████▋ | 262/300 [07:38<01:05,  1.74s/it]Epoch 6:  88%|████████▊ | 263/300 [07:40<01:01,  1.67s/it]Epoch 6:  88%|████████▊ | 264/300 [07:42<01:01,  1.71s/it]Epoch 6:  88%|████████▊ | 265/300 [07:43<00:57,  1.66s/it]Epoch 6:  89%|████████▊ | 266/300 [07:45<00:55,  1.63s/it]Epoch 6:  89%|████████▉ | 267/300 [07:46<00:53,  1.61s/it]Epoch 6:  89%|████████▉ | 268/300 [07:48<00:56,  1.76s/it]Epoch 6:  90%|████████▉ | 269/300 [07:51<00:56,  1.84s/it]Epoch 6:  90%|█████████ | 270/300 [07:52<00:56,  1.88s/it]Epoch 6:  90%|█████████ | 271/300 [07:55<00:56,  1.95s/it]Epoch 6:  91%|█████████ | 272/300 [07:57<00:54,  1.94s/it]Epoch 6:  91%|█████████ | 273/300 [07:58<00:49,  1.84s/it]Epoch 6:  91%|█████████▏| 274/300 [08:00<00:46,  1.80s/it]Epoch 6:  92%|█████████▏| 275/300 [08:02<00:45,  1.82s/it]Epoch 6:  92%|█████████▏| 276/300 [08:03<00:42,  1.77s/it]Epoch 6:  92%|█████████▏| 277/300 [08:05<00:41,  1.79s/it]Epoch 6:  93%|█████████▎| 278/300 [08:07<00:39,  1.79s/it]Epoch 6:  93%|█████████▎| 279/300 [08:09<00:36,  1.76s/it]06/19/2022 15:37:46 - INFO - __main__ - global step: 1040; train loss: 8.087130546569824; dev loss
Epoch 6:  93%|█████████▎| 280/300 [08:10<00:34,  1.70s/it]Epoch 6:  94%|█████████▎| 281/300 [08:12<00:31,  1.68s/it]Epoch 6:  94%|█████████▍| 282/300 [08:14<00:30,  1.67s/it]Epoch 6:  94%|█████████▍| 283/300 [08:15<00:29,  1.75s/it]Epoch 6:  95%|█████████▍| 284/300 [08:17<00:27,  1.71s/it]Epoch 6:  95%|█████████▌| 285/300 [08:19<00:26,  1.76s/it]Epoch 6:  95%|█████████▌| 286/300 [08:21<00:25,  1.86s/it]Epoch 6:  96%|█████████▌| 287/300 [08:23<00:24,  1.87s/it]Epoch 6:  96%|█████████▌| 288/300 [08:24<00:21,  1.76s/it]Epoch 6:  96%|█████████▋| 289/300 [08:26<00:18,  1.68s/it]Epoch 6:  97%|█████████▋| 290/300 [08:28<00:17,  1.74s/it]Epoch 6:  97%|█████████▋| 291/300 [08:30<00:15,  1.77s/it]Epoch 6:  97%|█████████▋| 292/300 [08:31<00:13,  1.70s/it]Epoch 6:  98%|█████████▊| 293/300 [08:33<00:12,  1.72s/it]Epoch 6:  98%|█████████▊| 294/300 [08:34<00:09,  1.67s/it]Epoch 6:  98%|█████████▊| 295/300 [08:37<00:08,  1.79s/it]Epoch 6:  99%|█████████▊| 296/300 [08:38<00:06,  1.70s/it]Epoch 6:  99%|█████████▉| 297/300 [08:40<00:05,  1.75s/it]Epoch 6:  99%|█████████▉| 298/300 [08:42<00:03,  1.79s/it]Epoch 6: 100%|█████████▉| 299/300 [08:44<00:01,  1.82s/it]06/19/2022 15:38:21 - INFO - __main__ - global step: 1050; train loss: 7.79491662979126; dev loss
Epoch 6: 100%|██████████| 300/300 [08:45<00:00,  1.73s/it]Epoch 6: 100%|██████████| 300/300 [08:45<00:00,  1.75s/it]
Epoch 7:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 7:   0%|          | 1/300 [00:01<09:29,  1.91s/it]Epoch 7:   1%|          | 2/300 [00:03<08:19,  1.68s/it]Epoch 7:   1%|          | 3/300 [00:05<08:45,  1.77s/it]Epoch 7:   1%|▏         | 4/300 [00:06<08:29,  1.72s/it]Epoch 7:   2%|▏         | 5/300 [00:08<08:54,  1.81s/it]Epoch 7:   2%|▏         | 6/300 [00:10<08:56,  1.82s/it]Epoch 7:   2%|▏         | 7/300 [00:12<08:24,  1.72s/it]Epoch 7:   3%|▎         | 8/300 [00:14<08:55,  1.83s/it]Epoch 7:   3%|▎         | 9/300 [00:15<08:29,  1.75s/it]Epoch 7:   3%|▎         | 10/300 [00:17<08:09,  1.69s/it]Epoch 7:   4%|▎         | 11/300 [00:19<08:23,  1.74s/it]Epoch 7:   4%|▍         | 12/300 [00:21<08:45,  1.82s/it]Epoch 7:   4%|▍         | 13/300 [00:22<08:11,  1.71s/it]Epoch 7:   5%|▍         | 14/300 [00:24<08:02,  1.69s/it]Epoch 7:   5%|▌         | 15/300 [00:26<08:08,  1.71s/it]Epoch 7:   5%|▌         | 16/300 [00:28<08:23,  1.77s/it]Epoch 7:   6%|▌         | 17/300 [00:29<08:21,  1.77s/it]Epoch 7:   6%|▌         | 18/300 [00:31<08:00,  1.70s/it]Epoch 7:   6%|▋         | 19/300 [00:32<07:39,  1.63s/it]06/19/2022 15:38:56 - INFO - __main__ - global step: 1060; train loss: 7.762861728668213; dev loss
Epoch 7:   7%|▋         | 20/300 [00:34<08:09,  1.75s/it]Epoch 7:   7%|▋         | 21/300 [00:36<08:00,  1.72s/it]Epoch 7:   7%|▋         | 22/300 [00:38<07:52,  1.70s/it]Epoch 7:   8%|▊         | 23/300 [00:39<07:49,  1.70s/it]Epoch 7:   8%|▊         | 24/300 [00:41<07:52,  1.71s/it]Epoch 7:   8%|▊         | 25/300 [00:43<08:07,  1.77s/it]Epoch 7:   9%|▊         | 26/300 [00:45<07:59,  1.75s/it]Epoch 7:   9%|▉         | 27/300 [00:46<07:35,  1.67s/it]Epoch 7:   9%|▉         | 28/300 [00:48<07:47,  1.72s/it]Epoch 7:  10%|▉         | 29/300 [00:50<07:46,  1.72s/it]Epoch 7:  10%|█         | 30/300 [00:52<07:53,  1.75s/it]Epoch 7:  10%|█         | 31/300 [00:53<07:27,  1.66s/it]Epoch 7:  11%|█         | 32/300 [00:55<07:18,  1.63s/it]Epoch 7:  11%|█         | 33/300 [00:56<07:11,  1.62s/it]Epoch 7:  11%|█▏        | 34/300 [00:58<07:29,  1.69s/it]Epoch 7:  12%|█▏        | 35/300 [01:00<07:57,  1.80s/it]Epoch 7:  12%|█▏        | 36/300 [01:02<08:03,  1.83s/it]Epoch 7:  12%|█▏        | 37/300 [01:04<08:20,  1.90s/it]Epoch 7:  13%|█▎        | 38/300 [01:06<08:16,  1.89s/it]Epoch 7:  13%|█▎        | 39/300 [01:08<07:52,  1.81s/it]06/19/2022 15:39:31 - INFO - __main__ - global step: 1070; train loss: 7.62054967880249; dev loss
Epoch 7:  13%|█▎        | 40/300 [01:09<07:46,  1.79s/it]Epoch 7:  14%|█▎        | 41/300 [01:11<07:39,  1.77s/it]Epoch 7:  14%|█▍        | 42/300 [01:13<07:41,  1.79s/it]Epoch 7:  14%|█▍        | 43/300 [01:15<07:26,  1.74s/it]Epoch 7:  15%|█▍        | 44/300 [01:16<07:08,  1.67s/it]Epoch 7:  15%|█▌        | 45/300 [01:18<07:35,  1.79s/it]Epoch 7:  15%|█▌        | 46/300 [01:20<07:46,  1.84s/it]Epoch 7:  16%|█▌        | 47/300 [01:22<07:20,  1.74s/it]Epoch 7:  16%|█▌        | 48/300 [01:23<07:21,  1.75s/it]Epoch 7:  16%|█▋        | 49/300 [01:25<07:45,  1.86s/it]Epoch 7:  17%|█▋        | 50/300 [01:27<07:39,  1.84s/it]Epoch 7:  17%|█▋        | 51/300 [01:29<07:37,  1.84s/it]Epoch 7:  17%|█▋        | 52/300 [01:31<07:47,  1.88s/it]Epoch 7:  18%|█▊        | 53/300 [01:33<07:23,  1.80s/it]Epoch 7:  18%|█▊        | 54/300 [01:35<07:47,  1.90s/it]Epoch 7:  18%|█▊        | 55/300 [01:36<07:15,  1.78s/it]Epoch 7:  19%|█▊        | 56/300 [01:38<07:06,  1.75s/it]Epoch 7:  19%|█▉        | 57/300 [01:39<06:46,  1.67s/it]Epoch 7:  19%|█▉        | 58/300 [01:41<06:43,  1.67s/it]Epoch 7:  20%|█▉        | 59/300 [01:43<06:35,  1.64s/it]06/19/2022 15:40:06 - INFO - __main__ - global step: 1080; train loss: 8.352293968200684; dev loss
Epoch 7:  20%|██        | 60/300 [01:45<06:44,  1.69s/it]Epoch 7:  20%|██        | 61/300 [01:47<07:05,  1.78s/it]Epoch 7:  21%|██        | 62/300 [01:48<07:15,  1.83s/it]Epoch 7:  21%|██        | 63/300 [01:50<07:16,  1.84s/it]Epoch 7:  21%|██▏       | 64/300 [01:52<06:46,  1.72s/it]Epoch 7:  22%|██▏       | 65/300 [01:54<06:56,  1.77s/it]Epoch 7:  22%|██▏       | 66/300 [01:55<06:45,  1.73s/it]Epoch 7:  22%|██▏       | 67/300 [01:57<06:54,  1.78s/it]Epoch 7:  23%|██▎       | 68/300 [01:59<07:02,  1.82s/it]Epoch 7:  23%|██▎       | 69/300 [02:01<07:04,  1.84s/it]Epoch 7:  23%|██▎       | 70/300 [02:03<06:57,  1.82s/it]Epoch 7:  24%|██▎       | 71/300 [02:04<06:33,  1.72s/it]Epoch 7:  24%|██▍       | 72/300 [02:06<06:25,  1.69s/it]Epoch 7:  24%|██▍       | 73/300 [02:07<06:09,  1.63s/it]Epoch 7:  25%|██▍       | 74/300 [02:09<06:03,  1.61s/it]Epoch 7:  25%|██▌       | 75/300 [02:11<06:05,  1.63s/it]Epoch 7:  25%|██▌       | 76/300 [02:12<05:52,  1.58s/it]Epoch 7:  26%|██▌       | 77/300 [02:14<05:56,  1.60s/it]Epoch 7:  26%|██▌       | 78/300 [02:15<05:49,  1.57s/it]Epoch 7:  26%|██▋       | 79/300 [02:17<05:47,  1.57s/it]06/19/2022 15:40:40 - INFO - __main__ - global step: 1090; train loss: 7.715399265289307; dev loss
Epoch 7:  27%|██▋       | 80/300 [02:18<05:44,  1.57s/it]Epoch 7:  27%|██▋       | 81/300 [02:20<06:01,  1.65s/it]Epoch 7:  27%|██▋       | 82/300 [02:22<05:53,  1.62s/it]Epoch 7:  28%|██▊       | 83/300 [02:23<05:50,  1.61s/it]Epoch 7:  28%|██▊       | 84/300 [02:25<05:41,  1.58s/it]Epoch 7:  28%|██▊       | 85/300 [02:26<05:37,  1.57s/it]Epoch 7:  29%|██▊       | 86/300 [02:28<05:34,  1.56s/it]Epoch 7:  29%|██▉       | 87/300 [02:30<06:03,  1.71s/it]Epoch 7:  29%|██▉       | 88/300 [02:32<06:18,  1.79s/it]Epoch 7:  30%|██▉       | 89/300 [02:34<06:24,  1.82s/it]Epoch 7:  30%|███       | 90/300 [02:36<06:26,  1.84s/it]Epoch 7:  30%|███       | 91/300 [02:37<06:09,  1.77s/it]Epoch 7:  31%|███       | 92/300 [02:39<05:48,  1.68s/it]Epoch 7:  31%|███       | 93/300 [02:41<05:58,  1.73s/it]Epoch 7:  31%|███▏      | 94/300 [02:43<06:06,  1.78s/it]Epoch 7:  32%|███▏      | 95/300 [02:45<06:17,  1.84s/it]Epoch 7:  32%|███▏      | 96/300 [02:46<05:54,  1.74s/it]Epoch 7:  32%|███▏      | 97/300 [02:48<05:37,  1.66s/it]Epoch 7:  33%|███▎      | 98/300 [02:49<05:49,  1.73s/it]Epoch 7:  33%|███▎      | 99/300 [02:51<06:02,  1.81s/it]06/19/2022 15:41:14 - INFO - __main__ - global step: 1100; train loss: 8.24616527557373; dev loss
Epoch 7:  33%|███▎      | 100/300 [02:53<05:42,  1.71s/it]Epoch 7:  34%|███▎      | 101/300 [02:54<05:27,  1.64s/it]Epoch 7:  34%|███▍      | 102/300 [02:56<05:15,  1.59s/it]Epoch 7:  34%|███▍      | 103/300 [02:57<05:15,  1.60s/it]Epoch 7:  35%|███▍      | 104/300 [02:59<05:06,  1.56s/it]Epoch 7:  35%|███▌      | 105/300 [03:00<05:00,  1.54s/it]Epoch 7:  35%|███▌      | 106/300 [03:02<05:17,  1.64s/it]Epoch 7:  36%|███▌      | 107/300 [03:04<05:07,  1.59s/it]Epoch 7:  36%|███▌      | 108/300 [03:05<05:05,  1.59s/it]Epoch 7:  36%|███▋      | 109/300 [03:07<05:19,  1.67s/it]Epoch 7:  37%|███▋      | 110/300 [03:09<05:06,  1.61s/it]Epoch 7:  37%|███▋      | 111/300 [03:10<04:56,  1.57s/it]Epoch 7:  37%|███▋      | 112/300 [03:12<04:56,  1.58s/it]Epoch 7:  38%|███▊      | 113/300 [03:13<04:50,  1.56s/it]Epoch 7:  38%|███▊      | 114/300 [03:15<05:08,  1.66s/it]Epoch 7:  38%|███▊      | 115/300 [03:17<05:06,  1.65s/it]Epoch 7:  39%|███▊      | 116/300 [03:19<05:24,  1.76s/it]Epoch 7:  39%|███▉      | 117/300 [03:20<05:17,  1.73s/it]Epoch 7:  39%|███▉      | 118/300 [03:22<05:23,  1.78s/it]Epoch 7:  40%|███▉      | 119/300 [03:24<05:26,  1.81s/it]06/19/2022 15:41:48 - INFO - __main__ - global step: 1110; train loss: 7.8390793800354; dev loss
Epoch 7:  40%|████      | 120/300 [03:26<05:35,  1.86s/it]Epoch 7:  40%|████      | 121/300 [03:28<05:12,  1.75s/it]Epoch 7:  41%|████      | 122/300 [03:29<04:57,  1.67s/it]Epoch 7:  41%|████      | 123/300 [03:31<04:44,  1.61s/it]Epoch 7:  41%|████▏     | 124/300 [03:32<04:40,  1.60s/it]Epoch 7:  42%|████▏     | 125/300 [03:34<04:48,  1.65s/it]Epoch 7:  42%|████▏     | 126/300 [03:36<04:52,  1.68s/it]Epoch 7:  42%|████▏     | 127/300 [03:37<04:40,  1.62s/it]Epoch 7:  43%|████▎     | 128/300 [03:39<04:52,  1.70s/it]Epoch 7:  43%|████▎     | 129/300 [03:41<04:58,  1.75s/it]Epoch 7:  43%|████▎     | 130/300 [03:42<04:43,  1.67s/it]Epoch 7:  44%|████▎     | 131/300 [03:44<04:52,  1.73s/it]Epoch 7:  44%|████▍     | 132/300 [03:46<04:38,  1.65s/it]Epoch 7:  44%|████▍     | 133/300 [03:48<04:52,  1.75s/it]Epoch 7:  45%|████▍     | 134/300 [03:49<04:39,  1.68s/it]Epoch 7:  45%|████▌     | 135/300 [03:51<04:45,  1.73s/it]Epoch 7:  45%|████▌     | 136/300 [03:53<04:31,  1.65s/it]Epoch 7:  46%|████▌     | 137/300 [03:55<04:45,  1.75s/it]Epoch 7:  46%|████▌     | 138/300 [03:56<04:31,  1.68s/it]Epoch 7:  46%|████▋     | 139/300 [03:58<04:38,  1.73s/it]06/19/2022 15:42:21 - INFO - __main__ - global step: 1120; train loss: 7.952813148498535; dev loss
Epoch 7:  47%|████▋     | 140/300 [04:00<04:44,  1.78s/it]Epoch 7:  47%|████▋     | 141/300 [04:02<04:51,  1.83s/it]Epoch 7:  47%|████▋     | 142/300 [04:03<04:33,  1.73s/it]Epoch 7:  48%|████▊     | 143/300 [04:05<04:37,  1.77s/it]Epoch 7:  48%|████▊     | 144/300 [04:07<04:21,  1.68s/it]Epoch 7:  48%|████▊     | 145/300 [04:08<04:14,  1.64s/it]Epoch 7:  49%|████▊     | 146/300 [04:10<04:04,  1.59s/it]Epoch 7:  49%|████▉     | 147/300 [04:11<03:57,  1.56s/it]Epoch 7:  49%|████▉     | 148/300 [04:13<03:53,  1.54s/it]Epoch 7:  50%|████▉     | 149/300 [04:15<04:12,  1.67s/it]Epoch 7:  50%|█████     | 150/300 [04:16<04:03,  1.62s/it]Epoch 7:  50%|█████     | 151/300 [04:18<04:11,  1.69s/it]Epoch 7:  51%|█████     | 152/300 [04:19<04:00,  1.62s/it]Epoch 7:  51%|█████     | 153/300 [04:21<03:56,  1.61s/it]Epoch 7:  51%|█████▏    | 154/300 [04:22<03:48,  1.56s/it]Epoch 7:  52%|█████▏    | 155/300 [04:24<03:58,  1.65s/it]Epoch 7:  52%|█████▏    | 156/300 [04:26<04:06,  1.71s/it]Epoch 7:  52%|█████▏    | 157/300 [04:28<04:00,  1.68s/it]Epoch 7:  53%|█████▎    | 158/300 [04:30<04:05,  1.73s/it]Epoch 7:  53%|█████▎    | 159/300 [04:31<04:09,  1.77s/it]06/19/2022 15:42:55 - INFO - __main__ - global step: 1130; train loss: 7.922037601470947; dev loss
Epoch 7:  53%|█████▎    | 160/300 [04:33<04:11,  1.79s/it]Epoch 7:  54%|█████▎    | 161/300 [04:35<03:56,  1.70s/it]Epoch 7:  54%|█████▍    | 162/300 [04:36<03:50,  1.67s/it]Epoch 7:  54%|█████▍    | 163/300 [04:38<03:56,  1.73s/it]Epoch 7:  55%|█████▍    | 164/300 [04:40<03:44,  1.65s/it]Epoch 7:  55%|█████▌    | 165/300 [04:42<03:51,  1.71s/it]Epoch 7:  55%|█████▌    | 166/300 [04:44<03:59,  1.79s/it]Epoch 7:  56%|█████▌    | 167/300 [04:45<04:00,  1.81s/it]Epoch 7:  56%|█████▌    | 168/300 [04:47<03:44,  1.70s/it]Epoch 7:  56%|█████▋    | 169/300 [04:48<03:33,  1.63s/it]Epoch 7:  57%|█████▋    | 170/300 [04:50<03:44,  1.72s/it]Epoch 7:  57%|█████▋    | 171/300 [04:52<03:47,  1.76s/it]Epoch 7:  57%|█████▋    | 172/300 [04:54<03:34,  1.67s/it]Epoch 7:  58%|█████▊    | 173/300 [04:55<03:39,  1.73s/it]Epoch 7:  58%|█████▊    | 174/300 [04:57<03:46,  1.80s/it]Epoch 7:  58%|█████▊    | 175/300 [04:59<03:34,  1.71s/it]Epoch 7:  59%|█████▊    | 176/300 [05:00<03:24,  1.65s/it]Epoch 7:  59%|█████▉    | 177/300 [05:02<03:30,  1.71s/it]Epoch 7:  59%|█████▉    | 178/300 [05:04<03:25,  1.68s/it]Epoch 7:  60%|█████▉    | 179/300 [05:05<03:15,  1.62s/it]06/19/2022 15:43:29 - INFO - __main__ - global step: 1140; train loss: 7.555050849914551; dev loss
Epoch 7:  60%|██████    | 180/300 [05:07<03:22,  1.69s/it]Epoch 7:  60%|██████    | 181/300 [05:09<03:13,  1.62s/it]Epoch 7:  61%|██████    | 182/300 [05:11<03:23,  1.72s/it]Epoch 7:  61%|██████    | 183/300 [05:13<03:26,  1.76s/it]Epoch 7:  61%|██████▏   | 184/300 [05:14<03:14,  1.67s/it]Epoch 7:  62%|██████▏   | 185/300 [05:15<03:05,  1.61s/it]Epoch 7:  62%|██████▏   | 186/300 [05:17<02:58,  1.57s/it]Epoch 7:  62%|██████▏   | 187/300 [05:18<02:57,  1.57s/it]Epoch 7:  63%|██████▎   | 188/300 [05:20<02:52,  1.54s/it]Epoch 7:  63%|██████▎   | 189/300 [05:21<02:49,  1.52s/it]Epoch 7:  63%|██████▎   | 190/300 [05:23<02:58,  1.62s/it]Epoch 7:  64%|██████▎   | 191/300 [05:25<02:56,  1.62s/it]Epoch 7:  64%|██████▍   | 192/300 [05:26<02:50,  1.58s/it]Epoch 7:  64%|██████▍   | 193/300 [05:28<02:46,  1.56s/it]Epoch 7:  65%|██████▍   | 194/300 [05:29<02:43,  1.54s/it]Epoch 7:  65%|██████▌   | 195/300 [05:31<02:43,  1.56s/it]Epoch 7:  65%|██████▌   | 196/300 [05:32<02:40,  1.54s/it]Epoch 7:  66%|██████▌   | 197/300 [05:34<02:48,  1.64s/it]Epoch 7:  66%|██████▌   | 198/300 [05:36<02:53,  1.70s/it]Epoch 7:  66%|██████▋   | 199/300 [05:38<03:00,  1.78s/it]06/19/2022 15:44:01 - INFO - __main__ - global step: 1150; train loss: 7.816086769104004; dev loss
Epoch 7:  67%|██████▋   | 200/300 [05:40<02:49,  1.69s/it]Epoch 7:  67%|██████▋   | 201/300 [05:41<02:41,  1.63s/it]Epoch 7:  67%|██████▋   | 202/300 [05:43<02:34,  1.58s/it]Epoch 7:  68%|██████▊   | 203/300 [05:44<02:33,  1.58s/it]Epoch 7:  68%|██████▊   | 204/300 [05:46<02:28,  1.55s/it]Epoch 7:  68%|██████▊   | 205/300 [05:48<02:35,  1.64s/it]Epoch 7:  69%|██████▊   | 206/300 [05:49<02:40,  1.71s/it]Epoch 7:  69%|██████▉   | 207/300 [05:51<02:46,  1.79s/it]Epoch 7:  69%|██████▉   | 208/300 [05:53<02:46,  1.81s/it]Epoch 7:  70%|██████▉   | 209/300 [05:55<02:45,  1.82s/it]Epoch 7:  70%|███████   | 210/300 [05:57<02:34,  1.72s/it]Epoch 7:  70%|███████   | 211/300 [05:58<02:39,  1.79s/it]Epoch 7:  71%|███████   | 212/300 [06:00<02:28,  1.69s/it]Epoch 7:  71%|███████   | 213/300 [06:01<02:21,  1.62s/it]Epoch 7:  71%|███████▏  | 214/300 [06:03<02:25,  1.69s/it]Epoch 7:  72%|███████▏  | 215/300 [06:05<02:29,  1.76s/it]Epoch 7:  72%|███████▏  | 216/300 [06:07<02:32,  1.82s/it]Epoch 7:  72%|███████▏  | 217/300 [06:09<02:21,  1.71s/it]Epoch 7:  73%|███████▎  | 218/300 [06:10<02:13,  1.63s/it]Epoch 7:  73%|███████▎  | 219/300 [06:12<02:07,  1.58s/it]06/19/2022 15:44:35 - INFO - __main__ - global step: 1160; train loss: 8.151145935058594; dev loss
Epoch 7:  73%|███████▎  | 220/300 [06:13<02:15,  1.70s/it]Epoch 7:  74%|███████▎  | 221/300 [06:15<02:10,  1.65s/it]Epoch 7:  74%|███████▍  | 222/300 [06:17<02:13,  1.71s/it]Epoch 7:  74%|███████▍  | 223/300 [06:19<02:14,  1.75s/it]Epoch 7:  75%|███████▍  | 224/300 [06:21<02:17,  1.81s/it]Epoch 7:  75%|███████▌  | 225/300 [06:22<02:08,  1.71s/it]Epoch 7:  75%|███████▌  | 226/300 [06:24<02:09,  1.75s/it]Epoch 7:  76%|███████▌  | 227/300 [06:26<02:09,  1.78s/it]Epoch 7:  76%|███████▌  | 228/300 [06:28<02:12,  1.84s/it]Epoch 7:  76%|███████▋  | 229/300 [06:29<02:02,  1.73s/it]Epoch 7:  77%|███████▋  | 230/300 [06:31<02:03,  1.77s/it]Epoch 7:  77%|███████▋  | 231/300 [06:33<02:04,  1.80s/it]Epoch 7:  77%|███████▋  | 232/300 [06:35<01:58,  1.74s/it]Epoch 7:  78%|███████▊  | 233/300 [06:36<01:50,  1.65s/it]Epoch 7:  78%|███████▊  | 234/300 [06:38<01:45,  1.60s/it]Epoch 7:  78%|███████▊  | 235/300 [06:39<01:41,  1.56s/it]Epoch 7:  79%|███████▊  | 236/300 [06:41<01:40,  1.57s/it]Epoch 7:  79%|███████▉  | 237/300 [06:42<01:37,  1.55s/it]Epoch 7:  79%|███████▉  | 238/300 [06:44<01:41,  1.64s/it]Epoch 7:  80%|███████▉  | 239/300 [06:45<01:36,  1.59s/it]06/19/2022 15:45:09 - INFO - __main__ - global step: 1170; train loss: 8.027044296264648; dev loss
Epoch 7:  80%|████████  | 240/300 [06:47<01:39,  1.66s/it]Epoch 7:  80%|████████  | 241/300 [06:49<01:37,  1.65s/it]Epoch 7:  81%|████████  | 242/300 [06:50<01:32,  1.60s/it]Epoch 7:  81%|████████  | 243/300 [06:52<01:29,  1.56s/it]Epoch 7:  81%|████████▏ | 244/300 [06:53<01:26,  1.54s/it]Epoch 7:  82%|████████▏ | 245/300 [06:55<01:25,  1.55s/it]Epoch 7:  82%|████████▏ | 246/300 [06:56<01:22,  1.53s/it]Epoch 7:  82%|████████▏ | 247/300 [06:58<01:26,  1.63s/it]Epoch 7:  83%|████████▎ | 248/300 [07:00<01:28,  1.70s/it]Epoch 7:  83%|████████▎ | 249/300 [07:02<01:25,  1.67s/it]Epoch 7:  83%|████████▎ | 250/300 [07:03<01:20,  1.61s/it]Epoch 7:  84%|████████▎ | 251/300 [07:05<01:23,  1.69s/it]Epoch 7:  84%|████████▍ | 252/300 [07:07<01:23,  1.75s/it]Epoch 7:  84%|████████▍ | 253/300 [07:08<01:19,  1.70s/it]Epoch 7:  85%|████████▍ | 254/300 [07:10<01:14,  1.62s/it]Epoch 7:  85%|████████▌ | 255/300 [07:11<01:11,  1.58s/it]Epoch 7:  85%|████████▌ | 256/300 [07:13<01:08,  1.56s/it]Epoch 7:  86%|████████▌ | 257/300 [07:15<01:12,  1.68s/it]Epoch 7:  86%|████████▌ | 258/300 [07:16<01:08,  1.64s/it]Epoch 7:  86%|████████▋ | 259/300 [07:18<01:04,  1.58s/it]06/19/2022 15:45:41 - INFO - __main__ - global step: 1180; train loss: 7.764189720153809; dev loss
Epoch 7:  87%|████████▋ | 260/300 [07:19<01:01,  1.54s/it]Epoch 7:  87%|████████▋ | 261/300 [07:21<01:00,  1.56s/it]Epoch 7:  87%|████████▋ | 262/300 [07:22<00:58,  1.53s/it]Epoch 7:  88%|████████▊ | 263/300 [07:24<01:01,  1.65s/it]Epoch 7:  88%|████████▊ | 264/300 [07:26<01:02,  1.73s/it]Epoch 7:  88%|████████▊ | 265/300 [07:28<01:02,  1.80s/it]Epoch 7:  89%|████████▊ | 266/300 [07:30<00:57,  1.70s/it]Epoch 7:  89%|████████▉ | 267/300 [07:32<00:57,  1.75s/it]Epoch 7:  89%|████████▉ | 268/300 [07:33<00:53,  1.67s/it]Epoch 7:  90%|████████▉ | 269/300 [07:34<00:49,  1.60s/it]Epoch 7:  90%|█████████ | 270/300 [07:36<00:50,  1.70s/it]Epoch 7:  90%|█████████ | 271/300 [07:38<00:47,  1.62s/it]Epoch 7:  91%|█████████ | 272/300 [07:40<00:47,  1.69s/it]Epoch 7:  91%|█████████ | 273/300 [07:42<00:47,  1.75s/it]Epoch 7:  91%|█████████▏| 274/300 [07:44<00:47,  1.82s/it]Epoch 7:  92%|█████████▏| 275/300 [07:45<00:43,  1.73s/it]Epoch 7:  92%|█████████▏| 276/300 [07:47<00:42,  1.79s/it]Epoch 7:  92%|█████████▏| 277/300 [07:48<00:38,  1.69s/it]Epoch 7:  93%|█████████▎| 278/300 [07:50<00:39,  1.78s/it]Epoch 7:  93%|█████████▎| 279/300 [07:52<00:38,  1.82s/it]06/19/2022 15:46:16 - INFO - __main__ - global step: 1190; train loss: 8.372404098510742; dev loss
Epoch 7:  93%|█████████▎| 280/300 [07:54<00:37,  1.85s/it]Epoch 7:  94%|█████████▎| 281/300 [07:56<00:35,  1.88s/it]Epoch 7:  94%|█████████▍| 282/300 [07:58<00:32,  1.79s/it]Epoch 7:  94%|█████████▍| 283/300 [07:59<00:28,  1.70s/it]Epoch 7:  95%|█████████▍| 284/300 [08:01<00:28,  1.75s/it]Epoch 7:  95%|█████████▌| 285/300 [08:03<00:26,  1.80s/it]Epoch 7:  95%|█████████▌| 286/300 [08:05<00:25,  1.85s/it]Epoch 7:  96%|█████████▌| 287/300 [08:07<00:22,  1.74s/it]Epoch 7:  96%|█████████▌| 288/300 [08:08<00:21,  1.78s/it]Epoch 7:  96%|█████████▋| 289/300 [08:10<00:19,  1.81s/it]Epoch 7:  97%|█████████▋| 290/300 [08:12<00:18,  1.87s/it]Epoch 7:  97%|█████████▋| 291/300 [08:14<00:17,  1.90s/it]Epoch 7:  97%|█████████▋| 292/300 [08:16<00:14,  1.80s/it]Epoch 7:  98%|█████████▊| 293/300 [08:17<00:11,  1.70s/it]Epoch 7:  98%|█████████▊| 294/300 [08:19<00:09,  1.64s/it]Epoch 7:  98%|█████████▊| 295/300 [08:20<00:08,  1.62s/it]Epoch 7:  99%|█████████▊| 296/300 [08:22<00:06,  1.71s/it]Epoch 7:  99%|█████████▉| 297/300 [08:24<00:04,  1.66s/it]Epoch 7:  99%|█████████▉| 298/300 [08:25<00:03,  1.60s/it]Epoch 7: 100%|█████████▉| 299/300 [08:27<00:01,  1.73s/it]06/19/2022 15:46:51 - INFO - __main__ - global step: 1200; train loss: 7.374209403991699; dev loss
Epoch 7: 100%|██████████| 300/300 [08:29<00:00,  1.80s/it]Epoch 7: 100%|██████████| 300/300 [08:29<00:00,  1.70s/it]
Epoch 8:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 8:   0%|          | 1/300 [00:01<09:37,  1.93s/it]Epoch 8:   1%|          | 2/300 [00:03<08:16,  1.67s/it]Epoch 8:   1%|          | 3/300 [00:05<09:04,  1.83s/it]Epoch 8:   1%|▏         | 4/300 [00:06<08:25,  1.71s/it]Epoch 8:   2%|▏         | 5/300 [00:08<08:47,  1.79s/it]Epoch 8:   2%|▏         | 6/300 [00:10<08:59,  1.84s/it]Epoch 8:   2%|▏         | 7/300 [00:12<08:37,  1.77s/it]Epoch 8:   3%|▎         | 8/300 [00:14<08:50,  1.82s/it]Epoch 8:   3%|▎         | 9/300 [00:16<08:58,  1.85s/it]Epoch 8:   3%|▎         | 10/300 [00:17<08:25,  1.74s/it]Epoch 8:   4%|▎         | 11/300 [00:19<08:48,  1.83s/it]Epoch 8:   4%|▍         | 12/300 [00:21<08:55,  1.86s/it]Epoch 8:   4%|▍         | 13/300 [00:23<08:22,  1.75s/it]Epoch 8:   5%|▍         | 14/300 [00:25<08:38,  1.81s/it]Epoch 8:   5%|▌         | 15/300 [00:27<08:56,  1.88s/it]Epoch 8:   5%|▌         | 16/300 [00:28<08:20,  1.76s/it]Epoch 8:   6%|▌         | 17/300 [00:30<08:32,  1.81s/it]Epoch 8:   6%|▌         | 18/300 [00:32<08:41,  1.85s/it]Epoch 8:   6%|▋         | 19/300 [00:34<08:18,  1.77s/it]06/19/2022 15:47:27 - INFO - __main__ - global step: 1210; train loss: 8.039118766784668; dev loss
Epoch 8:   7%|▋         | 20/300 [00:36<08:28,  1.81s/it]Epoch 8:   7%|▋         | 21/300 [00:38<08:37,  1.85s/it]Epoch 8:   7%|▋         | 22/300 [00:39<08:06,  1.75s/it]Epoch 8:   8%|▊         | 23/300 [00:41<08:19,  1.80s/it]Epoch 8:   8%|▊         | 24/300 [00:43<08:01,  1.74s/it]Epoch 8:   8%|▊         | 25/300 [00:45<08:13,  1.80s/it]Epoch 8:   9%|▊         | 26/300 [00:46<07:46,  1.70s/it]Epoch 8:   9%|▉         | 27/300 [00:47<07:25,  1.63s/it]Epoch 8:   9%|▉         | 28/300 [00:49<07:20,  1.62s/it]Epoch 8:  10%|▉         | 29/300 [00:51<07:39,  1.70s/it]Epoch 8:  10%|█         | 30/300 [00:52<07:22,  1.64s/it]Epoch 8:  10%|█         | 31/300 [00:54<07:05,  1.58s/it]Epoch 8:  11%|█         | 32/300 [00:55<07:03,  1.58s/it]Epoch 8:  11%|█         | 33/300 [00:57<06:50,  1.54s/it]Epoch 8:  11%|█▏        | 34/300 [00:58<06:45,  1.52s/it]Epoch 8:  12%|█▏        | 35/300 [01:00<06:37,  1.50s/it]Epoch 8:  12%|█▏        | 36/300 [01:02<07:13,  1.64s/it]Epoch 8:  12%|█▏        | 37/300 [01:03<06:59,  1.60s/it]Epoch 8:  13%|█▎        | 38/300 [01:05<07:19,  1.68s/it]Epoch 8:  13%|█▎        | 39/300 [01:07<07:02,  1.62s/it]06/19/2022 15:48:00 - INFO - __main__ - global step: 1220; train loss: 8.048388481140137; dev loss
Epoch 8:  13%|█▎        | 40/300 [01:08<06:55,  1.60s/it]Epoch 8:  14%|█▎        | 41/300 [01:10<07:12,  1.67s/it]Epoch 8:  14%|█▍        | 42/300 [01:12<06:57,  1.62s/it]Epoch 8:  14%|█▍        | 43/300 [01:13<06:43,  1.57s/it]Epoch 8:  15%|█▍        | 44/300 [01:15<06:46,  1.59s/it]Epoch 8:  15%|█▌        | 45/300 [01:16<07:03,  1.66s/it]Epoch 8:  15%|█▌        | 46/300 [01:18<07:17,  1.72s/it]Epoch 8:  16%|█▌        | 47/300 [01:20<07:26,  1.76s/it]Epoch 8:  16%|█▌        | 48/300 [01:22<07:04,  1.68s/it]Epoch 8:  16%|█▋        | 49/300 [01:24<07:19,  1.75s/it]Epoch 8:  17%|█▋        | 50/300 [01:25<07:21,  1.76s/it]Epoch 8:  17%|█▋        | 51/300 [01:27<07:19,  1.76s/it]Epoch 8:  17%|█▋        | 52/300 [01:29<07:17,  1.76s/it]Epoch 8:  18%|█▊        | 53/300 [01:31<07:22,  1.79s/it]Epoch 8:  18%|█▊        | 54/300 [01:33<07:23,  1.80s/it]Epoch 8:  18%|█▊        | 55/300 [01:34<07:24,  1.82s/it]Epoch 8:  19%|█▊        | 56/300 [01:36<06:57,  1.71s/it]Epoch 8:  19%|█▉        | 57/300 [01:38<07:14,  1.79s/it]Epoch 8:  19%|█▉        | 58/300 [01:40<07:16,  1.80s/it]Epoch 8:  20%|█▉        | 59/300 [01:42<07:19,  1.83s/it]06/19/2022 15:48:34 - INFO - __main__ - global step: 1230; train loss: 7.815533638000488; dev loss
Epoch 8:  20%|██        | 60/300 [01:43<06:51,  1.71s/it]Epoch 8:  20%|██        | 61/300 [01:45<06:42,  1.68s/it]Epoch 8:  21%|██        | 62/300 [01:47<06:55,  1.75s/it]Epoch 8:  21%|██        | 63/300 [01:48<06:38,  1.68s/it]Epoch 8:  21%|██▏       | 64/300 [01:50<06:51,  1.74s/it]Epoch 8:  22%|██▏       | 65/300 [01:52<07:09,  1.83s/it]Epoch 8:  22%|██▏       | 66/300 [01:53<06:45,  1.73s/it]Epoch 8:  22%|██▏       | 67/300 [01:55<06:55,  1.78s/it]Epoch 8:  23%|██▎       | 68/300 [01:57<07:01,  1.82s/it]Epoch 8:  23%|██▎       | 69/300 [01:59<07:10,  1.86s/it]Epoch 8:  23%|██▎       | 70/300 [02:01<06:41,  1.74s/it]Epoch 8:  24%|██▎       | 71/300 [02:03<06:47,  1.78s/it]Epoch 8:  24%|██▍       | 72/300 [02:04<06:52,  1.81s/it]Epoch 8:  24%|██▍       | 73/300 [02:06<07:00,  1.85s/it]Epoch 8:  25%|██▍       | 74/300 [02:08<06:33,  1.74s/it]Epoch 8:  25%|██▌       | 75/300 [02:10<06:38,  1.77s/it]Epoch 8:  25%|██▌       | 76/300 [02:11<06:16,  1.68s/it]Epoch 8:  26%|██▌       | 77/300 [02:13<06:27,  1.74s/it]Epoch 8:  26%|██▌       | 78/300 [02:15<06:40,  1.80s/it]Epoch 8:  26%|██▋       | 79/300 [02:17<06:16,  1.70s/it]06/19/2022 15:49:10 - INFO - __main__ - global step: 1240; train loss: 7.8736772537231445; dev loss
Epoch 8:  27%|██▋       | 80/300 [02:18<06:23,  1.74s/it]Epoch 8:  27%|██▋       | 81/300 [02:20<06:05,  1.67s/it]Epoch 8:  27%|██▋       | 82/300 [02:21<05:56,  1.64s/it]Epoch 8:  28%|██▊       | 83/300 [02:23<06:07,  1.70s/it]Epoch 8:  28%|██▊       | 84/300 [02:25<05:50,  1.62s/it]Epoch 8:  28%|██▊       | 85/300 [02:26<05:36,  1.57s/it]Epoch 8:  29%|██▊       | 86/300 [02:28<06:00,  1.68s/it]Epoch 8:  29%|██▉       | 87/300 [02:30<05:54,  1.66s/it]Epoch 8:  29%|██▉       | 88/300 [02:32<06:06,  1.73s/it]Epoch 8:  30%|██▉       | 89/300 [02:33<05:49,  1.66s/it]Epoch 8:  30%|███       | 90/300 [02:35<06:07,  1.75s/it]Epoch 8:  30%|███       | 91/300 [02:36<05:47,  1.66s/it]Epoch 8:  31%|███       | 92/300 [02:38<05:58,  1.73s/it]Epoch 8:  31%|███       | 93/300 [02:40<05:50,  1.69s/it]Epoch 8:  31%|███▏      | 94/300 [02:42<05:44,  1.67s/it]Epoch 8:  32%|███▏      | 95/300 [02:43<05:29,  1.61s/it]Epoch 8:  32%|███▏      | 96/300 [02:45<05:41,  1.67s/it]Epoch 8:  32%|███▏      | 97/300 [02:47<05:53,  1.74s/it]Epoch 8:  33%|███▎      | 98/300 [02:48<05:44,  1.70s/it]Epoch 8:  33%|███▎      | 99/300 [02:50<05:29,  1.64s/it]06/19/2022 15:49:43 - INFO - __main__ - global step: 1250; train loss: 7.935845851898193; dev loss
Epoch 8:  33%|███▎      | 100/300 [02:52<05:40,  1.70s/it]Epoch 8:  34%|███▎      | 101/300 [02:53<05:27,  1.65s/it]Epoch 8:  34%|███▍      | 102/300 [02:55<05:33,  1.69s/it]Epoch 8:  34%|███▍      | 103/300 [02:57<05:49,  1.77s/it]Epoch 8:  35%|███▍      | 104/300 [02:59<05:32,  1.70s/it]Epoch 8:  35%|███▌      | 105/300 [03:00<05:17,  1.63s/it]Epoch 8:  35%|███▌      | 106/300 [03:02<05:32,  1.72s/it]Epoch 8:  36%|███▌      | 107/300 [03:04<05:46,  1.79s/it]Epoch 8:  36%|███▌      | 108/300 [03:06<05:50,  1.82s/it]Epoch 8:  36%|███▋      | 109/300 [03:07<05:28,  1.72s/it]Epoch 8:  37%|███▋      | 110/300 [03:09<05:34,  1.76s/it]Epoch 8:  37%|███▋      | 111/300 [03:11<05:23,  1.71s/it]Epoch 8:  37%|███▋      | 112/300 [03:12<05:06,  1.63s/it]Epoch 8:  38%|███▊      | 113/300 [03:14<04:55,  1.58s/it]Epoch 8:  38%|███▊      | 114/300 [03:15<05:09,  1.66s/it]Epoch 8:  38%|███▊      | 115/300 [03:17<05:24,  1.75s/it]Epoch 8:  39%|███▊      | 116/300 [03:19<05:29,  1.79s/it]Epoch 8:  39%|███▉      | 117/300 [03:21<05:29,  1.80s/it]Epoch 8:  39%|███▉      | 118/300 [03:23<05:09,  1.70s/it]Epoch 8:  40%|███▉      | 119/300 [03:24<05:00,  1.66s/it]06/19/2022 15:50:17 - INFO - __main__ - global step: 1260; train loss: 7.869870662689209; dev loss
Epoch 8:  40%|████      | 120/300 [03:26<04:48,  1.60s/it]Epoch 8:  40%|████      | 121/300 [03:27<04:40,  1.56s/it]Epoch 8:  41%|████      | 122/300 [03:29<04:32,  1.53s/it]Epoch 8:  41%|████      | 123/300 [03:31<04:54,  1.67s/it]Epoch 8:  41%|████▏     | 124/300 [03:32<04:41,  1.60s/it]Epoch 8:  42%|████▏     | 125/300 [03:33<04:32,  1.56s/it]Epoch 8:  42%|████▏     | 126/300 [03:35<04:46,  1.65s/it]Epoch 8:  42%|████▏     | 127/300 [03:37<04:41,  1.63s/it]Epoch 8:  43%|████▎     | 128/300 [03:38<04:32,  1.59s/it]Epoch 8:  43%|████▎     | 129/300 [03:40<04:24,  1.54s/it]Epoch 8:  43%|████▎     | 130/300 [03:41<04:18,  1.52s/it]Epoch 8:  44%|████▎     | 131/300 [03:43<04:14,  1.50s/it]Epoch 8:  44%|████▍     | 132/300 [03:44<04:16,  1.53s/it]Epoch 8:  44%|████▍     | 133/300 [03:46<04:31,  1.63s/it]Epoch 8:  45%|████▍     | 134/300 [03:48<04:42,  1.70s/it]Epoch 8:  45%|████▌     | 135/300 [03:50<04:49,  1.75s/it]Epoch 8:  45%|████▌     | 136/300 [03:52<05:00,  1.83s/it]Epoch 8:  46%|████▌     | 137/300 [03:54<05:01,  1.85s/it]Epoch 8:  46%|████▌     | 138/300 [03:56<05:00,  1.86s/it]Epoch 8:  46%|████▋     | 139/300 [03:58<05:00,  1.87s/it]06/19/2022 15:50:51 - INFO - __main__ - global step: 1270; train loss: 8.00093936920166; dev loss
Epoch 8:  47%|████▋     | 140/300 [04:00<05:06,  1.91s/it]Epoch 8:  47%|████▋     | 141/300 [04:02<05:01,  1.90s/it]Epoch 8:  47%|████▋     | 142/300 [04:03<04:59,  1.89s/it]Epoch 8:  48%|████▊     | 143/300 [04:05<04:56,  1.89s/it]Epoch 8:  48%|████▊     | 144/300 [04:07<04:59,  1.92s/it]Epoch 8:  48%|████▊     | 145/300 [04:09<04:37,  1.79s/it]Epoch 8:  49%|████▊     | 146/300 [04:10<04:20,  1.69s/it]Epoch 8:  49%|████▉     | 147/300 [04:12<04:27,  1.75s/it]Epoch 8:  49%|████▉     | 148/300 [04:14<04:17,  1.70s/it]Epoch 8:  50%|████▉     | 149/300 [04:15<04:07,  1.64s/it]Epoch 8:  50%|█████     | 150/300 [04:17<03:57,  1.58s/it]Epoch 8:  50%|█████     | 151/300 [04:18<04:07,  1.66s/it]Epoch 8:  51%|█████     | 152/300 [04:20<04:03,  1.64s/it]Epoch 8:  51%|█████     | 153/300 [04:22<04:11,  1.71s/it]Epoch 8:  51%|█████▏    | 154/300 [04:23<04:00,  1.65s/it]Epoch 8:  52%|█████▏    | 155/300 [04:25<03:49,  1.58s/it]Epoch 8:  52%|█████▏    | 156/300 [04:26<03:43,  1.55s/it]Epoch 8:  52%|█████▏    | 157/300 [04:28<03:59,  1.68s/it]Epoch 8:  53%|█████▎    | 158/300 [04:30<03:50,  1.62s/it]Epoch 8:  53%|█████▎    | 159/300 [04:31<03:41,  1.57s/it]06/19/2022 15:51:24 - INFO - __main__ - global step: 1280; train loss: 8.010228157043457; dev loss
Epoch 8:  53%|█████▎    | 160/300 [04:33<03:51,  1.66s/it]Epoch 8:  54%|█████▎    | 161/300 [04:35<03:47,  1.64s/it]Epoch 8:  54%|█████▍    | 162/300 [04:37<03:55,  1.70s/it]Epoch 8:  54%|█████▍    | 163/300 [04:38<04:00,  1.75s/it]Epoch 8:  55%|█████▍    | 164/300 [04:40<03:47,  1.67s/it]Epoch 8:  55%|█████▌    | 165/300 [04:42<03:44,  1.66s/it]Epoch 8:  55%|█████▌    | 166/300 [04:43<03:36,  1.62s/it]Epoch 8:  56%|█████▌    | 167/300 [04:45<03:30,  1.58s/it]Epoch 8:  56%|█████▌    | 168/300 [04:46<03:39,  1.67s/it]Epoch 8:  56%|█████▋    | 169/300 [04:48<03:34,  1.64s/it]Epoch 8:  57%|█████▋    | 170/300 [04:49<03:25,  1.58s/it]Epoch 8:  57%|█████▋    | 171/300 [04:51<03:34,  1.66s/it]Epoch 8:  57%|█████▋    | 172/300 [04:53<03:26,  1.61s/it]Epoch 8:  58%|█████▊    | 173/300 [04:54<03:24,  1.61s/it]Epoch 8:  58%|█████▊    | 174/300 [04:56<03:32,  1.69s/it]Epoch 8:  58%|█████▊    | 175/300 [04:58<03:36,  1.74s/it]Epoch 8:  59%|█████▊    | 176/300 [05:00<03:40,  1.78s/it]Epoch 8:  59%|█████▉    | 177/300 [05:02<03:32,  1.73s/it]Epoch 8:  59%|█████▉    | 178/300 [05:03<03:21,  1.65s/it]Epoch 8:  60%|█████▉    | 179/300 [05:05<03:27,  1.71s/it]06/19/2022 15:51:58 - INFO - __main__ - global step: 1290; train loss: 7.4764084815979; dev loss
Epoch 8:  60%|██████    | 180/300 [05:07<03:31,  1.77s/it]Epoch 8:  60%|██████    | 181/300 [05:08<03:24,  1.72s/it]Epoch 8:  61%|██████    | 182/300 [05:10<03:27,  1.76s/it]Epoch 8:  61%|██████    | 183/300 [05:12<03:30,  1.79s/it]Epoch 8:  61%|██████▏   | 184/300 [05:14<03:17,  1.70s/it]Epoch 8:  62%|██████▏   | 185/300 [05:15<03:07,  1.63s/it]Epoch 8:  62%|██████▏   | 186/300 [05:17<03:18,  1.74s/it]Epoch 8:  62%|██████▏   | 187/300 [05:19<03:08,  1.67s/it]Epoch 8:  63%|██████▎   | 188/300 [05:20<03:00,  1.61s/it]Epoch 8:  63%|██████▎   | 189/300 [05:22<02:53,  1.56s/it]Epoch 8:  63%|██████▎   | 190/300 [05:24<03:05,  1.69s/it]Epoch 8:  64%|██████▎   | 191/300 [05:25<02:57,  1.63s/it]Epoch 8:  64%|██████▍   | 192/300 [05:27<03:03,  1.70s/it]Epoch 8:  64%|██████▍   | 193/300 [05:29<03:06,  1.75s/it]Epoch 8:  65%|██████▍   | 194/300 [05:30<03:00,  1.70s/it]Epoch 8:  65%|██████▌   | 195/300 [05:32<03:03,  1.75s/it]Epoch 8:  65%|██████▌   | 196/300 [05:34<02:53,  1.67s/it]Epoch 8:  66%|██████▌   | 197/300 [05:35<02:45,  1.61s/it]Epoch 8:  66%|██████▌   | 198/300 [05:37<02:43,  1.61s/it]Epoch 8:  66%|██████▋   | 199/300 [05:38<02:38,  1.56s/it]06/19/2022 15:52:31 - INFO - __main__ - global step: 1300; train loss: 8.334733963012695; dev loss
Epoch 8:  67%|██████▋   | 200/300 [05:40<02:33,  1.54s/it]Epoch 8:  67%|██████▋   | 201/300 [05:41<02:30,  1.52s/it]Epoch 8:  67%|██████▋   | 202/300 [05:43<02:43,  1.67s/it]Epoch 8:  68%|██████▊   | 203/300 [05:45<02:37,  1.62s/it]Epoch 8:  68%|██████▊   | 204/300 [05:47<02:41,  1.69s/it]Epoch 8:  68%|██████▊   | 205/300 [05:48<02:45,  1.74s/it]Epoch 8:  69%|██████▊   | 206/300 [05:50<02:51,  1.82s/it]Epoch 8:  69%|██████▉   | 207/300 [05:52<02:38,  1.71s/it]Epoch 8:  69%|██████▉   | 208/300 [05:53<02:30,  1.63s/it]Epoch 8:  70%|██████▉   | 209/300 [05:55<02:24,  1.59s/it]Epoch 8:  70%|███████   | 210/300 [05:57<02:30,  1.67s/it]Epoch 8:  70%|███████   | 211/300 [05:59<02:36,  1.76s/it]Epoch 8:  71%|███████   | 212/300 [06:00<02:27,  1.67s/it]Epoch 8:  71%|███████   | 213/300 [06:02<02:31,  1.74s/it]Epoch 8:  71%|███████▏  | 214/300 [06:04<02:32,  1.77s/it]Epoch 8:  72%|███████▏  | 215/300 [06:06<02:36,  1.84s/it]Epoch 8:  72%|███████▏  | 216/300 [06:07<02:26,  1.74s/it]Epoch 8:  72%|███████▏  | 217/300 [06:09<02:27,  1.78s/it]Epoch 8:  73%|███████▎  | 218/300 [06:11<02:18,  1.69s/it]Epoch 8:  73%|███████▎  | 219/300 [06:12<02:14,  1.65s/it]06/19/2022 15:53:05 - INFO - __main__ - global step: 1310; train loss: 8.023622512817383; dev loss
Epoch 8:  73%|███████▎  | 220/300 [06:14<02:17,  1.72s/it]Epoch 8:  74%|███████▎  | 221/300 [06:16<02:19,  1.77s/it]Epoch 8:  74%|███████▍  | 222/300 [06:18<02:12,  1.70s/it]Epoch 8:  74%|███████▍  | 223/300 [06:20<02:18,  1.79s/it]Epoch 8:  75%|███████▍  | 224/300 [06:21<02:18,  1.82s/it]Epoch 8:  75%|███████▌  | 225/300 [06:23<02:09,  1.73s/it]Epoch 8:  75%|███████▌  | 226/300 [06:25<02:10,  1.76s/it]Epoch 8:  76%|███████▌  | 227/300 [06:27<02:13,  1.82s/it]Epoch 8:  76%|███████▌  | 228/300 [06:29<02:11,  1.83s/it]Epoch 8:  76%|███████▋  | 229/300 [06:30<02:03,  1.74s/it]Epoch 8:  77%|███████▋  | 230/300 [06:32<01:57,  1.69s/it]Epoch 8:  77%|███████▋  | 231/300 [06:33<01:55,  1.67s/it]Epoch 8:  77%|███████▋  | 232/300 [06:35<01:49,  1.61s/it]Epoch 8:  78%|███████▊  | 233/300 [06:37<01:52,  1.68s/it]Epoch 8:  78%|███████▊  | 234/300 [06:39<01:54,  1.73s/it]Epoch 8:  78%|███████▊  | 235/300 [06:40<01:50,  1.69s/it]Epoch 8:  79%|███████▊  | 236/300 [06:42<01:51,  1.75s/it]Epoch 8:  79%|███████▉  | 237/300 [06:44<01:46,  1.68s/it]Epoch 8:  79%|███████▉  | 238/300 [06:45<01:40,  1.63s/it]Epoch 8:  80%|███████▉  | 239/300 [06:47<01:37,  1.60s/it]06/19/2022 15:53:40 - INFO - __main__ - global step: 1320; train loss: 7.765472412109375; dev loss
Epoch 8:  80%|████████  | 240/300 [06:49<01:42,  1.71s/it]Epoch 8:  80%|████████  | 241/300 [06:50<01:37,  1.65s/it]Epoch 8:  81%|████████  | 242/300 [06:52<01:34,  1.62s/it]Epoch 8:  81%|████████  | 243/300 [06:53<01:36,  1.70s/it]Epoch 8:  81%|████████▏ | 244/300 [06:55<01:39,  1.78s/it]Epoch 8:  82%|████████▏ | 245/300 [06:57<01:32,  1.68s/it]Epoch 8:  82%|████████▏ | 246/300 [06:58<01:28,  1.64s/it]Epoch 8:  82%|████████▏ | 247/300 [07:00<01:30,  1.70s/it]Epoch 8:  83%|████████▎ | 248/300 [07:02<01:26,  1.66s/it]Epoch 8:  83%|████████▎ | 249/300 [07:04<01:28,  1.73s/it]Epoch 8:  83%|████████▎ | 250/300 [07:06<01:28,  1.77s/it]Epoch 8:  84%|████████▎ | 251/300 [07:07<01:28,  1.80s/it]Epoch 8:  84%|████████▍ | 252/300 [07:09<01:23,  1.75s/it]Epoch 8:  84%|████████▍ | 253/300 [07:11<01:18,  1.67s/it]Epoch 8:  85%|████████▍ | 254/300 [07:13<01:20,  1.75s/it]Epoch 8:  85%|████████▌ | 255/300 [07:14<01:16,  1.70s/it]Epoch 8:  85%|████████▌ | 256/300 [07:16<01:18,  1.78s/it]Epoch 8:  86%|████████▌ | 257/300 [07:18<01:12,  1.69s/it]Epoch 8:  86%|████████▌ | 258/300 [07:19<01:08,  1.63s/it]Epoch 8:  86%|████████▋ | 259/300 [07:21<01:09,  1.70s/it]06/19/2022 15:54:14 - INFO - __main__ - global step: 1330; train loss: 7.765595436096191; dev loss
Epoch 8:  87%|████████▋ | 260/300 [07:23<01:11,  1.78s/it]Epoch 8:  87%|████████▋ | 261/300 [07:25<01:10,  1.81s/it]Epoch 8:  87%|████████▋ | 262/300 [07:26<01:05,  1.71s/it]Epoch 8:  88%|████████▊ | 263/300 [07:28<01:04,  1.75s/it]Epoch 8:  88%|████████▊ | 264/300 [07:30<01:00,  1.68s/it]Epoch 8:  88%|████████▊ | 265/300 [07:32<01:02,  1.78s/it]Epoch 8:  89%|████████▊ | 266/300 [07:33<00:58,  1.71s/it]Epoch 8:  89%|████████▉ | 267/300 [07:35<00:57,  1.75s/it]Epoch 8:  89%|████████▉ | 268/300 [07:37<00:57,  1.78s/it]Epoch 8:  90%|████████▉ | 269/300 [07:38<00:53,  1.73s/it]Epoch 8:  90%|█████████ | 270/300 [07:40<00:49,  1.67s/it]Epoch 8:  90%|█████████ | 271/300 [07:41<00:46,  1.62s/it]Epoch 8:  91%|█████████ | 272/300 [07:43<00:44,  1.59s/it]Epoch 8:  91%|█████████ | 273/300 [07:45<00:42,  1.59s/it]Epoch 8:  91%|█████████▏| 274/300 [07:46<00:40,  1.56s/it]Epoch 8:  92%|█████████▏| 275/300 [07:48<00:41,  1.65s/it]Epoch 8:  92%|█████████▏| 276/300 [07:50<00:41,  1.71s/it]Epoch 8:  92%|█████████▏| 277/300 [07:52<00:40,  1.78s/it]Epoch 8:  93%|█████████▎| 278/300 [07:53<00:37,  1.69s/it]Epoch 8:  93%|█████████▎| 279/300 [07:55<00:36,  1.74s/it]06/19/2022 15:54:48 - INFO - __main__ - global step: 1340; train loss: 7.929650783538818; dev loss
Epoch 8:  93%|█████████▎| 280/300 [07:57<00:35,  1.77s/it]Epoch 8:  94%|█████████▎| 281/300 [07:59<00:32,  1.72s/it]Epoch 8:  94%|█████████▍| 282/300 [08:00<00:29,  1.64s/it]Epoch 8:  94%|█████████▍| 283/300 [08:02<00:28,  1.70s/it]Epoch 8:  95%|█████████▍| 284/300 [08:04<00:27,  1.75s/it]Epoch 8:  95%|█████████▌| 285/300 [08:06<00:27,  1.83s/it]Epoch 8:  95%|█████████▌| 286/300 [08:08<00:25,  1.83s/it]Epoch 8:  96%|█████████▌| 287/300 [08:09<00:23,  1.84s/it]Epoch 8:  96%|█████████▌| 288/300 [08:11<00:20,  1.74s/it]Epoch 8:  96%|█████████▋| 289/300 [08:12<00:18,  1.69s/it]Epoch 8:  97%|█████████▋| 290/300 [08:14<00:17,  1.74s/it]Epoch 8:  97%|█████████▋| 291/300 [08:16<00:15,  1.77s/it]Epoch 8:  97%|█████████▋| 292/300 [08:18<00:13,  1.68s/it]Epoch 8:  98%|█████████▊| 293/300 [08:19<00:11,  1.61s/it]Epoch 8:  98%|█████████▊| 294/300 [08:21<00:09,  1.60s/it]Epoch 8:  98%|█████████▊| 295/300 [08:23<00:08,  1.67s/it]Epoch 8:  99%|█████████▊| 296/300 [08:24<00:06,  1.72s/it]Epoch 8:  99%|█████████▉| 297/300 [08:26<00:04,  1.64s/it]Epoch 8:  99%|█████████▉| 298/300 [08:28<00:03,  1.74s/it]Epoch 8: 100%|█████████▉| 299/300 [08:29<00:01,  1.67s/it]06/19/2022 15:55:23 - INFO - __main__ - global step: 1350; train loss: 8.193912506103516; dev loss
Epoch 8: 100%|██████████| 300/300 [08:31<00:00,  1.73s/it]Epoch 8: 100%|██████████| 300/300 [08:31<00:00,  1.71s/it]
Epoch 9:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 9:   0%|          | 1/300 [00:01<09:19,  1.87s/it]Epoch 9:   1%|          | 2/300 [00:03<09:32,  1.92s/it]Epoch 9:   1%|          | 3/300 [00:05<09:21,  1.89s/it]Epoch 9:   1%|▏         | 4/300 [00:07<09:15,  1.88s/it]Epoch 9:   2%|▏         | 5/300 [00:08<08:29,  1.73s/it]Epoch 9:   2%|▏         | 6/300 [00:10<08:14,  1.68s/it]Epoch 9:   2%|▏         | 7/300 [00:12<08:29,  1.74s/it]Epoch 9:   3%|▎         | 8/300 [00:13<08:03,  1.66s/it]Epoch 9:   3%|▎         | 9/300 [00:15<07:48,  1.61s/it]Epoch 9:   3%|▎         | 10/300 [00:17<08:18,  1.72s/it]Epoch 9:   4%|▎         | 11/300 [00:18<07:56,  1.65s/it]Epoch 9:   4%|▍         | 12/300 [00:20<08:12,  1.71s/it]Epoch 9:   4%|▍         | 13/300 [00:22<08:22,  1.75s/it]Epoch 9:   5%|▍         | 14/300 [00:24<08:41,  1.82s/it]Epoch 9:   5%|▌         | 15/300 [00:26<08:42,  1.83s/it]Epoch 9:   5%|▌         | 16/300 [00:27<08:09,  1.72s/it]Epoch 9:   6%|▌         | 17/300 [00:29<08:19,  1.76s/it]Epoch 9:   6%|▌         | 18/300 [00:31<07:57,  1.69s/it]Epoch 9:   6%|▋         | 19/300 [00:33<08:16,  1.77s/it]06/19/2022 15:55:58 - INFO - __main__ - global step: 1360; train loss: 7.731121063232422; dev loss
Epoch 9:   7%|▋         | 20/300 [00:35<08:22,  1.80s/it]Epoch 9:   7%|▋         | 21/300 [00:36<08:25,  1.81s/it]Epoch 9:   7%|▋         | 22/300 [00:38<07:55,  1.71s/it]Epoch 9:   8%|▊         | 23/300 [00:40<08:14,  1.78s/it]Epoch 9:   8%|▊         | 24/300 [00:41<07:45,  1.69s/it]Epoch 9:   8%|▊         | 25/300 [00:43<07:28,  1.63s/it]Epoch 9:   9%|▊         | 26/300 [00:44<07:13,  1.58s/it]Epoch 9:   9%|▉         | 27/300 [00:46<07:42,  1.69s/it]Epoch 9:   9%|▉         | 28/300 [00:48<07:53,  1.74s/it]Epoch 9:  10%|▉         | 29/300 [00:50<08:02,  1.78s/it]Epoch 9:  10%|█         | 30/300 [00:52<08:06,  1.80s/it]Epoch 9:  10%|█         | 31/300 [00:53<07:47,  1.74s/it]Epoch 9:  11%|█         | 32/300 [00:55<07:53,  1.77s/it]Epoch 9:  11%|█         | 33/300 [00:57<08:00,  1.80s/it]Epoch 9:  11%|█▏        | 34/300 [00:59<08:05,  1.82s/it]Epoch 9:  12%|█▏        | 35/300 [01:01<08:13,  1.86s/it]Epoch 9:  12%|█▏        | 36/300 [01:02<07:41,  1.75s/it]Epoch 9:  12%|█▏        | 37/300 [01:04<07:20,  1.68s/it]Epoch 9:  13%|█▎        | 38/300 [01:05<07:04,  1.62s/it]Epoch 9:  13%|█▎        | 39/300 [01:07<07:29,  1.72s/it]06/19/2022 15:56:32 - INFO - __main__ - global step: 1370; train loss: 7.5496697425842285; dev loss
Epoch 9:  13%|█▎        | 40/300 [01:09<07:09,  1.65s/it]Epoch 9:  14%|█▎        | 41/300 [01:10<06:54,  1.60s/it]Epoch 9:  14%|█▍        | 42/300 [01:12<07:12,  1.68s/it]Epoch 9:  14%|█▍        | 43/300 [01:14<07:35,  1.77s/it]Epoch 9:  15%|█▍        | 44/300 [01:16<07:39,  1.80s/it]Epoch 9:  15%|█▌        | 45/300 [01:18<07:13,  1.70s/it]Epoch 9:  15%|█▌        | 46/300 [01:19<07:24,  1.75s/it]Epoch 9:  16%|█▌        | 47/300 [01:21<07:02,  1.67s/it]Epoch 9:  16%|█▌        | 48/300 [01:23<06:56,  1.65s/it]Epoch 9:  16%|█▋        | 49/300 [01:24<07:09,  1.71s/it]Epoch 9:  17%|█▋        | 50/300 [01:26<06:51,  1.65s/it]Epoch 9:  17%|█▋        | 51/300 [01:27<06:37,  1.60s/it]Epoch 9:  17%|█▋        | 52/300 [01:29<06:35,  1.60s/it]Epoch 9:  18%|█▊        | 53/300 [01:31<06:53,  1.67s/it]Epoch 9:  18%|█▊        | 54/300 [01:32<06:36,  1.61s/it]Epoch 9:  18%|█▊        | 55/300 [01:34<06:24,  1.57s/it]Epoch 9:  19%|█▊        | 56/300 [01:35<06:26,  1.59s/it]Epoch 9:  19%|█▉        | 57/300 [01:37<06:21,  1.57s/it]Epoch 9:  19%|█▉        | 58/300 [01:38<06:13,  1.54s/it]Epoch 9:  20%|█▉        | 59/300 [01:40<06:07,  1.53s/it]06/19/2022 15:57:05 - INFO - __main__ - global step: 1380; train loss: 8.040223121643066; dev loss
Epoch 9:  20%|██        | 60/300 [01:42<06:38,  1.66s/it]Epoch 9:  20%|██        | 61/300 [01:43<06:23,  1.60s/it]Epoch 9:  21%|██        | 62/300 [01:45<06:39,  1.68s/it]Epoch 9:  21%|██        | 63/300 [01:47<06:24,  1.62s/it]Epoch 9:  21%|██▏       | 64/300 [01:49<06:48,  1.73s/it]Epoch 9:  22%|██▏       | 65/300 [01:50<06:28,  1.66s/it]Epoch 9:  22%|██▏       | 66/300 [01:52<06:14,  1.60s/it]Epoch 9:  22%|██▏       | 67/300 [01:53<06:04,  1.56s/it]Epoch 9:  23%|██▎       | 68/300 [01:55<06:30,  1.68s/it]Epoch 9:  23%|██▎       | 69/300 [01:57<06:42,  1.74s/it]Epoch 9:  23%|██▎       | 70/300 [01:58<06:22,  1.66s/it]Epoch 9:  24%|██▎       | 71/300 [02:00<06:08,  1.61s/it]Epoch 9:  24%|██▍       | 72/300 [02:01<05:56,  1.56s/it]Epoch 9:  24%|██▍       | 73/300 [02:03<05:56,  1.57s/it]Epoch 9:  25%|██▍       | 74/300 [02:04<05:48,  1.54s/it]Epoch 9:  25%|██▌       | 75/300 [02:06<05:41,  1.52s/it]Epoch 9:  25%|██▌       | 76/300 [02:07<05:37,  1.50s/it]Epoch 9:  26%|██▌       | 77/300 [02:09<06:07,  1.65s/it]Epoch 9:  26%|██▌       | 78/300 [02:11<05:54,  1.60s/it]Epoch 9:  26%|██▋       | 79/300 [02:12<05:44,  1.56s/it]06/19/2022 15:57:37 - INFO - __main__ - global step: 1390; train loss: 7.9129133224487305; dev loss
Epoch 9:  27%|██▋       | 80/300 [02:14<05:40,  1.55s/it]Epoch 9:  27%|██▋       | 81/300 [02:15<05:44,  1.57s/it]Epoch 9:  27%|██▋       | 82/300 [02:17<05:38,  1.55s/it]Epoch 9:  28%|██▊       | 83/300 [02:18<05:31,  1.53s/it]Epoch 9:  28%|██▊       | 84/300 [02:20<05:50,  1.62s/it]Epoch 9:  28%|██▊       | 85/300 [02:22<06:11,  1.73s/it]Epoch 9:  29%|██▊       | 86/300 [02:24<05:54,  1.66s/it]Epoch 9:  29%|██▉       | 87/300 [02:26<06:04,  1.71s/it]Epoch 9:  29%|██▉       | 88/300 [02:27<05:48,  1.65s/it]Epoch 9:  30%|██▉       | 89/300 [02:29<05:45,  1.64s/it]Epoch 9:  30%|███       | 90/300 [02:30<05:35,  1.60s/it]Epoch 9:  30%|███       | 91/300 [02:32<05:51,  1.68s/it]Epoch 9:  31%|███       | 92/300 [02:34<06:02,  1.74s/it]Epoch 9:  31%|███       | 93/300 [02:35<05:51,  1.70s/it]Epoch 9:  31%|███▏      | 94/300 [02:37<05:35,  1.63s/it]Epoch 9:  32%|███▏      | 95/300 [02:39<05:48,  1.70s/it]Epoch 9:  32%|███▏      | 96/300 [02:41<05:56,  1.75s/it]Epoch 9:  32%|███▏      | 97/300 [02:43<06:07,  1.81s/it]Epoch 9:  33%|███▎      | 98/300 [02:44<06:08,  1.82s/it]Epoch 9:  33%|███▎      | 99/300 [02:46<06:09,  1.84s/it]06/19/2022 15:58:11 - INFO - __main__ - global step: 1400; train loss: 8.07032585144043; dev loss
Epoch 9:  33%|███▎      | 100/300 [02:48<05:48,  1.74s/it]Epoch 9:  34%|███▎      | 101/300 [02:50<05:54,  1.78s/it]Epoch 9:  34%|███▍      | 102/300 [02:52<06:05,  1.84s/it]Epoch 9:  34%|███▍      | 103/300 [02:53<05:41,  1.73s/it]Epoch 9:  35%|███▍      | 104/300 [02:55<05:24,  1.66s/it]Epoch 9:  35%|███▌      | 105/300 [02:57<05:37,  1.73s/it]Epoch 9:  35%|███▌      | 106/300 [02:58<05:30,  1.70s/it]Epoch 9:  36%|███▌      | 107/300 [03:00<05:18,  1.65s/it]Epoch 9:  36%|███▌      | 108/300 [03:01<05:09,  1.61s/it]Epoch 9:  36%|███▋      | 109/300 [03:03<05:26,  1.71s/it]Epoch 9:  37%|███▋      | 110/300 [03:05<05:43,  1.81s/it]Epoch 9:  37%|███▋      | 111/300 [03:07<05:23,  1.71s/it]Epoch 9:  37%|███▋      | 112/300 [03:09<05:31,  1.76s/it]Epoch 9:  38%|███▊      | 113/300 [03:10<05:34,  1.79s/it]Epoch 9:  38%|███▊      | 114/300 [03:12<05:42,  1.84s/it]Epoch 9:  38%|███▊      | 115/300 [03:14<05:41,  1.85s/it]Epoch 9:  39%|███▊      | 116/300 [03:16<05:42,  1.86s/it]Epoch 9:  39%|███▉      | 117/300 [03:18<05:40,  1.86s/it]Epoch 9:  39%|███▉      | 118/300 [03:20<05:25,  1.79s/it]Epoch 9:  40%|███▉      | 119/300 [03:21<05:08,  1.70s/it]06/19/2022 15:58:46 - INFO - __main__ - global step: 1410; train loss: 7.641329765319824; dev loss
Epoch 9:  40%|████      | 120/300 [03:23<05:14,  1.74s/it]Epoch 9:  40%|████      | 121/300 [03:25<05:19,  1.78s/it]Epoch 9:  41%|████      | 122/300 [03:27<05:28,  1.84s/it]Epoch 9:  41%|████      | 123/300 [03:29<05:28,  1.85s/it]Epoch 9:  41%|████▏     | 124/300 [03:30<05:06,  1.74s/it]Epoch 9:  42%|████▏     | 125/300 [03:32<04:50,  1.66s/it]Epoch 9:  42%|████▏     | 126/300 [03:33<04:39,  1.60s/it]Epoch 9:  42%|████▏     | 127/300 [03:35<04:38,  1.61s/it]Epoch 9:  43%|████▎     | 128/300 [03:37<04:51,  1.69s/it]Epoch 9:  43%|████▎     | 129/300 [03:39<04:58,  1.74s/it]Epoch 9:  43%|████▎     | 130/300 [03:40<05:02,  1.78s/it]Epoch 9:  44%|████▎     | 131/300 [03:42<05:10,  1.84s/it]Epoch 9:  44%|████▍     | 132/300 [03:44<05:09,  1.84s/it]Epoch 9:  44%|████▍     | 133/300 [03:46<04:52,  1.75s/it]Epoch 9:  45%|████▍     | 134/300 [03:47<04:38,  1.68s/it]Epoch 9:  45%|████▌     | 135/300 [03:49<04:33,  1.66s/it]Epoch 9:  45%|████▌     | 136/300 [03:51<04:41,  1.72s/it]Epoch 9:  46%|████▌     | 137/300 [03:53<04:46,  1.76s/it]Epoch 9:  46%|████▌     | 138/300 [03:54<04:50,  1.79s/it]Epoch 9:  46%|████▋     | 139/300 [03:56<04:39,  1.73s/it]06/19/2022 15:59:21 - INFO - __main__ - global step: 1420; train loss: 7.90765905380249; dev loss
Epoch 9:  47%|████▋     | 140/300 [03:58<04:45,  1.78s/it]Epoch 9:  47%|████▋     | 141/300 [03:59<04:31,  1.71s/it]Epoch 9:  47%|████▋     | 142/300 [04:01<04:39,  1.77s/it]Epoch 9:  48%|████▊     | 143/300 [04:03<04:49,  1.84s/it]Epoch 9:  48%|████▊     | 144/300 [04:05<04:50,  1.86s/it]Epoch 9:  48%|████▊     | 145/300 [04:07<04:49,  1.87s/it]Epoch 9:  49%|████▊     | 146/300 [04:09<04:50,  1.89s/it]Epoch 9:  49%|████▉     | 147/300 [04:11<04:55,  1.93s/it]Epoch 9:  49%|████▉     | 148/300 [04:13<04:33,  1.80s/it]Epoch 9:  50%|████▉     | 149/300 [04:14<04:18,  1.71s/it]Epoch 9:  50%|█████     | 150/300 [04:16<04:25,  1.77s/it]Epoch 9:  50%|█████     | 151/300 [04:18<04:34,  1.84s/it]Epoch 9:  51%|█████     | 152/300 [04:20<04:35,  1.86s/it]Epoch 9:  51%|█████     | 153/300 [04:22<04:18,  1.76s/it]Epoch 9:  51%|█████▏    | 154/300 [04:23<04:05,  1.68s/it]Epoch 9:  52%|█████▏    | 155/300 [04:25<04:13,  1.75s/it]Epoch 9:  52%|█████▏    | 156/300 [04:27<04:05,  1.71s/it]Epoch 9:  52%|█████▏    | 157/300 [04:28<03:55,  1.64s/it]Epoch 9:  53%|█████▎    | 158/300 [04:30<03:47,  1.60s/it]Epoch 9:  53%|█████▎    | 159/300 [04:31<03:59,  1.70s/it]06/19/2022 15:59:56 - INFO - __main__ - global step: 1430; train loss: 8.153380393981934; dev loss
Epoch 9:  53%|█████▎    | 160/300 [04:33<04:11,  1.79s/it]Epoch 9:  54%|█████▎    | 161/300 [04:35<03:57,  1.71s/it]Epoch 9:  54%|█████▍    | 162/300 [04:37<04:04,  1.77s/it]Epoch 9:  54%|█████▍    | 163/300 [04:38<03:51,  1.69s/it]Epoch 9:  55%|█████▍    | 164/300 [04:40<03:47,  1.68s/it]Epoch 9:  55%|█████▌    | 165/300 [04:42<03:39,  1.62s/it]Epoch 9:  55%|█████▌    | 166/300 [04:43<03:32,  1.59s/it]Epoch 9:  56%|█████▌    | 167/300 [04:45<03:26,  1.56s/it]Epoch 9:  56%|█████▌    | 168/300 [04:47<03:43,  1.69s/it]Epoch 9:  56%|█████▋    | 169/300 [04:48<03:51,  1.76s/it]Epoch 9:  57%|█████▋    | 170/300 [04:50<03:54,  1.81s/it]Epoch 9:  57%|█████▋    | 171/300 [04:52<03:56,  1.83s/it]Epoch 9:  57%|█████▋    | 172/300 [04:54<03:48,  1.79s/it]Epoch 9:  58%|█████▊    | 173/300 [04:55<03:35,  1.70s/it]Epoch 9:  58%|█████▊    | 174/300 [04:57<03:39,  1.74s/it]Epoch 9:  58%|█████▊    | 175/300 [04:59<03:30,  1.68s/it]Epoch 9:  59%|█████▊    | 176/300 [05:00<03:25,  1.66s/it]Epoch 9:  59%|█████▉    | 177/300 [05:02<03:31,  1.72s/it]Epoch 9:  59%|█████▉    | 178/300 [05:04<03:36,  1.77s/it]Epoch 9:  60%|█████▉    | 179/300 [05:06<03:39,  1.82s/it]06/19/2022 16:00:31 - INFO - __main__ - global step: 1440; train loss: 8.550153732299805; dev loss
Epoch 9:  60%|██████    | 180/300 [05:08<03:27,  1.73s/it]Epoch 9:  60%|██████    | 181/300 [05:10<03:35,  1.81s/it]Epoch 9:  61%|██████    | 182/300 [05:12<03:36,  1.84s/it]Epoch 9:  61%|██████    | 183/300 [05:13<03:23,  1.74s/it]Epoch 9:  61%|██████▏   | 184/300 [05:15<03:14,  1.68s/it]Epoch 9:  62%|██████▏   | 185/300 [05:16<03:09,  1.65s/it]Epoch 9:  62%|██████▏   | 186/300 [05:18<03:15,  1.72s/it]Epoch 9:  62%|██████▏   | 187/300 [05:20<03:07,  1.66s/it]Epoch 9:  63%|██████▎   | 188/300 [05:21<03:13,  1.73s/it]Epoch 9:  63%|██████▎   | 189/300 [05:23<03:21,  1.82s/it]Epoch 9:  63%|██████▎   | 190/300 [05:25<03:09,  1.73s/it]Epoch 9:  64%|██████▎   | 191/300 [05:26<02:59,  1.65s/it]Epoch 9:  64%|██████▍   | 192/300 [05:28<02:54,  1.61s/it]Epoch 9:  64%|██████▍   | 193/300 [05:30<02:52,  1.61s/it]Epoch 9:  65%|██████▍   | 194/300 [05:31<02:59,  1.69s/it]Epoch 9:  65%|██████▌   | 195/300 [05:33<02:51,  1.64s/it]Epoch 9:  65%|██████▌   | 196/300 [05:34<02:46,  1.60s/it]Epoch 9:  66%|██████▌   | 197/300 [05:37<02:57,  1.72s/it]Epoch 9:  66%|██████▌   | 198/300 [05:38<03:01,  1.78s/it]Epoch 9:  66%|██████▋   | 199/300 [05:40<03:03,  1.82s/it]06/19/2022 16:01:05 - INFO - __main__ - global step: 1450; train loss: 7.872819423675537; dev loss
Epoch 9:  67%|██████▋   | 200/300 [05:42<03:04,  1.84s/it]Epoch 9:  67%|██████▋   | 201/300 [05:44<03:07,  1.90s/it]Epoch 9:  67%|██████▋   | 202/300 [05:46<02:55,  1.79s/it]Epoch 9:  68%|██████▊   | 203/300 [05:48<02:55,  1.81s/it]Epoch 9:  68%|██████▊   | 204/300 [05:49<02:45,  1.72s/it]Epoch 9:  68%|██████▊   | 205/300 [05:51<02:40,  1.69s/it]Epoch 9:  69%|██████▊   | 206/300 [05:53<02:44,  1.75s/it]Epoch 9:  69%|██████▉   | 207/300 [05:54<02:35,  1.67s/it]Epoch 9:  69%|██████▉   | 208/300 [05:56<02:39,  1.74s/it]Epoch 9:  70%|██████▉   | 209/300 [05:58<02:31,  1.66s/it]Epoch 9:  70%|███████   | 210/300 [05:59<02:29,  1.66s/it]Epoch 9:  70%|███████   | 211/300 [06:01<02:33,  1.73s/it]Epoch 9:  71%|███████   | 212/300 [06:03<02:37,  1.79s/it]Epoch 9:  71%|███████   | 213/300 [06:04<02:28,  1.70s/it]Epoch 9:  71%|███████▏  | 214/300 [06:07<02:34,  1.80s/it]Epoch 9:  72%|███████▏  | 215/300 [06:08<02:25,  1.72s/it]Epoch 9:  72%|███████▏  | 216/300 [06:10<02:18,  1.65s/it]Epoch 9:  72%|███████▏  | 217/300 [06:11<02:13,  1.61s/it]Epoch 9:  73%|███████▎  | 218/300 [06:13<02:12,  1.61s/it]Epoch 9:  73%|███████▎  | 219/300 [06:15<02:17,  1.70s/it]06/19/2022 16:01:39 - INFO - __main__ - global step: 1460; train loss: 8.049263000488281; dev loss
Epoch 9:  73%|███████▎  | 220/300 [06:16<02:20,  1.76s/it]Epoch 9:  74%|███████▎  | 221/300 [06:18<02:13,  1.68s/it]Epoch 9:  74%|███████▍  | 222/300 [06:20<02:19,  1.79s/it]Epoch 9:  74%|███████▍  | 223/300 [06:22<02:11,  1.70s/it]Epoch 9:  75%|███████▍  | 224/300 [06:23<02:05,  1.65s/it]Epoch 9:  75%|███████▌  | 225/300 [06:25<02:09,  1.73s/it]Epoch 9:  75%|███████▌  | 226/300 [06:27<02:05,  1.69s/it]Epoch 9:  76%|███████▌  | 227/300 [06:28<02:08,  1.76s/it]Epoch 9:  76%|███████▌  | 228/300 [06:30<02:09,  1.80s/it]Epoch 9:  76%|███████▋  | 229/300 [06:32<02:10,  1.84s/it]Epoch 9:  77%|███████▋  | 230/300 [06:34<02:04,  1.77s/it]Epoch 9:  77%|███████▋  | 231/300 [06:35<01:56,  1.68s/it]Epoch 9:  77%|███████▋  | 232/300 [06:37<01:50,  1.62s/it]Epoch 9:  78%|███████▊  | 233/300 [06:39<01:53,  1.70s/it]Epoch 9:  78%|███████▊  | 234/300 [06:41<01:55,  1.75s/it]Epoch 9:  78%|███████▊  | 235/300 [06:43<01:58,  1.82s/it]Epoch 9:  79%|███████▊  | 236/300 [06:44<01:57,  1.84s/it]Epoch 9:  79%|███████▉  | 237/300 [06:46<01:49,  1.74s/it]Epoch 9:  79%|███████▉  | 238/300 [06:48<01:50,  1.77s/it]Epoch 9:  80%|███████▉  | 239/300 [06:50<01:46,  1.74s/it]06/19/2022 16:02:14 - INFO - __main__ - global step: 1470; train loss: 8.256035804748535; dev loss
Epoch 9:  80%|████████  | 240/300 [06:51<01:47,  1.78s/it]Epoch 9:  80%|████████  | 241/300 [06:53<01:47,  1.81s/it]Epoch 9:  81%|████████  | 242/300 [06:55<01:44,  1.81s/it]Epoch 9:  81%|████████  | 243/300 [06:57<01:44,  1.84s/it]Epoch 9:  81%|████████▏ | 244/300 [06:59<01:38,  1.76s/it]Epoch 9:  82%|████████▏ | 245/300 [07:00<01:38,  1.80s/it]Epoch 9:  82%|████████▏ | 246/300 [07:02<01:38,  1.82s/it]Epoch 9:  82%|████████▏ | 247/300 [07:04<01:39,  1.88s/it]Epoch 9:  83%|████████▎ | 248/300 [07:06<01:38,  1.89s/it]Epoch 9:  83%|████████▎ | 249/300 [07:08<01:30,  1.77s/it]Epoch 9:  83%|████████▎ | 250/300 [07:09<01:24,  1.69s/it]Epoch 9:  84%|████████▎ | 251/300 [07:11<01:21,  1.66s/it]Epoch 9:  84%|████████▍ | 252/300 [07:13<01:22,  1.72s/it]Epoch 9:  84%|████████▍ | 253/300 [07:15<01:23,  1.77s/it]Epoch 9:  85%|████████▍ | 254/300 [07:16<01:17,  1.69s/it]Epoch 9:  85%|████████▌ | 255/300 [07:18<01:19,  1.78s/it]Epoch 9:  85%|████████▌ | 256/300 [07:20<01:14,  1.69s/it]Epoch 9:  86%|████████▌ | 257/300 [07:21<01:10,  1.63s/it]Epoch 9:  86%|████████▌ | 258/300 [07:23<01:06,  1.59s/it]Epoch 9:  86%|████████▋ | 259/300 [07:25<01:10,  1.72s/it]06/19/2022 16:02:49 - INFO - __main__ - global step: 1480; train loss: 7.601995944976807; dev loss
Epoch 9:  87%|████████▋ | 260/300 [07:26<01:10,  1.77s/it]Epoch 9:  87%|████████▋ | 261/300 [07:28<01:05,  1.68s/it]Epoch 9:  87%|████████▋ | 262/300 [07:30<01:06,  1.74s/it]Epoch 9:  88%|████████▊ | 263/300 [07:32<01:06,  1.79s/it]Epoch 9:  88%|████████▊ | 264/300 [07:34<01:06,  1.85s/it]Epoch 9:  88%|████████▊ | 265/300 [07:35<01:00,  1.74s/it]Epoch 9:  89%|████████▊ | 266/300 [07:37<00:56,  1.66s/it]Epoch 9:  89%|████████▉ | 267/300 [07:39<00:57,  1.73s/it]Epoch 9:  89%|████████▉ | 268/300 [07:41<00:57,  1.81s/it]Epoch 9:  90%|████████▉ | 269/300 [07:42<00:56,  1.83s/it]Epoch 9:  90%|█████████ | 270/300 [07:44<00:52,  1.74s/it]Epoch 9:  90%|█████████ | 271/300 [07:45<00:48,  1.67s/it]Epoch 9:  91%|█████████ | 272/300 [07:47<00:45,  1.64s/it]Epoch 9:  91%|█████████ | 273/300 [07:49<00:46,  1.72s/it]Epoch 9:  91%|█████████▏| 274/300 [07:50<00:42,  1.65s/it]Epoch 9:  92%|█████████▏| 275/300 [07:52<00:40,  1.60s/it]Epoch 9:  92%|█████████▏| 276/300 [07:54<00:41,  1.73s/it]Epoch 9:  92%|█████████▏| 277/300 [07:55<00:38,  1.65s/it]Epoch 9:  93%|█████████▎| 278/300 [07:57<00:35,  1.62s/it]Epoch 9:  93%|█████████▎| 279/300 [07:58<00:33,  1.58s/it]06/19/2022 16:03:23 - INFO - __main__ - global step: 1490; train loss: 8.024378776550293; dev loss
Epoch 9:  93%|█████████▎| 280/300 [08:00<00:31,  1.59s/it]Epoch 9:  94%|█████████▎| 281/300 [08:02<00:31,  1.68s/it]Epoch 9:  94%|█████████▍| 282/300 [08:03<00:29,  1.62s/it]Epoch 9:  94%|█████████▍| 283/300 [08:05<00:26,  1.58s/it]Epoch 9:  95%|█████████▍| 284/300 [08:07<00:27,  1.70s/it]Epoch 9:  95%|█████████▌| 285/300 [08:09<00:26,  1.75s/it]Epoch 9:  95%|█████████▌| 286/300 [08:11<00:25,  1.80s/it]Epoch 9:  96%|█████████▌| 287/300 [08:13<00:23,  1.84s/it]Epoch 9:  96%|█████████▌| 288/300 [08:14<00:22,  1.85s/it]Epoch 9:  96%|█████████▋| 289/300 [08:16<00:19,  1.80s/it]Epoch 9:  97%|█████████▋| 290/300 [08:18<00:16,  1.70s/it]Epoch 9:  97%|█████████▋| 291/300 [08:19<00:15,  1.73s/it]Epoch 9:  97%|█████████▋| 292/300 [08:21<00:13,  1.66s/it]Epoch 9:  98%|█████████▊| 293/300 [08:23<00:12,  1.76s/it]Epoch 9:  98%|█████████▊| 294/300 [08:25<00:10,  1.80s/it]Epoch 9:  98%|█████████▊| 295/300 [08:27<00:09,  1.84s/it]Epoch 9:  99%|█████████▊| 296/300 [08:28<00:06,  1.73s/it]Epoch 9:  99%|█████████▉| 297/300 [08:30<00:05,  1.82s/it]Epoch 9:  99%|█████████▉| 298/300 [08:32<00:03,  1.72s/it]Epoch 9: 100%|█████████▉| 299/300 [08:33<00:01,  1.65s/it]06/19/2022 16:03:58 - INFO - __main__ - global step: 1500; train loss: 8.26707649230957; dev loss
Epoch 9: 100%|██████████| 300/300 [08:35<00:00,  1.61s/it]Epoch 9: 100%|██████████| 300/300 [08:35<00:00,  1.72s/it]
Epoch 10:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 10:   0%|          | 1/300 [00:01<09:55,  1.99s/it]Epoch 10:   1%|          | 2/300 [00:03<09:33,  1.92s/it]Epoch 10:   1%|          | 3/300 [00:05<09:25,  1.90s/it]Epoch 10:   1%|▏         | 4/300 [00:07<08:26,  1.71s/it]Epoch 10:   2%|▏         | 5/300 [00:09<08:47,  1.79s/it]Epoch 10:   2%|▏         | 6/300 [00:10<08:24,  1.71s/it]Epoch 10:   2%|▏         | 7/300 [00:12<08:14,  1.69s/it]Epoch 10:   3%|▎         | 8/300 [00:13<08:05,  1.66s/it]Epoch 10:   3%|▎         | 9/300 [00:15<08:10,  1.69s/it]Epoch 10:   3%|▎         | 10/300 [00:17<07:44,  1.60s/it]Epoch 10:   4%|▎         | 11/300 [00:18<07:44,  1.61s/it]Epoch 10:   4%|▍         | 12/300 [00:20<07:43,  1.61s/it]Epoch 10:   4%|▍         | 13/300 [00:21<07:47,  1.63s/it]Epoch 10:   5%|▍         | 14/300 [00:23<07:28,  1.57s/it]Epoch 10:   5%|▌         | 15/300 [00:24<07:17,  1.53s/it]Epoch 10:   5%|▌         | 16/300 [00:26<07:08,  1.51s/it]Epoch 10:   6%|▌         | 17/300 [00:27<06:59,  1.48s/it]Epoch 10:   6%|▌         | 18/300 [00:29<07:04,  1.51s/it]Epoch 10:   6%|▋         | 19/300 [00:30<07:09,  1.53s/it]06/19/2022 16:04:30 - INFO - __main__ - global step: 1510; train loss: 7.890671730041504; dev loss
Epoch 10:   7%|▋         | 20/300 [00:32<07:25,  1.59s/it]Epoch 10:   7%|▋         | 21/300 [00:34<07:44,  1.66s/it]Epoch 10:   7%|▋         | 22/300 [00:35<07:31,  1.62s/it]Epoch 10:   8%|▊         | 23/300 [00:37<07:47,  1.69s/it]Epoch 10:   8%|▊         | 24/300 [00:39<07:25,  1.61s/it]Epoch 10:   8%|▊         | 25/300 [00:40<07:07,  1.55s/it]Epoch 10:   9%|▊         | 26/300 [00:42<07:03,  1.55s/it]Epoch 10:   9%|▉         | 27/300 [00:44<07:25,  1.63s/it]Epoch 10:   9%|▉         | 28/300 [00:45<07:07,  1.57s/it]Epoch 10:  10%|▉         | 29/300 [00:46<06:53,  1.53s/it]Epoch 10:  10%|█         | 30/300 [00:48<06:53,  1.53s/it]Epoch 10:  10%|█         | 31/300 [00:50<07:17,  1.63s/it]Epoch 10:  11%|█         | 32/300 [00:51<06:59,  1.57s/it]Epoch 10:  11%|█         | 33/300 [00:53<06:46,  1.52s/it]Epoch 10:  11%|█▏        | 34/300 [00:55<07:16,  1.64s/it]Epoch 10:  12%|█▏        | 35/300 [00:56<07:29,  1.70s/it]Epoch 10:  12%|█▏        | 36/300 [00:58<07:05,  1.61s/it]Epoch 10:  12%|█▏        | 37/300 [00:59<06:47,  1.55s/it]Epoch 10:  13%|█▎        | 38/300 [01:01<06:43,  1.54s/it]Epoch 10:  13%|█▎        | 39/300 [01:02<06:32,  1.50s/it]06/19/2022 16:05:02 - INFO - __main__ - global step: 1520; train loss: 8.122659683227539; dev loss
Epoch 10:  13%|█▎        | 40/300 [01:04<06:23,  1.48s/it]Epoch 10:  14%|█▎        | 41/300 [01:05<06:18,  1.46s/it]Epoch 10:  14%|█▍        | 42/300 [01:07<06:46,  1.57s/it]Epoch 10:  14%|█▍        | 43/300 [01:08<06:40,  1.56s/it]Epoch 10:  15%|█▍        | 44/300 [01:10<06:26,  1.51s/it]Epoch 10:  15%|█▌        | 45/300 [01:11<06:29,  1.53s/it]Epoch 10:  15%|█▌        | 46/300 [01:13<06:33,  1.55s/it]Epoch 10:  16%|█▌        | 47/300 [01:15<06:43,  1.59s/it]Epoch 10:  16%|█▌        | 48/300 [01:16<06:42,  1.60s/it]Epoch 10:  16%|█▋        | 49/300 [01:18<06:40,  1.60s/it]Epoch 10:  17%|█▋        | 50/300 [01:20<06:51,  1.65s/it]Epoch 10:  17%|█▋        | 51/300 [01:21<06:39,  1.60s/it]Epoch 10:  17%|█▋        | 52/300 [01:22<06:24,  1.55s/it]Epoch 10:  18%|█▊        | 53/300 [01:24<06:14,  1.52s/it]Epoch 10:  18%|█▊        | 54/300 [01:26<06:38,  1.62s/it]Epoch 10:  18%|█▊        | 55/300 [01:27<06:34,  1.61s/it]Epoch 10:  19%|█▊        | 56/300 [01:29<06:24,  1.57s/it]Epoch 10:  19%|█▉        | 57/300 [01:30<06:16,  1.55s/it]Epoch 10:  19%|█▉        | 58/300 [01:32<06:39,  1.65s/it]Epoch 10:  20%|█▉        | 59/300 [01:34<06:36,  1.64s/it]06/19/2022 16:05:34 - INFO - __main__ - global step: 1530; train loss: 7.748631477355957; dev loss
Epoch 10:  20%|██        | 60/300 [01:35<06:22,  1.60s/it]Epoch 10:  20%|██        | 61/300 [01:37<06:13,  1.56s/it]Epoch 10:  21%|██        | 62/300 [01:38<06:11,  1.56s/it]Epoch 10:  21%|██        | 63/300 [01:40<06:40,  1.69s/it]Epoch 10:  21%|██▏       | 64/300 [01:42<06:54,  1.76s/it]Epoch 10:  22%|██▏       | 65/300 [01:44<07:02,  1.80s/it]Epoch 10:  22%|██▏       | 66/300 [01:46<07:08,  1.83s/it]Epoch 10:  22%|██▏       | 67/300 [01:48<06:52,  1.77s/it]Epoch 10:  23%|██▎       | 68/300 [01:50<07:01,  1.82s/it]Epoch 10:  23%|██▎       | 69/300 [01:52<07:04,  1.84s/it]Epoch 10:  23%|██▎       | 70/300 [01:53<07:07,  1.86s/it]Epoch 10:  24%|██▎       | 71/300 [01:55<07:09,  1.88s/it]Epoch 10:  24%|██▍       | 72/300 [01:57<07:17,  1.92s/it]Epoch 10:  24%|██▍       | 73/300 [01:59<06:45,  1.79s/it]Epoch 10:  25%|██▍       | 74/300 [02:01<06:39,  1.77s/it]Epoch 10:  25%|██▌       | 75/300 [02:02<06:20,  1.69s/it]Epoch 10:  25%|██▌       | 76/300 [02:04<06:12,  1.66s/it]Epoch 10:  26%|██▌       | 77/300 [02:06<06:27,  1.74s/it]Epoch 10:  26%|██▌       | 78/300 [02:07<06:11,  1.67s/it]Epoch 10:  26%|██▋       | 79/300 [02:09<05:58,  1.62s/it]06/19/2022 16:06:09 - INFO - __main__ - global step: 1540; train loss: 8.170841217041016; dev loss
Epoch 10:  27%|██▋       | 80/300 [02:11<06:22,  1.74s/it]Epoch 10:  27%|██▋       | 81/300 [02:13<06:31,  1.79s/it]Epoch 10:  27%|██▋       | 82/300 [02:14<06:13,  1.71s/it]Epoch 10:  28%|██▊       | 83/300 [02:16<06:22,  1.76s/it]Epoch 10:  28%|██▊       | 84/300 [02:18<06:11,  1.72s/it]Epoch 10:  28%|██▊       | 85/300 [02:19<06:19,  1.76s/it]Epoch 10:  29%|██▊       | 86/300 [02:21<06:25,  1.80s/it]Epoch 10:  29%|██▉       | 87/300 [02:23<06:03,  1.71s/it]Epoch 10:  29%|██▉       | 88/300 [02:25<06:20,  1.79s/it]Epoch 10:  30%|██▉       | 89/300 [02:26<06:00,  1.71s/it]Epoch 10:  30%|███       | 90/300 [02:28<05:44,  1.64s/it]Epoch 10:  30%|███       | 91/300 [02:30<05:58,  1.71s/it]Epoch 10:  31%|███       | 92/300 [02:32<06:14,  1.80s/it]Epoch 10:  31%|███       | 93/300 [02:33<05:52,  1.70s/it]Epoch 10:  31%|███▏      | 94/300 [02:35<06:02,  1.76s/it]Epoch 10:  32%|███▏      | 95/300 [02:37<06:09,  1.80s/it]Epoch 10:  32%|███▏      | 96/300 [02:38<05:49,  1.71s/it]Epoch 10:  32%|███▏      | 97/300 [02:40<06:05,  1.80s/it]Epoch 10:  33%|███▎      | 98/300 [02:42<06:09,  1.83s/it]Epoch 10:  33%|███▎      | 99/300 [02:44<05:47,  1.73s/it]06/19/2022 16:06:44 - INFO - __main__ - global step: 1550; train loss: 8.071283340454102; dev loss
Epoch 10:  33%|███▎      | 100/300 [02:46<05:53,  1.77s/it]Epoch 10:  34%|███▎      | 101/300 [02:48<06:04,  1.83s/it]Epoch 10:  34%|███▍      | 102/300 [02:49<05:42,  1.73s/it]Epoch 10:  34%|███▍      | 103/300 [02:51<05:48,  1.77s/it]Epoch 10:  35%|███▍      | 104/300 [02:53<05:54,  1.81s/it]Epoch 10:  35%|███▌      | 105/300 [02:55<05:40,  1.74s/it]Epoch 10:  35%|███▌      | 106/300 [02:56<05:47,  1.79s/it]Epoch 10:  36%|███▌      | 107/300 [02:58<05:50,  1.82s/it]Epoch 10:  36%|███▌      | 108/300 [03:00<05:30,  1.72s/it]Epoch 10:  36%|███▋      | 109/300 [03:02<05:43,  1.80s/it]Epoch 10:  37%|███▋      | 110/300 [03:04<05:48,  1.83s/it]Epoch 10:  37%|███▋      | 111/300 [03:05<05:41,  1.81s/it]Epoch 10:  37%|███▋      | 112/300 [03:07<05:31,  1.76s/it]Epoch 10:  38%|███▊      | 113/300 [03:09<05:32,  1.78s/it]Epoch 10:  38%|███▊      | 114/300 [03:11<05:20,  1.72s/it]Epoch 10:  38%|███▊      | 115/300 [03:12<05:31,  1.79s/it]Epoch 10:  39%|███▊      | 116/300 [03:14<05:22,  1.75s/it]Epoch 10:  39%|███▉      | 117/300 [03:16<05:34,  1.83s/it]Epoch 10:  39%|███▉      | 118/300 [03:18<05:27,  1.80s/it]Epoch 10:  40%|███▉      | 119/300 [03:19<05:16,  1.75s/it]06/19/2022 16:07:19 - INFO - __main__ - global step: 1560; train loss: 8.350580215454102; dev loss
Epoch 10:  40%|████      | 120/300 [03:21<05:08,  1.72s/it]Epoch 10:  40%|████      | 121/300 [03:23<05:13,  1.75s/it]Epoch 10:  41%|████      | 122/300 [03:25<05:21,  1.81s/it]Epoch 10:  41%|████      | 123/300 [03:27<05:29,  1.86s/it]Epoch 10:  41%|████▏     | 124/300 [03:29<05:15,  1.79s/it]Epoch 10:  42%|████▏     | 125/300 [03:30<05:19,  1.83s/it]Epoch 10:  42%|████▏     | 126/300 [03:32<05:13,  1.80s/it]Epoch 10:  42%|████▏     | 127/300 [03:34<05:03,  1.75s/it]Epoch 10:  43%|████▎     | 128/300 [03:35<04:55,  1.72s/it]Epoch 10:  43%|████▎     | 129/300 [03:37<05:05,  1.79s/it]Epoch 10:  43%|████▎     | 130/300 [03:39<05:01,  1.77s/it]Epoch 10:  44%|████▎     | 131/300 [03:41<04:55,  1.75s/it]Epoch 10:  44%|████▍     | 132/300 [03:43<05:02,  1.80s/it]Epoch 10:  44%|████▍     | 133/300 [03:45<05:07,  1.84s/it]Epoch 10:  45%|████▍     | 134/300 [03:47<05:04,  1.84s/it]Epoch 10:  45%|████▌     | 135/300 [03:48<04:49,  1.75s/it]Epoch 10:  45%|████▌     | 136/300 [03:50<04:55,  1.80s/it]Epoch 10:  46%|████▌     | 137/300 [03:51<04:37,  1.71s/it]Epoch 10:  46%|████▌     | 138/300 [03:53<04:44,  1.75s/it]Epoch 10:  46%|████▋     | 139/300 [03:55<04:28,  1.67s/it]06/19/2022 16:07:55 - INFO - __main__ - global step: 1570; train loss: 7.580631256103516; dev loss
Epoch 10:  47%|████▋     | 140/300 [03:57<04:37,  1.74s/it]Epoch 10:  47%|████▋     | 141/300 [03:59<04:44,  1.79s/it]Epoch 10:  47%|████▋     | 142/300 [04:00<04:29,  1.71s/it]Epoch 10:  48%|████▊     | 143/300 [04:02<04:16,  1.63s/it]Epoch 10:  48%|████▊     | 144/300 [04:03<04:04,  1.57s/it]Epoch 10:  48%|████▊     | 145/300 [04:05<04:10,  1.62s/it]Epoch 10:  49%|████▊     | 146/300 [04:06<04:14,  1.65s/it]Epoch 10:  49%|████▉     | 147/300 [04:08<04:02,  1.59s/it]Epoch 10:  49%|████▉     | 148/300 [04:09<04:01,  1.59s/it]Epoch 10:  50%|████▉     | 149/300 [04:11<04:00,  1.59s/it]Epoch 10:  50%|█████     | 150/300 [04:13<03:59,  1.60s/it]Epoch 10:  50%|█████     | 151/300 [04:14<03:54,  1.57s/it]Epoch 10:  51%|█████     | 152/300 [04:16<03:46,  1.53s/it]Epoch 10:  51%|█████     | 153/300 [04:17<03:41,  1.50s/it]Epoch 10:  51%|█████▏    | 154/300 [04:18<03:35,  1.47s/it]Epoch 10:  52%|█████▏    | 155/300 [04:20<03:35,  1.48s/it]Epoch 10:  52%|█████▏    | 156/300 [04:21<03:31,  1.47s/it]Epoch 10:  52%|█████▏    | 157/300 [04:23<03:27,  1.45s/it]Epoch 10:  53%|█████▎    | 158/300 [04:24<03:33,  1.50s/it]Epoch 10:  53%|█████▎    | 159/300 [04:26<03:40,  1.56s/it]06/19/2022 16:08:26 - INFO - __main__ - global step: 1580; train loss: 8.012955665588379; dev loss
Epoch 10:  53%|█████▎    | 160/300 [04:28<03:39,  1.57s/it]Epoch 10:  54%|█████▎    | 161/300 [04:29<03:31,  1.52s/it]Epoch 10:  54%|█████▍    | 162/300 [04:31<03:26,  1.49s/it]Epoch 10:  54%|█████▍    | 163/300 [04:32<03:25,  1.50s/it]Epoch 10:  55%|█████▍    | 164/300 [04:34<03:27,  1.52s/it]Epoch 10:  55%|█████▌    | 165/300 [04:35<03:22,  1.50s/it]Epoch 10:  55%|█████▌    | 166/300 [04:37<03:18,  1.48s/it]Epoch 10:  56%|█████▌    | 167/300 [04:38<03:18,  1.49s/it]Epoch 10:  56%|█████▌    | 168/300 [04:40<03:20,  1.52s/it]Epoch 10:  56%|█████▋    | 169/300 [04:41<03:22,  1.54s/it]Epoch 10:  57%|█████▋    | 170/300 [04:43<03:15,  1.51s/it]Epoch 10:  57%|█████▋    | 171/300 [04:44<03:22,  1.57s/it]Epoch 10:  57%|█████▋    | 172/300 [04:46<03:21,  1.57s/it]Epoch 10:  58%|█████▊    | 173/300 [04:47<03:15,  1.54s/it]Epoch 10:  58%|█████▊    | 174/300 [04:49<03:08,  1.50s/it]Epoch 10:  58%|█████▊    | 175/300 [04:50<03:08,  1.51s/it]Epoch 10:  59%|█████▊    | 176/300 [04:52<03:03,  1.48s/it]Epoch 10:  59%|█████▉    | 177/300 [04:53<02:59,  1.46s/it]Epoch 10:  59%|█████▉    | 178/300 [04:55<03:04,  1.51s/it]Epoch 10:  60%|█████▉    | 179/300 [04:56<02:59,  1.48s/it]06/19/2022 16:08:56 - INFO - __main__ - global step: 1590; train loss: 7.5634331703186035; dev loss
Epoch 10:  60%|██████    | 180/300 [04:58<03:05,  1.55s/it]Epoch 10:  60%|██████    | 181/300 [05:00<03:06,  1.57s/it]Epoch 10:  61%|██████    | 182/300 [05:01<02:59,  1.52s/it]Epoch 10:  61%|██████    | 183/300 [05:02<02:54,  1.49s/it]Epoch 10:  61%|██████▏   | 184/300 [05:04<02:59,  1.55s/it]Epoch 10:  62%|██████▏   | 185/300 [05:06<02:54,  1.52s/it]Epoch 10:  62%|██████▏   | 186/300 [05:07<02:55,  1.54s/it]Epoch 10:  62%|██████▏   | 187/300 [05:09<02:50,  1.51s/it]Epoch 10:  63%|██████▎   | 188/300 [05:10<02:50,  1.52s/it]Epoch 10:  63%|██████▎   | 189/300 [05:12<02:45,  1.49s/it]Epoch 10:  63%|██████▎   | 190/300 [05:13<02:41,  1.47s/it]Epoch 10:  64%|██████▎   | 191/300 [05:14<02:38,  1.45s/it]Epoch 10:  64%|██████▍   | 192/300 [05:16<02:44,  1.53s/it]Epoch 10:  64%|██████▍   | 193/300 [05:17<02:39,  1.49s/it]Epoch 10:  65%|██████▍   | 194/300 [05:19<02:41,  1.53s/it]Epoch 10:  65%|██████▌   | 195/300 [05:21<02:42,  1.55s/it]Epoch 10:  65%|██████▌   | 196/300 [05:22<02:39,  1.53s/it]Epoch 10:  66%|██████▌   | 197/300 [05:24<02:44,  1.60s/it]Epoch 10:  66%|██████▌   | 198/300 [05:26<02:51,  1.68s/it]Epoch 10:  66%|██████▋   | 199/300 [05:27<02:43,  1.61s/it]06/19/2022 16:09:27 - INFO - __main__ - global step: 1600; train loss: 8.209542274475098; dev loss
Epoch 10:  67%|██████▋   | 200/300 [05:29<02:50,  1.71s/it]Epoch 10:  67%|██████▋   | 201/300 [05:31<02:53,  1.76s/it]Epoch 10:  67%|██████▋   | 202/300 [05:33<02:55,  1.79s/it]Epoch 10:  68%|██████▊   | 203/300 [05:34<02:43,  1.68s/it]Epoch 10:  68%|██████▊   | 204/300 [05:36<02:46,  1.73s/it]Epoch 10:  68%|██████▊   | 205/300 [05:38<02:51,  1.80s/it]Epoch 10:  69%|██████▊   | 206/300 [05:40<02:51,  1.82s/it]Epoch 10:  69%|██████▉   | 207/300 [05:42<02:49,  1.82s/it]Epoch 10:  69%|██████▉   | 208/300 [05:43<02:37,  1.71s/it]Epoch 10:  70%|██████▉   | 209/300 [05:45<02:42,  1.78s/it]Epoch 10:  70%|███████   | 210/300 [05:47<02:42,  1.80s/it]Epoch 10:  70%|███████   | 211/300 [05:49<02:30,  1.69s/it]Epoch 10:  71%|███████   | 212/300 [05:50<02:21,  1.61s/it]Epoch 10:  71%|███████   | 213/300 [05:51<02:17,  1.58s/it]Epoch 10:  71%|███████▏  | 214/300 [05:53<02:11,  1.53s/it]Epoch 10:  72%|███████▏  | 215/300 [05:55<02:18,  1.63s/it]Epoch 10:  72%|███████▏  | 216/300 [05:57<02:23,  1.70s/it]Epoch 10:  72%|███████▏  | 217/300 [05:59<02:28,  1.79s/it]Epoch 10:  73%|███████▎  | 218/300 [06:00<02:18,  1.69s/it]Epoch 10:  73%|███████▎  | 219/300 [06:02<02:20,  1.73s/it]06/19/2022 16:10:02 - INFO - __main__ - global step: 1610; train loss: 7.460999965667725; dev loss
Epoch 10:  73%|███████▎  | 220/300 [06:04<02:22,  1.78s/it]Epoch 10:  74%|███████▎  | 221/300 [06:05<02:14,  1.70s/it]Epoch 10:  74%|███████▍  | 222/300 [06:07<02:05,  1.61s/it]Epoch 10:  74%|███████▍  | 223/300 [06:09<02:09,  1.68s/it]Epoch 10:  75%|███████▍  | 224/300 [06:10<02:11,  1.73s/it]Epoch 10:  75%|███████▌  | 225/300 [06:12<02:05,  1.68s/it]Epoch 10:  75%|███████▌  | 226/300 [06:13<01:59,  1.61s/it]Epoch 10:  76%|███████▌  | 227/300 [06:15<01:53,  1.56s/it]Epoch 10:  76%|███████▌  | 228/300 [06:17<01:58,  1.64s/it]Epoch 10:  76%|███████▋  | 229/300 [06:18<01:54,  1.61s/it]Epoch 10:  77%|███████▋  | 230/300 [06:20<01:58,  1.69s/it]Epoch 10:  77%|███████▋  | 231/300 [06:22<01:59,  1.74s/it]Epoch 10:  77%|███████▋  | 232/300 [06:23<01:52,  1.65s/it]Epoch 10:  78%|███████▊  | 233/300 [06:25<01:54,  1.71s/it]Epoch 10:  78%|███████▊  | 234/300 [06:27<01:49,  1.66s/it]Epoch 10:  78%|███████▊  | 235/300 [06:28<01:43,  1.59s/it]Epoch 10:  79%|███████▊  | 236/300 [06:30<01:46,  1.66s/it]Epoch 10:  79%|███████▉  | 237/300 [06:32<01:47,  1.71s/it]Epoch 10:  79%|███████▉  | 238/300 [06:33<01:42,  1.66s/it]Epoch 10:  80%|███████▉  | 239/300 [06:35<01:45,  1.72s/it]06/19/2022 16:10:35 - INFO - __main__ - global step: 1620; train loss: 7.767856597900391; dev loss
Epoch 10:  80%|████████  | 240/300 [06:37<01:38,  1.65s/it]Epoch 10:  80%|████████  | 241/300 [06:39<01:40,  1.71s/it]Epoch 10:  81%|████████  | 242/300 [06:41<01:43,  1.78s/it]Epoch 10:  81%|████████  | 243/300 [06:42<01:42,  1.80s/it]Epoch 10:  81%|████████▏ | 244/300 [06:44<01:34,  1.70s/it]Epoch 10:  82%|████████▏ | 245/300 [06:46<01:35,  1.74s/it]Epoch 10:  82%|████████▏ | 246/300 [06:48<01:38,  1.82s/it]Epoch 10:  82%|████████▏ | 247/300 [06:50<01:36,  1.83s/it]Epoch 10:  83%|████████▎ | 248/300 [06:51<01:28,  1.71s/it]Epoch 10:  83%|████████▎ | 249/300 [06:52<01:22,  1.62s/it]Epoch 10:  83%|████████▎ | 250/300 [06:54<01:26,  1.73s/it]Epoch 10:  84%|████████▎ | 251/300 [06:56<01:24,  1.72s/it]Epoch 10:  84%|████████▍ | 252/300 [06:58<01:19,  1.65s/it]Epoch 10:  84%|████████▍ | 253/300 [06:59<01:15,  1.61s/it]Epoch 10:  85%|████████▍ | 254/300 [07:01<01:13,  1.60s/it]Epoch 10:  85%|████████▌ | 255/300 [07:02<01:10,  1.56s/it]Epoch 10:  85%|████████▌ | 256/300 [07:04<01:08,  1.55s/it]Epoch 10:  86%|████████▌ | 257/300 [07:05<01:06,  1.54s/it]Epoch 10:  86%|████████▌ | 258/300 [07:07<01:10,  1.67s/it]Epoch 10:  86%|████████▋ | 259/300 [07:09<01:14,  1.82s/it]06/19/2022 16:11:10 - INFO - __main__ - global step: 1630; train loss: 7.996774196624756; dev loss
Epoch 10:  87%|████████▋ | 260/300 [07:11<01:15,  1.88s/it]Epoch 10:  87%|████████▋ | 261/300 [07:13<01:09,  1.79s/it]Epoch 10:  87%|████████▋ | 262/300 [07:15<01:09,  1.82s/it]Epoch 10:  88%|████████▊ | 263/300 [07:17<01:09,  1.87s/it]Epoch 10:  88%|████████▊ | 264/300 [07:18<01:03,  1.75s/it]Epoch 10:  88%|████████▊ | 265/300 [07:20<01:02,  1.79s/it]Epoch 10:  89%|████████▊ | 266/300 [07:22<00:58,  1.71s/it]Epoch 10:  89%|████████▉ | 267/300 [07:24<00:59,  1.80s/it]Epoch 10:  89%|████████▉ | 268/300 [07:25<00:55,  1.72s/it]Epoch 10:  90%|████████▉ | 269/300 [07:27<00:54,  1.77s/it]Epoch 10:  90%|█████████ | 270/300 [07:29<00:50,  1.69s/it]Epoch 10:  90%|█████████ | 271/300 [07:31<00:51,  1.78s/it]Epoch 10:  91%|█████████ | 272/300 [07:32<00:47,  1.70s/it]Epoch 10:  91%|█████████ | 273/300 [07:34<00:44,  1.63s/it]Epoch 10:  91%|█████████▏| 274/300 [07:35<00:44,  1.71s/it]Epoch 10:  92%|█████████▏| 275/300 [07:37<00:44,  1.80s/it]Epoch 10:  92%|█████████▏| 276/300 [07:39<00:43,  1.83s/it]Epoch 10:  92%|█████████▏| 277/300 [07:41<00:42,  1.84s/it]Epoch 10:  93%|█████████▎| 278/300 [07:43<00:38,  1.74s/it]Epoch 10:  93%|█████████▎| 279/300 [07:44<00:35,  1.70s/it]06/19/2022 16:11:44 - INFO - __main__ - global step: 1640; train loss: 8.442832946777344; dev loss
Epoch 10:  93%|█████████▎| 280/300 [07:46<00:35,  1.76s/it]Epoch 10:  94%|█████████▎| 281/300 [07:48<00:34,  1.81s/it]Epoch 10:  94%|█████████▍| 282/300 [07:50<00:32,  1.83s/it]Epoch 10:  94%|█████████▍| 283/300 [07:52<00:29,  1.76s/it]Epoch 10:  95%|█████████▍| 284/300 [07:53<00:26,  1.68s/it]Epoch 10:  95%|█████████▌| 285/300 [07:55<00:26,  1.74s/it]Epoch 10:  95%|█████████▌| 286/300 [07:57<00:25,  1.79s/it]Epoch 10:  96%|█████████▌| 287/300 [07:59<00:23,  1.82s/it]Epoch 10:  96%|█████████▌| 288/300 [08:00<00:21,  1.76s/it]Epoch 10:  96%|█████████▋| 289/300 [08:02<00:18,  1.68s/it]Epoch 10:  97%|█████████▋| 290/300 [08:04<00:17,  1.75s/it]Epoch 10:  97%|█████████▋| 291/300 [08:05<00:15,  1.67s/it]Epoch 10:  97%|█████████▋| 292/300 [08:07<00:14,  1.78s/it]Epoch 10:  98%|█████████▊| 293/300 [08:09<00:11,  1.69s/it]Epoch 10:  98%|█████████▊| 294/300 [08:11<00:10,  1.75s/it]Epoch 10:  98%|█████████▊| 295/300 [08:13<00:08,  1.80s/it]Epoch 10:  99%|█████████▊| 296/300 [08:14<00:06,  1.75s/it]Epoch 10:  99%|█████████▉| 297/300 [08:16<00:05,  1.67s/it]Epoch 10:  99%|█████████▉| 298/300 [08:18<00:03,  1.74s/it]Epoch 10: 100%|█████████▉| 299/300 [08:20<00:01,  1.79s/it]06/19/2022 16:12:19 - INFO - __main__ - global step: 1650; train loss: 7.642535209655762; dev loss
Epoch 10: 100%|██████████| 300/300 [08:21<00:00,  1.75s/it]Epoch 10: 100%|██████████| 300/300 [08:21<00:00,  1.67s/it]
Epoch 11:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 11:   0%|          | 1/300 [00:01<09:25,  1.89s/it]Epoch 11:   1%|          | 2/300 [00:03<08:09,  1.64s/it]Epoch 11:   1%|          | 3/300 [00:05<08:42,  1.76s/it]Epoch 11:   1%|▏         | 4/300 [00:06<08:22,  1.70s/it]Epoch 11:   2%|▏         | 5/300 [00:08<08:43,  1.78s/it]Epoch 11:   2%|▏         | 6/300 [00:10<08:13,  1.68s/it]Epoch 11:   2%|▏         | 7/300 [00:12<08:32,  1.75s/it]Epoch 11:   3%|▎         | 8/300 [00:13<08:16,  1.70s/it]Epoch 11:   3%|▎         | 9/300 [00:15<07:57,  1.64s/it]Epoch 11:   3%|▎         | 10/300 [00:17<08:18,  1.72s/it]Epoch 11:   4%|▎         | 11/300 [00:18<07:56,  1.65s/it]Epoch 11:   4%|▍         | 12/300 [00:20<07:43,  1.61s/it]Epoch 11:   4%|▍         | 13/300 [00:22<08:17,  1.73s/it]Epoch 11:   5%|▍         | 14/300 [00:24<08:36,  1.80s/it]Epoch 11:   5%|▌         | 15/300 [00:25<08:23,  1.77s/it]Epoch 11:   5%|▌         | 16/300 [00:27<08:19,  1.76s/it]Epoch 11:   6%|▌         | 17/300 [00:29<08:41,  1.84s/it]Epoch 11:   6%|▌         | 18/300 [00:31<08:23,  1.79s/it]Epoch 11:   6%|▋         | 19/300 [00:33<08:17,  1.77s/it]06/19/2022 16:12:54 - INFO - __main__ - global step: 1660; train loss: 7.94842004776001; dev loss
Epoch 11:   7%|▋         | 20/300 [00:34<08:12,  1.76s/it]Epoch 11:   7%|▋         | 21/300 [00:36<08:18,  1.79s/it]Epoch 11:   7%|▋         | 22/300 [00:38<08:30,  1.84s/it]Epoch 11:   8%|▊         | 23/300 [00:40<08:14,  1.78s/it]Epoch 11:   8%|▊         | 24/300 [00:41<08:10,  1.78s/it]Epoch 11:   8%|▊         | 25/300 [00:44<08:30,  1.86s/it]Epoch 11:   9%|▊         | 26/300 [00:45<08:34,  1.88s/it]Epoch 11:   9%|▉         | 27/300 [00:47<08:38,  1.90s/it]Epoch 11:   9%|▉         | 28/300 [00:49<08:40,  1.91s/it]Epoch 11:  10%|▉         | 29/300 [00:51<08:52,  1.96s/it]Epoch 11:  10%|█         | 30/300 [00:53<08:33,  1.90s/it]Epoch 11:  10%|█         | 31/300 [00:55<08:18,  1.85s/it]Epoch 11:  11%|█         | 32/300 [00:57<08:24,  1.88s/it]Epoch 11:  11%|█         | 33/300 [00:59<08:38,  1.94s/it]Epoch 11:  11%|█▏        | 34/300 [01:01<08:39,  1.95s/it]Epoch 11:  12%|█▏        | 35/300 [01:03<08:37,  1.95s/it]Epoch 11:  12%|█▏        | 36/300 [01:05<08:18,  1.89s/it]Epoch 11:  12%|█▏        | 37/300 [01:07<08:31,  1.94s/it]Epoch 11:  13%|█▎        | 38/300 [01:08<08:03,  1.84s/it]Epoch 11:  13%|█▎        | 39/300 [01:10<07:46,  1.79s/it]06/19/2022 16:13:32 - INFO - __main__ - global step: 1670; train loss: 8.155631065368652; dev loss
Epoch 11:  13%|█▎        | 40/300 [01:12<07:34,  1.75s/it]Epoch 11:  14%|█▎        | 41/300 [01:13<07:23,  1.71s/it]Epoch 11:  14%|█▍        | 42/300 [01:15<07:30,  1.75s/it]Epoch 11:  14%|█▍        | 43/300 [01:17<07:45,  1.81s/it]Epoch 11:  15%|█▍        | 44/300 [01:19<07:39,  1.80s/it]Epoch 11:  15%|█▌        | 45/300 [01:21<07:50,  1.85s/it]Epoch 11:  15%|█▌        | 46/300 [01:23<07:49,  1.85s/it]Epoch 11:  16%|█▌        | 47/300 [01:24<07:39,  1.82s/it]Epoch 11:  16%|█▌        | 48/300 [01:26<07:48,  1.86s/it]Epoch 11:  16%|█▋        | 49/300 [01:28<07:34,  1.81s/it]Epoch 11:  17%|█▋        | 50/300 [01:30<07:48,  1.87s/it]Epoch 11:  17%|█▋        | 51/300 [01:32<07:50,  1.89s/it]Epoch 11:  17%|█▋        | 52/300 [01:34<07:32,  1.82s/it]Epoch 11:  18%|█▊        | 53/300 [01:35<07:17,  1.77s/it]Epoch 11:  18%|█▊        | 54/300 [01:37<07:20,  1.79s/it]Epoch 11:  18%|█▊        | 55/300 [01:39<07:31,  1.84s/it]Epoch 11:  19%|█▊        | 56/300 [01:41<07:38,  1.88s/it]Epoch 11:  19%|█▉        | 57/300 [01:43<07:42,  1.90s/it]Epoch 11:  19%|█▉        | 58/300 [01:45<07:50,  1.94s/it]Epoch 11:  20%|█▉        | 59/300 [01:47<07:47,  1.94s/it]06/19/2022 16:14:09 - INFO - __main__ - global step: 1680; train loss: 7.623394966125488; dev loss
Epoch 11:  20%|██        | 60/300 [01:49<07:24,  1.85s/it]Epoch 11:  20%|██        | 61/300 [01:51<07:26,  1.87s/it]Epoch 11:  21%|██        | 62/300 [01:52<07:19,  1.85s/it]Epoch 11:  21%|██        | 63/300 [01:54<07:23,  1.87s/it]Epoch 11:  21%|██▏       | 64/300 [01:56<07:08,  1.81s/it]Epoch 11:  22%|██▏       | 65/300 [01:58<06:59,  1.79s/it]Epoch 11:  22%|██▏       | 66/300 [01:59<06:56,  1.78s/it]Epoch 11:  22%|██▏       | 67/300 [02:01<07:14,  1.87s/it]Epoch 11:  23%|██▎       | 68/300 [02:03<06:57,  1.80s/it]Epoch 11:  23%|██▎       | 69/300 [02:05<06:43,  1.75s/it]Epoch 11:  23%|██▎       | 70/300 [02:07<06:56,  1.81s/it]Epoch 11:  24%|██▎       | 71/300 [02:09<07:04,  1.86s/it]Epoch 11:  24%|██▍       | 72/300 [02:11<07:05,  1.87s/it]Epoch 11:  24%|██▍       | 73/300 [02:12<06:38,  1.75s/it]Epoch 11:  25%|██▍       | 74/300 [02:14<06:16,  1.67s/it]Epoch 11:  25%|██▌       | 75/300 [02:15<06:09,  1.64s/it]Epoch 11:  25%|██▌       | 76/300 [02:17<06:24,  1.72s/it]Epoch 11:  26%|██▌       | 77/300 [02:18<06:07,  1.65s/it]Epoch 11:  26%|██▌       | 78/300 [02:20<06:22,  1.72s/it]Epoch 11:  26%|██▋       | 79/300 [02:22<06:38,  1.80s/it]06/19/2022 16:14:44 - INFO - __main__ - global step: 1690; train loss: 7.959002494812012; dev loss
Epoch 11:  27%|██▋       | 80/300 [02:24<06:42,  1.83s/it]Epoch 11:  27%|██▋       | 81/300 [02:26<06:44,  1.85s/it]Epoch 11:  27%|██▋       | 82/300 [02:28<06:47,  1.87s/it]Epoch 11:  28%|██▊       | 83/300 [02:30<06:57,  1.92s/it]Epoch 11:  28%|██▊       | 84/300 [02:32<06:29,  1.81s/it]Epoch 11:  28%|██▊       | 85/300 [02:34<06:31,  1.82s/it]Epoch 11:  29%|██▊       | 86/300 [02:35<06:09,  1.73s/it]Epoch 11:  29%|██▉       | 87/300 [02:37<05:59,  1.69s/it]Epoch 11:  29%|██▉       | 88/300 [02:38<05:44,  1.63s/it]Epoch 11:  30%|██▉       | 89/300 [02:40<05:57,  1.70s/it]Epoch 11:  30%|███       | 90/300 [02:42<06:08,  1.75s/it]Epoch 11:  30%|███       | 91/300 [02:44<06:23,  1.83s/it]Epoch 11:  31%|███       | 92/300 [02:46<06:25,  1.85s/it]Epoch 11:  31%|███       | 93/300 [02:47<06:00,  1.74s/it]Epoch 11:  31%|███▏      | 94/300 [02:49<06:07,  1.78s/it]Epoch 11:  32%|███▏      | 95/300 [02:51<06:11,  1.81s/it]Epoch 11:  32%|███▏      | 96/300 [02:53<05:58,  1.76s/it]Epoch 11:  32%|███▏      | 97/300 [02:54<05:44,  1.70s/it]Epoch 11:  33%|███▎      | 98/300 [02:56<05:32,  1.65s/it]Epoch 11:  33%|███▎      | 99/300 [02:57<05:23,  1.61s/it]06/19/2022 16:15:19 - INFO - __main__ - global step: 1700; train loss: 7.932871341705322; dev loss
Epoch 11:  33%|███▎      | 100/300 [02:59<05:41,  1.71s/it]Epoch 11:  34%|███▎      | 101/300 [03:01<05:53,  1.78s/it]Epoch 11:  34%|███▍      | 102/300 [03:03<06:00,  1.82s/it]Epoch 11:  34%|███▍      | 103/300 [03:05<06:04,  1.85s/it]Epoch 11:  35%|███▍      | 104/300 [03:07<05:50,  1.79s/it]Epoch 11:  35%|███▌      | 105/300 [03:08<05:53,  1.81s/it]Epoch 11:  35%|███▌      | 106/300 [03:10<05:55,  1.83s/it]Epoch 11:  36%|███▌      | 107/300 [03:12<05:33,  1.73s/it]Epoch 11:  36%|███▌      | 108/300 [03:14<05:46,  1.80s/it]Epoch 11:  36%|███▋      | 109/300 [03:15<05:25,  1.70s/it]Epoch 11:  37%|███▋      | 110/300 [03:17<05:10,  1.63s/it]Epoch 11:  37%|███▋      | 111/300 [03:18<05:00,  1.59s/it]Epoch 11:  37%|███▋      | 112/300 [03:20<04:59,  1.59s/it]Epoch 11:  38%|███▊      | 113/300 [03:21<04:51,  1.56s/it]Epoch 11:  38%|███▊      | 114/300 [03:23<04:43,  1.53s/it]Epoch 11:  38%|███▊      | 115/300 [03:25<05:01,  1.63s/it]Epoch 11:  39%|███▊      | 116/300 [03:26<04:57,  1.62s/it]Epoch 11:  39%|███▉      | 117/300 [03:28<04:48,  1.58s/it]Epoch 11:  39%|███▉      | 118/300 [03:30<05:02,  1.66s/it]Epoch 11:  40%|███▉      | 119/300 [03:31<04:50,  1.61s/it]06/19/2022 16:15:53 - INFO - __main__ - global step: 1710; train loss: 8.035841941833496; dev loss
Epoch 11:  40%|████      | 120/300 [03:33<04:43,  1.57s/it]Epoch 11:  40%|████      | 121/300 [03:34<04:42,  1.58s/it]Epoch 11:  41%|████      | 122/300 [03:36<04:34,  1.54s/it]Epoch 11:  41%|████      | 123/300 [03:37<04:49,  1.64s/it]Epoch 11:  41%|████▏     | 124/300 [03:39<04:39,  1.59s/it]Epoch 11:  42%|████▏     | 125/300 [03:41<04:57,  1.70s/it]Epoch 11:  42%|████▏     | 126/300 [03:42<04:45,  1.64s/it]Epoch 11:  42%|████▏     | 127/300 [03:44<04:56,  1.71s/it]Epoch 11:  43%|████▎     | 128/300 [03:46<05:04,  1.77s/it]Epoch 11:  43%|████▎     | 129/300 [03:48<04:53,  1.72s/it]Epoch 11:  43%|████▎     | 130/300 [03:50<05:00,  1.77s/it]Epoch 11:  44%|████▎     | 131/300 [03:52<05:04,  1.80s/it]Epoch 11:  44%|████▍     | 132/300 [03:53<04:46,  1.70s/it]Epoch 11:  44%|████▍     | 133/300 [03:55<04:39,  1.68s/it]Epoch 11:  45%|████▍     | 134/300 [03:56<04:27,  1.61s/it]Epoch 11:  45%|████▌     | 135/300 [03:58<04:39,  1.69s/it]Epoch 11:  45%|████▌     | 136/300 [04:00<04:48,  1.76s/it]Epoch 11:  46%|████▌     | 137/300 [04:01<04:37,  1.71s/it]Epoch 11:  46%|████▌     | 138/300 [04:03<04:24,  1.64s/it]Epoch 11:  46%|████▋     | 139/300 [04:04<04:15,  1.59s/it]06/19/2022 16:16:26 - INFO - __main__ - global step: 1720; train loss: 7.826925754547119; dev loss
Epoch 11:  47%|████▋     | 140/300 [04:06<04:09,  1.56s/it]Epoch 11:  47%|████▋     | 141/300 [04:08<04:10,  1.57s/it]Epoch 11:  47%|████▋     | 142/300 [04:09<04:04,  1.55s/it]Epoch 11:  48%|████▊     | 143/300 [04:11<04:17,  1.64s/it]Epoch 11:  48%|████▊     | 144/300 [04:12<04:07,  1.59s/it]Epoch 11:  48%|████▊     | 145/300 [04:14<04:05,  1.59s/it]Epoch 11:  49%|████▊     | 146/300 [04:16<04:17,  1.67s/it]Epoch 11:  49%|████▉     | 147/300 [04:17<04:06,  1.61s/it]Epoch 11:  49%|████▉     | 148/300 [04:19<04:16,  1.68s/it]Epoch 11:  50%|████▉     | 149/300 [04:21<04:23,  1.75s/it]Epoch 11:  50%|█████     | 150/300 [04:23<04:14,  1.70s/it]Epoch 11:  50%|█████     | 151/300 [04:24<04:03,  1.63s/it]Epoch 11:  51%|█████     | 152/300 [04:26<03:54,  1.59s/it]Epoch 11:  51%|█████     | 153/300 [04:27<03:48,  1.56s/it]Epoch 11:  51%|█████▏    | 154/300 [04:29<03:49,  1.57s/it]Epoch 11:  52%|█████▏    | 155/300 [04:31<04:00,  1.66s/it]Epoch 11:  52%|█████▏    | 156/300 [04:32<04:09,  1.74s/it]Epoch 11:  52%|█████▏    | 157/300 [04:34<03:58,  1.67s/it]Epoch 11:  53%|█████▎    | 158/300 [04:36<03:54,  1.65s/it]Epoch 11:  53%|█████▎    | 159/300 [04:37<04:03,  1.72s/it]06/19/2022 16:16:59 - INFO - __main__ - global step: 1730; train loss: 8.037150382995605; dev loss
Epoch 11:  53%|█████▎    | 160/300 [04:40<04:16,  1.83s/it]Epoch 11:  54%|█████▎    | 161/300 [04:41<04:19,  1.87s/it]Epoch 11:  54%|█████▍    | 162/300 [04:43<04:05,  1.78s/it]Epoch 11:  54%|█████▍    | 163/300 [04:45<03:52,  1.69s/it]Epoch 11:  55%|█████▍    | 164/300 [04:46<04:00,  1.77s/it]Epoch 11:  55%|█████▌    | 165/300 [04:48<03:48,  1.69s/it]Epoch 11:  55%|█████▌    | 166/300 [04:50<03:44,  1.67s/it]Epoch 11:  56%|█████▌    | 167/300 [04:52<03:51,  1.74s/it]Epoch 11:  56%|█████▌    | 168/300 [04:53<03:40,  1.67s/it]Epoch 11:  56%|█████▋    | 169/300 [04:55<03:47,  1.74s/it]Epoch 11:  57%|█████▋    | 170/300 [04:57<03:56,  1.82s/it]Epoch 11:  57%|█████▋    | 171/300 [04:58<03:41,  1.72s/it]Epoch 11:  57%|█████▋    | 172/300 [05:00<03:32,  1.66s/it]Epoch 11:  58%|█████▊    | 173/300 [05:02<03:39,  1.73s/it]Epoch 11:  58%|█████▊    | 174/300 [05:03<03:29,  1.66s/it]Epoch 11:  58%|█████▊    | 175/300 [05:05<03:41,  1.77s/it]Epoch 11:  59%|█████▊    | 176/300 [05:07<03:44,  1.81s/it]Epoch 11:  59%|█████▉    | 177/300 [05:09<03:31,  1.72s/it]Epoch 11:  59%|█████▉    | 178/300 [05:10<03:21,  1.65s/it]Epoch 11:  60%|█████▉    | 179/300 [05:12<03:18,  1.64s/it]06/19/2022 16:17:33 - INFO - __main__ - global step: 1740; train loss: 8.2185697555542; dev loss
Epoch 11:  60%|██████    | 180/300 [05:13<03:11,  1.60s/it]Epoch 11:  60%|██████    | 181/300 [05:15<03:05,  1.56s/it]Epoch 11:  61%|██████    | 182/300 [05:17<03:15,  1.66s/it]Epoch 11:  61%|██████    | 183/300 [05:18<03:13,  1.65s/it]Epoch 11:  61%|██████▏   | 184/300 [05:20<03:05,  1.60s/it]Epoch 11:  62%|██████▏   | 185/300 [05:22<03:13,  1.69s/it]Epoch 11:  62%|██████▏   | 186/300 [05:24<03:19,  1.75s/it]Epoch 11:  62%|██████▏   | 187/300 [05:26<03:25,  1.82s/it]Epoch 11:  63%|██████▎   | 188/300 [05:28<03:26,  1.84s/it]Epoch 11:  63%|██████▎   | 189/300 [05:29<03:13,  1.74s/it]Epoch 11:  63%|██████▎   | 190/300 [05:31<03:03,  1.67s/it]Epoch 11:  64%|██████▎   | 191/300 [05:32<03:11,  1.76s/it]Epoch 11:  64%|██████▍   | 192/300 [05:34<03:02,  1.69s/it]Epoch 11:  64%|██████▍   | 193/300 [05:36<03:07,  1.75s/it]Epoch 11:  65%|██████▍   | 194/300 [05:38<03:09,  1.79s/it]Epoch 11:  65%|██████▌   | 195/300 [05:40<03:14,  1.86s/it]Epoch 11:  65%|██████▌   | 196/300 [05:42<03:14,  1.87s/it]Epoch 11:  66%|██████▌   | 197/300 [05:43<03:01,  1.76s/it]Epoch 11:  66%|██████▌   | 198/300 [05:45<03:04,  1.80s/it]Epoch 11:  66%|██████▋   | 199/300 [05:47<03:08,  1.87s/it]06/19/2022 16:18:09 - INFO - __main__ - global step: 1750; train loss: 7.8083295822143555; dev loss
Epoch 11:  67%|██████▋   | 200/300 [05:49<03:07,  1.87s/it]Epoch 11:  67%|██████▋   | 201/300 [05:51<03:06,  1.88s/it]Epoch 11:  67%|██████▋   | 202/300 [05:53<03:05,  1.89s/it]Epoch 11:  68%|██████▊   | 203/300 [05:55<03:03,  1.89s/it]Epoch 11:  68%|██████▊   | 204/300 [05:57<03:05,  1.93s/it]Epoch 11:  68%|██████▊   | 205/300 [05:58<02:50,  1.80s/it]Epoch 11:  69%|██████▊   | 206/300 [06:00<02:40,  1.70s/it]Epoch 11:  69%|██████▉   | 207/300 [06:01<02:32,  1.64s/it]Epoch 11:  69%|██████▉   | 208/300 [06:03<02:30,  1.64s/it]Epoch 11:  70%|██████▉   | 209/300 [06:04<02:24,  1.59s/it]Epoch 11:  70%|███████   | 210/300 [06:06<02:31,  1.68s/it]Epoch 11:  70%|███████   | 211/300 [06:08<02:35,  1.74s/it]Epoch 11:  71%|███████   | 212/300 [06:10<02:30,  1.71s/it]Epoch 11:  71%|███████   | 213/300 [06:11<02:24,  1.66s/it]Epoch 11:  71%|███████▏  | 214/300 [06:13<02:28,  1.73s/it]Epoch 11:  72%|███████▏  | 215/300 [06:15<02:20,  1.65s/it]Epoch 11:  72%|███████▏  | 216/300 [06:16<02:17,  1.63s/it]Epoch 11:  72%|███████▏  | 217/300 [06:18<02:21,  1.71s/it]Epoch 11:  73%|███████▎  | 218/300 [06:20<02:25,  1.77s/it]Epoch 11:  73%|███████▎  | 219/300 [06:22<02:16,  1.69s/it]06/19/2022 16:18:43 - INFO - __main__ - global step: 1760; train loss: 7.963222503662109; dev loss
Epoch 11:  73%|███████▎  | 220/300 [06:23<02:13,  1.67s/it]Epoch 11:  74%|███████▎  | 221/300 [06:25<02:17,  1.74s/it]Epoch 11:  74%|███████▍  | 222/300 [06:27<02:10,  1.67s/it]Epoch 11:  74%|███████▍  | 223/300 [06:28<02:13,  1.74s/it]Epoch 11:  75%|███████▍  | 224/300 [06:30<02:09,  1.70s/it]Epoch 11:  75%|███████▌  | 225/300 [06:32<02:02,  1.63s/it]Epoch 11:  75%|███████▌  | 226/300 [06:33<01:57,  1.59s/it]Epoch 11:  76%|███████▌  | 227/300 [06:35<02:02,  1.68s/it]Epoch 11:  76%|███████▌  | 228/300 [06:37<02:05,  1.75s/it]Epoch 11:  76%|███████▋  | 229/300 [06:38<02:00,  1.70s/it]Epoch 11:  77%|███████▋  | 230/300 [06:40<01:54,  1.64s/it]Epoch 11:  77%|███████▋  | 231/300 [06:42<01:58,  1.71s/it]Epoch 11:  77%|███████▋  | 232/300 [06:44<02:00,  1.78s/it]Epoch 11:  78%|███████▊  | 233/300 [06:45<01:55,  1.73s/it]Epoch 11:  78%|███████▊  | 234/300 [06:47<01:48,  1.65s/it]Epoch 11:  78%|███████▊  | 235/300 [06:48<01:43,  1.60s/it]Epoch 11:  79%|███████▊  | 236/300 [06:50<01:47,  1.68s/it]Epoch 11:  79%|███████▉  | 237/300 [06:52<01:44,  1.67s/it]Epoch 11:  79%|███████▉  | 238/300 [06:53<01:40,  1.62s/it]Epoch 11:  80%|███████▉  | 239/300 [06:55<01:36,  1.58s/it]06/19/2022 16:19:17 - INFO - __main__ - global step: 1770; train loss: 7.944052696228027; dev loss
Epoch 11:  80%|████████  | 240/300 [06:57<01:40,  1.67s/it]Epoch 11:  80%|████████  | 241/300 [06:58<01:37,  1.66s/it]Epoch 11:  81%|████████  | 242/300 [07:00<01:40,  1.73s/it]Epoch 11:  81%|████████  | 243/300 [07:02<01:34,  1.66s/it]Epoch 11:  81%|████████▏ | 244/300 [07:04<01:36,  1.72s/it]Epoch 11:  82%|████████▏ | 245/300 [07:05<01:32,  1.69s/it]Epoch 11:  82%|████████▏ | 246/300 [07:07<01:34,  1.74s/it]Epoch 11:  82%|████████▏ | 247/300 [07:09<01:34,  1.79s/it]Epoch 11:  83%|████████▎ | 248/300 [07:11<01:34,  1.82s/it]Epoch 11:  83%|████████▎ | 249/300 [07:12<01:29,  1.76s/it]Epoch 11:  83%|████████▎ | 250/300 [07:14<01:30,  1.80s/it]Epoch 11:  84%|████████▎ | 251/300 [07:16<01:24,  1.72s/it]Epoch 11:  84%|████████▍ | 252/300 [07:18<01:24,  1.77s/it]Epoch 11:  84%|████████▍ | 253/300 [07:20<01:26,  1.85s/it]Epoch 11:  85%|████████▍ | 254/300 [07:22<01:25,  1.86s/it]Epoch 11:  85%|████████▌ | 255/300 [07:23<01:18,  1.75s/it]Epoch 11:  85%|████████▌ | 256/300 [07:25<01:18,  1.79s/it]Epoch 11:  86%|████████▌ | 257/300 [07:27<01:18,  1.83s/it]Epoch 11:  86%|████████▌ | 258/300 [07:29<01:19,  1.88s/it]Epoch 11:  86%|████████▋ | 259/300 [07:31<01:17,  1.89s/it]06/19/2022 16:19:53 - INFO - __main__ - global step: 1780; train loss: 7.800071716308594; dev loss
Epoch 11:  87%|████████▋ | 260/300 [07:33<01:15,  1.89s/it]Epoch 11:  87%|████████▋ | 261/300 [07:34<01:09,  1.78s/it]Epoch 11:  87%|████████▋ | 262/300 [07:36<01:10,  1.84s/it]Epoch 11:  88%|████████▊ | 263/300 [07:38<01:04,  1.73s/it]Epoch 11:  88%|████████▊ | 264/300 [07:40<01:04,  1.78s/it]Epoch 11:  88%|████████▊ | 265/300 [07:42<01:03,  1.81s/it]Epoch 11:  89%|████████▊ | 266/300 [07:43<00:59,  1.75s/it]Epoch 11:  89%|████████▉ | 267/300 [07:45<00:55,  1.67s/it]Epoch 11:  89%|████████▉ | 268/300 [07:47<00:55,  1.74s/it]Epoch 11:  90%|████████▉ | 269/300 [07:48<00:51,  1.66s/it]Epoch 11:  90%|█████████ | 270/300 [07:50<00:53,  1.77s/it]Epoch 11:  90%|█████████ | 271/300 [07:52<00:48,  1.69s/it]Epoch 11:  91%|█████████ | 272/300 [07:53<00:48,  1.75s/it]Epoch 11:  91%|█████████ | 273/300 [07:55<00:45,  1.67s/it]Epoch 11:  91%|█████████▏| 274/300 [07:57<00:43,  1.66s/it]Epoch 11:  92%|█████████▏| 275/300 [07:58<00:40,  1.61s/it]Epoch 11:  92%|█████████▏| 276/300 [08:00<00:40,  1.69s/it]Epoch 11:  92%|█████████▏| 277/300 [08:01<00:37,  1.64s/it]Epoch 11:  93%|█████████▎| 278/300 [08:03<00:38,  1.75s/it]Epoch 11:  93%|█████████▎| 279/300 [08:05<00:35,  1.68s/it]06/19/2022 16:20:26 - INFO - __main__ - global step: 1790; train loss: 8.367326736450195; dev loss
Epoch 11:  93%|█████████▎| 280/300 [08:06<00:32,  1.62s/it]Epoch 11:  94%|█████████▎| 281/300 [08:08<00:30,  1.58s/it]Epoch 11:  94%|█████████▍| 282/300 [08:10<00:30,  1.67s/it]Epoch 11:  94%|█████████▍| 283/300 [08:12<00:30,  1.78s/it]Epoch 11:  95%|█████████▍| 284/300 [08:14<00:29,  1.82s/it]Epoch 11:  95%|█████████▌| 285/300 [08:15<00:25,  1.72s/it]Epoch 11:  95%|█████████▌| 286/300 [08:17<00:24,  1.77s/it]Epoch 11:  96%|█████████▌| 287/300 [08:19<00:22,  1.71s/it]Epoch 11:  96%|█████████▌| 288/300 [08:21<00:21,  1.77s/it]Epoch 11:  96%|█████████▋| 289/300 [08:22<00:18,  1.69s/it]Epoch 11:  97%|█████████▋| 290/300 [08:24<00:17,  1.75s/it]Epoch 11:  97%|█████████▋| 291/300 [08:26<00:15,  1.70s/it]Epoch 11:  97%|█████████▋| 292/300 [08:27<00:13,  1.65s/it]Epoch 11:  98%|█████████▊| 293/300 [08:29<00:12,  1.71s/it]Epoch 11:  98%|█████████▊| 294/300 [08:30<00:09,  1.65s/it]Epoch 11:  98%|█████████▊| 295/300 [08:32<00:08,  1.64s/it]Epoch 11:  99%|█████████▊| 296/300 [08:34<00:06,  1.72s/it]Epoch 11:  99%|█████████▉| 297/300 [08:36<00:05,  1.77s/it]Epoch 11:  99%|█████████▉| 298/300 [08:37<00:03,  1.69s/it]Epoch 11: 100%|█████████▉| 299/300 [08:39<00:01,  1.78s/it]06/19/2022 16:21:01 - INFO - __main__ - global step: 1800; train loss: 7.300529479980469; dev loss
Epoch 11: 100%|██████████| 300/300 [08:41<00:00,  1.69s/it]Epoch 11: 100%|██████████| 300/300 [08:41<00:00,  1.74s/it]
Epoch 12:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 12:   0%|          | 1/300 [00:01<09:24,  1.89s/it]Epoch 12:   1%|          | 2/300 [00:03<09:20,  1.88s/it]Epoch 12:   1%|          | 3/300 [00:05<08:40,  1.75s/it]Epoch 12:   1%|▏         | 4/300 [00:07<08:57,  1.81s/it]Epoch 12:   2%|▏         | 5/300 [00:09<09:02,  1.84s/it]Epoch 12:   2%|▏         | 6/300 [00:10<08:26,  1.72s/it]Epoch 12:   2%|▏         | 7/300 [00:12<08:14,  1.69s/it]Epoch 12:   3%|▎         | 8/300 [00:14<08:32,  1.76s/it]Epoch 12:   3%|▎         | 9/300 [00:16<08:43,  1.80s/it]Epoch 12:   3%|▎         | 10/300 [00:17<08:13,  1.70s/it]Epoch 12:   4%|▎         | 11/300 [00:19<08:28,  1.76s/it]Epoch 12:   4%|▍         | 12/300 [00:21<08:15,  1.72s/it]Epoch 12:   4%|▍         | 13/300 [00:22<08:26,  1.77s/it]Epoch 12:   5%|▍         | 14/300 [00:24<08:05,  1.70s/it]Epoch 12:   5%|▌         | 15/300 [00:26<08:22,  1.76s/it]Epoch 12:   5%|▌         | 16/300 [00:28<08:43,  1.84s/it]Epoch 12:   6%|▌         | 17/300 [00:29<08:11,  1.74s/it]Epoch 12:   6%|▌         | 18/300 [00:31<07:50,  1.67s/it]Epoch 12:   6%|▋         | 19/300 [00:33<08:07,  1.73s/it]06/19/2022 16:21:36 - INFO - __main__ - global step: 1810; train loss: 7.798635959625244; dev loss
Epoch 12:   7%|▋         | 20/300 [00:34<07:54,  1.70s/it]Epoch 12:   7%|▋         | 21/300 [00:36<07:33,  1.63s/it]Epoch 12:   7%|▋         | 22/300 [00:38<07:55,  1.71s/it]Epoch 12:   8%|▊         | 23/300 [00:39<07:36,  1.65s/it]Epoch 12:   8%|▊         | 24/300 [00:41<08:06,  1.76s/it]Epoch 12:   8%|▊         | 25/300 [00:43<08:16,  1.81s/it]Epoch 12:   9%|▊         | 26/300 [00:45<07:54,  1.73s/it]Epoch 12:   9%|▉         | 27/300 [00:47<08:07,  1.79s/it]Epoch 12:   9%|▉         | 28/300 [00:49<08:25,  1.86s/it]Epoch 12:  10%|▉         | 29/300 [00:50<07:53,  1.75s/it]Epoch 12:  10%|█         | 30/300 [00:52<08:01,  1.78s/it]Epoch 12:  10%|█         | 31/300 [00:54<07:38,  1.70s/it]Epoch 12:  11%|█         | 32/300 [00:55<07:29,  1.68s/it]Epoch 12:  11%|█         | 33/300 [00:57<07:45,  1.74s/it]Epoch 12:  11%|█▏        | 34/300 [00:59<07:23,  1.67s/it]Epoch 12:  12%|█▏        | 35/300 [01:00<07:40,  1.74s/it]Epoch 12:  12%|█▏        | 36/300 [01:02<07:18,  1.66s/it]Epoch 12:  12%|█▏        | 37/300 [01:04<07:13,  1.65s/it]Epoch 12:  13%|█▎        | 38/300 [01:05<07:30,  1.72s/it]Epoch 12:  13%|█▎        | 39/300 [01:07<07:11,  1.65s/it]06/19/2022 16:22:10 - INFO - __main__ - global step: 1820; train loss: 8.100529670715332; dev loss
Epoch 12:  13%|█▎        | 40/300 [01:09<07:28,  1.73s/it]Epoch 12:  14%|█▎        | 41/300 [01:11<07:48,  1.81s/it]Epoch 12:  14%|█▍        | 42/300 [01:13<07:53,  1.84s/it]Epoch 12:  14%|█▍        | 43/300 [01:15<07:55,  1.85s/it]Epoch 12:  15%|█▍        | 44/300 [01:16<07:27,  1.75s/it]Epoch 12:  15%|█▌        | 45/300 [01:18<07:45,  1.83s/it]Epoch 12:  15%|█▌        | 46/300 [01:20<07:20,  1.73s/it]Epoch 12:  16%|█▌        | 47/300 [01:21<07:01,  1.67s/it]Epoch 12:  16%|█▌        | 48/300 [01:23<06:48,  1.62s/it]Epoch 12:  16%|█▋        | 49/300 [01:25<07:14,  1.73s/it]Epoch 12:  17%|█▋        | 50/300 [01:26<06:56,  1.67s/it]Epoch 12:  17%|█▋        | 51/300 [01:28<07:11,  1.73s/it]Epoch 12:  17%|█▋        | 52/300 [01:30<07:21,  1.78s/it]Epoch 12:  18%|█▊        | 53/300 [01:32<07:36,  1.85s/it]Epoch 12:  18%|█▊        | 54/300 [01:34<07:10,  1.75s/it]Epoch 12:  18%|█▊        | 55/300 [01:35<06:48,  1.67s/it]Epoch 12:  19%|█▊        | 56/300 [01:37<06:37,  1.63s/it]Epoch 12:  19%|█▉        | 57/300 [01:38<06:34,  1.62s/it]Epoch 12:  19%|█▉        | 58/300 [01:40<06:22,  1.58s/it]Epoch 12:  20%|█▉        | 59/300 [01:42<06:44,  1.68s/it]06/19/2022 16:22:44 - INFO - __main__ - global step: 1830; train loss: 7.7426652908325195; dev loss
Epoch 12:  20%|██        | 60/300 [01:43<06:29,  1.62s/it]Epoch 12:  20%|██        | 61/300 [01:45<06:54,  1.74s/it]Epoch 12:  21%|██        | 62/300 [01:47<06:35,  1.66s/it]Epoch 12:  21%|██        | 63/300 [01:48<06:49,  1.73s/it]Epoch 12:  21%|██▏       | 64/300 [01:50<07:00,  1.78s/it]Epoch 12:  22%|██▏       | 65/300 [01:52<06:40,  1.70s/it]Epoch 12:  22%|██▏       | 66/300 [01:53<06:31,  1.67s/it]Epoch 12:  22%|██▏       | 67/300 [01:55<06:46,  1.75s/it]Epoch 12:  23%|██▎       | 68/300 [01:57<06:55,  1.79s/it]Epoch 12:  23%|██▎       | 69/300 [01:59<06:33,  1.70s/it]Epoch 12:  23%|██▎       | 70/300 [02:00<06:23,  1.67s/it]Epoch 12:  24%|██▎       | 71/300 [02:02<06:09,  1.61s/it]Epoch 12:  24%|██▍       | 72/300 [02:03<05:58,  1.57s/it]Epoch 12:  24%|██▍       | 73/300 [02:05<05:52,  1.55s/it]Epoch 12:  25%|██▍       | 74/300 [02:07<06:21,  1.69s/it]Epoch 12:  25%|██▌       | 75/300 [02:09<06:33,  1.75s/it]Epoch 12:  25%|██▌       | 76/300 [02:11<06:41,  1.79s/it]Epoch 12:  26%|██▌       | 77/300 [02:12<06:45,  1.82s/it]Epoch 12:  26%|██▌       | 78/300 [02:14<06:55,  1.87s/it]Epoch 12:  26%|██▋       | 79/300 [02:16<06:30,  1.77s/it]06/19/2022 16:23:19 - INFO - __main__ - global step: 1840; train loss: 7.4028425216674805; dev loss
Epoch 12:  27%|██▋       | 80/300 [02:18<06:35,  1.80s/it]Epoch 12:  27%|██▋       | 81/300 [02:20<06:40,  1.83s/it]Epoch 12:  27%|██▋       | 82/300 [02:22<06:50,  1.88s/it]Epoch 12:  28%|██▊       | 83/300 [02:24<06:48,  1.88s/it]Epoch 12:  28%|██▊       | 84/300 [02:26<06:47,  1.89s/it]Epoch 12:  28%|██▊       | 85/300 [02:27<06:22,  1.78s/it]Epoch 12:  29%|██▊       | 86/300 [02:29<06:33,  1.84s/it]Epoch 12:  29%|██▉       | 87/300 [02:31<06:10,  1.74s/it]Epoch 12:  29%|██▉       | 88/300 [02:32<06:18,  1.78s/it]Epoch 12:  30%|██▉       | 89/300 [02:34<06:00,  1.71s/it]Epoch 12:  30%|███       | 90/300 [02:35<05:44,  1.64s/it]Epoch 12:  30%|███       | 91/300 [02:37<06:04,  1.75s/it]Epoch 12:  31%|███       | 92/300 [02:39<05:49,  1.68s/it]Epoch 12:  31%|███       | 93/300 [02:40<05:35,  1.62s/it]Epoch 12:  31%|███▏      | 94/300 [02:42<05:50,  1.70s/it]Epoch 12:  32%|███▏      | 95/300 [02:44<06:07,  1.79s/it]Epoch 12:  32%|███▏      | 96/300 [02:46<06:13,  1.83s/it]Epoch 12:  32%|███▏      | 97/300 [02:48<05:51,  1.73s/it]Epoch 12:  33%|███▎      | 98/300 [02:49<05:36,  1.66s/it]Epoch 12:  33%|███▎      | 99/300 [02:51<05:31,  1.65s/it]06/19/2022 16:23:54 - INFO - __main__ - global step: 1850; train loss: 8.085229873657227; dev loss
Epoch 12:  33%|███▎      | 100/300 [02:52<05:20,  1.60s/it]Epoch 12:  34%|███▎      | 101/300 [02:54<05:13,  1.58s/it]Epoch 12:  34%|███▍      | 102/300 [02:56<05:29,  1.67s/it]Epoch 12:  34%|███▍      | 103/300 [02:57<05:28,  1.67s/it]Epoch 12:  35%|███▍      | 104/300 [02:59<05:40,  1.74s/it]Epoch 12:  35%|███▌      | 105/300 [03:01<05:48,  1.79s/it]Epoch 12:  35%|███▌      | 106/300 [03:03<05:27,  1.69s/it]Epoch 12:  36%|███▌      | 107/300 [03:05<05:39,  1.76s/it]Epoch 12:  36%|███▌      | 108/300 [03:06<05:40,  1.78s/it]Epoch 12:  36%|███▋      | 109/300 [03:08<05:18,  1.67s/it]Epoch 12:  37%|███▋      | 110/300 [03:09<05:02,  1.59s/it]Epoch 12:  37%|███▋      | 111/300 [03:11<05:19,  1.69s/it]Epoch 12:  37%|███▋      | 112/300 [03:13<05:02,  1.61s/it]Epoch 12:  38%|███▊      | 113/300 [03:14<04:50,  1.55s/it]Epoch 12:  38%|███▊      | 114/300 [03:16<05:03,  1.63s/it]Epoch 12:  38%|███▊      | 115/300 [03:17<04:56,  1.60s/it]Epoch 12:  39%|███▊      | 116/300 [03:19<05:07,  1.67s/it]Epoch 12:  39%|███▉      | 117/300 [03:21<04:50,  1.59s/it]Epoch 12:  39%|███▉      | 118/300 [03:22<04:38,  1.53s/it]Epoch 12:  40%|███▉      | 119/300 [03:23<04:28,  1.48s/it]06/19/2022 16:24:26 - INFO - __main__ - global step: 1860; train loss: 8.357366561889648; dev loss
Epoch 12:  40%|████      | 120/300 [03:25<04:28,  1.49s/it]Epoch 12:  40%|████      | 121/300 [03:27<04:44,  1.59s/it]Epoch 12:  41%|████      | 122/300 [03:28<04:37,  1.56s/it]Epoch 12:  41%|████      | 123/300 [03:30<04:54,  1.66s/it]Epoch 12:  41%|████▏     | 124/300 [03:32<04:52,  1.66s/it]Epoch 12:  42%|████▏     | 125/300 [03:33<04:43,  1.62s/it]Epoch 12:  42%|████▏     | 126/300 [03:35<04:38,  1.60s/it]Epoch 12:  42%|████▏     | 127/300 [03:36<04:32,  1.57s/it]Epoch 12:  43%|████▎     | 128/300 [03:38<04:33,  1.59s/it]Epoch 12:  43%|████▎     | 129/300 [03:40<04:49,  1.69s/it]Epoch 12:  43%|████▎     | 130/300 [03:42<04:59,  1.76s/it]Epoch 12:  44%|████▎     | 131/300 [03:44<05:06,  1.81s/it]Epoch 12:  44%|████▍     | 132/300 [03:46<05:07,  1.83s/it]Epoch 12:  44%|████▍     | 133/300 [03:47<04:49,  1.73s/it]Epoch 12:  45%|████▍     | 134/300 [03:49<04:35,  1.66s/it]Epoch 12:  45%|████▌     | 135/300 [03:50<04:25,  1.61s/it]Epoch 12:  45%|████▌     | 136/300 [03:52<04:21,  1.59s/it]Epoch 12:  46%|████▌     | 137/300 [03:53<04:14,  1.56s/it]Epoch 12:  46%|████▌     | 138/300 [03:55<04:30,  1.67s/it]Epoch 12:  46%|████▋     | 139/300 [03:57<04:40,  1.74s/it]06/19/2022 16:25:00 - INFO - __main__ - global step: 1870; train loss: 7.712533473968506; dev loss
Epoch 12:  47%|████▋     | 140/300 [03:59<04:31,  1.70s/it]Epoch 12:  47%|████▋     | 141/300 [04:00<04:20,  1.64s/it]Epoch 12:  47%|████▋     | 142/300 [04:02<04:13,  1.60s/it]Epoch 12:  48%|████▊     | 143/300 [04:03<04:25,  1.69s/it]Epoch 12:  48%|████▊     | 144/300 [04:05<04:34,  1.76s/it]Epoch 12:  48%|████▊     | 145/300 [04:07<04:43,  1.83s/it]Epoch 12:  49%|████▊     | 146/300 [04:09<04:25,  1.73s/it]Epoch 12:  49%|████▉     | 147/300 [04:11<04:32,  1.78s/it]Epoch 12:  49%|████▉     | 148/300 [04:13<04:37,  1.83s/it]Epoch 12:  50%|████▉     | 149/300 [04:14<04:24,  1.75s/it]Epoch 12:  50%|█████     | 150/300 [04:16<04:10,  1.67s/it]Epoch 12:  50%|█████     | 151/300 [04:17<04:00,  1.62s/it]Epoch 12:  51%|█████     | 152/300 [04:19<04:13,  1.71s/it]Epoch 12:  51%|█████     | 153/300 [04:21<04:06,  1.68s/it]Epoch 12:  51%|█████▏    | 154/300 [04:23<04:16,  1.76s/it]Epoch 12:  52%|█████▏    | 155/300 [04:24<04:02,  1.68s/it]Epoch 12:  52%|█████▏    | 156/300 [04:26<04:11,  1.75s/it]Epoch 12:  52%|█████▏    | 157/300 [04:28<04:02,  1.70s/it]Epoch 12:  53%|█████▎    | 158/300 [04:29<03:52,  1.64s/it]Epoch 12:  53%|█████▎    | 159/300 [04:31<04:02,  1.72s/it]06/19/2022 16:25:34 - INFO - __main__ - global step: 1880; train loss: 8.622945785522461; dev loss
Epoch 12:  53%|█████▎    | 160/300 [04:33<04:08,  1.78s/it]Epoch 12:  54%|█████▎    | 161/300 [04:35<03:58,  1.72s/it]Epoch 12:  54%|█████▍    | 162/300 [04:36<03:48,  1.66s/it]Epoch 12:  54%|█████▍    | 163/300 [04:38<03:39,  1.61s/it]Epoch 12:  55%|█████▍    | 164/300 [04:39<03:46,  1.67s/it]Epoch 12:  55%|█████▌    | 165/300 [04:41<03:41,  1.64s/it]Epoch 12:  55%|█████▌    | 166/300 [04:43<03:33,  1.60s/it]Epoch 12:  56%|█████▌    | 167/300 [04:44<03:28,  1.57s/it]Epoch 12:  56%|█████▌    | 168/300 [04:46<03:38,  1.65s/it]Epoch 12:  56%|█████▋    | 169/300 [04:48<03:50,  1.76s/it]Epoch 12:  57%|█████▋    | 170/300 [04:49<03:39,  1.68s/it]Epoch 12:  57%|█████▋    | 171/300 [04:51<03:46,  1.75s/it]Epoch 12:  57%|█████▋    | 172/300 [04:53<03:49,  1.80s/it]Epoch 12:  58%|█████▊    | 173/300 [04:55<03:37,  1.71s/it]Epoch 12:  58%|█████▊    | 174/300 [04:56<03:31,  1.68s/it]Epoch 12:  58%|█████▊    | 175/300 [04:58<03:39,  1.76s/it]Epoch 12:  59%|█████▊    | 176/300 [05:00<03:44,  1.81s/it]Epoch 12:  59%|█████▉    | 177/300 [05:02<03:46,  1.84s/it]Epoch 12:  59%|█████▉    | 178/300 [05:04<03:35,  1.77s/it]Epoch 12:  60%|█████▉    | 179/300 [05:06<03:38,  1.81s/it]06/19/2022 16:26:08 - INFO - __main__ - global step: 1890; train loss: 8.047266960144043; dev loss
Epoch 12:  60%|██████    | 180/300 [05:07<03:25,  1.71s/it]Epoch 12:  60%|██████    | 181/300 [05:09<03:30,  1.77s/it]Epoch 12:  61%|██████    | 182/300 [05:11<03:36,  1.84s/it]Epoch 12:  61%|██████    | 183/300 [05:12<03:23,  1.74s/it]Epoch 12:  61%|██████▏   | 184/300 [05:14<03:27,  1.79s/it]Epoch 12:  62%|██████▏   | 185/300 [05:16<03:30,  1.83s/it]Epoch 12:  62%|██████▏   | 186/300 [05:18<03:34,  1.88s/it]Epoch 12:  62%|██████▏   | 187/300 [05:20<03:34,  1.89s/it]Epoch 12:  63%|██████▎   | 188/300 [05:22<03:19,  1.78s/it]Epoch 12:  63%|██████▎   | 189/300 [05:23<03:08,  1.70s/it]Epoch 12:  63%|██████▎   | 190/300 [05:25<03:02,  1.66s/it]Epoch 12:  64%|██████▎   | 191/300 [05:27<03:09,  1.74s/it]Epoch 12:  64%|██████▍   | 192/300 [05:28<03:00,  1.67s/it]Epoch 12:  64%|██████▍   | 193/300 [05:30<02:52,  1.61s/it]Epoch 12:  65%|██████▍   | 194/300 [05:31<02:50,  1.61s/it]Epoch 12:  65%|██████▌   | 195/300 [05:33<02:58,  1.70s/it]Epoch 12:  65%|██████▌   | 196/300 [05:35<02:50,  1.64s/it]Epoch 12:  66%|██████▌   | 197/300 [05:36<02:43,  1.58s/it]Epoch 12:  66%|██████▌   | 198/300 [05:38<02:38,  1.56s/it]Epoch 12:  66%|██████▋   | 199/300 [05:40<02:50,  1.69s/it]06/19/2022 16:26:43 - INFO - __main__ - global step: 1900; train loss: 8.245893478393555; dev loss
Epoch 12:  67%|██████▋   | 200/300 [05:41<02:43,  1.63s/it]Epoch 12:  67%|██████▋   | 201/300 [05:43<02:49,  1.72s/it]Epoch 12:  67%|██████▋   | 202/300 [05:45<02:42,  1.66s/it]Epoch 12:  68%|██████▊   | 203/300 [05:46<02:39,  1.64s/it]Epoch 12:  68%|██████▊   | 204/300 [05:48<02:32,  1.59s/it]Epoch 12:  68%|██████▊   | 205/300 [05:49<02:29,  1.57s/it]Epoch 12:  69%|██████▊   | 206/300 [05:51<02:25,  1.55s/it]Epoch 12:  69%|██████▉   | 207/300 [05:53<02:33,  1.65s/it]Epoch 12:  69%|██████▉   | 208/300 [05:55<02:39,  1.74s/it]Epoch 12:  70%|██████▉   | 209/300 [05:57<02:43,  1.80s/it]Epoch 12:  70%|███████   | 210/300 [05:58<02:45,  1.84s/it]Epoch 12:  70%|███████   | 211/300 [06:00<02:48,  1.90s/it]Epoch 12:  71%|███████   | 212/300 [06:02<02:47,  1.90s/it]Epoch 12:  71%|███████   | 213/300 [06:04<02:35,  1.78s/it]Epoch 12:  71%|███████▏  | 214/300 [06:06<02:36,  1.82s/it]Epoch 12:  72%|███████▏  | 215/300 [06:08<02:39,  1.87s/it]Epoch 12:  72%|███████▏  | 216/300 [06:09<02:27,  1.76s/it]Epoch 12:  72%|███████▏  | 217/300 [06:11<02:19,  1.68s/it]Epoch 12:  73%|███████▎  | 218/300 [06:12<02:12,  1.61s/it]Epoch 12:  73%|███████▎  | 219/300 [06:14<02:10,  1.61s/it]06/19/2022 16:27:17 - INFO - __main__ - global step: 1910; train loss: 8.156394004821777; dev loss
Epoch 12:  73%|███████▎  | 220/300 [06:15<02:06,  1.58s/it]Epoch 12:  74%|███████▎  | 221/300 [06:17<02:02,  1.55s/it]Epoch 12:  74%|███████▍  | 222/300 [06:19<02:08,  1.65s/it]Epoch 12:  74%|███████▍  | 223/300 [06:20<02:05,  1.64s/it]Epoch 12:  75%|███████▍  | 224/300 [06:22<02:01,  1.59s/it]Epoch 12:  75%|███████▌  | 225/300 [06:24<02:07,  1.70s/it]Epoch 12:  75%|███████▌  | 226/300 [06:25<02:01,  1.64s/it]Epoch 12:  76%|███████▌  | 227/300 [06:27<01:56,  1.60s/it]Epoch 12:  76%|███████▌  | 228/300 [06:29<02:02,  1.70s/it]Epoch 12:  76%|███████▋  | 229/300 [06:30<01:56,  1.64s/it]Epoch 12:  77%|███████▋  | 230/300 [06:32<02:00,  1.72s/it]Epoch 12:  77%|███████▋  | 231/300 [06:34<01:54,  1.66s/it]Epoch 12:  77%|███████▋  | 232/300 [06:36<02:00,  1.77s/it]Epoch 12:  78%|███████▊  | 233/300 [06:37<01:52,  1.68s/it]Epoch 12:  78%|███████▊  | 234/300 [06:39<01:55,  1.75s/it]Epoch 12:  78%|███████▊  | 235/300 [06:41<01:48,  1.68s/it]Epoch 12:  79%|███████▊  | 236/300 [06:43<01:53,  1.77s/it]Epoch 12:  79%|███████▉  | 237/300 [06:44<01:54,  1.81s/it]Epoch 12:  79%|███████▉  | 238/300 [06:46<01:46,  1.72s/it]Epoch 12:  80%|███████▉  | 239/300 [06:47<01:40,  1.65s/it]06/19/2022 16:27:50 - INFO - __main__ - global step: 1920; train loss: 7.637631893157959; dev loss
Epoch 12:  80%|████████  | 240/300 [06:49<01:38,  1.64s/it]Epoch 12:  80%|████████  | 241/300 [06:51<01:41,  1.72s/it]Epoch 12:  81%|████████  | 242/300 [06:53<01:42,  1.77s/it]Epoch 12:  81%|████████  | 243/300 [06:54<01:36,  1.70s/it]Epoch 12:  81%|████████▏ | 244/300 [06:56<01:33,  1.67s/it]Epoch 12:  82%|████████▏ | 245/300 [06:58<01:36,  1.76s/it]Epoch 12:  82%|████████▏ | 246/300 [07:00<01:38,  1.83s/it]Epoch 12:  82%|████████▏ | 247/300 [07:02<01:39,  1.87s/it]Epoch 12:  83%|████████▎ | 248/300 [07:04<01:33,  1.80s/it]Epoch 12:  83%|████████▎ | 249/300 [07:05<01:33,  1.84s/it]Epoch 12:  83%|████████▎ | 250/300 [07:07<01:27,  1.75s/it]Epoch 12:  84%|████████▎ | 251/300 [07:09<01:28,  1.80s/it]Epoch 12:  84%|████████▍ | 252/300 [07:11<01:27,  1.83s/it]Epoch 12:  84%|████████▍ | 253/300 [07:13<01:29,  1.90s/it]Epoch 12:  85%|████████▍ | 254/300 [07:15<01:27,  1.90s/it]Epoch 12:  85%|████████▌ | 255/300 [07:17<01:25,  1.91s/it]Epoch 12:  85%|████████▌ | 256/300 [07:18<01:18,  1.78s/it]Epoch 12:  86%|████████▌ | 257/300 [07:20<01:14,  1.72s/it]Epoch 12:  86%|████████▌ | 258/300 [07:22<01:14,  1.78s/it]Epoch 12:  86%|████████▋ | 259/300 [07:23<01:09,  1.69s/it]06/19/2022 16:28:26 - INFO - __main__ - global step: 1930; train loss: 7.866353511810303; dev loss
Epoch 12:  87%|████████▋ | 260/300 [07:25<01:10,  1.76s/it]Epoch 12:  87%|████████▋ | 261/300 [07:27<01:06,  1.70s/it]Epoch 12:  87%|████████▋ | 262/300 [07:29<01:07,  1.77s/it]Epoch 12:  88%|████████▊ | 263/300 [07:30<01:06,  1.81s/it]Epoch 12:  88%|████████▊ | 264/300 [07:32<01:06,  1.85s/it]Epoch 12:  88%|████████▊ | 265/300 [07:34<01:06,  1.90s/it]Epoch 12:  89%|████████▊ | 266/300 [07:36<01:04,  1.91s/it]Epoch 12:  89%|████████▉ | 267/300 [07:38<00:58,  1.78s/it]Epoch 12:  89%|████████▉ | 268/300 [07:39<00:54,  1.69s/it]Epoch 12:  90%|████████▉ | 269/300 [07:41<00:55,  1.79s/it]Epoch 12:  90%|█████████ | 270/300 [07:43<00:54,  1.83s/it]Epoch 12:  90%|█████████ | 271/300 [07:45<00:53,  1.86s/it]Epoch 12:  91%|█████████ | 272/300 [07:47<00:49,  1.76s/it]Epoch 12:  91%|█████████ | 273/300 [07:49<00:50,  1.85s/it]Epoch 12:  91%|█████████▏| 274/300 [07:51<00:48,  1.88s/it]Epoch 12:  92%|█████████▏| 275/300 [07:52<00:44,  1.77s/it]Epoch 12:  92%|█████████▏| 276/300 [07:54<00:40,  1.69s/it]Epoch 12:  92%|█████████▏| 277/300 [07:55<00:38,  1.66s/it]Epoch 12:  93%|█████████▎| 278/300 [07:57<00:35,  1.60s/it]Epoch 12:  93%|█████████▎| 279/300 [07:58<00:32,  1.56s/it]06/19/2022 16:29:01 - INFO - __main__ - global step: 1940; train loss: 8.00481128692627; dev loss
Epoch 12:  93%|█████████▎| 280/300 [08:00<00:31,  1.56s/it]Epoch 12:  94%|█████████▎| 281/300 [08:01<00:29,  1.55s/it]Epoch 12:  94%|█████████▍| 282/300 [08:03<00:29,  1.65s/it]Epoch 12:  94%|█████████▍| 283/300 [08:05<00:28,  1.70s/it]Epoch 12:  95%|█████████▍| 284/300 [08:07<00:26,  1.63s/it]Epoch 12:  95%|█████████▌| 285/300 [08:08<00:23,  1.58s/it]Epoch 12:  95%|█████████▌| 286/300 [08:10<00:22,  1.58s/it]Epoch 12:  96%|█████████▌| 287/300 [08:11<00:20,  1.55s/it]Epoch 12:  96%|█████████▌| 288/300 [08:13<00:19,  1.64s/it]Epoch 12:  96%|█████████▋| 289/300 [08:14<00:17,  1.58s/it]Epoch 12:  97%|█████████▋| 290/300 [08:16<00:15,  1.55s/it]Epoch 12:  97%|█████████▋| 291/300 [08:18<00:14,  1.63s/it]Epoch 12:  97%|█████████▋| 292/300 [08:19<00:12,  1.59s/it]Epoch 12:  98%|█████████▊| 293/300 [08:21<00:11,  1.69s/it]Epoch 12:  98%|█████████▊| 294/300 [08:23<00:10,  1.78s/it]Epoch 12:  98%|█████████▊| 295/300 [08:25<00:09,  1.80s/it]Epoch 12:  99%|█████████▊| 296/300 [08:27<00:07,  1.81s/it]Epoch 12:  99%|█████████▉| 297/300 [08:29<00:05,  1.83s/it]Epoch 12:  99%|█████████▉| 298/300 [08:31<00:03,  1.86s/it]Epoch 12: 100%|█████████▉| 299/300 [08:32<00:01,  1.73s/it]06/19/2022 16:29:35 - INFO - __main__ - global step: 1950; train loss: 7.841177463531494; dev loss
Epoch 12: 100%|██████████| 300/300 [08:34<00:00,  1.76s/it]Epoch 12: 100%|██████████| 300/300 [08:34<00:00,  1.71s/it]
Epoch 13:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 13:   0%|          | 1/300 [00:01<09:11,  1.85s/it]Epoch 13:   1%|          | 2/300 [00:03<09:31,  1.92s/it]Epoch 13:   1%|          | 3/300 [00:05<08:17,  1.67s/it]Epoch 13:   1%|▏         | 4/300 [00:06<07:40,  1.56s/it]Epoch 13:   2%|▏         | 5/300 [00:08<08:08,  1.66s/it]Epoch 13:   2%|▏         | 6/300 [00:09<07:45,  1.58s/it]Epoch 13:   2%|▏         | 7/300 [00:11<08:17,  1.70s/it]Epoch 13:   3%|▎         | 8/300 [00:13<07:51,  1.62s/it]Epoch 13:   3%|▎         | 9/300 [00:14<07:30,  1.55s/it]Epoch 13:   3%|▎         | 10/300 [00:16<07:53,  1.63s/it]Epoch 13:   4%|▎         | 11/300 [00:18<08:18,  1.72s/it]Epoch 13:   4%|▍         | 12/300 [00:19<07:50,  1.63s/it]Epoch 13:   4%|▍         | 13/300 [00:21<08:06,  1.70s/it]Epoch 13:   5%|▍         | 14/300 [00:23<07:40,  1.61s/it]Epoch 13:   5%|▌         | 15/300 [00:25<08:08,  1.71s/it]Epoch 13:   5%|▌         | 16/300 [00:26<08:17,  1.75s/it]Epoch 13:   6%|▌         | 17/300 [00:28<07:46,  1.65s/it]Epoch 13:   6%|▌         | 18/300 [00:30<08:00,  1.70s/it]Epoch 13:   6%|▋         | 19/300 [00:31<07:44,  1.65s/it]06/19/2022 16:30:08 - INFO - __main__ - global step: 1960; train loss: 7.480837345123291; dev loss
Epoch 13:   7%|▋         | 20/300 [00:33<07:22,  1.58s/it]Epoch 13:   7%|▋         | 21/300 [00:34<07:42,  1.66s/it]Epoch 13:   7%|▋         | 22/300 [00:36<07:58,  1.72s/it]Epoch 13:   8%|▊         | 23/300 [00:38<07:39,  1.66s/it]Epoch 13:   8%|▊         | 24/300 [00:40<07:51,  1.71s/it]Epoch 13:   8%|▊         | 25/300 [00:41<07:27,  1.63s/it]Epoch 13:   9%|▊         | 26/300 [00:42<07:09,  1.57s/it]Epoch 13:   9%|▉         | 27/300 [00:44<07:38,  1.68s/it]Epoch 13:   9%|▉         | 28/300 [00:46<07:17,  1.61s/it]Epoch 13:  10%|▉         | 29/300 [00:47<06:59,  1.55s/it]Epoch 13:  10%|█         | 30/300 [00:49<07:21,  1.64s/it]Epoch 13:  10%|█         | 31/300 [00:51<07:11,  1.60s/it]Epoch 13:  11%|█         | 32/300 [00:52<07:28,  1.67s/it]Epoch 13:  11%|█         | 33/300 [00:54<07:06,  1.60s/it]Epoch 13:  11%|█▏        | 34/300 [00:56<07:22,  1.66s/it]Epoch 13:  12%|█▏        | 35/300 [00:57<07:02,  1.60s/it]Epoch 13:  12%|█▏        | 36/300 [00:59<07:28,  1.70s/it]Epoch 13:  12%|█▏        | 37/300 [01:01<07:38,  1.74s/it]Epoch 13:  13%|█▎        | 38/300 [01:03<07:43,  1.77s/it]Epoch 13:  13%|█▎        | 39/300 [01:05<07:46,  1.79s/it]06/19/2022 16:30:42 - INFO - __main__ - global step: 1970; train loss: 8.273236274719238; dev loss
Epoch 13:  13%|█▎        | 40/300 [01:06<07:23,  1.71s/it]Epoch 13:  14%|█▎        | 41/300 [01:07<06:59,  1.62s/it]Epoch 13:  14%|█▍        | 42/300 [01:09<07:14,  1.68s/it]Epoch 13:  14%|█▍        | 43/300 [01:11<07:25,  1.73s/it]Epoch 13:  15%|█▍        | 44/300 [01:13<07:07,  1.67s/it]Epoch 13:  15%|█▌        | 45/300 [01:14<06:46,  1.59s/it]Epoch 13:  15%|█▌        | 46/300 [01:16<07:03,  1.67s/it]Epoch 13:  16%|█▌        | 47/300 [01:17<06:43,  1.60s/it]Epoch 13:  16%|█▌        | 48/300 [01:19<06:35,  1.57s/it]Epoch 13:  16%|█▋        | 49/300 [01:21<06:53,  1.65s/it]Epoch 13:  17%|█▋        | 50/300 [01:23<07:08,  1.71s/it]Epoch 13:  17%|█▋        | 51/300 [01:24<06:44,  1.63s/it]Epoch 13:  17%|█▋        | 52/300 [01:26<06:36,  1.60s/it]Epoch 13:  18%|█▊        | 53/300 [01:27<06:19,  1.54s/it]Epoch 13:  18%|█▊        | 54/300 [01:29<06:41,  1.63s/it]Epoch 13:  18%|█▊        | 55/300 [01:30<06:24,  1.57s/it]Epoch 13:  19%|█▊        | 56/300 [01:32<06:49,  1.68s/it]Epoch 13:  19%|█▉        | 57/300 [01:34<06:29,  1.60s/it]Epoch 13:  19%|█▉        | 58/300 [01:35<06:43,  1.67s/it]Epoch 13:  20%|█▉        | 59/300 [01:37<06:25,  1.60s/it]06/19/2022 16:31:14 - INFO - __main__ - global step: 1980; train loss: 8.200043678283691; dev loss
Epoch 13:  20%|██        | 60/300 [01:38<06:10,  1.54s/it]Epoch 13:  20%|██        | 61/300 [01:40<06:07,  1.54s/it]Epoch 13:  21%|██        | 62/300 [01:42<06:25,  1.62s/it]Epoch 13:  21%|██        | 63/300 [01:43<06:08,  1.55s/it]Epoch 13:  21%|██▏       | 64/300 [01:44<05:56,  1.51s/it]Epoch 13:  22%|██▏       | 65/300 [01:46<06:24,  1.63s/it]Epoch 13:  22%|██▏       | 66/300 [01:48<06:07,  1.57s/it]Epoch 13:  22%|██▏       | 67/300 [01:49<05:56,  1.53s/it]Epoch 13:  23%|██▎       | 68/300 [01:51<06:15,  1.62s/it]Epoch 13:  23%|██▎       | 69/300 [01:53<06:34,  1.71s/it]Epoch 13:  23%|██▎       | 70/300 [01:54<06:12,  1.62s/it]Epoch 13:  24%|██▎       | 71/300 [01:56<05:57,  1.56s/it]Epoch 13:  24%|██▍       | 72/300 [01:57<05:46,  1.52s/it]Epoch 13:  24%|██▍       | 73/300 [01:59<05:45,  1.52s/it]Epoch 13:  25%|██▍       | 74/300 [02:00<05:36,  1.49s/it]Epoch 13:  25%|██▌       | 75/300 [02:02<05:29,  1.47s/it]Epoch 13:  25%|██▌       | 76/300 [02:03<05:25,  1.45s/it]Epoch 13:  26%|██▌       | 77/300 [02:05<05:56,  1.60s/it]Epoch 13:  26%|██▌       | 78/300 [02:06<05:42,  1.54s/it]Epoch 13:  26%|██▋       | 79/300 [02:08<05:30,  1.49s/it]06/19/2022 16:31:45 - INFO - __main__ - global step: 1990; train loss: 8.074991226196289; dev loss
Epoch 13:  27%|██▋       | 80/300 [02:10<05:51,  1.60s/it]Epoch 13:  27%|██▋       | 81/300 [02:11<05:44,  1.57s/it]Epoch 13:  27%|██▋       | 82/300 [02:13<06:02,  1.66s/it]Epoch 13:  28%|██▊       | 83/300 [02:14<05:44,  1.59s/it]Epoch 13:  28%|██▊       | 84/300 [02:16<05:58,  1.66s/it]Epoch 13:  28%|██▊       | 85/300 [02:18<05:49,  1.63s/it]Epoch 13:  29%|██▊       | 86/300 [02:19<06:00,  1.68s/it]Epoch 13:  29%|██▉       | 87/300 [02:21<06:10,  1.74s/it]Epoch 13:  29%|██▉       | 88/300 [02:23<05:48,  1.64s/it]Epoch 13:  30%|██▉       | 89/300 [02:25<05:57,  1.70s/it]Epoch 13:  30%|███       | 90/300 [02:26<05:45,  1.65s/it]Epoch 13:  30%|███       | 91/300 [02:28<05:30,  1.58s/it]Epoch 13:  31%|███       | 92/300 [02:29<05:44,  1.66s/it]Epoch 13:  31%|███       | 93/300 [02:31<05:30,  1.60s/it]Epoch 13:  31%|███▏      | 94/300 [02:33<05:47,  1.69s/it]Epoch 13:  32%|███▏      | 95/300 [02:35<05:55,  1.73s/it]Epoch 13:  32%|███▏      | 96/300 [02:36<05:35,  1.64s/it]Epoch 13:  32%|███▏      | 97/300 [02:38<05:44,  1.70s/it]Epoch 13:  33%|███▎      | 98/300 [02:40<05:57,  1.77s/it]Epoch 13:  33%|███▎      | 99/300 [02:41<05:35,  1.67s/it]06/19/2022 16:32:19 - INFO - __main__ - global step: 2000; train loss: 7.9995222091674805; dev loss
Epoch 13:  33%|███▎      | 100/300 [02:43<05:42,  1.71s/it]Epoch 13:  34%|███▎      | 101/300 [02:44<05:23,  1.63s/it]Epoch 13:  34%|███▍      | 102/300 [02:46<05:35,  1.69s/it]Epoch 13:  34%|███▍      | 103/300 [02:48<05:18,  1.62s/it]Epoch 13:  35%|███▍      | 104/300 [02:50<05:25,  1.66s/it]Epoch 13:  35%|███▌      | 105/300 [02:51<05:32,  1.70s/it]Epoch 13:  35%|███▌      | 106/300 [02:53<05:44,  1.77s/it]Epoch 13:  36%|███▌      | 107/300 [02:55<05:22,  1.67s/it]Epoch 13:  36%|███▌      | 108/300 [02:57<05:31,  1.72s/it]Epoch 13:  36%|███▋      | 109/300 [02:58<05:13,  1.64s/it]Epoch 13:  37%|███▋      | 110/300 [03:00<05:29,  1.73s/it]Epoch 13:  37%|███▋      | 111/300 [03:01<05:10,  1.64s/it]Epoch 13:  37%|███▋      | 112/300 [03:03<04:56,  1.58s/it]Epoch 13:  38%|███▊      | 113/300 [03:04<04:45,  1.53s/it]Epoch 13:  38%|███▊      | 114/300 [03:06<05:01,  1.62s/it]Epoch 13:  38%|███▊      | 115/300 [03:08<04:54,  1.59s/it]Epoch 13:  39%|███▊      | 116/300 [03:09<05:07,  1.67s/it]Epoch 13:  39%|███▉      | 117/300 [03:11<04:53,  1.60s/it]Epoch 13:  39%|███▉      | 118/300 [03:12<04:41,  1.55s/it]Epoch 13:  40%|███▉      | 119/300 [03:14<04:38,  1.54s/it]06/19/2022 16:32:51 - INFO - __main__ - global step: 2010; train loss: 7.9714155197143555; dev loss
Epoch 13:  40%|████      | 120/300 [03:16<04:52,  1.62s/it]Epoch 13:  40%|████      | 121/300 [03:17<05:02,  1.69s/it]Epoch 13:  41%|████      | 122/300 [03:19<04:47,  1.61s/it]Epoch 13:  41%|████      | 123/300 [03:21<05:00,  1.70s/it]Epoch 13:  41%|████▏     | 124/300 [03:22<04:43,  1.61s/it]Epoch 13:  42%|████▏     | 125/300 [03:24<04:31,  1.55s/it]Epoch 13:  42%|████▏     | 126/300 [03:25<04:44,  1.63s/it]Epoch 13:  42%|████▏     | 127/300 [03:27<04:36,  1.60s/it]Epoch 13:  43%|████▎     | 128/300 [03:28<04:26,  1.55s/it]Epoch 13:  43%|████▎     | 129/300 [03:30<04:39,  1.64s/it]Epoch 13:  43%|████▎     | 130/300 [03:32<04:26,  1.57s/it]Epoch 13:  44%|████▎     | 131/300 [03:34<04:43,  1.68s/it]Epoch 13:  44%|████▍     | 132/300 [03:35<04:50,  1.73s/it]Epoch 13:  44%|████▍     | 133/300 [03:37<04:34,  1.64s/it]Epoch 13:  45%|████▍     | 134/300 [03:39<04:41,  1.70s/it]Epoch 13:  45%|████▌     | 135/300 [03:41<04:52,  1.77s/it]Epoch 13:  45%|████▌     | 136/300 [03:42<04:53,  1.79s/it]Epoch 13:  46%|████▌     | 137/300 [03:44<04:33,  1.68s/it]Epoch 13:  46%|████▌     | 138/300 [03:46<04:38,  1.72s/it]Epoch 13:  46%|████▋     | 139/300 [03:47<04:26,  1.65s/it]06/19/2022 16:33:25 - INFO - __main__ - global step: 2020; train loss: 7.908772945404053; dev loss
Epoch 13:  47%|████▋     | 140/300 [03:49<04:34,  1.71s/it]Epoch 13:  47%|████▋     | 141/300 [03:50<04:19,  1.63s/it]Epoch 13:  47%|████▋     | 142/300 [03:52<04:26,  1.69s/it]Epoch 13:  48%|████▊     | 143/300 [03:54<04:13,  1.61s/it]Epoch 13:  48%|████▊     | 144/300 [03:56<04:25,  1.70s/it]Epoch 13:  48%|████▊     | 145/300 [03:57<04:29,  1.74s/it]Epoch 13:  49%|████▊     | 146/300 [03:59<04:32,  1.77s/it]Epoch 13:  49%|████▉     | 147/300 [04:01<04:15,  1.67s/it]Epoch 13:  49%|████▉     | 148/300 [04:02<04:05,  1.62s/it]Epoch 13:  50%|████▉     | 149/300 [04:04<03:53,  1.55s/it]Epoch 13:  50%|█████     | 150/300 [04:05<03:46,  1.51s/it]Epoch 13:  50%|█████     | 151/300 [04:07<03:59,  1.61s/it]Epoch 13:  51%|█████     | 152/300 [04:09<04:12,  1.71s/it]Epoch 13:  51%|█████     | 153/300 [04:11<04:16,  1.74s/it]Epoch 13:  51%|█████▏    | 154/300 [04:12<04:18,  1.77s/it]Epoch 13:  52%|█████▏    | 155/300 [04:14<04:20,  1.80s/it]Epoch 13:  52%|█████▏    | 156/300 [04:16<04:06,  1.71s/it]Epoch 13:  52%|█████▏    | 157/300 [04:18<04:11,  1.76s/it]Epoch 13:  53%|█████▎    | 158/300 [04:20<04:13,  1.79s/it]Epoch 13:  53%|█████▎    | 159/300 [04:21<04:13,  1.80s/it]06/19/2022 16:33:59 - INFO - __main__ - global step: 2030; train loss: 7.9167585372924805; dev loss
Epoch 13:  53%|█████▎    | 160/300 [04:23<04:17,  1.84s/it]Epoch 13:  54%|█████▎    | 161/300 [04:25<04:15,  1.84s/it]Epoch 13:  54%|█████▍    | 162/300 [04:27<04:13,  1.84s/it]Epoch 13:  54%|█████▍    | 163/300 [04:29<04:12,  1.84s/it]Epoch 13:  55%|█████▍    | 164/300 [04:31<04:14,  1.87s/it]Epoch 13:  55%|█████▌    | 165/300 [04:33<04:10,  1.86s/it]Epoch 13:  55%|█████▌    | 166/300 [04:34<03:51,  1.73s/it]Epoch 13:  56%|█████▌    | 167/300 [04:36<03:52,  1.75s/it]Epoch 13:  56%|█████▌    | 168/300 [04:38<03:53,  1.77s/it]Epoch 13:  56%|█████▋    | 169/300 [04:40<03:59,  1.83s/it]Epoch 13:  57%|█████▋    | 170/300 [04:41<03:41,  1.70s/it]Epoch 13:  57%|█████▋    | 171/300 [04:43<03:43,  1.73s/it]Epoch 13:  57%|█████▋    | 172/300 [04:45<03:46,  1.77s/it]Epoch 13:  58%|█████▊    | 173/300 [04:47<03:51,  1.82s/it]Epoch 13:  58%|█████▊    | 174/300 [04:48<03:33,  1.69s/it]Epoch 13:  58%|█████▊    | 175/300 [04:49<03:21,  1.62s/it]Epoch 13:  59%|█████▊    | 176/300 [04:51<03:12,  1.56s/it]Epoch 13:  59%|█████▉    | 177/300 [04:52<03:08,  1.54s/it]Epoch 13:  59%|█████▉    | 178/300 [04:54<03:02,  1.50s/it]Epoch 13:  60%|█████▉    | 179/300 [04:55<02:57,  1.47s/it]06/19/2022 16:34:33 - INFO - __main__ - global step: 2040; train loss: 7.8823561668396; dev loss
Epoch 13:  60%|██████    | 180/300 [04:57<03:09,  1.58s/it]Epoch 13:  60%|██████    | 181/300 [04:59<03:04,  1.55s/it]Epoch 13:  61%|██████    | 182/300 [05:00<03:13,  1.64s/it]Epoch 13:  61%|██████    | 183/300 [05:02<03:03,  1.57s/it]Epoch 13:  61%|██████▏   | 184/300 [05:03<02:57,  1.53s/it]Epoch 13:  62%|██████▏   | 185/300 [05:05<02:55,  1.52s/it]Epoch 13:  62%|██████▏   | 186/300 [05:06<02:49,  1.49s/it]Epoch 13:  62%|██████▏   | 187/300 [05:08<02:45,  1.46s/it]Epoch 13:  63%|██████▎   | 188/300 [05:09<02:42,  1.45s/it]Epoch 13:  63%|██████▎   | 189/300 [05:10<02:42,  1.47s/it]Epoch 13:  63%|██████▎   | 190/300 [05:12<02:52,  1.57s/it]Epoch 13:  64%|██████▎   | 191/300 [05:14<02:59,  1.65s/it]Epoch 13:  64%|██████▍   | 192/300 [05:15<02:50,  1.58s/it]Epoch 13:  64%|██████▍   | 193/300 [05:17<02:46,  1.55s/it]Epoch 13:  65%|██████▍   | 194/300 [05:19<02:53,  1.64s/it]Epoch 13:  65%|██████▌   | 195/300 [05:20<02:46,  1.58s/it]Epoch 13:  65%|██████▌   | 196/300 [05:22<02:51,  1.65s/it]Epoch 13:  66%|██████▌   | 197/300 [05:24<02:43,  1.58s/it]Epoch 13:  66%|██████▌   | 198/300 [05:25<02:51,  1.68s/it]Epoch 13:  66%|██████▋   | 199/300 [05:27<02:53,  1.72s/it]06/19/2022 16:35:05 - INFO - __main__ - global step: 2050; train loss: 8.310009956359863; dev loss
Epoch 13:  67%|██████▋   | 200/300 [05:29<02:55,  1.76s/it]Epoch 13:  67%|██████▋   | 201/300 [05:31<02:56,  1.78s/it]Epoch 13:  67%|██████▋   | 202/300 [05:33<02:58,  1.82s/it]Epoch 13:  68%|██████▊   | 203/300 [05:34<02:45,  1.71s/it]Epoch 13:  68%|██████▊   | 204/300 [05:36<02:47,  1.74s/it]Epoch 13:  68%|██████▊   | 205/300 [05:38<02:48,  1.77s/it]Epoch 13:  69%|██████▊   | 206/300 [05:40<02:51,  1.82s/it]Epoch 13:  69%|██████▉   | 207/300 [05:41<02:39,  1.71s/it]Epoch 13:  69%|██████▉   | 208/300 [05:43<02:28,  1.62s/it]Epoch 13:  70%|██████▉   | 209/300 [05:44<02:22,  1.56s/it]Epoch 13:  70%|███████   | 210/300 [05:46<02:19,  1.55s/it]Epoch 13:  70%|███████   | 211/300 [05:47<02:13,  1.50s/it]Epoch 13:  71%|███████   | 212/300 [05:49<02:21,  1.61s/it]Epoch 13:  71%|███████   | 213/300 [05:51<02:26,  1.68s/it]Epoch 13:  71%|███████▏  | 214/300 [05:53<02:31,  1.76s/it]Epoch 13:  72%|███████▏  | 215/300 [05:54<02:21,  1.66s/it]Epoch 13:  72%|███████▏  | 216/300 [05:56<02:13,  1.59s/it]Epoch 13:  72%|███████▏  | 217/300 [05:57<02:17,  1.66s/it]Epoch 13:  73%|███████▎  | 218/300 [05:59<02:22,  1.74s/it]Epoch 13:  73%|███████▎  | 219/300 [06:01<02:24,  1.78s/it]06/19/2022 16:35:39 - INFO - __main__ - global step: 2060; train loss: 7.820706367492676; dev loss
Epoch 13:  73%|███████▎  | 220/300 [06:03<02:23,  1.80s/it]Epoch 13:  74%|███████▎  | 221/300 [06:05<02:22,  1.81s/it]Epoch 13:  74%|███████▍  | 222/300 [06:07<02:21,  1.82s/it]Epoch 13:  74%|███████▍  | 223/300 [06:08<02:13,  1.73s/it]Epoch 13:  75%|███████▍  | 224/300 [06:10<02:14,  1.77s/it]Epoch 13:  75%|███████▌  | 225/300 [06:12<02:14,  1.80s/it]Epoch 13:  75%|███████▌  | 226/300 [06:13<02:04,  1.69s/it]Epoch 13:  76%|███████▌  | 227/300 [06:15<01:58,  1.63s/it]Epoch 13:  76%|███████▌  | 228/300 [06:17<02:02,  1.70s/it]Epoch 13:  76%|███████▋  | 229/300 [06:18<01:54,  1.61s/it]Epoch 13:  77%|███████▋  | 230/300 [06:20<01:57,  1.68s/it]Epoch 13:  77%|███████▋  | 231/300 [06:22<02:00,  1.75s/it]Epoch 13:  77%|███████▋  | 232/300 [06:23<01:52,  1.65s/it]Epoch 13:  78%|███████▊  | 233/300 [06:25<01:53,  1.70s/it]Epoch 13:  78%|███████▊  | 234/300 [06:27<01:54,  1.74s/it]Epoch 13:  78%|███████▊  | 235/300 [06:28<01:48,  1.67s/it]Epoch 13:  79%|███████▊  | 236/300 [06:30<01:42,  1.59s/it]Epoch 13:  79%|███████▉  | 237/300 [06:31<01:37,  1.54s/it]Epoch 13:  79%|███████▉  | 238/300 [06:33<01:33,  1.51s/it]Epoch 13:  80%|███████▉  | 239/300 [06:34<01:31,  1.50s/it]06/19/2022 16:36:11 - INFO - __main__ - global step: 2070; train loss: 7.8155951499938965; dev loss
Epoch 13:  80%|████████  | 240/300 [06:36<01:28,  1.48s/it]Epoch 13:  80%|████████  | 241/300 [06:37<01:25,  1.46s/it]Epoch 13:  81%|████████  | 242/300 [06:38<01:23,  1.44s/it]Epoch 13:  81%|████████  | 243/300 [06:40<01:30,  1.58s/it]Epoch 13:  81%|████████▏ | 244/300 [06:42<01:25,  1.53s/it]Epoch 13:  82%|████████▏ | 245/300 [06:43<01:22,  1.50s/it]Epoch 13:  82%|████████▏ | 246/300 [06:45<01:20,  1.48s/it]Epoch 13:  82%|████████▏ | 247/300 [06:47<01:25,  1.62s/it]Epoch 13:  83%|████████▎ | 248/300 [06:48<01:20,  1.55s/it]Epoch 13:  83%|████████▎ | 249/300 [06:50<01:23,  1.63s/it]Epoch 13:  83%|████████▎ | 250/300 [06:51<01:18,  1.57s/it]Epoch 13:  84%|████████▎ | 251/300 [06:53<01:20,  1.65s/it]Epoch 13:  84%|████████▍ | 252/300 [06:55<01:17,  1.61s/it]Epoch 13:  84%|████████▍ | 253/300 [06:56<01:12,  1.55s/it]Epoch 13:  85%|████████▍ | 254/300 [06:57<01:09,  1.51s/it]Epoch 13:  85%|████████▌ | 255/300 [06:59<01:12,  1.60s/it]Epoch 13:  85%|████████▌ | 256/300 [07:01<01:14,  1.70s/it]Epoch 13:  86%|████████▌ | 257/300 [07:03<01:14,  1.74s/it]Epoch 13:  86%|████████▌ | 258/300 [07:04<01:08,  1.64s/it]Epoch 13:  86%|████████▋ | 259/300 [07:06<01:09,  1.69s/it]06/19/2022 16:36:44 - INFO - __main__ - global step: 2080; train loss: 7.53317403793335; dev loss
Epoch 13:  87%|████████▋ | 260/300 [07:08<01:10,  1.76s/it]Epoch 13:  87%|████████▋ | 261/300 [07:10<01:09,  1.79s/it]Epoch 13:  87%|████████▋ | 262/300 [07:11<01:04,  1.68s/it]Epoch 13:  88%|████████▊ | 263/300 [07:13<00:59,  1.60s/it]Epoch 13:  88%|████████▊ | 264/300 [07:14<00:56,  1.57s/it]Epoch 13:  88%|████████▊ | 265/300 [07:16<00:53,  1.53s/it]Epoch 13:  89%|████████▊ | 266/300 [07:17<00:50,  1.49s/it]Epoch 13:  89%|████████▉ | 267/300 [07:19<00:52,  1.59s/it]Epoch 13:  89%|████████▉ | 268/300 [07:21<00:54,  1.69s/it]Epoch 13:  90%|████████▉ | 269/300 [07:22<00:49,  1.61s/it]Epoch 13:  90%|█████████ | 270/300 [07:24<00:50,  1.68s/it]Epoch 13:  90%|█████████ | 271/300 [07:26<00:50,  1.73s/it]Epoch 13:  91%|█████████ | 272/300 [07:28<00:46,  1.66s/it]Epoch 13:  91%|█████████ | 273/300 [07:29<00:46,  1.71s/it]Epoch 13:  91%|█████████▏| 274/300 [07:31<00:42,  1.62s/it]Epoch 13:  92%|█████████▏| 275/300 [07:33<00:41,  1.68s/it]Epoch 13:  92%|█████████▏| 276/300 [07:34<00:41,  1.73s/it]Epoch 13:  92%|█████████▏| 277/300 [07:36<00:41,  1.79s/it]Epoch 13:  93%|█████████▎| 278/300 [07:38<00:39,  1.80s/it]Epoch 13:  93%|█████████▎| 279/300 [07:40<00:35,  1.69s/it]06/19/2022 16:37:17 - INFO - __main__ - global step: 2090; train loss: 7.879908084869385; dev loss
Epoch 13:  93%|█████████▎| 280/300 [07:41<00:32,  1.61s/it]Epoch 13:  94%|█████████▎| 281/300 [07:43<00:29,  1.58s/it]Epoch 13:  94%|█████████▍| 282/300 [07:44<00:29,  1.66s/it]Epoch 13:  94%|█████████▍| 283/300 [07:46<00:27,  1.59s/it]Epoch 13:  95%|█████████▍| 284/300 [07:48<00:26,  1.67s/it]Epoch 13:  95%|█████████▌| 285/300 [07:50<00:26,  1.75s/it]Epoch 13:  95%|█████████▌| 286/300 [07:51<00:23,  1.66s/it]Epoch 13:  96%|█████████▌| 287/300 [07:53<00:20,  1.59s/it]Epoch 13:  96%|█████████▌| 288/300 [07:54<00:18,  1.54s/it]Epoch 13:  96%|█████████▋| 289/300 [07:55<00:16,  1.53s/it]Epoch 13:  97%|█████████▋| 290/300 [07:57<00:16,  1.62s/it]Epoch 13:  97%|█████████▋| 291/300 [07:59<00:14,  1.56s/it]Epoch 13:  97%|█████████▋| 292/300 [08:00<00:12,  1.51s/it]Epoch 13:  98%|█████████▊| 293/300 [08:02<00:10,  1.52s/it]Epoch 13:  98%|█████████▊| 294/300 [08:03<00:09,  1.51s/it]Epoch 13:  98%|█████████▊| 295/300 [08:05<00:08,  1.64s/it]Epoch 13:  99%|█████████▊| 296/300 [08:07<00:06,  1.72s/it]Epoch 13:  99%|█████████▉| 297/300 [08:09<00:05,  1.82s/it]Epoch 13:  99%|█████████▉| 298/300 [08:11<00:03,  1.85s/it]Epoch 13: 100%|█████████▉| 299/300 [08:13<00:01,  1.87s/it]06/19/2022 16:37:50 - INFO - __main__ - global step: 2100; train loss: 7.9290571212768555; dev loss
Epoch 13: 100%|██████████| 300/300 [08:14<00:00,  1.74s/it]Epoch 13: 100%|██████████| 300/300 [08:14<00:00,  1.65s/it]
Epoch 14:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 14:   0%|          | 1/300 [00:01<07:41,  1.54s/it]Epoch 14:   1%|          | 2/300 [00:03<08:25,  1.69s/it]Epoch 14:   1%|          | 3/300 [00:05<08:43,  1.76s/it]Epoch 14:   1%|▏         | 4/300 [00:07<08:51,  1.79s/it]Epoch 14:   2%|▏         | 5/300 [00:08<08:53,  1.81s/it]Epoch 14:   2%|▏         | 6/300 [00:10<09:03,  1.85s/it]Epoch 14:   2%|▏         | 7/300 [00:12<08:17,  1.70s/it]Epoch 14:   3%|▎         | 8/300 [00:13<07:48,  1.61s/it]Epoch 14:   3%|▎         | 9/300 [00:15<07:29,  1.55s/it]Epoch 14:   3%|▎         | 10/300 [00:16<07:28,  1.55s/it]Epoch 14:   4%|▎         | 11/300 [00:18<07:50,  1.63s/it]Epoch 14:   4%|▍         | 12/300 [00:20<08:00,  1.67s/it]Epoch 14:   4%|▍         | 13/300 [00:21<07:34,  1.58s/it]Epoch 14:   5%|▍         | 14/300 [00:23<08:02,  1.69s/it]Epoch 14:   5%|▌         | 15/300 [00:25<08:10,  1.72s/it]Epoch 14:   5%|▌         | 16/300 [00:26<07:42,  1.63s/it]Epoch 14:   6%|▌         | 17/300 [00:28<07:55,  1.68s/it]Epoch 14:   6%|▌         | 18/300 [00:30<08:13,  1.75s/it]Epoch 14:   6%|▋         | 19/300 [00:31<07:42,  1.65s/it]06/19/2022 16:38:23 - INFO - __main__ - global step: 2110; train loss: 7.970180511474609; dev loss
Epoch 14:   7%|▋         | 20/300 [00:33<07:20,  1.57s/it]Epoch 14:   7%|▋         | 21/300 [00:34<07:05,  1.52s/it]Epoch 14:   7%|▋         | 22/300 [00:36<07:03,  1.53s/it]Epoch 14:   8%|▊         | 23/300 [00:37<07:24,  1.61s/it]Epoch 14:   8%|▊         | 24/300 [00:39<07:07,  1.55s/it]Epoch 14:   8%|▊         | 25/300 [00:40<06:53,  1.50s/it]Epoch 14:   9%|▊         | 26/300 [00:42<07:23,  1.62s/it]Epoch 14:   9%|▉         | 27/300 [00:44<07:37,  1.67s/it]Epoch 14:   9%|▉         | 28/300 [00:46<07:46,  1.72s/it]Epoch 14:  10%|▉         | 29/300 [00:47<07:20,  1.62s/it]Epoch 14:  10%|█         | 30/300 [00:49<06:59,  1.55s/it]Epoch 14:  10%|█         | 31/300 [00:50<07:29,  1.67s/it]Epoch 14:  11%|█         | 32/300 [00:52<07:45,  1.74s/it]Epoch 14:  11%|█         | 33/300 [00:54<07:23,  1.66s/it]Epoch 14:  11%|█▏        | 34/300 [00:55<07:07,  1.61s/it]Epoch 14:  12%|█▏        | 35/300 [00:57<07:02,  1.60s/it]Epoch 14:  12%|█▏        | 36/300 [00:58<06:52,  1.56s/it]Epoch 14:  12%|█▏        | 37/300 [01:00<07:09,  1.63s/it]Epoch 14:  13%|█▎        | 38/300 [01:02<06:56,  1.59s/it]Epoch 14:  13%|█▎        | 39/300 [01:03<06:53,  1.58s/it]06/19/2022 16:38:55 - INFO - __main__ - global step: 2120; train loss: 7.896372318267822; dev loss
Epoch 14:  13%|█▎        | 40/300 [01:05<06:45,  1.56s/it]Epoch 14:  14%|█▎        | 41/300 [01:07<06:59,  1.62s/it]Epoch 14:  14%|█▍        | 42/300 [01:08<06:48,  1.58s/it]Epoch 14:  14%|█▍        | 43/300 [01:10<07:14,  1.69s/it]Epoch 14:  15%|█▍        | 44/300 [01:12<07:07,  1.67s/it]Epoch 14:  15%|█▌        | 45/300 [01:13<06:58,  1.64s/it]Epoch 14:  15%|█▌        | 46/300 [01:15<07:15,  1.71s/it]Epoch 14:  16%|█▌        | 47/300 [01:17<07:11,  1.70s/it]Epoch 14:  16%|█▌        | 48/300 [01:19<07:31,  1.79s/it]Epoch 14:  16%|█▋        | 49/300 [01:21<07:40,  1.83s/it]Epoch 14:  17%|█▋        | 50/300 [01:22<07:12,  1.73s/it]Epoch 14:  17%|█▋        | 51/300 [01:24<07:30,  1.81s/it]Epoch 14:  17%|█▋        | 52/300 [01:26<07:03,  1.71s/it]Epoch 14:  18%|█▊        | 53/300 [01:27<07:15,  1.76s/it]Epoch 14:  18%|█▊        | 54/300 [01:29<07:23,  1.80s/it]Epoch 14:  18%|█▊        | 55/300 [01:31<07:05,  1.74s/it]Epoch 14:  19%|█▊        | 56/300 [01:33<07:14,  1.78s/it]Epoch 14:  19%|█▉        | 57/300 [01:34<06:51,  1.69s/it]Epoch 14:  19%|█▉        | 58/300 [01:36<07:06,  1.76s/it]Epoch 14:  20%|█▉        | 59/300 [01:38<07:13,  1.80s/it]06/19/2022 16:39:30 - INFO - __main__ - global step: 2130; train loss: 8.187949180603027; dev loss
Epoch 14:  20%|██        | 60/300 [01:40<06:59,  1.75s/it]Epoch 14:  20%|██        | 61/300 [01:41<06:39,  1.67s/it]Epoch 14:  21%|██        | 62/300 [01:43<06:52,  1.73s/it]Epoch 14:  21%|██        | 63/300 [01:45<07:03,  1.79s/it]Epoch 14:  21%|██▏       | 64/300 [01:47<07:17,  1.85s/it]Epoch 14:  22%|██▏       | 65/300 [01:49<06:48,  1.74s/it]Epoch 14:  22%|██▏       | 66/300 [01:50<06:57,  1.78s/it]Epoch 14:  22%|██▏       | 67/300 [01:52<06:35,  1.70s/it]Epoch 14:  23%|██▎       | 68/300 [01:54<06:53,  1.78s/it]Epoch 14:  23%|██▎       | 69/300 [01:56<07:01,  1.83s/it]Epoch 14:  23%|██▎       | 70/300 [01:57<06:38,  1.73s/it]Epoch 14:  24%|██▎       | 71/300 [01:59<06:20,  1.66s/it]Epoch 14:  24%|██▍       | 72/300 [02:00<06:13,  1.64s/it]Epoch 14:  24%|██▍       | 73/300 [02:02<06:31,  1.72s/it]Epoch 14:  25%|██▍       | 74/300 [02:04<06:41,  1.77s/it]Epoch 14:  25%|██▌       | 75/300 [02:06<06:19,  1.69s/it]Epoch 14:  25%|██▌       | 76/300 [02:07<06:10,  1.65s/it]Epoch 14:  26%|██▌       | 77/300 [02:09<05:57,  1.60s/it]Epoch 14:  26%|██▌       | 78/300 [02:11<06:14,  1.69s/it]Epoch 14:  26%|██▋       | 79/300 [02:12<05:59,  1.63s/it]06/19/2022 16:40:04 - INFO - __main__ - global step: 2140; train loss: 7.529458522796631; dev loss
Epoch 14:  27%|██▋       | 80/300 [02:14<05:53,  1.61s/it]Epoch 14:  27%|██▋       | 81/300 [02:16<06:09,  1.69s/it]Epoch 14:  27%|██▋       | 82/300 [02:17<05:55,  1.63s/it]Epoch 14:  28%|██▊       | 83/300 [02:19<06:10,  1.71s/it]Epoch 14:  28%|██▊       | 84/300 [02:21<06:23,  1.77s/it]Epoch 14:  28%|██▊       | 85/300 [02:23<06:35,  1.84s/it]Epoch 14:  29%|██▊       | 86/300 [02:24<06:10,  1.73s/it]Epoch 14:  29%|██▉       | 87/300 [02:26<05:53,  1.66s/it]Epoch 14:  29%|██▉       | 88/300 [02:27<05:41,  1.61s/it]Epoch 14:  30%|██▉       | 89/300 [02:29<05:39,  1.61s/it]Epoch 14:  30%|███       | 90/300 [02:31<06:00,  1.72s/it]Epoch 14:  30%|███       | 91/300 [02:32<05:48,  1.67s/it]Epoch 14:  31%|███       | 92/300 [02:34<05:35,  1.61s/it]Epoch 14:  31%|███       | 93/300 [02:36<05:58,  1.73s/it]Epoch 14:  31%|███▏      | 94/300 [02:37<05:40,  1.65s/it]Epoch 14:  32%|███▏      | 95/300 [02:39<05:27,  1.60s/it]Epoch 14:  32%|███▏      | 96/300 [02:41<05:43,  1.69s/it]Epoch 14:  32%|███▏      | 97/300 [02:42<05:36,  1.66s/it]Epoch 14:  33%|███▎      | 98/300 [02:44<05:49,  1.73s/it]Epoch 14:  33%|███▎      | 99/300 [02:46<05:34,  1.66s/it]06/19/2022 16:40:38 - INFO - __main__ - global step: 2150; train loss: 8.586499214172363; dev loss
Epoch 14:  33%|███▎      | 100/300 [02:48<05:46,  1.73s/it]Epoch 14:  34%|███▎      | 101/300 [02:49<05:35,  1.68s/it]Epoch 14:  34%|███▍      | 102/300 [02:51<05:20,  1.62s/it]Epoch 14:  34%|███▍      | 103/300 [02:52<05:08,  1.57s/it]Epoch 14:  35%|███▍      | 104/300 [02:54<05:00,  1.53s/it]Epoch 14:  35%|███▌      | 105/300 [02:56<05:24,  1.67s/it]Epoch 14:  35%|███▌      | 106/300 [02:57<05:12,  1.61s/it]Epoch 14:  36%|███▌      | 107/300 [02:59<05:01,  1.56s/it]Epoch 14:  36%|███▌      | 108/300 [03:00<05:18,  1.66s/it]Epoch 14:  36%|███▋      | 109/300 [03:02<05:35,  1.76s/it]Epoch 14:  37%|███▋      | 110/300 [03:04<05:17,  1.67s/it]Epoch 14:  37%|███▋      | 111/300 [03:06<05:26,  1.73s/it]Epoch 14:  37%|███▋      | 112/300 [03:08<05:34,  1.78s/it]Epoch 14:  38%|███▊      | 113/300 [03:09<05:15,  1.69s/it]Epoch 14:  38%|███▊      | 114/300 [03:11<05:30,  1.78s/it]Epoch 14:  38%|███▊      | 115/300 [03:13<05:11,  1.69s/it]Epoch 14:  39%|███▊      | 116/300 [03:14<05:19,  1.74s/it]Epoch 14:  39%|███▉      | 117/300 [03:16<05:03,  1.66s/it]Epoch 14:  39%|███▉      | 118/300 [03:18<05:20,  1.76s/it]Epoch 14:  40%|███▉      | 119/300 [03:19<05:03,  1.68s/it]06/19/2022 16:41:12 - INFO - __main__ - global step: 2160; train loss: 8.011306762695312; dev loss
Epoch 14:  40%|████      | 120/300 [03:21<05:11,  1.73s/it]Epoch 14:  40%|████      | 121/300 [03:23<05:16,  1.77s/it]Epoch 14:  41%|████      | 122/300 [03:25<05:03,  1.71s/it]Epoch 14:  41%|████      | 123/300 [03:26<04:51,  1.65s/it]Epoch 14:  41%|████▏     | 124/300 [03:28<05:01,  1.71s/it]Epoch 14:  42%|████▏     | 125/300 [03:30<05:08,  1.76s/it]Epoch 14:  42%|████▏     | 126/300 [03:32<05:01,  1.73s/it]Epoch 14:  42%|████▏     | 127/300 [03:33<05:09,  1.79s/it]Epoch 14:  43%|████▎     | 128/300 [03:35<04:54,  1.71s/it]Epoch 14:  43%|████▎     | 129/300 [03:37<04:45,  1.67s/it]Epoch 14:  43%|████▎     | 130/300 [03:39<04:58,  1.76s/it]Epoch 14:  44%|████▎     | 131/300 [03:40<04:42,  1.67s/it]Epoch 14:  44%|████▍     | 132/300 [03:41<04:29,  1.60s/it]Epoch 14:  44%|████▍     | 133/300 [03:43<04:20,  1.56s/it]Epoch 14:  45%|████▍     | 134/300 [03:45<04:20,  1.57s/it]Epoch 14:  45%|████▌     | 135/300 [03:46<04:34,  1.67s/it]Epoch 14:  45%|████▌     | 136/300 [03:48<04:42,  1.72s/it]Epoch 14:  46%|████▌     | 137/300 [03:50<04:47,  1.77s/it]Epoch 14:  46%|████▌     | 138/300 [03:52<04:50,  1.79s/it]Epoch 14:  46%|████▋     | 139/300 [03:54<04:58,  1.85s/it]06/19/2022 16:41:46 - INFO - __main__ - global step: 2170; train loss: 7.542817115783691; dev loss
Epoch 14:  47%|████▋     | 140/300 [03:56<04:57,  1.86s/it]Epoch 14:  47%|████▋     | 141/300 [03:57<04:36,  1.74s/it]Epoch 14:  47%|████▋     | 142/300 [03:59<04:22,  1.66s/it]Epoch 14:  48%|████▊     | 143/300 [04:00<04:16,  1.63s/it]Epoch 14:  48%|████▊     | 144/300 [04:02<04:06,  1.58s/it]Epoch 14:  48%|████▊     | 145/300 [04:04<04:17,  1.66s/it]Epoch 14:  49%|████▊     | 146/300 [04:05<04:07,  1.61s/it]Epoch 14:  49%|████▉     | 147/300 [04:07<04:22,  1.71s/it]Epoch 14:  49%|████▉     | 148/300 [04:09<04:26,  1.75s/it]Epoch 14:  50%|████▉     | 149/300 [04:10<04:13,  1.68s/it]Epoch 14:  50%|█████     | 150/300 [04:12<04:03,  1.63s/it]Epoch 14:  50%|█████     | 151/300 [04:14<04:19,  1.74s/it]Epoch 14:  51%|█████     | 152/300 [04:16<04:22,  1.78s/it]Epoch 14:  51%|█████     | 153/300 [04:18<04:25,  1.81s/it]Epoch 14:  51%|█████▏    | 154/300 [04:20<04:27,  1.83s/it]Epoch 14:  52%|█████▏    | 155/300 [04:22<04:31,  1.87s/it]Epoch 14:  52%|█████▏    | 156/300 [04:23<04:28,  1.87s/it]Epoch 14:  52%|█████▏    | 157/300 [04:25<04:10,  1.75s/it]Epoch 14:  53%|█████▎    | 158/300 [04:27<04:14,  1.79s/it]Epoch 14:  53%|█████▎    | 159/300 [04:28<04:07,  1.76s/it]06/19/2022 16:42:20 - INFO - __main__ - global step: 2180; train loss: 7.464559078216553; dev loss
Epoch 14:  53%|█████▎    | 160/300 [04:30<03:53,  1.67s/it]Epoch 14:  54%|█████▎    | 161/300 [04:31<03:43,  1.61s/it]Epoch 14:  54%|█████▍    | 162/300 [04:33<03:51,  1.68s/it]Epoch 14:  54%|█████▍    | 163/300 [04:35<04:04,  1.79s/it]Epoch 14:  55%|█████▍    | 164/300 [04:37<04:07,  1.82s/it]Epoch 14:  55%|█████▌    | 165/300 [04:39<04:01,  1.79s/it]Epoch 14:  55%|█████▌    | 166/300 [04:41<03:54,  1.75s/it]Epoch 14:  56%|█████▌    | 167/300 [04:42<03:51,  1.74s/it]Epoch 14:  56%|█████▌    | 168/300 [04:44<04:00,  1.82s/it]Epoch 14:  56%|█████▋    | 169/300 [04:46<03:49,  1.75s/it]Epoch 14:  57%|█████▋    | 170/300 [04:48<03:47,  1.75s/it]Epoch 14:  57%|█████▋    | 171/300 [04:49<03:42,  1.72s/it]Epoch 14:  57%|█████▋    | 172/300 [04:51<03:52,  1.81s/it]Epoch 14:  58%|█████▊    | 173/300 [04:53<03:52,  1.83s/it]Epoch 14:  58%|█████▊    | 174/300 [04:55<03:53,  1.85s/it]Epoch 14:  58%|█████▊    | 175/300 [04:57<03:53,  1.86s/it]Epoch 14:  59%|█████▊    | 176/300 [04:59<03:43,  1.80s/it]Epoch 14:  59%|█████▉    | 177/300 [05:00<03:28,  1.69s/it]Epoch 14:  59%|█████▉    | 178/300 [05:02<03:19,  1.63s/it]Epoch 14:  60%|█████▉    | 179/300 [05:03<03:11,  1.58s/it]06/19/2022 16:42:55 - INFO - __main__ - global step: 2190; train loss: 7.785547733306885; dev loss
Epoch 14:  60%|██████    | 180/300 [05:05<03:09,  1.58s/it]Epoch 14:  60%|██████    | 181/300 [05:06<03:04,  1.55s/it]Epoch 14:  61%|██████    | 182/300 [05:08<02:59,  1.52s/it]Epoch 14:  61%|██████    | 183/300 [05:09<02:55,  1.50s/it]Epoch 14:  61%|██████▏   | 184/300 [05:11<02:57,  1.53s/it]Epoch 14:  62%|██████▏   | 185/300 [05:12<02:54,  1.51s/it]Epoch 14:  62%|██████▏   | 186/300 [05:14<02:51,  1.50s/it]Epoch 14:  62%|██████▏   | 187/300 [05:15<02:48,  1.49s/it]Epoch 14:  63%|██████▎   | 188/300 [05:17<02:51,  1.53s/it]Epoch 14:  63%|██████▎   | 189/300 [05:18<02:47,  1.51s/it]Epoch 14:  63%|██████▎   | 190/300 [05:20<02:58,  1.62s/it]Epoch 14:  64%|██████▎   | 191/300 [05:22<03:04,  1.69s/it]Epoch 14:  64%|██████▍   | 192/300 [05:24<03:08,  1.74s/it]Epoch 14:  64%|██████▍   | 193/300 [05:26<03:13,  1.81s/it]Epoch 14:  65%|██████▍   | 194/300 [05:27<03:00,  1.70s/it]Epoch 14:  65%|██████▌   | 195/300 [05:29<02:51,  1.63s/it]Epoch 14:  65%|██████▌   | 196/300 [05:30<02:57,  1.70s/it]Epoch 14:  66%|██████▌   | 197/300 [05:32<03:03,  1.78s/it]Epoch 14:  66%|██████▌   | 198/300 [05:34<02:53,  1.70s/it]Epoch 14:  66%|██████▋   | 199/300 [05:35<02:44,  1.63s/it]06/19/2022 16:43:28 - INFO - __main__ - global step: 2200; train loss: 8.514713287353516; dev loss
Epoch 14:  67%|██████▋   | 200/300 [05:37<02:50,  1.70s/it]Epoch 14:  67%|██████▋   | 201/300 [05:39<02:45,  1.67s/it]Epoch 14:  67%|██████▋   | 202/300 [05:41<02:48,  1.72s/it]Epoch 14:  68%|██████▊   | 203/300 [05:43<02:51,  1.77s/it]Epoch 14:  68%|██████▊   | 204/300 [05:44<02:52,  1.80s/it]Epoch 14:  68%|██████▊   | 205/300 [05:46<02:56,  1.85s/it]Epoch 14:  69%|██████▊   | 206/300 [05:48<02:54,  1.86s/it]Epoch 14:  69%|██████▉   | 207/300 [05:50<02:42,  1.75s/it]Epoch 14:  69%|██████▉   | 208/300 [05:51<02:33,  1.67s/it]Epoch 14:  70%|██████▉   | 209/300 [05:53<02:39,  1.76s/it]Epoch 14:  70%|███████   | 210/300 [05:55<02:41,  1.79s/it]Epoch 14:  70%|███████   | 211/300 [05:57<02:41,  1.81s/it]Epoch 14:  71%|███████   | 212/300 [05:59<02:40,  1.82s/it]Epoch 14:  71%|███████   | 213/300 [06:00<02:33,  1.76s/it]Epoch 14:  71%|███████▏  | 214/300 [06:02<02:24,  1.68s/it]Epoch 14:  72%|███████▏  | 215/300 [06:04<02:27,  1.74s/it]Epoch 14:  72%|███████▏  | 216/300 [06:06<02:29,  1.77s/it]Epoch 14:  72%|███████▏  | 217/300 [06:08<02:32,  1.83s/it]Epoch 14:  73%|███████▎  | 218/300 [06:09<02:21,  1.73s/it]Epoch 14:  73%|███████▎  | 219/300 [06:11<02:23,  1.77s/it]06/19/2022 16:44:03 - INFO - __main__ - global step: 2210; train loss: 7.809262275695801; dev loss
Epoch 14:  73%|███████▎  | 220/300 [06:12<02:15,  1.69s/it]Epoch 14:  74%|███████▎  | 221/300 [06:14<02:08,  1.62s/it]Epoch 14:  74%|███████▍  | 222/300 [06:16<02:05,  1.61s/it]Epoch 14:  74%|███████▍  | 223/300 [06:17<02:00,  1.57s/it]Epoch 14:  75%|███████▍  | 224/300 [06:19<02:06,  1.66s/it]Epoch 14:  75%|███████▌  | 225/300 [06:20<02:01,  1.61s/it]Epoch 14:  75%|███████▌  | 226/300 [06:22<01:59,  1.61s/it]Epoch 14:  76%|███████▌  | 227/300 [06:24<02:03,  1.69s/it]Epoch 14:  76%|███████▌  | 228/300 [06:25<01:57,  1.63s/it]Epoch 14:  76%|███████▋  | 229/300 [06:27<01:52,  1.59s/it]Epoch 14:  77%|███████▋  | 230/300 [06:29<01:59,  1.70s/it]Epoch 14:  77%|███████▋  | 231/300 [06:31<02:01,  1.76s/it]Epoch 14:  77%|███████▋  | 232/300 [06:32<01:54,  1.68s/it]Epoch 14:  78%|███████▊  | 233/300 [06:34<01:55,  1.73s/it]Epoch 14:  78%|███████▊  | 234/300 [06:36<01:51,  1.68s/it]Epoch 14:  78%|███████▊  | 235/300 [06:37<01:45,  1.62s/it]Epoch 14:  79%|███████▊  | 236/300 [06:39<01:48,  1.69s/it]Epoch 14:  79%|███████▉  | 237/300 [06:41<01:49,  1.74s/it]Epoch 14:  79%|███████▉  | 238/300 [06:43<01:52,  1.81s/it]Epoch 14:  80%|███████▉  | 239/300 [06:45<01:51,  1.83s/it]06/19/2022 16:44:37 - INFO - __main__ - global step: 2220; train loss: 8.503859519958496; dev loss
Epoch 14:  80%|████████  | 240/300 [06:46<01:43,  1.72s/it]Epoch 14:  80%|████████  | 241/300 [06:48<01:43,  1.76s/it]Epoch 14:  81%|████████  | 242/300 [06:50<01:38,  1.70s/it]Epoch 14:  81%|████████  | 243/300 [06:51<01:32,  1.63s/it]Epoch 14:  81%|████████▏ | 244/300 [06:53<01:35,  1.70s/it]Epoch 14:  82%|████████▏ | 245/300 [06:54<01:29,  1.63s/it]Epoch 14:  82%|████████▏ | 246/300 [06:56<01:25,  1.59s/it]Epoch 14:  82%|████████▏ | 247/300 [06:57<01:24,  1.60s/it]Epoch 14:  83%|████████▎ | 248/300 [06:59<01:20,  1.56s/it]Epoch 14:  83%|████████▎ | 249/300 [07:01<01:24,  1.65s/it]Epoch 14:  83%|████████▎ | 250/300 [07:03<01:26,  1.72s/it]Epoch 14:  84%|████████▎ | 251/300 [07:05<01:27,  1.79s/it]Epoch 14:  84%|████████▍ | 252/300 [07:06<01:26,  1.81s/it]Epoch 14:  84%|████████▍ | 253/300 [07:08<01:25,  1.82s/it]Epoch 14:  85%|████████▍ | 254/300 [07:10<01:24,  1.83s/it]Epoch 14:  85%|████████▌ | 255/300 [07:12<01:19,  1.77s/it]Epoch 14:  85%|████████▌ | 256/300 [07:13<01:13,  1.68s/it]Epoch 14:  86%|████████▌ | 257/300 [07:15<01:14,  1.74s/it]Epoch 14:  86%|████████▌ | 258/300 [07:17<01:10,  1.67s/it]Epoch 14:  86%|████████▋ | 259/300 [07:19<01:11,  1.76s/it]06/19/2022 16:45:11 - INFO - __main__ - global step: 2230; train loss: 7.583178520202637; dev loss
Epoch 14:  87%|████████▋ | 260/300 [07:20<01:07,  1.68s/it]Epoch 14:  87%|████████▋ | 261/300 [07:22<01:07,  1.74s/it]Epoch 14:  87%|████████▋ | 262/300 [07:24<01:07,  1.78s/it]Epoch 14:  88%|████████▊ | 263/300 [07:26<01:08,  1.84s/it]Epoch 14:  88%|████████▊ | 264/300 [07:28<01:06,  1.85s/it]Epoch 14:  88%|████████▊ | 265/300 [07:29<01:01,  1.76s/it]Epoch 14:  89%|████████▊ | 266/300 [07:31<00:56,  1.66s/it]Epoch 14:  89%|████████▉ | 267/300 [07:33<00:57,  1.75s/it]Epoch 14:  89%|████████▉ | 268/300 [07:34<00:54,  1.69s/it]Epoch 14:  90%|████████▉ | 269/300 [07:36<00:50,  1.63s/it]Epoch 14:  90%|█████████ | 270/300 [07:38<00:50,  1.70s/it]Epoch 14:  90%|█████████ | 271/300 [07:39<00:48,  1.67s/it]Epoch 14:  91%|█████████ | 272/300 [07:41<00:45,  1.61s/it]Epoch 14:  91%|█████████ | 273/300 [07:42<00:42,  1.57s/it]Epoch 14:  91%|█████████▏| 274/300 [07:44<00:40,  1.55s/it]Epoch 14:  92%|█████████▏| 275/300 [07:45<00:38,  1.53s/it]Epoch 14:  92%|█████████▏| 276/300 [07:47<00:39,  1.67s/it]Epoch 14:  92%|█████████▏| 277/300 [07:49<00:37,  1.61s/it]Epoch 14:  93%|█████████▎| 278/300 [07:50<00:37,  1.69s/it]Epoch 14:  93%|█████████▎| 279/300 [07:52<00:36,  1.74s/it]06/19/2022 16:45:44 - INFO - __main__ - global step: 2240; train loss: 7.828511714935303; dev loss
Epoch 14:  93%|█████████▎| 280/300 [07:54<00:34,  1.70s/it]Epoch 14:  94%|█████████▎| 281/300 [07:55<00:31,  1.64s/it]Epoch 14:  94%|█████████▍| 282/300 [07:57<00:28,  1.59s/it]Epoch 14:  94%|█████████▍| 283/300 [07:59<00:28,  1.68s/it]Epoch 14:  95%|█████████▍| 284/300 [08:01<00:28,  1.77s/it]Epoch 14:  95%|█████████▌| 285/300 [08:02<00:25,  1.68s/it]Epoch 14:  95%|█████████▌| 286/300 [08:04<00:24,  1.74s/it]Epoch 14:  96%|█████████▌| 287/300 [08:06<00:21,  1.66s/it]Epoch 14:  96%|█████████▌| 288/300 [08:08<00:21,  1.75s/it]Epoch 14:  96%|█████████▋| 289/300 [08:09<00:18,  1.67s/it]Epoch 14:  97%|█████████▋| 290/300 [08:11<00:16,  1.62s/it]Epoch 14:  97%|█████████▋| 291/300 [08:12<00:14,  1.58s/it]Epoch 14:  97%|█████████▋| 292/300 [08:14<00:13,  1.70s/it]Epoch 14:  98%|█████████▊| 293/300 [08:16<00:12,  1.76s/it]Epoch 14:  98%|█████████▊| 294/300 [08:17<00:10,  1.67s/it]Epoch 14:  98%|█████████▊| 295/300 [08:19<00:08,  1.74s/it]Epoch 14:  99%|█████████▊| 296/300 [08:21<00:07,  1.82s/it]Epoch 14:  99%|█████████▉| 297/300 [08:23<00:05,  1.72s/it]Epoch 14:  99%|█████████▉| 298/300 [08:25<00:03,  1.76s/it]Epoch 14: 100%|█████████▉| 299/300 [08:26<00:01,  1.79s/it]06/19/2022 16:46:18 - INFO - __main__ - global step: 2250; train loss: 8.064350128173828; dev loss
Epoch 14: 100%|██████████| 300/300 [08:28<00:00,  1.69s/it]Epoch 14: 100%|██████████| 300/300 [08:28<00:00,  1.69s/it]
Epoch 15:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 15:   0%|          | 1/300 [00:01<08:06,  1.63s/it]Epoch 15:   1%|          | 2/300 [00:03<08:47,  1.77s/it]Epoch 15:   1%|          | 3/300 [00:04<08:06,  1.64s/it]Epoch 15:   1%|▏         | 4/300 [00:06<08:29,  1.72s/it]Epoch 15:   2%|▏         | 5/300 [00:08<08:15,  1.68s/it]Epoch 15:   2%|▏         | 6/300 [00:10<08:30,  1.74s/it]Epoch 15:   2%|▏         | 7/300 [00:12<08:39,  1.77s/it]Epoch 15:   3%|▎         | 8/300 [00:13<08:45,  1.80s/it]Epoch 15:   3%|▎         | 9/300 [00:15<08:25,  1.74s/it]Epoch 15:   3%|▎         | 10/300 [00:17<08:37,  1.79s/it]Epoch 15:   4%|▎         | 11/300 [00:19<08:43,  1.81s/it]Epoch 15:   4%|▍         | 12/300 [00:20<08:11,  1.71s/it]Epoch 15:   4%|▍         | 13/300 [00:22<08:31,  1.78s/it]Epoch 15:   5%|▍         | 14/300 [00:24<08:05,  1.70s/it]Epoch 15:   5%|▌         | 15/300 [00:25<07:45,  1.63s/it]Epoch 15:   5%|▌         | 16/300 [00:27<08:02,  1.70s/it]Epoch 15:   6%|▌         | 17/300 [00:29<08:26,  1.79s/it]Epoch 15:   6%|▌         | 18/300 [00:31<08:00,  1.70s/it]Epoch 15:   6%|▋         | 19/300 [00:33<08:14,  1.76s/it]06/19/2022 16:46:53 - INFO - __main__ - global step: 2260; train loss: 7.885079383850098; dev loss
Epoch 15:   7%|▋         | 20/300 [00:34<07:53,  1.69s/it]Epoch 15:   7%|▋         | 21/300 [00:36<08:19,  1.79s/it]Epoch 15:   7%|▋         | 22/300 [00:38<07:51,  1.70s/it]Epoch 15:   8%|▊         | 23/300 [00:39<08:02,  1.74s/it]Epoch 15:   8%|▊         | 24/300 [00:41<07:38,  1.66s/it]Epoch 15:   8%|▊         | 25/300 [00:43<08:03,  1.76s/it]Epoch 15:   9%|▊         | 26/300 [00:45<08:10,  1.79s/it]Epoch 15:   9%|▉         | 27/300 [00:46<07:44,  1.70s/it]Epoch 15:   9%|▉         | 28/300 [00:48<07:24,  1.63s/it]Epoch 15:  10%|▉         | 29/300 [00:49<07:10,  1.59s/it]Epoch 15:  10%|█         | 30/300 [00:51<07:39,  1.70s/it]Epoch 15:  10%|█         | 31/300 [00:53<07:19,  1.63s/it]Epoch 15:  11%|█         | 32/300 [00:54<07:04,  1.58s/it]Epoch 15:  11%|█         | 33/300 [00:55<06:50,  1.54s/it]Epoch 15:  11%|█▏        | 34/300 [00:57<06:52,  1.55s/it]Epoch 15:  12%|█▏        | 35/300 [00:59<07:14,  1.64s/it]Epoch 15:  12%|█▏        | 36/300 [01:00<06:58,  1.58s/it]Epoch 15:  12%|█▏        | 37/300 [01:02<06:48,  1.55s/it]Epoch 15:  13%|█▎        | 38/300 [01:04<07:21,  1.68s/it]Epoch 15:  13%|█▎        | 39/300 [01:05<07:01,  1.62s/it]06/19/2022 16:47:26 - INFO - __main__ - global step: 2270; train loss: 7.067652702331543; dev loss
Epoch 15:  13%|█▎        | 40/300 [01:07<06:47,  1.57s/it]Epoch 15:  14%|█▎        | 41/300 [01:08<06:39,  1.54s/it]Epoch 15:  14%|█▍        | 42/300 [01:10<07:09,  1.67s/it]Epoch 15:  14%|█▍        | 43/300 [01:12<06:50,  1.60s/it]Epoch 15:  15%|█▍        | 44/300 [01:13<07:08,  1.68s/it]Epoch 15:  15%|█▌        | 45/300 [01:15<06:54,  1.62s/it]Epoch 15:  15%|█▌        | 46/300 [01:17<06:47,  1.61s/it]Epoch 15:  16%|█▌        | 47/300 [01:18<06:38,  1.58s/it]Epoch 15:  16%|█▌        | 48/300 [01:20<06:56,  1.65s/it]Epoch 15:  16%|█▋        | 49/300 [01:21<06:39,  1.59s/it]Epoch 15:  17%|█▋        | 50/300 [01:23<06:37,  1.59s/it]Epoch 15:  17%|█▋        | 51/300 [01:25<06:53,  1.66s/it]Epoch 15:  17%|█▋        | 52/300 [01:27<07:03,  1.71s/it]Epoch 15:  18%|█▊        | 53/300 [01:28<07:11,  1.75s/it]Epoch 15:  18%|█▊        | 54/300 [01:30<06:50,  1.67s/it]Epoch 15:  18%|█▊        | 55/300 [01:31<06:41,  1.64s/it]Epoch 15:  19%|█▊        | 56/300 [01:33<06:26,  1.58s/it]Epoch 15:  19%|█▉        | 57/300 [01:34<06:19,  1.56s/it]Epoch 15:  19%|█▉        | 58/300 [01:36<06:11,  1.54s/it]Epoch 15:  20%|█▉        | 59/300 [01:38<06:38,  1.65s/it]06/19/2022 16:47:58 - INFO - __main__ - global step: 2280; train loss: 7.967844486236572; dev loss
Epoch 15:  20%|██        | 60/300 [01:39<06:21,  1.59s/it]Epoch 15:  20%|██        | 61/300 [01:41<06:10,  1.55s/it]Epoch 15:  21%|██        | 62/300 [01:43<06:29,  1.64s/it]Epoch 15:  21%|██        | 63/300 [01:44<06:23,  1.62s/it]Epoch 15:  21%|██▏       | 64/300 [01:46<06:08,  1.56s/it]Epoch 15:  22%|██▏       | 65/300 [01:47<06:27,  1.65s/it]Epoch 15:  22%|██▏       | 66/300 [01:49<06:11,  1.59s/it]Epoch 15:  22%|██▏       | 67/300 [01:50<06:09,  1.59s/it]Epoch 15:  23%|██▎       | 68/300 [01:52<06:24,  1.66s/it]Epoch 15:  23%|██▎       | 69/300 [01:54<06:10,  1.60s/it]Epoch 15:  23%|██▎       | 70/300 [01:55<06:00,  1.57s/it]Epoch 15:  24%|██▎       | 71/300 [01:57<05:57,  1.56s/it]Epoch 15:  24%|██▍       | 72/300 [01:59<06:14,  1.64s/it]Epoch 15:  24%|██▍       | 73/300 [02:00<06:25,  1.70s/it]Epoch 15:  25%|██▍       | 74/300 [02:02<06:08,  1.63s/it]Epoch 15:  25%|██▌       | 75/300 [02:03<06:00,  1.60s/it]Epoch 15:  25%|██▌       | 76/300 [02:05<05:49,  1.56s/it]Epoch 15:  26%|██▌       | 77/300 [02:06<05:39,  1.52s/it]Epoch 15:  26%|██▌       | 78/300 [02:08<05:57,  1.61s/it]Epoch 15:  26%|██▋       | 79/300 [02:10<06:18,  1.71s/it]06/19/2022 16:48:31 - INFO - __main__ - global step: 2290; train loss: 7.869596004486084; dev loss
Epoch 15:  27%|██▋       | 80/300 [02:12<06:27,  1.76s/it]Epoch 15:  27%|██▋       | 81/300 [02:13<06:04,  1.66s/it]Epoch 15:  27%|██▋       | 82/300 [02:15<05:49,  1.60s/it]Epoch 15:  28%|██▊       | 83/300 [02:17<06:02,  1.67s/it]Epoch 15:  28%|██▊       | 84/300 [02:18<05:54,  1.64s/it]Epoch 15:  28%|██▊       | 85/300 [02:20<06:03,  1.69s/it]Epoch 15:  29%|██▊       | 86/300 [02:22<06:12,  1.74s/it]Epoch 15:  29%|██▉       | 87/300 [02:23<05:54,  1.66s/it]Epoch 15:  29%|██▉       | 88/300 [02:25<06:11,  1.75s/it]Epoch 15:  30%|██▉       | 89/300 [02:27<06:15,  1.78s/it]Epoch 15:  30%|███       | 90/300 [02:29<05:53,  1.69s/it]Epoch 15:  30%|███       | 91/300 [02:30<05:38,  1.62s/it]Epoch 15:  31%|███       | 92/300 [02:32<05:56,  1.72s/it]Epoch 15:  31%|███       | 93/300 [02:34<06:03,  1.76s/it]Epoch 15:  31%|███▏      | 94/300 [02:36<06:08,  1.79s/it]Epoch 15:  32%|███▏      | 95/300 [02:38<06:09,  1.80s/it]Epoch 15:  32%|███▏      | 96/300 [02:40<06:20,  1.86s/it]Epoch 15:  32%|███▏      | 97/300 [02:41<05:52,  1.73s/it]Epoch 15:  33%|███▎      | 98/300 [02:43<05:32,  1.65s/it]Epoch 15:  33%|███▎      | 99/300 [02:44<05:19,  1.59s/it]06/19/2022 16:49:04 - INFO - __main__ - global step: 2300; train loss: 8.444910049438477; dev loss
Epoch 15:  33%|███▎      | 100/300 [02:46<05:17,  1.59s/it]Epoch 15:  34%|███▎      | 101/300 [02:47<05:30,  1.66s/it]Epoch 15:  34%|███▍      | 102/300 [02:49<05:38,  1.71s/it]Epoch 15:  34%|███▍      | 103/300 [02:51<05:23,  1.64s/it]Epoch 15:  35%|███▍      | 104/300 [02:52<05:17,  1.62s/it]Epoch 15:  35%|███▌      | 105/300 [02:54<05:28,  1.69s/it]Epoch 15:  35%|███▌      | 106/300 [02:56<05:14,  1.62s/it]Epoch 15:  36%|███▌      | 107/300 [02:57<05:26,  1.69s/it]Epoch 15:  36%|███▌      | 108/300 [02:59<05:11,  1.62s/it]Epoch 15:  36%|███▋      | 109/300 [03:01<05:06,  1.60s/it]Epoch 15:  37%|███▋      | 110/300 [03:02<04:57,  1.56s/it]Epoch 15:  37%|███▋      | 111/300 [03:04<05:10,  1.64s/it]Epoch 15:  37%|███▋      | 112/300 [03:06<05:19,  1.70s/it]Epoch 15:  38%|███▊      | 113/300 [03:08<05:33,  1.78s/it]Epoch 15:  38%|███▊      | 114/300 [03:09<05:13,  1.69s/it]Epoch 15:  38%|███▊      | 115/300 [03:11<04:58,  1.61s/it]Epoch 15:  39%|███▊      | 116/300 [03:12<05:08,  1.68s/it]Epoch 15:  39%|███▉      | 117/300 [03:14<05:02,  1.65s/it]Epoch 15:  39%|███▉      | 118/300 [03:16<05:12,  1.72s/it]Epoch 15:  40%|███▉      | 119/300 [03:17<04:57,  1.64s/it]06/19/2022 16:49:38 - INFO - __main__ - global step: 2310; train loss: 7.72036600112915; dev loss
Epoch 15:  40%|████      | 120/300 [03:19<05:07,  1.71s/it]Epoch 15:  40%|████      | 121/300 [03:21<05:21,  1.80s/it]Epoch 15:  41%|████      | 122/300 [03:23<05:22,  1.81s/it]Epoch 15:  41%|████      | 123/300 [03:24<05:03,  1.71s/it]Epoch 15:  41%|████▏     | 124/300 [03:26<05:07,  1.75s/it]Epoch 15:  42%|████▏     | 125/300 [03:28<05:16,  1.81s/it]Epoch 15:  42%|████▏     | 126/300 [03:30<05:17,  1.82s/it]Epoch 15:  42%|████▏     | 127/300 [03:32<05:17,  1.83s/it]Epoch 15:  43%|████▎     | 128/300 [03:33<04:57,  1.73s/it]Epoch 15:  43%|████▎     | 129/300 [03:35<05:07,  1.80s/it]Epoch 15:  43%|████▎     | 130/300 [03:37<04:47,  1.69s/it]Epoch 15:  44%|████▎     | 131/300 [03:39<04:52,  1.73s/it]Epoch 15:  44%|████▍     | 132/300 [03:41<04:55,  1.76s/it]Epoch 15:  44%|████▍     | 133/300 [03:42<05:04,  1.82s/it]Epoch 15:  45%|████▍     | 134/300 [03:44<05:03,  1.83s/it]Epoch 15:  45%|████▌     | 135/300 [03:46<04:44,  1.72s/it]Epoch 15:  45%|████▌     | 136/300 [03:48<04:47,  1.76s/it]Epoch 15:  46%|████▌     | 137/300 [03:49<04:49,  1.78s/it]Epoch 15:  46%|████▌     | 138/300 [03:51<04:56,  1.83s/it]Epoch 15:  46%|████▋     | 139/300 [03:53<04:37,  1.73s/it]06/19/2022 16:50:13 - INFO - __main__ - global step: 2320; train loss: 7.8622589111328125; dev loss
Epoch 15:  47%|████▋     | 140/300 [03:54<04:22,  1.64s/it]Epoch 15:  47%|████▋     | 141/300 [03:56<04:11,  1.58s/it]Epoch 15:  47%|████▋     | 142/300 [03:58<04:27,  1.69s/it]Epoch 15:  48%|████▊     | 143/300 [04:00<04:31,  1.73s/it]Epoch 15:  48%|████▊     | 144/300 [04:01<04:19,  1.66s/it]Epoch 15:  48%|████▊     | 145/300 [04:03<04:26,  1.72s/it]Epoch 15:  49%|████▊     | 146/300 [04:05<04:36,  1.80s/it]Epoch 15:  49%|████▉     | 147/300 [04:07<04:37,  1.81s/it]Epoch 15:  49%|████▉     | 148/300 [04:08<04:19,  1.71s/it]Epoch 15:  50%|████▉     | 149/300 [04:10<04:23,  1.75s/it]Epoch 15:  50%|█████     | 150/300 [04:12<04:31,  1.81s/it]Epoch 15:  50%|█████     | 151/300 [04:13<04:13,  1.70s/it]Epoch 15:  51%|█████     | 152/300 [04:15<04:00,  1.62s/it]Epoch 15:  51%|█████     | 153/300 [04:17<04:07,  1.68s/it]Epoch 15:  51%|█████▏    | 154/300 [04:19<04:17,  1.76s/it]Epoch 15:  52%|█████▏    | 155/300 [04:20<04:03,  1.68s/it]Epoch 15:  52%|█████▏    | 156/300 [04:22<03:52,  1.62s/it]Epoch 15:  52%|█████▏    | 157/300 [04:23<03:44,  1.57s/it]Epoch 15:  53%|█████▎    | 158/300 [04:25<03:59,  1.69s/it]Epoch 15:  53%|█████▎    | 159/300 [04:26<03:48,  1.62s/it]06/19/2022 16:50:47 - INFO - __main__ - global step: 2330; train loss: 8.349950790405273; dev loss
Epoch 15:  53%|█████▎    | 160/300 [04:28<03:55,  1.68s/it]Epoch 15:  54%|█████▎    | 161/300 [04:30<03:46,  1.63s/it]Epoch 15:  54%|█████▍    | 162/300 [04:31<03:36,  1.57s/it]Epoch 15:  54%|█████▍    | 163/300 [04:33<03:36,  1.58s/it]Epoch 15:  55%|█████▍    | 164/300 [04:35<03:45,  1.66s/it]Epoch 15:  55%|█████▌    | 165/300 [04:37<03:52,  1.72s/it]Epoch 15:  55%|█████▌    | 166/300 [04:38<03:57,  1.77s/it]Epoch 15:  56%|█████▌    | 167/300 [04:40<04:03,  1.83s/it]Epoch 15:  56%|█████▌    | 168/300 [04:42<04:02,  1.83s/it]Epoch 15:  56%|█████▋    | 169/300 [04:44<03:46,  1.73s/it]Epoch 15:  57%|█████▋    | 170/300 [04:45<03:35,  1.66s/it]Epoch 15:  57%|█████▋    | 171/300 [04:47<03:45,  1.75s/it]Epoch 15:  57%|█████▋    | 172/300 [04:49<03:46,  1.77s/it]Epoch 15:  58%|█████▊    | 173/300 [04:50<03:32,  1.68s/it]Epoch 15:  58%|█████▊    | 174/300 [04:52<03:37,  1.73s/it]Epoch 15:  58%|█████▊    | 175/300 [04:54<03:44,  1.79s/it]Epoch 15:  59%|█████▊    | 176/300 [04:56<03:31,  1.70s/it]Epoch 15:  59%|█████▉    | 177/300 [04:58<03:34,  1.75s/it]Epoch 15:  59%|█████▉    | 178/300 [04:59<03:23,  1.67s/it]Epoch 15:  60%|█████▉    | 179/300 [05:01<03:17,  1.64s/it]06/19/2022 16:51:21 - INFO - __main__ - global step: 2340; train loss: 7.579554080963135; dev loss
Epoch 15:  60%|██████    | 180/300 [05:02<03:10,  1.59s/it]Epoch 15:  60%|██████    | 181/300 [05:04<03:17,  1.66s/it]Epoch 15:  61%|██████    | 182/300 [05:06<03:22,  1.72s/it]Epoch 15:  61%|██████    | 183/300 [05:08<03:30,  1.80s/it]Epoch 15:  61%|██████▏   | 184/300 [05:09<03:18,  1.71s/it]Epoch 15:  62%|██████▏   | 185/300 [05:11<03:07,  1.63s/it]Epoch 15:  62%|██████▏   | 186/300 [05:12<03:00,  1.59s/it]Epoch 15:  62%|██████▏   | 187/300 [05:14<02:59,  1.59s/it]Epoch 15:  63%|██████▎   | 188/300 [05:15<02:53,  1.55s/it]Epoch 15:  63%|██████▎   | 189/300 [05:17<03:01,  1.64s/it]Epoch 15:  63%|██████▎   | 190/300 [05:19<03:09,  1.72s/it]Epoch 15:  64%|██████▎   | 191/300 [05:21<03:13,  1.78s/it]Epoch 15:  64%|██████▍   | 192/300 [05:23<03:07,  1.74s/it]Epoch 15:  64%|██████▍   | 193/300 [05:24<03:10,  1.78s/it]Epoch 15:  65%|██████▍   | 194/300 [05:26<03:12,  1.81s/it]Epoch 15:  65%|██████▌   | 195/300 [05:28<03:00,  1.72s/it]Epoch 15:  65%|██████▌   | 196/300 [05:30<02:58,  1.72s/it]Epoch 15:  66%|██████▌   | 197/300 [05:31<02:49,  1.64s/it]Epoch 15:  66%|██████▌   | 198/300 [05:33<02:55,  1.72s/it]Epoch 15:  66%|██████▋   | 199/300 [05:34<02:48,  1.66s/it]06/19/2022 16:51:55 - INFO - __main__ - global step: 2350; train loss: 8.12300968170166; dev loss
Epoch 15:  67%|██████▋   | 200/300 [05:36<02:54,  1.74s/it]Epoch 15:  67%|██████▋   | 201/300 [05:38<02:44,  1.66s/it]Epoch 15:  67%|██████▋   | 202/300 [05:39<02:37,  1.61s/it]Epoch 15:  68%|██████▊   | 203/300 [05:41<02:31,  1.57s/it]Epoch 15:  68%|██████▊   | 204/300 [05:42<02:31,  1.58s/it]Epoch 15:  68%|██████▊   | 205/300 [05:44<02:38,  1.66s/it]Epoch 15:  69%|██████▊   | 206/300 [05:46<02:42,  1.73s/it]Epoch 15:  69%|██████▉   | 207/300 [05:48<02:33,  1.65s/it]Epoch 15:  69%|██████▉   | 208/300 [05:50<02:40,  1.75s/it]Epoch 15:  70%|██████▉   | 209/300 [05:51<02:42,  1.78s/it]Epoch 15:  70%|███████   | 210/300 [05:53<02:43,  1.82s/it]Epoch 15:  70%|███████   | 211/300 [05:55<02:32,  1.71s/it]Epoch 15:  71%|███████   | 212/300 [05:56<02:27,  1.67s/it]Epoch 15:  71%|███████   | 213/300 [05:58<02:30,  1.73s/it]Epoch 15:  71%|███████▏  | 214/300 [06:00<02:32,  1.77s/it]Epoch 15:  72%|███████▏  | 215/300 [06:02<02:33,  1.80s/it]Epoch 15:  72%|███████▏  | 216/300 [06:04<02:22,  1.70s/it]Epoch 15:  72%|███████▏  | 217/300 [06:05<02:17,  1.66s/it]Epoch 15:  73%|███████▎  | 218/300 [06:07<02:11,  1.60s/it]Epoch 15:  73%|███████▎  | 219/300 [06:08<02:07,  1.57s/it]06/19/2022 16:52:29 - INFO - __main__ - global step: 2360; train loss: 7.838054656982422; dev loss
Epoch 15:  73%|███████▎  | 220/300 [06:10<02:12,  1.65s/it]Epoch 15:  74%|███████▎  | 221/300 [06:12<02:17,  1.74s/it]Epoch 15:  74%|███████▍  | 222/300 [06:13<02:09,  1.66s/it]Epoch 15:  74%|███████▍  | 223/300 [06:15<02:03,  1.61s/it]Epoch 15:  75%|███████▍  | 224/300 [06:17<02:07,  1.68s/it]Epoch 15:  75%|███████▌  | 225/300 [06:18<02:03,  1.64s/it]Epoch 15:  75%|███████▌  | 226/300 [06:20<01:57,  1.58s/it]Epoch 15:  76%|███████▌  | 227/300 [06:21<01:53,  1.55s/it]Epoch 15:  76%|███████▌  | 228/300 [06:23<01:58,  1.64s/it]Epoch 15:  76%|███████▋  | 229/300 [06:25<02:04,  1.75s/it]Epoch 15:  77%|███████▋  | 230/300 [06:27<02:04,  1.78s/it]Epoch 15:  77%|███████▋  | 231/300 [06:29<02:04,  1.81s/it]Epoch 15:  77%|███████▋  | 232/300 [06:30<01:56,  1.72s/it]Epoch 15:  78%|███████▊  | 233/300 [06:32<01:52,  1.68s/it]Epoch 15:  78%|███████▊  | 234/300 [06:34<01:54,  1.73s/it]Epoch 15:  78%|███████▊  | 235/300 [06:35<01:47,  1.65s/it]Epoch 15:  79%|███████▊  | 236/300 [06:37<01:42,  1.61s/it]Epoch 15:  79%|███████▉  | 237/300 [06:39<01:47,  1.71s/it]Epoch 15:  79%|███████▉  | 238/300 [06:40<01:41,  1.64s/it]Epoch 15:  80%|███████▉  | 239/300 [06:42<01:44,  1.71s/it]06/19/2022 16:53:03 - INFO - __main__ - global step: 2370; train loss: 7.691143989562988; dev loss
Epoch 15:  80%|████████  | 240/300 [06:44<01:45,  1.75s/it]Epoch 15:  80%|████████  | 241/300 [06:46<01:47,  1.82s/it]Epoch 15:  81%|████████  | 242/300 [06:48<01:46,  1.83s/it]Epoch 15:  81%|████████  | 243/300 [06:49<01:38,  1.73s/it]Epoch 15:  81%|████████▏ | 244/300 [06:51<01:32,  1.65s/it]Epoch 15:  82%|████████▏ | 245/300 [06:52<01:34,  1.71s/it]Epoch 15:  82%|████████▏ | 246/300 [06:54<01:30,  1.68s/it]Epoch 15:  82%|████████▏ | 247/300 [06:56<01:30,  1.71s/it]Epoch 15:  83%|████████▎ | 248/300 [06:57<01:28,  1.71s/it]Epoch 15:  83%|████████▎ | 249/300 [06:59<01:27,  1.71s/it]Epoch 15:  83%|████████▎ | 250/300 [07:01<01:30,  1.81s/it]Epoch 15:  84%|████████▎ | 251/300 [07:03<01:23,  1.71s/it]Epoch 15:  84%|████████▍ | 252/300 [07:04<01:18,  1.64s/it]Epoch 15:  84%|████████▍ | 253/300 [07:06<01:20,  1.71s/it]Epoch 15:  85%|████████▍ | 254/300 [07:08<01:16,  1.67s/it]Epoch 15:  85%|████████▌ | 255/300 [07:09<01:12,  1.61s/it]Epoch 15:  85%|████████▌ | 256/300 [07:11<01:14,  1.68s/it]Epoch 15:  86%|████████▌ | 257/300 [07:13<01:15,  1.76s/it]Epoch 15:  86%|████████▌ | 258/300 [07:15<01:16,  1.83s/it]Epoch 15:  86%|████████▋ | 259/300 [07:16<01:11,  1.75s/it]06/19/2022 16:53:37 - INFO - __main__ - global step: 2380; train loss: 8.627198219299316; dev loss
Epoch 15:  87%|████████▋ | 260/300 [07:18<01:07,  1.68s/it]Epoch 15:  87%|████████▋ | 261/300 [07:19<01:03,  1.63s/it]Epoch 15:  87%|████████▋ | 262/300 [07:21<01:01,  1.62s/it]Epoch 15:  88%|████████▊ | 263/300 [07:23<01:02,  1.69s/it]Epoch 15:  88%|████████▊ | 264/300 [07:24<00:58,  1.63s/it]Epoch 15:  88%|████████▊ | 265/300 [07:26<00:55,  1.58s/it]Epoch 15:  89%|████████▊ | 266/300 [07:28<00:57,  1.70s/it]Epoch 15:  89%|████████▉ | 267/300 [07:30<00:57,  1.75s/it]Epoch 15:  89%|████████▉ | 268/300 [07:31<00:53,  1.67s/it]Epoch 15:  90%|████████▉ | 269/300 [07:33<00:49,  1.60s/it]Epoch 15:  90%|█████████ | 270/300 [07:34<00:47,  1.57s/it]Epoch 15:  90%|█████████ | 271/300 [07:36<00:49,  1.72s/it]Epoch 15:  91%|█████████ | 272/300 [07:38<00:49,  1.76s/it]Epoch 15:  91%|█████████ | 273/300 [07:40<00:48,  1.80s/it]Epoch 15:  91%|█████████▏| 274/300 [07:41<00:44,  1.71s/it]Epoch 15:  92%|█████████▏| 275/300 [07:43<00:41,  1.67s/it]Epoch 15:  92%|█████████▏| 276/300 [07:45<00:41,  1.73s/it]Epoch 15:  92%|█████████▏| 277/300 [07:46<00:38,  1.65s/it]Epoch 15:  93%|█████████▎| 278/300 [07:48<00:35,  1.60s/it]Epoch 15:  93%|█████████▎| 279/300 [07:50<00:35,  1.71s/it]06/19/2022 16:54:11 - INFO - __main__ - global step: 2390; train loss: 8.165590286254883; dev loss
Epoch 15:  93%|█████████▎| 280/300 [07:52<00:35,  1.75s/it]Epoch 15:  94%|█████████▎| 281/300 [07:53<00:31,  1.66s/it]Epoch 15:  94%|█████████▍| 282/300 [07:55<00:30,  1.72s/it]Epoch 15:  94%|█████████▍| 283/300 [07:57<00:30,  1.79s/it]Epoch 15:  95%|█████████▍| 284/300 [07:58<00:27,  1.69s/it]Epoch 15:  95%|█████████▌| 285/300 [08:00<00:24,  1.63s/it]Epoch 15:  95%|█████████▌| 286/300 [08:02<00:23,  1.69s/it]Epoch 15:  96%|█████████▌| 287/300 [08:03<00:21,  1.66s/it]Epoch 15:  96%|█████████▌| 288/300 [08:05<00:19,  1.61s/it]Epoch 15:  96%|█████████▋| 289/300 [08:07<00:18,  1.68s/it]Epoch 15:  97%|█████████▋| 290/300 [08:09<00:17,  1.73s/it]Epoch 15:  97%|█████████▋| 291/300 [08:11<00:16,  1.80s/it]Epoch 15:  97%|█████████▋| 292/300 [08:12<00:14,  1.82s/it]Epoch 15:  98%|█████████▊| 293/300 [08:14<00:12,  1.72s/it]Epoch 15:  98%|█████████▊| 294/300 [08:15<00:09,  1.64s/it]Epoch 15:  98%|█████████▊| 295/300 [08:17<00:08,  1.62s/it]Epoch 15:  99%|█████████▊| 296/300 [08:19<00:06,  1.71s/it]Epoch 15:  99%|█████████▉| 297/300 [08:21<00:05,  1.76s/it]Epoch 15:  99%|█████████▉| 298/300 [08:23<00:03,  1.80s/it]Epoch 15: 100%|█████████▉| 299/300 [08:24<00:01,  1.83s/it]06/19/2022 16:54:45 - INFO - __main__ - global step: 2400; train loss: 7.7799577713012695; dev loss
Epoch 15: 100%|██████████| 300/300 [08:26<00:00,  1.78s/it]Epoch 15: 100%|██████████| 300/300 [08:26<00:00,  1.69s/it]
Epoch 16:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 16:   0%|          | 1/300 [00:01<07:33,  1.52s/it]Epoch 16:   1%|          | 2/300 [00:03<07:26,  1.50s/it]Epoch 16:   1%|          | 3/300 [00:04<08:14,  1.67s/it]Epoch 16:   1%|▏         | 4/300 [00:06<08:06,  1.64s/it]Epoch 16:   2%|▏         | 5/300 [00:08<08:32,  1.74s/it]Epoch 16:   2%|▏         | 6/300 [00:10<08:42,  1.78s/it]Epoch 16:   2%|▏         | 7/300 [00:12<08:46,  1.80s/it]Epoch 16:   3%|▎         | 8/300 [00:13<08:25,  1.73s/it]Epoch 16:   3%|▎         | 9/300 [00:15<08:34,  1.77s/it]Epoch 16:   3%|▎         | 10/300 [00:16<08:05,  1.68s/it]Epoch 16:   4%|▎         | 11/300 [00:18<07:48,  1.62s/it]Epoch 16:   4%|▍         | 12/300 [00:20<07:43,  1.61s/it]Epoch 16:   4%|▍         | 13/300 [00:21<07:26,  1.56s/it]Epoch 16:   5%|▍         | 14/300 [00:23<07:52,  1.65s/it]Epoch 16:   5%|▌         | 15/300 [00:24<07:35,  1.60s/it]Epoch 16:   5%|▌         | 16/300 [00:26<07:32,  1.59s/it]Epoch 16:   6%|▌         | 17/300 [00:27<07:19,  1.55s/it]Epoch 16:   6%|▌         | 18/300 [00:29<07:41,  1.64s/it]Epoch 16:   6%|▋         | 19/300 [00:31<07:59,  1.71s/it]06/19/2022 16:55:18 - INFO - __main__ - global step: 2410; train loss: 8.064286231994629; dev loss
Epoch 16:   7%|▋         | 20/300 [00:33<07:46,  1.67s/it]Epoch 16:   7%|▋         | 21/300 [00:34<07:59,  1.72s/it]Epoch 16:   7%|▋         | 22/300 [00:36<07:41,  1.66s/it]Epoch 16:   8%|▊         | 23/300 [00:38<07:56,  1.72s/it]Epoch 16:   8%|▊         | 24/300 [00:40<08:06,  1.76s/it]Epoch 16:   8%|▊         | 25/300 [00:41<07:47,  1.70s/it]Epoch 16:   9%|▊         | 26/300 [00:43<07:58,  1.75s/it]Epoch 16:   9%|▉         | 27/300 [00:45<07:34,  1.66s/it]Epoch 16:   9%|▉         | 28/300 [00:46<07:48,  1.72s/it]Epoch 16:  10%|▉         | 29/300 [00:48<08:06,  1.79s/it]Epoch 16:  10%|█         | 30/300 [00:50<08:12,  1.83s/it]Epoch 16:  10%|█         | 31/300 [00:52<07:44,  1.73s/it]Epoch 16:  11%|█         | 32/300 [00:54<07:53,  1.77s/it]Epoch 16:  11%|█         | 33/300 [00:56<08:06,  1.82s/it]Epoch 16:  11%|█▏        | 34/300 [00:57<08:05,  1.83s/it]Epoch 16:  12%|█▏        | 35/300 [00:59<08:08,  1.84s/it]Epoch 16:  12%|█▏        | 36/300 [01:01<07:38,  1.74s/it]Epoch 16:  12%|█▏        | 37/300 [01:03<07:52,  1.80s/it]Epoch 16:  13%|█▎        | 38/300 [01:04<07:25,  1.70s/it]Epoch 16:  13%|█▎        | 39/300 [01:06<07:34,  1.74s/it]06/19/2022 16:55:53 - INFO - __main__ - global step: 2420; train loss: 7.3741865158081055; dev loss
Epoch 16:  13%|█▎        | 40/300 [01:08<07:12,  1.66s/it]Epoch 16:  14%|█▎        | 41/300 [01:10<07:32,  1.75s/it]Epoch 16:  14%|█▍        | 42/300 [01:11<07:08,  1.66s/it]Epoch 16:  14%|█▍        | 43/300 [01:13<07:20,  1.71s/it]Epoch 16:  15%|█▍        | 44/300 [01:14<07:03,  1.65s/it]Epoch 16:  15%|█▌        | 45/300 [01:16<06:58,  1.64s/it]Epoch 16:  15%|█▌        | 46/300 [01:17<06:45,  1.60s/it]Epoch 16:  16%|█▌        | 47/300 [01:19<07:01,  1.67s/it]Epoch 16:  16%|█▌        | 48/300 [01:21<06:46,  1.61s/it]Epoch 16:  16%|█▋        | 49/300 [01:23<07:09,  1.71s/it]Epoch 16:  17%|█▋        | 50/300 [01:25<07:17,  1.75s/it]Epoch 16:  17%|█▋        | 51/300 [01:26<07:22,  1.78s/it]Epoch 16:  17%|█▋        | 52/300 [01:28<06:59,  1.69s/it]Epoch 16:  18%|█▊        | 53/300 [01:29<06:40,  1.62s/it]Epoch 16:  18%|█▊        | 54/300 [01:31<06:37,  1.62s/it]Epoch 16:  18%|█▊        | 55/300 [01:33<06:53,  1.69s/it]Epoch 16:  19%|█▊        | 56/300 [01:34<06:38,  1.64s/it]Epoch 16:  19%|█▉        | 57/300 [01:36<06:24,  1.58s/it]Epoch 16:  19%|█▉        | 58/300 [01:38<06:52,  1.71s/it]Epoch 16:  20%|█▉        | 59/300 [01:40<07:01,  1.75s/it]06/19/2022 16:56:27 - INFO - __main__ - global step: 2430; train loss: 7.9573869705200195; dev loss
Epoch 16:  20%|██        | 60/300 [01:41<06:38,  1.66s/it]Epoch 16:  20%|██        | 61/300 [01:43<06:50,  1.72s/it]Epoch 16:  21%|██        | 62/300 [01:45<07:06,  1.79s/it]Epoch 16:  21%|██        | 63/300 [01:46<06:42,  1.70s/it]Epoch 16:  21%|██▏       | 64/300 [01:48<06:25,  1.63s/it]Epoch 16:  22%|██▏       | 65/300 [01:49<06:12,  1.59s/it]Epoch 16:  22%|██▏       | 66/300 [01:51<06:12,  1.59s/it]Epoch 16:  22%|██▏       | 67/300 [01:52<06:00,  1.55s/it]Epoch 16:  23%|██▎       | 68/300 [01:54<06:21,  1.64s/it]Epoch 16:  23%|██▎       | 69/300 [01:56<06:35,  1.71s/it]Epoch 16:  23%|██▎       | 70/300 [01:58<06:51,  1.79s/it]Epoch 16:  24%|██▎       | 71/300 [02:00<06:26,  1.69s/it]Epoch 16:  24%|██▍       | 72/300 [02:01<06:35,  1.73s/it]Epoch 16:  24%|██▍       | 73/300 [02:03<06:18,  1.67s/it]Epoch 16:  25%|██▍       | 74/300 [02:04<06:13,  1.65s/it]Epoch 16:  25%|██▌       | 75/300 [02:06<06:27,  1.72s/it]Epoch 16:  25%|██▌       | 76/300 [02:08<06:35,  1.76s/it]Epoch 16:  26%|██▌       | 77/300 [02:10<06:13,  1.67s/it]Epoch 16:  26%|██▌       | 78/300 [02:12<06:24,  1.73s/it]Epoch 16:  26%|██▋       | 79/300 [02:14<06:37,  1.80s/it]06/19/2022 16:57:01 - INFO - __main__ - global step: 2440; train loss: 7.622550964355469; dev loss
Epoch 16:  27%|██▋       | 80/300 [02:15<06:39,  1.82s/it]Epoch 16:  27%|██▋       | 81/300 [02:17<06:16,  1.72s/it]Epoch 16:  27%|██▋       | 82/300 [02:19<06:22,  1.76s/it]Epoch 16:  28%|██▊       | 83/300 [02:21<06:34,  1.82s/it]Epoch 16:  28%|██▊       | 84/300 [02:22<06:08,  1.71s/it]Epoch 16:  28%|██▊       | 85/300 [02:24<06:14,  1.74s/it]Epoch 16:  29%|██▊       | 86/300 [02:26<06:19,  1.77s/it]Epoch 16:  29%|██▉       | 87/300 [02:27<06:05,  1.71s/it]Epoch 16:  29%|██▉       | 88/300 [02:29<06:13,  1.76s/it]Epoch 16:  30%|██▉       | 89/300 [02:31<06:17,  1.79s/it]Epoch 16:  30%|███       | 90/300 [02:33<06:21,  1.82s/it]Epoch 16:  30%|███       | 91/300 [02:35<06:30,  1.87s/it]Epoch 16:  31%|███       | 92/300 [02:36<06:06,  1.76s/it]Epoch 16:  31%|███       | 93/300 [02:38<06:09,  1.79s/it]Epoch 16:  31%|███▏      | 94/300 [02:40<05:50,  1.70s/it]Epoch 16:  32%|███▏      | 95/300 [02:42<06:04,  1.78s/it]Epoch 16:  32%|███▏      | 96/300 [02:44<06:07,  1.80s/it]Epoch 16:  32%|███▏      | 97/300 [02:45<05:45,  1.70s/it]Epoch 16:  33%|███▎      | 98/300 [02:47<05:30,  1.64s/it]Epoch 16:  33%|███▎      | 99/300 [02:48<05:25,  1.62s/it]06/19/2022 16:57:35 - INFO - __main__ - global step: 2450; train loss: 7.816683769226074; dev loss
Epoch 16:  33%|███▎      | 100/300 [02:50<05:15,  1.58s/it]Epoch 16:  34%|███▎      | 101/300 [02:51<05:29,  1.66s/it]Epoch 16:  34%|███▍      | 102/300 [02:53<05:39,  1.71s/it]Epoch 16:  34%|███▍      | 103/300 [02:55<05:31,  1.68s/it]Epoch 16:  35%|███▍      | 104/300 [02:56<05:15,  1.61s/it]Epoch 16:  35%|███▌      | 105/300 [02:58<05:27,  1.68s/it]Epoch 16:  35%|███▌      | 106/300 [03:00<05:36,  1.73s/it]Epoch 16:  36%|███▌      | 107/300 [03:02<05:18,  1.65s/it]Epoch 16:  36%|███▌      | 108/300 [03:03<05:13,  1.63s/it]Epoch 16:  36%|███▋      | 109/300 [03:05<05:23,  1.69s/it]Epoch 16:  37%|███▋      | 110/300 [03:06<05:09,  1.63s/it]Epoch 16:  37%|███▋      | 111/300 [03:08<05:20,  1.70s/it]Epoch 16:  37%|███▋      | 112/300 [03:10<05:12,  1.66s/it]Epoch 16:  38%|███▊      | 113/300 [03:12<05:20,  1.72s/it]Epoch 16:  38%|███▊      | 114/300 [03:14<05:28,  1.76s/it]Epoch 16:  38%|███▊      | 115/300 [03:15<05:32,  1.80s/it]Epoch 16:  39%|███▊      | 116/300 [03:17<05:40,  1.85s/it]Epoch 16:  39%|███▉      | 117/300 [03:19<05:17,  1.73s/it]Epoch 16:  39%|███▉      | 118/300 [03:20<05:01,  1.66s/it]Epoch 16:  40%|███▉      | 119/300 [03:22<05:10,  1.71s/it]06/19/2022 16:58:10 - INFO - __main__ - global step: 2460; train loss: 7.814277648925781; dev loss
Epoch 16:  40%|████      | 120/300 [03:24<05:21,  1.79s/it]Epoch 16:  40%|████      | 121/300 [03:26<05:01,  1.69s/it]Epoch 16:  41%|████      | 122/300 [03:27<05:08,  1.73s/it]Epoch 16:  41%|████      | 123/300 [03:29<04:53,  1.66s/it]Epoch 16:  41%|████▏     | 124/300 [03:31<04:47,  1.63s/it]Epoch 16:  42%|████▏     | 125/300 [03:32<04:57,  1.70s/it]Epoch 16:  42%|████▏     | 126/300 [03:34<04:58,  1.71s/it]Epoch 16:  42%|████▏     | 127/300 [03:36<04:45,  1.65s/it]Epoch 16:  43%|████▎     | 128/300 [03:37<04:39,  1.62s/it]Epoch 16:  43%|████▎     | 129/300 [03:39<04:29,  1.57s/it]Epoch 16:  43%|████▎     | 130/300 [03:40<04:21,  1.54s/it]Epoch 16:  44%|████▎     | 131/300 [03:42<04:29,  1.59s/it]Epoch 16:  44%|████▍     | 132/300 [03:43<04:21,  1.56s/it]Epoch 16:  44%|████▍     | 133/300 [03:45<04:21,  1.56s/it]Epoch 16:  45%|████▍     | 134/300 [03:46<04:14,  1.54s/it]Epoch 16:  45%|████▌     | 135/300 [03:48<04:23,  1.60s/it]Epoch 16:  45%|████▌     | 136/300 [03:50<04:32,  1.66s/it]Epoch 16:  46%|████▌     | 137/300 [03:52<04:34,  1.69s/it]Epoch 16:  46%|████▌     | 138/300 [03:54<04:42,  1.74s/it]Epoch 16:  46%|████▋     | 139/300 [03:55<04:46,  1.78s/it]06/19/2022 16:58:42 - INFO - __main__ - global step: 2470; train loss: 7.825855255126953; dev loss
Epoch 16:  47%|████▋     | 140/300 [03:57<04:29,  1.69s/it]Epoch 16:  47%|████▋     | 141/300 [03:58<04:22,  1.65s/it]Epoch 16:  47%|████▋     | 142/300 [04:00<04:29,  1.70s/it]Epoch 16:  48%|████▊     | 143/300 [04:02<04:34,  1.75s/it]Epoch 16:  48%|████▊     | 144/300 [04:04<04:19,  1.66s/it]Epoch 16:  48%|████▊     | 145/300 [04:06<04:30,  1.75s/it]Epoch 16:  49%|████▊     | 146/300 [04:07<04:16,  1.66s/it]Epoch 16:  49%|████▉     | 147/300 [04:09<04:22,  1.72s/it]Epoch 16:  49%|████▉     | 148/300 [04:11<04:27,  1.76s/it]Epoch 16:  50%|████▉     | 149/300 [04:12<04:16,  1.70s/it]Epoch 16:  50%|█████     | 150/300 [04:14<04:04,  1.63s/it]Epoch 16:  50%|█████     | 151/300 [04:15<03:57,  1.59s/it]Epoch 16:  51%|█████     | 152/300 [04:17<04:06,  1.66s/it]Epoch 16:  51%|█████     | 153/300 [04:19<04:19,  1.77s/it]Epoch 16:  51%|█████▏    | 154/300 [04:21<04:04,  1.67s/it]Epoch 16:  52%|█████▏    | 155/300 [04:22<03:53,  1.61s/it]Epoch 16:  52%|█████▏    | 156/300 [04:24<04:02,  1.68s/it]Epoch 16:  52%|█████▏    | 157/300 [04:25<03:56,  1.65s/it]Epoch 16:  53%|█████▎    | 158/300 [04:27<03:46,  1.59s/it]Epoch 16:  53%|█████▎    | 159/300 [04:28<03:39,  1.56s/it]06/19/2022 16:59:15 - INFO - __main__ - global step: 2480; train loss: 7.75115442276001; dev loss
Epoch 16:  53%|█████▎    | 160/300 [04:30<03:33,  1.53s/it]Epoch 16:  54%|█████▎    | 161/300 [04:31<03:29,  1.51s/it]Epoch 16:  54%|█████▍    | 162/300 [04:33<03:48,  1.65s/it]Epoch 16:  54%|█████▍    | 163/300 [04:35<03:54,  1.71s/it]Epoch 16:  55%|█████▍    | 164/300 [04:37<03:58,  1.75s/it]Epoch 16:  55%|█████▌    | 165/300 [04:39<04:00,  1.78s/it]Epoch 16:  55%|█████▌    | 166/300 [04:41<04:05,  1.83s/it]Epoch 16:  56%|█████▌    | 167/300 [04:43<04:04,  1.84s/it]Epoch 16:  56%|█████▌    | 168/300 [04:44<04:03,  1.84s/it]Epoch 16:  56%|█████▋    | 169/300 [04:46<03:45,  1.73s/it]Epoch 16:  57%|█████▋    | 170/300 [04:47<03:38,  1.68s/it]Epoch 16:  57%|█████▋    | 171/300 [04:49<03:29,  1.62s/it]Epoch 16:  57%|█████▋    | 172/300 [04:51<03:36,  1.69s/it]Epoch 16:  58%|█████▊    | 173/300 [04:52<03:25,  1.62s/it]Epoch 16:  58%|█████▊    | 174/300 [04:54<03:22,  1.61s/it]Epoch 16:  58%|█████▊    | 175/300 [04:56<03:29,  1.67s/it]Epoch 16:  59%|█████▊    | 176/300 [04:58<03:34,  1.73s/it]Epoch 16:  59%|█████▉    | 177/300 [04:59<03:37,  1.77s/it]Epoch 16:  59%|█████▉    | 178/300 [05:01<03:29,  1.72s/it]Epoch 16:  60%|█████▉    | 179/300 [05:03<03:20,  1.66s/it]06/19/2022 16:59:49 - INFO - __main__ - global step: 2490; train loss: 7.971150875091553; dev loss
Epoch 16:  60%|██████    | 180/300 [05:04<03:12,  1.61s/it]Epoch 16:  60%|██████    | 181/300 [05:06<03:20,  1.68s/it]Epoch 16:  61%|██████    | 182/300 [05:07<03:16,  1.66s/it]Epoch 16:  61%|██████    | 183/300 [05:09<03:07,  1.61s/it]Epoch 16:  61%|██████▏   | 184/300 [05:11<03:14,  1.68s/it]Epoch 16:  62%|██████▏   | 185/300 [05:13<03:19,  1.74s/it]Epoch 16:  62%|██████▏   | 186/300 [05:14<03:08,  1.65s/it]Epoch 16:  62%|██████▏   | 187/300 [05:16<03:04,  1.63s/it]Epoch 16:  63%|██████▎   | 188/300 [05:18<03:09,  1.69s/it]Epoch 16:  63%|██████▎   | 189/300 [05:19<03:13,  1.74s/it]Epoch 16:  63%|██████▎   | 190/300 [05:21<03:03,  1.66s/it]Epoch 16:  64%|██████▎   | 191/300 [05:22<02:58,  1.64s/it]Epoch 16:  64%|██████▍   | 192/300 [05:24<02:52,  1.59s/it]Epoch 16:  64%|██████▍   | 193/300 [05:25<02:47,  1.56s/it]Epoch 16:  65%|██████▍   | 194/300 [05:27<02:42,  1.54s/it]Epoch 16:  65%|██████▌   | 195/300 [05:29<02:54,  1.66s/it]Epoch 16:  65%|██████▌   | 196/300 [05:31<02:59,  1.72s/it]Epoch 16:  66%|██████▌   | 197/300 [05:32<02:51,  1.66s/it]Epoch 16:  66%|██████▌   | 198/300 [05:34<02:44,  1.61s/it]Epoch 16:  66%|██████▋   | 199/300 [05:35<02:42,  1.61s/it]06/19/2022 17:00:23 - INFO - __main__ - global step: 2500; train loss: 8.014617919921875; dev loss
Epoch 16:  67%|██████▋   | 200/300 [05:37<02:50,  1.71s/it]Epoch 16:  67%|██████▋   | 201/300 [05:39<02:42,  1.64s/it]Epoch 16:  67%|██████▋   | 202/300 [05:40<02:35,  1.59s/it]Epoch 16:  68%|██████▊   | 203/300 [05:42<02:34,  1.59s/it]Epoch 16:  68%|██████▊   | 204/300 [05:43<02:29,  1.55s/it]Epoch 16:  68%|██████▊   | 205/300 [05:45<02:35,  1.64s/it]Epoch 16:  69%|██████▊   | 206/300 [05:47<02:28,  1.58s/it]Epoch 16:  69%|██████▉   | 207/300 [05:48<02:27,  1.58s/it]Epoch 16:  69%|██████▉   | 208/300 [05:50<02:22,  1.55s/it]Epoch 16:  70%|██████▉   | 209/300 [05:51<02:29,  1.64s/it]Epoch 16:  70%|███████   | 210/300 [05:53<02:33,  1.70s/it]Epoch 16:  70%|███████   | 211/300 [05:55<02:28,  1.67s/it]Epoch 16:  71%|███████   | 212/300 [05:56<02:21,  1.60s/it]Epoch 16:  71%|███████   | 213/300 [05:58<02:15,  1.56s/it]Epoch 16:  71%|███████▏  | 214/300 [06:00<02:21,  1.65s/it]Epoch 16:  72%|███████▏  | 215/300 [06:01<02:16,  1.60s/it]Epoch 16:  72%|███████▏  | 216/300 [06:03<02:13,  1.58s/it]Epoch 16:  72%|███████▏  | 217/300 [06:04<02:08,  1.55s/it]Epoch 16:  73%|███████▎  | 218/300 [06:06<02:14,  1.63s/it]Epoch 16:  73%|███████▎  | 219/300 [06:08<02:17,  1.70s/it]06/19/2022 17:00:55 - INFO - __main__ - global step: 2510; train loss: 8.005990982055664; dev loss
Epoch 16:  73%|███████▎  | 220/300 [06:10<02:22,  1.78s/it]Epoch 16:  74%|███████▎  | 221/300 [06:12<02:22,  1.80s/it]Epoch 16:  74%|███████▍  | 222/300 [06:13<02:12,  1.70s/it]Epoch 16:  74%|███████▍  | 223/300 [06:15<02:04,  1.62s/it]Epoch 16:  75%|███████▍  | 224/300 [06:17<02:11,  1.73s/it]Epoch 16:  75%|███████▌  | 225/300 [06:18<02:12,  1.77s/it]Epoch 16:  75%|███████▌  | 226/300 [06:20<02:13,  1.80s/it]Epoch 16:  76%|███████▌  | 227/300 [06:22<02:04,  1.70s/it]Epoch 16:  76%|███████▌  | 228/300 [06:24<02:08,  1.79s/it]Epoch 16:  76%|███████▋  | 229/300 [06:25<02:00,  1.69s/it]Epoch 16:  77%|███████▋  | 230/300 [06:27<02:01,  1.74s/it]Epoch 16:  77%|███████▋  | 231/300 [06:29<01:54,  1.67s/it]Epoch 16:  77%|███████▋  | 232/300 [06:31<01:59,  1.75s/it]Epoch 16:  78%|███████▊  | 233/300 [06:32<01:59,  1.78s/it]Epoch 16:  78%|███████▊  | 234/300 [06:34<01:59,  1.81s/it]Epoch 16:  78%|███████▊  | 235/300 [06:36<01:58,  1.83s/it]Epoch 16:  79%|███████▊  | 236/300 [06:38<01:51,  1.75s/it]Epoch 16:  79%|███████▉  | 237/300 [06:39<01:45,  1.67s/it]Epoch 16:  79%|███████▉  | 238/300 [06:41<01:47,  1.73s/it]Epoch 16:  80%|███████▉  | 239/300 [06:43<01:48,  1.77s/it]06/19/2022 17:01:30 - INFO - __main__ - global step: 2520; train loss: 8.003339767456055; dev loss
Epoch 16:  80%|████████  | 240/300 [06:45<01:47,  1.80s/it]Epoch 16:  80%|████████  | 241/300 [06:47<01:48,  1.84s/it]Epoch 16:  81%|████████  | 242/300 [06:49<01:47,  1.85s/it]Epoch 16:  81%|████████  | 243/300 [06:50<01:45,  1.86s/it]Epoch 16:  81%|████████▏ | 244/300 [06:52<01:44,  1.86s/it]Epoch 16:  82%|████████▏ | 245/300 [06:54<01:37,  1.78s/it]Epoch 16:  82%|████████▏ | 246/300 [06:56<01:37,  1.81s/it]Epoch 16:  82%|████████▏ | 247/300 [06:57<01:30,  1.70s/it]Epoch 16:  83%|████████▎ | 248/300 [06:59<01:30,  1.74s/it]Epoch 16:  83%|████████▎ | 249/300 [07:01<01:32,  1.81s/it]Epoch 16:  83%|████████▎ | 250/300 [07:03<01:30,  1.82s/it]Epoch 16:  84%|████████▎ | 251/300 [07:04<01:24,  1.72s/it]Epoch 16:  84%|████████▍ | 252/300 [07:06<01:24,  1.76s/it]Epoch 16:  84%|████████▍ | 253/300 [07:08<01:25,  1.82s/it]Epoch 16:  85%|████████▍ | 254/300 [07:10<01:19,  1.72s/it]Epoch 16:  85%|████████▌ | 255/300 [07:11<01:14,  1.65s/it]Epoch 16:  85%|████████▌ | 256/300 [07:13<01:15,  1.71s/it]Epoch 16:  86%|████████▌ | 257/300 [07:15<01:16,  1.79s/it]Epoch 16:  86%|████████▌ | 258/300 [07:17<01:15,  1.81s/it]Epoch 16:  86%|████████▋ | 259/300 [07:19<01:14,  1.82s/it]06/19/2022 17:02:06 - INFO - __main__ - global step: 2530; train loss: 8.253786087036133; dev loss
Epoch 16:  87%|████████▋ | 260/300 [07:21<01:13,  1.83s/it]Epoch 16:  87%|████████▋ | 261/300 [07:22<01:08,  1.76s/it]Epoch 16:  87%|████████▋ | 262/300 [07:24<01:03,  1.67s/it]Epoch 16:  88%|████████▊ | 263/300 [07:25<01:03,  1.73s/it]Epoch 16:  88%|████████▊ | 264/300 [07:27<00:59,  1.65s/it]Epoch 16:  88%|████████▊ | 265/300 [07:29<01:00,  1.74s/it]Epoch 16:  89%|████████▊ | 266/300 [07:31<01:00,  1.78s/it]Epoch 16:  89%|████████▉ | 267/300 [07:32<00:55,  1.68s/it]Epoch 16:  89%|████████▉ | 268/300 [07:34<00:53,  1.66s/it]Epoch 16:  90%|████████▉ | 269/300 [07:35<00:49,  1.61s/it]Epoch 16:  90%|█████████ | 270/300 [07:37<00:51,  1.71s/it]Epoch 16:  90%|█████████ | 271/300 [07:39<00:50,  1.76s/it]Epoch 16:  91%|█████████ | 272/300 [07:41<00:46,  1.66s/it]Epoch 16:  91%|█████████ | 273/300 [07:42<00:46,  1.71s/it]Epoch 16:  91%|█████████▏| 274/300 [07:44<00:46,  1.78s/it]Epoch 16:  92%|█████████▏| 275/300 [07:46<00:42,  1.68s/it]Epoch 16:  92%|█████████▏| 276/300 [07:47<00:38,  1.61s/it]Epoch 16:  92%|█████████▏| 277/300 [07:49<00:36,  1.57s/it]Epoch 16:  93%|█████████▎| 278/300 [07:50<00:35,  1.59s/it]Epoch 16:  93%|█████████▎| 279/300 [07:52<00:32,  1.56s/it]06/19/2022 17:02:39 - INFO - __main__ - global step: 2540; train loss: 8.127908706665039; dev loss
Epoch 16:  93%|█████████▎| 280/300 [07:53<00:30,  1.53s/it]Epoch 16:  94%|█████████▎| 281/300 [07:55<00:28,  1.51s/it]Epoch 16:  94%|█████████▍| 282/300 [07:57<00:29,  1.65s/it]Epoch 16:  94%|█████████▍| 283/300 [07:58<00:27,  1.60s/it]Epoch 16:  95%|█████████▍| 284/300 [08:00<00:25,  1.57s/it]Epoch 16:  95%|█████████▌| 285/300 [08:02<00:24,  1.65s/it]Epoch 16:  95%|█████████▌| 286/300 [08:04<00:24,  1.74s/it]Epoch 16:  96%|█████████▌| 287/300 [08:05<00:23,  1.78s/it]Epoch 16:  96%|█████████▌| 288/300 [08:07<00:21,  1.79s/it]Epoch 16:  96%|█████████▋| 289/300 [08:09<00:18,  1.70s/it]Epoch 16:  97%|█████████▋| 290/300 [08:10<00:16,  1.65s/it]Epoch 16:  97%|█████████▋| 291/300 [08:12<00:15,  1.70s/it]Epoch 16:  97%|█████████▋| 292/300 [08:14<00:13,  1.74s/it]Epoch 16:  98%|█████████▊| 293/300 [08:15<00:11,  1.67s/it]Epoch 16:  98%|█████████▊| 294/300 [08:17<00:10,  1.72s/it]Epoch 16:  98%|█████████▊| 295/300 [08:19<00:08,  1.67s/it]Epoch 16:  99%|█████████▊| 296/300 [08:20<00:06,  1.61s/it]Epoch 16:  99%|█████████▉| 297/300 [08:22<00:05,  1.67s/it]Epoch 16:  99%|█████████▉| 298/300 [08:24<00:03,  1.60s/it]Epoch 16: 100%|█████████▉| 299/300 [08:25<00:01,  1.71s/it]06/19/2022 17:03:12 - INFO - __main__ - global step: 2550; train loss: 7.960511684417725; dev loss
Epoch 16: 100%|██████████| 300/300 [08:27<00:00,  1.63s/it]Epoch 16: 100%|██████████| 300/300 [08:27<00:00,  1.69s/it]
Epoch 17:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 17:   0%|          | 1/300 [00:01<07:11,  1.44s/it]Epoch 17:   1%|          | 2/300 [00:02<07:19,  1.47s/it]Epoch 17:   1%|          | 3/300 [00:04<07:27,  1.51s/it]Epoch 17:   1%|▏         | 4/300 [00:06<08:02,  1.63s/it]Epoch 17:   2%|▏         | 5/300 [00:07<07:40,  1.56s/it]Epoch 17:   2%|▏         | 6/300 [00:09<08:11,  1.67s/it]Epoch 17:   2%|▏         | 7/300 [00:11<08:36,  1.76s/it]Epoch 17:   3%|▎         | 8/300 [00:13<08:04,  1.66s/it]Epoch 17:   3%|▎         | 9/300 [00:14<08:18,  1.71s/it]Epoch 17:   3%|▎         | 10/300 [00:16<07:55,  1.64s/it]Epoch 17:   4%|▎         | 11/300 [00:18<08:22,  1.74s/it]Epoch 17:   4%|▍         | 12/300 [00:19<07:57,  1.66s/it]Epoch 17:   4%|▍         | 13/300 [00:21<08:10,  1.71s/it]Epoch 17:   5%|▍         | 14/300 [00:23<08:22,  1.76s/it]Epoch 17:   5%|▌         | 15/300 [00:25<08:07,  1.71s/it]Epoch 17:   5%|▌         | 16/300 [00:26<08:14,  1.74s/it]Epoch 17:   6%|▌         | 17/300 [00:28<07:49,  1.66s/it]Epoch 17:   6%|▌         | 18/300 [00:29<07:31,  1.60s/it]Epoch 17:   6%|▋         | 19/300 [00:31<07:58,  1.70s/it]06/19/2022 17:03:46 - INFO - __main__ - global step: 2560; train loss: 8.086320877075195; dev loss
Epoch 17:   7%|▋         | 20/300 [00:33<08:10,  1.75s/it]Epoch 17:   7%|▋         | 21/300 [00:35<08:21,  1.80s/it]Epoch 17:   7%|▋         | 22/300 [00:37<08:23,  1.81s/it]Epoch 17:   8%|▊         | 23/300 [00:38<07:51,  1.70s/it]Epoch 17:   8%|▊         | 24/300 [00:40<08:09,  1.77s/it]Epoch 17:   8%|▊         | 25/300 [00:42<08:13,  1.79s/it]Epoch 17:   9%|▊         | 26/300 [00:44<08:14,  1.80s/it]Epoch 17:   9%|▉         | 27/300 [00:45<07:42,  1.69s/it]Epoch 17:   9%|▉         | 28/300 [00:47<07:31,  1.66s/it]Epoch 17:  10%|▉         | 29/300 [00:49<07:43,  1.71s/it]Epoch 17:  10%|█         | 30/300 [00:51<07:50,  1.74s/it]Epoch 17:  10%|█         | 31/300 [00:52<07:55,  1.77s/it]Epoch 17:  11%|█         | 32/300 [00:54<08:07,  1.82s/it]Epoch 17:  11%|█         | 33/300 [00:56<07:37,  1.71s/it]Epoch 17:  11%|█▏        | 34/300 [00:57<07:17,  1.64s/it]Epoch 17:  12%|█▏        | 35/300 [00:59<07:00,  1.59s/it]Epoch 17:  12%|█▏        | 36/300 [01:01<07:26,  1.69s/it]Epoch 17:  12%|█▏        | 37/300 [01:03<07:36,  1.74s/it]Epoch 17:  13%|█▎        | 38/300 [01:04<07:15,  1.66s/it]Epoch 17:  13%|█▎        | 39/300 [01:06<07:29,  1.72s/it]06/19/2022 17:04:21 - INFO - __main__ - global step: 2570; train loss: 7.046909332275391; dev loss
Epoch 17:  13%|█▎        | 40/300 [01:08<07:44,  1.79s/it]Epoch 17:  14%|█▎        | 41/300 [01:10<07:47,  1.81s/it]Epoch 17:  14%|█▍        | 42/300 [01:11<07:19,  1.70s/it]Epoch 17:  14%|█▍        | 43/300 [01:13<07:26,  1.74s/it]Epoch 17:  15%|█▍        | 44/300 [01:15<07:11,  1.69s/it]Epoch 17:  15%|█▌        | 45/300 [01:16<06:52,  1.62s/it]Epoch 17:  15%|█▌        | 46/300 [01:17<06:36,  1.56s/it]Epoch 17:  16%|█▌        | 47/300 [01:19<06:28,  1.53s/it]Epoch 17:  16%|█▌        | 48/300 [01:20<06:21,  1.51s/it]Epoch 17:  16%|█▋        | 49/300 [01:22<06:54,  1.65s/it]Epoch 17:  17%|█▋        | 50/300 [01:24<07:05,  1.70s/it]Epoch 17:  17%|█▋        | 51/300 [01:26<06:48,  1.64s/it]Epoch 17:  17%|█▋        | 52/300 [01:27<07:00,  1.70s/it]Epoch 17:  18%|█▊        | 53/300 [01:29<06:51,  1.66s/it]Epoch 17:  18%|█▊        | 54/300 [01:31<07:02,  1.72s/it]Epoch 17:  18%|█▊        | 55/300 [01:32<06:40,  1.64s/it]Epoch 17:  19%|█▊        | 56/300 [01:34<06:58,  1.72s/it]Epoch 17:  19%|█▉        | 57/300 [01:36<06:55,  1.71s/it]Epoch 17:  19%|█▉        | 58/300 [01:38<06:49,  1.69s/it]Epoch 17:  20%|█▉        | 59/300 [01:40<07:05,  1.76s/it]06/19/2022 17:04:54 - INFO - __main__ - global step: 2580; train loss: 7.956928253173828; dev loss
Epoch 17:  20%|██        | 60/300 [01:41<06:53,  1.72s/it]Epoch 17:  20%|██        | 61/300 [01:43<06:58,  1.75s/it]Epoch 17:  21%|██        | 62/300 [01:45<06:55,  1.75s/it]Epoch 17:  21%|██        | 63/300 [01:47<07:10,  1.82s/it]Epoch 17:  21%|██▏       | 64/300 [01:48<06:58,  1.77s/it]Epoch 17:  22%|██▏       | 65/300 [01:50<07:15,  1.85s/it]Epoch 17:  22%|██▏       | 66/300 [01:52<07:20,  1.88s/it]Epoch 17:  22%|██▏       | 67/300 [01:54<07:22,  1.90s/it]Epoch 17:  23%|██▎       | 68/300 [01:56<07:04,  1.83s/it]Epoch 17:  23%|██▎       | 69/300 [01:58<06:59,  1.82s/it]Epoch 17:  23%|██▎       | 70/300 [02:00<07:06,  1.85s/it]Epoch 17:  24%|██▎       | 71/300 [02:01<06:50,  1.79s/it]Epoch 17:  24%|██▍       | 72/300 [02:03<07:01,  1.85s/it]Epoch 17:  24%|██▍       | 73/300 [02:05<07:17,  1.93s/it]Epoch 17:  25%|██▍       | 74/300 [02:07<06:56,  1.84s/it]Epoch 17:  25%|██▌       | 75/300 [02:09<07:01,  1.88s/it]Epoch 17:  25%|██▌       | 76/300 [02:11<06:42,  1.80s/it]Epoch 17:  26%|██▌       | 77/300 [02:12<06:33,  1.77s/it]Epoch 17:  26%|██▌       | 78/300 [02:14<06:51,  1.85s/it]Epoch 17:  26%|██▋       | 79/300 [02:16<06:54,  1.88s/it]06/19/2022 17:05:31 - INFO - __main__ - global step: 2590; train loss: 8.3265380859375; dev loss
Epoch 17:  27%|██▋       | 80/300 [02:18<06:38,  1.81s/it]Epoch 17:  27%|██▋       | 81/300 [02:20<06:46,  1.86s/it]Epoch 17:  27%|██▋       | 82/300 [02:22<06:40,  1.84s/it]Epoch 17:  28%|██▊       | 83/300 [02:24<06:45,  1.87s/it]Epoch 17:  28%|██▊       | 84/300 [02:25<06:18,  1.75s/it]Epoch 17:  28%|██▊       | 85/300 [02:27<05:57,  1.66s/it]Epoch 17:  29%|██▊       | 86/300 [02:28<05:49,  1.63s/it]Epoch 17:  29%|██▉       | 87/300 [02:30<05:36,  1.58s/it]Epoch 17:  29%|██▉       | 88/300 [02:31<05:28,  1.55s/it]Epoch 17:  30%|██▉       | 89/300 [02:33<05:19,  1.51s/it]Epoch 17:  30%|███       | 90/300 [02:34<05:21,  1.53s/it]Epoch 17:  30%|███       | 91/300 [02:36<05:15,  1.51s/it]Epoch 17:  31%|███       | 92/300 [02:37<05:08,  1.49s/it]Epoch 17:  31%|███       | 93/300 [02:38<05:07,  1.49s/it]Epoch 17:  31%|███▏      | 94/300 [02:40<05:08,  1.50s/it]Epoch 17:  32%|███▏      | 95/300 [02:41<05:04,  1.49s/it]Epoch 17:  32%|███▏      | 96/300 [02:43<05:04,  1.49s/it]Epoch 17:  32%|███▏      | 97/300 [02:44<05:00,  1.48s/it]Epoch 17:  33%|███▎      | 98/300 [02:46<05:02,  1.50s/it]Epoch 17:  33%|███▎      | 99/300 [02:47<04:58,  1.48s/it]06/19/2022 17:06:02 - INFO - __main__ - global step: 2600; train loss: 7.703561305999756; dev loss
Epoch 17:  33%|███▎      | 100/300 [02:49<04:52,  1.46s/it]Epoch 17:  34%|███▎      | 101/300 [02:51<05:13,  1.57s/it]Epoch 17:  34%|███▍      | 102/300 [02:52<05:18,  1.61s/it]Epoch 17:  34%|███▍      | 103/300 [02:54<05:15,  1.60s/it]Epoch 17:  35%|███▍      | 104/300 [02:55<05:04,  1.55s/it]Epoch 17:  35%|███▌      | 105/300 [02:57<05:16,  1.62s/it]Epoch 17:  35%|███▌      | 106/300 [02:59<05:03,  1.57s/it]Epoch 17:  36%|███▌      | 107/300 [03:00<05:00,  1.56s/it]Epoch 17:  36%|███▌      | 108/300 [03:02<05:15,  1.64s/it]Epoch 17:  36%|███▋      | 109/300 [03:03<05:03,  1.59s/it]Epoch 17:  37%|███▋      | 110/300 [03:05<05:17,  1.67s/it]Epoch 17:  37%|███▋      | 111/300 [03:07<05:07,  1.63s/it]Epoch 17:  37%|███▋      | 112/300 [03:09<05:18,  1.69s/it]Epoch 17:  38%|███▊      | 113/300 [03:10<05:02,  1.62s/it]Epoch 17:  38%|███▊      | 114/300 [03:12<04:50,  1.56s/it]Epoch 17:  38%|███▊      | 115/300 [03:14<05:11,  1.69s/it]Epoch 17:  39%|███▊      | 116/300 [03:15<05:19,  1.74s/it]Epoch 17:  39%|███▉      | 117/300 [03:17<05:02,  1.65s/it]Epoch 17:  39%|███▉      | 118/300 [03:18<04:48,  1.58s/it]Epoch 17:  40%|███▉      | 119/300 [03:20<04:44,  1.57s/it]06/19/2022 17:06:35 - INFO - __main__ - global step: 2610; train loss: 8.30565071105957; dev loss
Epoch 17:  40%|████      | 120/300 [03:22<04:58,  1.66s/it]Epoch 17:  40%|████      | 121/300 [03:23<04:45,  1.59s/it]Epoch 17:  41%|████      | 122/300 [03:25<04:59,  1.68s/it]Epoch 17:  41%|████      | 123/300 [03:27<04:57,  1.68s/it]Epoch 17:  41%|████▏     | 124/300 [03:28<04:42,  1.60s/it]Epoch 17:  42%|████▏     | 125/300 [03:30<04:31,  1.55s/it]Epoch 17:  42%|████▏     | 126/300 [03:31<04:39,  1.61s/it]Epoch 17:  42%|████▏     | 127/300 [03:33<05:00,  1.74s/it]Epoch 17:  43%|████▎     | 128/300 [03:35<04:52,  1.70s/it]Epoch 17:  43%|████▎     | 129/300 [03:37<04:59,  1.75s/it]Epoch 17:  43%|████▎     | 130/300 [03:39<05:01,  1.77s/it]Epoch 17:  44%|████▎     | 131/300 [03:40<05:02,  1.79s/it]Epoch 17:  44%|████▍     | 132/300 [03:42<04:48,  1.72s/it]Epoch 17:  44%|████▍     | 133/300 [03:43<04:33,  1.64s/it]Epoch 17:  45%|████▍     | 134/300 [03:45<04:40,  1.69s/it]Epoch 17:  45%|████▌     | 135/300 [03:47<04:45,  1.73s/it]Epoch 17:  45%|████▌     | 136/300 [03:49<04:53,  1.79s/it]Epoch 17:  46%|████▌     | 137/300 [03:50<04:34,  1.68s/it]Epoch 17:  46%|████▌     | 138/300 [03:52<04:20,  1.61s/it]Epoch 17:  46%|████▋     | 139/300 [03:54<04:29,  1.67s/it]06/19/2022 17:07:09 - INFO - __main__ - global step: 2620; train loss: 7.81752872467041; dev loss
Epoch 17:  47%|████▋     | 140/300 [03:56<04:40,  1.75s/it]Epoch 17:  47%|████▋     | 141/300 [03:57<04:23,  1.66s/it]Epoch 17:  47%|████▋     | 142/300 [03:58<04:10,  1.59s/it]Epoch 17:  48%|████▊     | 143/300 [04:00<04:00,  1.53s/it]Epoch 17:  48%|████▊     | 144/300 [04:02<04:17,  1.65s/it]Epoch 17:  48%|████▊     | 145/300 [04:04<04:26,  1.72s/it]Epoch 17:  49%|████▊     | 146/300 [04:06<04:33,  1.78s/it]Epoch 17:  49%|████▉     | 147/300 [04:08<04:38,  1.82s/it]Epoch 17:  49%|████▉     | 148/300 [04:09<04:23,  1.73s/it]Epoch 17:  50%|████▉     | 149/300 [04:11<04:25,  1.76s/it]Epoch 17:  50%|█████     | 150/300 [04:12<04:07,  1.65s/it]Epoch 17:  50%|█████     | 151/300 [04:14<03:54,  1.58s/it]Epoch 17:  51%|█████     | 152/300 [04:16<04:08,  1.68s/it]Epoch 17:  51%|█████     | 153/300 [04:17<04:14,  1.73s/it]Epoch 17:  51%|█████▏    | 154/300 [04:19<03:58,  1.63s/it]Epoch 17:  52%|█████▏    | 155/300 [04:21<04:04,  1.69s/it]Epoch 17:  52%|█████▏    | 156/300 [04:22<03:51,  1.61s/it]Epoch 17:  52%|█████▏    | 157/300 [04:24<04:03,  1.70s/it]Epoch 17:  53%|█████▎    | 158/300 [04:26<04:08,  1.75s/it]Epoch 17:  53%|█████▎    | 159/300 [04:28<04:10,  1.78s/it]06/19/2022 17:07:42 - INFO - __main__ - global step: 2630; train loss: 7.8324174880981445; dev loss
Epoch 17:  53%|█████▎    | 160/300 [04:30<04:11,  1.80s/it]Epoch 17:  54%|█████▎    | 161/300 [04:31<04:15,  1.84s/it]Epoch 17:  54%|█████▍    | 162/300 [04:33<03:55,  1.71s/it]Epoch 17:  54%|█████▍    | 163/300 [04:34<03:41,  1.62s/it]Epoch 17:  55%|█████▍    | 164/300 [04:36<03:30,  1.55s/it]Epoch 17:  55%|█████▌    | 165/300 [04:37<03:25,  1.52s/it]Epoch 17:  55%|█████▌    | 166/300 [04:39<03:36,  1.62s/it]Epoch 17:  56%|█████▌    | 167/300 [04:41<03:43,  1.68s/it]Epoch 17:  56%|█████▌    | 168/300 [04:42<03:31,  1.60s/it]Epoch 17:  56%|█████▋    | 169/300 [04:44<03:25,  1.57s/it]Epoch 17:  57%|█████▋    | 170/300 [04:45<03:17,  1.52s/it]Epoch 17:  57%|█████▋    | 171/300 [04:47<03:11,  1.48s/it]Epoch 17:  57%|█████▋    | 172/300 [04:48<03:23,  1.59s/it]Epoch 17:  58%|█████▊    | 173/300 [04:50<03:18,  1.56s/it]Epoch 17:  58%|█████▊    | 174/300 [04:52<03:26,  1.64s/it]Epoch 17:  58%|█████▊    | 175/300 [04:53<03:16,  1.57s/it]Epoch 17:  59%|█████▊    | 176/300 [04:55<03:24,  1.65s/it]Epoch 17:  59%|█████▉    | 177/300 [04:56<03:17,  1.60s/it]Epoch 17:  59%|█████▉    | 178/300 [04:58<03:08,  1.54s/it]Epoch 17:  60%|█████▉    | 179/300 [04:59<03:01,  1.50s/it]06/19/2022 17:08:14 - INFO - __main__ - global step: 2640; train loss: 7.332444190979004; dev loss
Epoch 17:  60%|██████    | 180/300 [05:01<03:11,  1.59s/it]Epoch 17:  60%|██████    | 181/300 [05:03<03:21,  1.69s/it]Epoch 17:  61%|██████    | 182/300 [05:05<03:24,  1.74s/it]Epoch 17:  61%|██████    | 183/300 [05:07<03:27,  1.77s/it]Epoch 17:  61%|██████▏   | 184/300 [05:08<03:14,  1.68s/it]Epoch 17:  62%|██████▏   | 185/300 [05:10<03:20,  1.75s/it]Epoch 17:  62%|██████▏   | 186/300 [05:12<03:29,  1.84s/it]Epoch 17:  62%|██████▏   | 187/300 [05:14<03:17,  1.75s/it]Epoch 17:  63%|██████▎   | 188/300 [05:15<03:07,  1.67s/it]Epoch 17:  63%|██████▎   | 189/300 [05:17<03:14,  1.75s/it]Epoch 17:  63%|██████▎   | 190/300 [05:19<03:07,  1.71s/it]Epoch 17:  64%|██████▎   | 191/300 [05:20<02:55,  1.61s/it]Epoch 17:  64%|██████▍   | 192/300 [05:21<02:47,  1.55s/it]Epoch 17:  64%|██████▍   | 193/300 [05:23<02:54,  1.63s/it]Epoch 17:  65%|██████▍   | 194/300 [05:25<02:48,  1.59s/it]Epoch 17:  65%|██████▌   | 195/300 [05:27<02:55,  1.67s/it]Epoch 17:  65%|██████▌   | 196/300 [05:28<02:45,  1.59s/it]Epoch 17:  66%|██████▌   | 197/300 [05:30<02:50,  1.65s/it]Epoch 17:  66%|██████▌   | 198/300 [05:31<02:44,  1.61s/it]Epoch 17:  66%|██████▋   | 199/300 [05:33<02:36,  1.55s/it]06/19/2022 17:08:47 - INFO - __main__ - global step: 2650; train loss: 8.256684303283691; dev loss
Epoch 17:  67%|██████▋   | 200/300 [05:34<02:30,  1.51s/it]Epoch 17:  67%|██████▋   | 201/300 [05:36<02:37,  1.60s/it]Epoch 17:  67%|██████▋   | 202/300 [05:37<02:34,  1.57s/it]Epoch 17:  68%|██████▊   | 203/300 [05:39<02:27,  1.52s/it]Epoch 17:  68%|██████▊   | 204/300 [05:40<02:22,  1.49s/it]Epoch 17:  68%|██████▊   | 205/300 [05:42<02:30,  1.59s/it]Epoch 17:  69%|██████▊   | 206/300 [05:44<02:38,  1.68s/it]Epoch 17:  69%|██████▉   | 207/300 [05:45<02:29,  1.60s/it]Epoch 17:  69%|██████▉   | 208/300 [05:47<02:22,  1.55s/it]Epoch 17:  70%|██████▉   | 209/300 [05:49<02:28,  1.63s/it]Epoch 17:  70%|███████   | 210/300 [05:50<02:20,  1.57s/it]Epoch 17:  70%|███████   | 211/300 [05:52<02:17,  1.54s/it]Epoch 17:  71%|███████   | 212/300 [05:53<02:11,  1.50s/it]Epoch 17:  71%|███████   | 213/300 [05:54<02:07,  1.47s/it]Epoch 17:  71%|███████▏  | 214/300 [05:56<02:04,  1.45s/it]Epoch 17:  72%|███████▏  | 215/300 [05:57<02:04,  1.47s/it]Epoch 17:  72%|███████▏  | 216/300 [05:59<02:11,  1.57s/it]Epoch 17:  72%|███████▏  | 217/300 [06:00<02:06,  1.53s/it]Epoch 17:  73%|███████▎  | 218/300 [06:02<02:03,  1.50s/it]Epoch 17:  73%|███████▎  | 219/300 [06:04<02:11,  1.62s/it]06/19/2022 17:09:19 - INFO - __main__ - global step: 2660; train loss: 8.074780464172363; dev loss
Epoch 17:  73%|███████▎  | 220/300 [06:06<02:14,  1.68s/it]Epoch 17:  74%|███████▎  | 221/300 [06:07<02:07,  1.61s/it]Epoch 17:  74%|███████▍  | 222/300 [06:09<02:01,  1.56s/it]Epoch 17:  74%|███████▍  | 223/300 [06:10<02:08,  1.66s/it]Epoch 17:  75%|███████▍  | 224/300 [06:12<02:10,  1.72s/it]Epoch 17:  75%|███████▌  | 225/300 [06:14<02:02,  1.64s/it]Epoch 17:  75%|███████▌  | 226/300 [06:15<01:56,  1.58s/it]Epoch 17:  76%|███████▌  | 227/300 [06:17<01:53,  1.56s/it]Epoch 17:  76%|███████▌  | 228/300 [06:18<01:49,  1.52s/it]Epoch 17:  76%|███████▋  | 229/300 [06:20<01:45,  1.49s/it]Epoch 17:  77%|███████▋  | 230/300 [06:21<01:42,  1.46s/it]Epoch 17:  77%|███████▋  | 231/300 [06:23<01:50,  1.60s/it]Epoch 17:  77%|███████▋  | 232/300 [06:24<01:45,  1.55s/it]Epoch 17:  78%|███████▊  | 233/300 [06:26<01:48,  1.63s/it]Epoch 17:  78%|███████▊  | 234/300 [06:28<01:51,  1.69s/it]Epoch 17:  78%|███████▊  | 235/300 [06:30<01:54,  1.76s/it]Epoch 17:  79%|███████▊  | 236/300 [06:32<01:54,  1.78s/it]Epoch 17:  79%|███████▉  | 237/300 [06:34<01:53,  1.80s/it]Epoch 17:  79%|███████▉  | 238/300 [06:35<01:45,  1.70s/it]Epoch 17:  80%|███████▉  | 239/300 [06:37<01:46,  1.74s/it]06/19/2022 17:09:51 - INFO - __main__ - global step: 2670; train loss: 7.7881035804748535; dev loss
Epoch 17:  80%|████████  | 240/300 [06:38<01:40,  1.68s/it]Epoch 17:  80%|████████  | 241/300 [06:40<01:41,  1.72s/it]Epoch 17:  81%|████████  | 242/300 [06:42<01:42,  1.76s/it]Epoch 17:  81%|████████  | 243/300 [06:44<01:41,  1.78s/it]Epoch 17:  81%|████████▏ | 244/300 [06:46<01:42,  1.82s/it]Epoch 17:  82%|████████▏ | 245/300 [06:48<01:40,  1.82s/it]Epoch 17:  82%|████████▏ | 246/300 [06:49<01:38,  1.82s/it]Epoch 17:  82%|████████▏ | 247/300 [06:51<01:30,  1.71s/it]Epoch 17:  83%|████████▎ | 248/300 [06:52<01:25,  1.65s/it]Epoch 17:  83%|████████▎ | 249/300 [06:54<01:27,  1.71s/it]Epoch 17:  83%|████████▎ | 250/300 [06:56<01:21,  1.63s/it]Epoch 17:  84%|████████▎ | 251/300 [06:57<01:22,  1.69s/it]Epoch 17:  84%|████████▍ | 252/300 [06:59<01:18,  1.65s/it]Epoch 17:  84%|████████▍ | 253/300 [07:00<01:14,  1.58s/it]Epoch 17:  85%|████████▍ | 254/300 [07:02<01:10,  1.54s/it]Epoch 17:  85%|████████▌ | 255/300 [07:04<01:13,  1.63s/it]Epoch 17:  85%|████████▌ | 256/300 [07:05<01:10,  1.60s/it]Epoch 17:  86%|████████▌ | 257/300 [07:07<01:06,  1.55s/it]Epoch 17:  86%|████████▌ | 258/300 [07:09<01:08,  1.64s/it]Epoch 17:  86%|████████▋ | 259/300 [07:10<01:04,  1.58s/it]06/19/2022 17:10:25 - INFO - __main__ - global step: 2680; train loss: 7.852622032165527; dev loss
Epoch 17:  87%|████████▋ | 260/300 [07:12<01:07,  1.68s/it]Epoch 17:  87%|████████▋ | 261/300 [07:13<01:02,  1.61s/it]Epoch 17:  87%|████████▋ | 262/300 [07:15<01:03,  1.67s/it]Epoch 17:  88%|████████▊ | 263/300 [07:17<01:03,  1.72s/it]Epoch 17:  88%|████████▊ | 264/300 [07:18<00:58,  1.63s/it]Epoch 17:  88%|████████▊ | 265/300 [07:20<00:55,  1.59s/it]Epoch 17:  89%|████████▊ | 266/300 [07:22<00:56,  1.66s/it]Epoch 17:  89%|████████▉ | 267/300 [07:24<00:56,  1.70s/it]Epoch 17:  89%|████████▉ | 268/300 [07:25<00:51,  1.62s/it]Epoch 17:  90%|████████▉ | 269/300 [07:26<00:49,  1.58s/it]Epoch 17:  90%|█████████ | 270/300 [07:28<00:49,  1.65s/it]Epoch 17:  90%|█████████ | 271/300 [07:30<00:45,  1.59s/it]Epoch 17:  91%|█████████ | 272/300 [07:31<00:42,  1.53s/it]Epoch 17:  91%|█████████ | 273/300 [07:33<00:41,  1.53s/it]Epoch 17:  91%|█████████▏| 274/300 [07:34<00:38,  1.50s/it]Epoch 17:  92%|█████████▏| 275/300 [07:35<00:36,  1.48s/it]Epoch 17:  92%|█████████▏| 276/300 [07:37<00:35,  1.47s/it]Epoch 17:  92%|█████████▏| 277/300 [07:38<00:34,  1.48s/it]Epoch 17:  93%|█████████▎| 278/300 [07:40<00:34,  1.58s/it]Epoch 17:  93%|█████████▎| 279/300 [07:42<00:34,  1.66s/it]06/19/2022 17:10:57 - INFO - __main__ - global step: 2690; train loss: 8.168174743652344; dev loss
Epoch 17:  93%|█████████▎| 280/300 [07:44<00:34,  1.72s/it]Epoch 17:  94%|█████████▎| 281/300 [07:46<00:33,  1.78s/it]Epoch 17:  94%|█████████▍| 282/300 [07:47<00:30,  1.68s/it]Epoch 17:  94%|█████████▍| 283/300 [07:49<00:29,  1.71s/it]Epoch 17:  95%|█████████▍| 284/300 [07:51<00:25,  1.62s/it]Epoch 17:  95%|█████████▌| 285/300 [07:52<00:24,  1.62s/it]Epoch 17:  95%|█████████▌| 286/300 [07:54<00:22,  1.60s/it]Epoch 17:  96%|█████████▌| 287/300 [07:55<00:20,  1.54s/it]Epoch 17:  96%|█████████▌| 288/300 [07:57<00:18,  1.55s/it]Epoch 17:  96%|█████████▋| 289/300 [07:58<00:17,  1.55s/it]Epoch 17:  97%|█████████▋| 290/300 [08:00<00:15,  1.55s/it]Epoch 17:  97%|█████████▋| 291/300 [08:01<00:13,  1.55s/it]Epoch 17:  97%|█████████▋| 292/300 [08:03<00:12,  1.57s/it]Epoch 17:  98%|█████████▊| 293/300 [08:04<00:10,  1.56s/it]Epoch 17:  98%|█████████▊| 294/300 [08:06<00:09,  1.54s/it]Epoch 17:  98%|█████████▊| 295/300 [08:08<00:07,  1.55s/it]Epoch 17:  99%|█████████▊| 296/300 [08:09<00:06,  1.51s/it]Epoch 17:  99%|█████████▉| 297/300 [08:11<00:04,  1.52s/it]Epoch 17:  99%|█████████▉| 298/300 [08:12<00:03,  1.56s/it]Epoch 17: 100%|█████████▉| 299/300 [08:14<00:01,  1.57s/it]06/19/2022 17:11:28 - INFO - __main__ - global step: 2700; train loss: 8.457395553588867; dev loss
Epoch 17: 100%|██████████| 300/300 [08:15<00:00,  1.57s/it]Epoch 17: 100%|██████████| 300/300 [08:15<00:00,  1.65s/it]
Epoch 18:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 18:   0%|          | 1/300 [00:01<07:50,  1.57s/it]Epoch 18:   1%|          | 2/300 [00:03<08:03,  1.62s/it]Epoch 18:   1%|          | 3/300 [00:04<07:31,  1.52s/it]Epoch 18:   1%|▏         | 4/300 [00:06<07:36,  1.54s/it]Epoch 18:   2%|▏         | 5/300 [00:07<07:19,  1.49s/it]Epoch 18:   2%|▏         | 6/300 [00:09<07:34,  1.54s/it]Epoch 18:   2%|▏         | 7/300 [00:10<07:17,  1.49s/it]Epoch 18:   3%|▎         | 8/300 [00:12<07:25,  1.53s/it]Epoch 18:   3%|▎         | 9/300 [00:13<07:13,  1.49s/it]Epoch 18:   3%|▎         | 10/300 [00:15<07:11,  1.49s/it]Epoch 18:   4%|▎         | 11/300 [00:16<07:01,  1.46s/it]Epoch 18:   4%|▍         | 12/300 [00:17<06:55,  1.44s/it]Epoch 18:   4%|▍         | 13/300 [00:19<06:50,  1.43s/it]Epoch 18:   5%|▍         | 14/300 [00:20<07:09,  1.50s/it]Epoch 18:   5%|▌         | 15/300 [00:22<07:00,  1.47s/it]Epoch 18:   5%|▌         | 16/300 [00:23<06:50,  1.45s/it]Epoch 18:   6%|▌         | 17/300 [00:25<07:00,  1.48s/it]Epoch 18:   6%|▌         | 18/300 [00:26<06:50,  1.46s/it]Epoch 18:   6%|▋         | 19/300 [00:28<07:06,  1.52s/it]06/19/2022 17:11:58 - INFO - __main__ - global step: 2710; train loss: 7.6382269859313965; dev loss
Epoch 18:   7%|▋         | 20/300 [00:29<06:54,  1.48s/it]Epoch 18:   7%|▋         | 21/300 [00:31<07:01,  1.51s/it]Epoch 18:   7%|▋         | 22/300 [00:32<06:50,  1.48s/it]Epoch 18:   8%|▊         | 23/300 [00:34<06:49,  1.48s/it]Epoch 18:   8%|▊         | 24/300 [00:35<06:41,  1.46s/it]Epoch 18:   8%|▊         | 25/300 [00:37<06:50,  1.49s/it]Epoch 18:   9%|▊         | 26/300 [00:38<06:42,  1.47s/it]Epoch 18:   9%|▉         | 27/300 [00:40<06:42,  1.47s/it]Epoch 18:   9%|▉         | 28/300 [00:41<06:51,  1.51s/it]Epoch 18:  10%|▉         | 29/300 [00:43<06:54,  1.53s/it]Epoch 18:  10%|█         | 30/300 [00:44<06:41,  1.49s/it]Epoch 18:  10%|█         | 31/300 [00:46<06:39,  1.49s/it]Epoch 18:  11%|█         | 32/300 [00:47<06:43,  1.50s/it]Epoch 18:  11%|█         | 33/300 [00:49<06:33,  1.47s/it]Epoch 18:  11%|█▏        | 34/300 [00:50<06:39,  1.50s/it]Epoch 18:  12%|█▏        | 35/300 [00:52<06:36,  1.50s/it]Epoch 18:  12%|█▏        | 36/300 [00:53<06:40,  1.52s/it]Epoch 18:  12%|█▏        | 37/300 [00:55<06:29,  1.48s/it]Epoch 18:  13%|█▎        | 38/300 [00:56<06:33,  1.50s/it]Epoch 18:  13%|█▎        | 39/300 [00:58<06:48,  1.57s/it]06/19/2022 17:12:28 - INFO - __main__ - global step: 2720; train loss: 8.124955177307129; dev loss
Epoch 18:  13%|█▎        | 40/300 [01:00<07:05,  1.64s/it]Epoch 18:  14%|█▎        | 41/300 [01:01<07:11,  1.67s/it]Epoch 18:  14%|█▍        | 42/300 [01:03<06:59,  1.63s/it]Epoch 18:  14%|█▍        | 43/300 [01:04<06:46,  1.58s/it]Epoch 18:  15%|█▍        | 44/300 [01:06<06:44,  1.58s/it]Epoch 18:  15%|█▌        | 45/300 [01:07<06:27,  1.52s/it]Epoch 18:  15%|█▌        | 46/300 [01:09<06:17,  1.48s/it]Epoch 18:  16%|█▌        | 47/300 [01:10<06:08,  1.46s/it]Epoch 18:  16%|█▌        | 48/300 [01:12<06:34,  1.56s/it]Epoch 18:  16%|█▋        | 49/300 [01:13<06:18,  1.51s/it]Epoch 18:  17%|█▋        | 50/300 [01:15<06:08,  1.47s/it]Epoch 18:  17%|█▋        | 51/300 [01:16<06:00,  1.45s/it]Epoch 18:  17%|█▋        | 52/300 [01:18<06:33,  1.59s/it]Epoch 18:  18%|█▊        | 53/300 [01:20<06:18,  1.53s/it]Epoch 18:  18%|█▊        | 54/300 [01:21<06:35,  1.61s/it]Epoch 18:  18%|█▊        | 55/300 [01:23<06:47,  1.66s/it]Epoch 18:  19%|█▊        | 56/300 [01:25<07:03,  1.73s/it]Epoch 18:  19%|█▉        | 57/300 [01:27<07:04,  1.75s/it]Epoch 18:  19%|█▉        | 58/300 [01:29<07:06,  1.76s/it]Epoch 18:  20%|█▉        | 59/300 [01:30<07:07,  1.78s/it]06/19/2022 17:13:01 - INFO - __main__ - global step: 2730; train loss: 7.906864166259766; dev loss
Epoch 18:  20%|██        | 60/300 [01:32<06:46,  1.69s/it]Epoch 18:  20%|██        | 61/300 [01:33<06:23,  1.60s/it]Epoch 18:  21%|██        | 62/300 [01:35<06:37,  1.67s/it]Epoch 18:  21%|██        | 63/300 [01:37<06:43,  1.70s/it]Epoch 18:  21%|██▏       | 64/300 [01:38<06:27,  1.64s/it]Epoch 18:  22%|██▏       | 65/300 [01:40<06:07,  1.56s/it]Epoch 18:  22%|██▏       | 66/300 [01:41<05:53,  1.51s/it]Epoch 18:  22%|██▏       | 67/300 [01:43<06:10,  1.59s/it]Epoch 18:  23%|██▎       | 68/300 [01:44<06:00,  1.55s/it]Epoch 18:  23%|██▎       | 69/300 [01:46<05:46,  1.50s/it]Epoch 18:  23%|██▎       | 70/300 [01:48<06:06,  1.59s/it]Epoch 18:  24%|██▎       | 71/300 [01:49<05:50,  1.53s/it]Epoch 18:  24%|██▍       | 72/300 [01:51<06:06,  1.61s/it]Epoch 18:  24%|██▍       | 73/300 [01:52<05:58,  1.58s/it]Epoch 18:  25%|██▍       | 74/300 [01:54<05:42,  1.52s/it]Epoch 18:  25%|██▌       | 75/300 [01:55<05:33,  1.48s/it]Epoch 18:  25%|██▌       | 76/300 [01:57<05:52,  1.57s/it]Epoch 18:  26%|██▌       | 77/300 [01:58<05:44,  1.54s/it]Epoch 18:  26%|██▌       | 78/300 [02:00<05:32,  1.50s/it]Epoch 18:  26%|██▋       | 79/300 [02:02<05:53,  1.60s/it]06/19/2022 17:13:32 - INFO - __main__ - global step: 2740; train loss: 7.845235347747803; dev loss
Epoch 18:  27%|██▋       | 80/300 [02:03<05:39,  1.54s/it]Epoch 18:  27%|██▋       | 81/300 [02:05<06:00,  1.64s/it]Epoch 18:  27%|██▋       | 82/300 [02:07<06:08,  1.69s/it]Epoch 18:  28%|██▊       | 83/300 [02:08<05:45,  1.59s/it]Epoch 18:  28%|██▊       | 84/300 [02:10<05:55,  1.65s/it]Epoch 18:  28%|██▊       | 85/300 [02:12<06:10,  1.72s/it]Epoch 18:  29%|██▊       | 86/300 [02:13<05:46,  1.62s/it]Epoch 18:  29%|██▉       | 87/300 [02:15<05:55,  1.67s/it]Epoch 18:  29%|██▉       | 88/300 [02:17<06:01,  1.71s/it]Epoch 18:  30%|██▉       | 89/300 [02:18<05:46,  1.64s/it]Epoch 18:  30%|███       | 90/300 [02:20<05:54,  1.69s/it]Epoch 18:  30%|███       | 91/300 [02:22<06:00,  1.72s/it]Epoch 18:  31%|███       | 92/300 [02:23<05:36,  1.62s/it]Epoch 18:  31%|███       | 93/300 [02:25<05:25,  1.57s/it]Epoch 18:  31%|███▏      | 94/300 [02:26<05:36,  1.64s/it]Epoch 18:  32%|███▏      | 95/300 [02:28<05:19,  1.56s/it]Epoch 18:  32%|███▏      | 96/300 [02:29<05:08,  1.51s/it]Epoch 18:  32%|███▏      | 97/300 [02:31<05:04,  1.50s/it]Epoch 18:  33%|███▎      | 98/300 [02:32<04:58,  1.48s/it]Epoch 18:  33%|███▎      | 99/300 [02:33<04:57,  1.48s/it]06/19/2022 17:14:04 - INFO - __main__ - global step: 2750; train loss: 7.576957702636719; dev loss
Epoch 18:  33%|███▎      | 100/300 [02:35<04:55,  1.48s/it]Epoch 18:  34%|███▎      | 101/300 [02:37<05:11,  1.57s/it]Epoch 18:  34%|███▍      | 102/300 [02:39<05:27,  1.66s/it]Epoch 18:  34%|███▍      | 103/300 [02:40<05:09,  1.57s/it]Epoch 18:  35%|███▍      | 104/300 [02:42<05:20,  1.63s/it]Epoch 18:  35%|███▌      | 105/300 [02:44<05:26,  1.67s/it]Epoch 18:  35%|███▌      | 106/300 [02:45<05:36,  1.74s/it]Epoch 18:  36%|███▌      | 107/300 [02:47<05:14,  1.63s/it]Epoch 18:  36%|███▌      | 108/300 [02:48<04:59,  1.56s/it]Epoch 18:  36%|███▋      | 109/300 [02:50<04:48,  1.51s/it]Epoch 18:  37%|███▋      | 110/300 [02:51<05:07,  1.62s/it]Epoch 18:  37%|███▋      | 111/300 [02:53<05:15,  1.67s/it]Epoch 18:  37%|███▋      | 112/300 [02:55<04:58,  1.59s/it]Epoch 18:  38%|███▊      | 113/300 [02:56<04:43,  1.52s/it]Epoch 18:  38%|███▊      | 114/300 [02:58<05:01,  1.62s/it]Epoch 18:  38%|███▊      | 115/300 [02:59<04:47,  1.55s/it]Epoch 18:  39%|███▊      | 116/300 [03:01<04:36,  1.50s/it]Epoch 18:  39%|███▉      | 117/300 [03:02<04:28,  1.47s/it]Epoch 18:  39%|███▉      | 118/300 [03:04<04:50,  1.59s/it]Epoch 18:  40%|███▉      | 119/300 [03:06<04:57,  1.64s/it]06/19/2022 17:14:36 - INFO - __main__ - global step: 2760; train loss: 7.616192817687988; dev loss
Epoch 18:  40%|████      | 120/300 [03:07<04:41,  1.57s/it]Epoch 18:  40%|████      | 121/300 [03:09<04:51,  1.63s/it]Epoch 18:  41%|████      | 122/300 [03:11<05:02,  1.70s/it]Epoch 18:  41%|████      | 123/300 [03:12<05:07,  1.74s/it]Epoch 18:  41%|████▏     | 124/300 [03:14<05:11,  1.77s/it]Epoch 18:  42%|████▏     | 125/300 [03:16<04:51,  1.67s/it]Epoch 18:  42%|████▏     | 126/300 [03:18<04:54,  1.69s/it]Epoch 18:  42%|████▏     | 127/300 [03:19<04:41,  1.63s/it]Epoch 18:  43%|████▎     | 128/300 [03:20<04:27,  1.55s/it]Epoch 18:  43%|████▎     | 129/300 [03:22<04:38,  1.63s/it]Epoch 18:  43%|████▎     | 130/300 [03:24<04:44,  1.67s/it]Epoch 18:  44%|████▎     | 131/300 [03:26<04:52,  1.73s/it]Epoch 18:  44%|████▍     | 132/300 [03:27<04:33,  1.63s/it]Epoch 18:  44%|████▍     | 133/300 [03:29<04:38,  1.67s/it]Epoch 18:  45%|████▍     | 134/300 [03:30<04:22,  1.58s/it]Epoch 18:  45%|████▌     | 135/300 [03:32<04:36,  1.68s/it]Epoch 18:  45%|████▌     | 136/300 [03:34<04:20,  1.59s/it]Epoch 18:  46%|████▌     | 137/300 [03:35<04:28,  1.65s/it]Epoch 18:  46%|████▌     | 138/300 [03:37<04:33,  1.69s/it]Epoch 18:  46%|████▋     | 139/300 [03:39<04:20,  1.62s/it]06/19/2022 17:15:09 - INFO - __main__ - global step: 2770; train loss: 8.135889053344727; dev loss
Epoch 18:  47%|████▋     | 140/300 [03:40<04:26,  1.66s/it]Epoch 18:  47%|████▋     | 141/300 [03:42<04:30,  1.70s/it]Epoch 18:  47%|████▋     | 142/300 [03:44<04:34,  1.74s/it]Epoch 18:  48%|████▊     | 143/300 [03:46<04:39,  1.78s/it]Epoch 18:  48%|████▊     | 144/300 [03:48<04:37,  1.78s/it]Epoch 18:  48%|████▊     | 145/300 [03:49<04:17,  1.66s/it]Epoch 18:  49%|████▊     | 146/300 [03:51<04:21,  1.70s/it]Epoch 18:  49%|████▉     | 147/300 [03:53<04:29,  1.76s/it]Epoch 18:  49%|████▉     | 148/300 [03:54<04:10,  1.65s/it]Epoch 18:  50%|████▉     | 149/300 [03:56<03:57,  1.57s/it]Epoch 18:  50%|█████     | 150/300 [03:57<04:04,  1.63s/it]Epoch 18:  50%|█████     | 151/300 [03:59<04:14,  1.71s/it]Epoch 18:  51%|█████     | 152/300 [04:01<04:16,  1.73s/it]Epoch 18:  51%|█████     | 153/300 [04:02<04:01,  1.64s/it]Epoch 18:  51%|█████▏    | 154/300 [04:04<03:47,  1.56s/it]Epoch 18:  52%|█████▏    | 155/300 [04:06<03:56,  1.63s/it]Epoch 18:  52%|█████▏    | 156/300 [04:07<03:48,  1.59s/it]Epoch 18:  52%|█████▏    | 157/300 [04:09<03:54,  1.64s/it]Epoch 18:  53%|█████▎    | 158/300 [04:11<03:58,  1.68s/it]Epoch 18:  53%|█████▎    | 159/300 [04:12<04:01,  1.72s/it]06/19/2022 17:15:43 - INFO - __main__ - global step: 2780; train loss: 7.610774040222168; dev loss
Epoch 18:  53%|█████▎    | 160/300 [04:14<04:06,  1.76s/it]Epoch 18:  54%|█████▎    | 161/300 [04:16<04:04,  1.76s/it]Epoch 18:  54%|█████▍    | 162/300 [04:18<04:03,  1.76s/it]Epoch 18:  54%|█████▍    | 163/300 [04:20<04:02,  1.77s/it]Epoch 18:  55%|█████▍    | 164/300 [04:21<03:48,  1.68s/it]Epoch 18:  55%|█████▌    | 165/300 [04:22<03:35,  1.60s/it]Epoch 18:  55%|█████▌    | 166/300 [04:24<03:40,  1.65s/it]Epoch 18:  56%|█████▌    | 167/300 [04:26<03:45,  1.70s/it]Epoch 18:  56%|█████▌    | 168/300 [04:28<03:35,  1.64s/it]Epoch 18:  56%|█████▋    | 169/300 [04:29<03:23,  1.56s/it]Epoch 18:  57%|█████▋    | 170/300 [04:31<03:30,  1.62s/it]Epoch 18:  57%|█████▋    | 171/300 [04:32<03:20,  1.55s/it]Epoch 18:  57%|█████▋    | 172/300 [04:34<03:15,  1.53s/it]Epoch 18:  58%|█████▊    | 173/300 [04:35<03:07,  1.48s/it]Epoch 18:  58%|█████▊    | 174/300 [04:37<03:17,  1.56s/it]Epoch 18:  58%|█████▊    | 175/300 [04:38<03:23,  1.63s/it]Epoch 18:  59%|█████▊    | 176/300 [04:40<03:16,  1.59s/it]Epoch 18:  59%|█████▉    | 177/300 [04:42<03:21,  1.64s/it]Epoch 18:  59%|█████▉    | 178/300 [04:43<03:25,  1.68s/it]Epoch 18:  60%|█████▉    | 179/300 [04:45<03:12,  1.59s/it]06/19/2022 17:16:15 - INFO - __main__ - global step: 2790; train loss: 8.29409122467041; dev loss
Epoch 18:  60%|██████    | 180/300 [04:47<03:17,  1.65s/it]Epoch 18:  60%|██████    | 181/300 [04:49<03:24,  1.72s/it]Epoch 18:  61%|██████    | 182/300 [04:50<03:10,  1.62s/it]Epoch 18:  61%|██████    | 183/300 [04:51<03:01,  1.55s/it]Epoch 18:  61%|██████▏   | 184/300 [04:53<03:08,  1.62s/it]Epoch 18:  62%|██████▏   | 185/300 [04:55<03:15,  1.70s/it]Epoch 18:  62%|██████▏   | 186/300 [04:56<03:02,  1.60s/it]Epoch 18:  62%|██████▏   | 187/300 [04:58<02:53,  1.53s/it]Epoch 18:  63%|██████▎   | 188/300 [05:00<03:00,  1.61s/it]Epoch 18:  63%|██████▎   | 189/300 [05:01<02:55,  1.58s/it]Epoch 18:  63%|██████▎   | 190/300 [05:03<03:01,  1.65s/it]Epoch 18:  64%|██████▎   | 191/300 [05:05<03:04,  1.69s/it]Epoch 18:  64%|██████▍   | 192/300 [05:06<02:52,  1.60s/it]Epoch 18:  64%|██████▍   | 193/300 [05:08<02:59,  1.68s/it]Epoch 18:  65%|██████▍   | 194/300 [05:09<02:48,  1.59s/it]Epoch 18:  65%|██████▌   | 195/300 [05:11<02:40,  1.53s/it]Epoch 18:  65%|██████▌   | 196/300 [05:12<02:34,  1.49s/it]Epoch 18:  66%|██████▌   | 197/300 [05:14<02:45,  1.61s/it]Epoch 18:  66%|██████▌   | 198/300 [05:15<02:37,  1.54s/it]Epoch 18:  66%|██████▋   | 199/300 [05:17<02:30,  1.49s/it]06/19/2022 17:16:47 - INFO - __main__ - global step: 2800; train loss: 8.258444786071777; dev loss
Epoch 18:  67%|██████▋   | 200/300 [05:18<02:25,  1.46s/it]Epoch 18:  67%|██████▋   | 201/300 [05:20<02:36,  1.58s/it]Epoch 18:  67%|██████▋   | 202/300 [05:22<02:40,  1.64s/it]Epoch 18:  68%|██████▊   | 203/300 [05:23<02:43,  1.69s/it]Epoch 18:  68%|██████▊   | 204/300 [05:25<02:33,  1.60s/it]Epoch 18:  68%|██████▊   | 205/300 [05:26<02:28,  1.56s/it]Epoch 18:  69%|██████▊   | 206/300 [05:28<02:21,  1.50s/it]Epoch 18:  69%|██████▉   | 207/300 [05:29<02:26,  1.58s/it]Epoch 18:  69%|██████▉   | 208/300 [05:31<02:31,  1.64s/it]Epoch 18:  70%|██████▉   | 209/300 [05:33<02:23,  1.57s/it]Epoch 18:  70%|███████   | 210/300 [05:34<02:19,  1.55s/it]Epoch 18:  70%|███████   | 211/300 [05:36<02:13,  1.50s/it]Epoch 18:  71%|███████   | 212/300 [05:37<02:08,  1.46s/it]Epoch 18:  71%|███████   | 213/300 [05:38<02:04,  1.44s/it]Epoch 18:  71%|███████▏  | 214/300 [05:40<02:04,  1.45s/it]Epoch 18:  72%|███████▏  | 215/300 [05:42<02:11,  1.55s/it]Epoch 18:  72%|███████▏  | 216/300 [05:43<02:06,  1.51s/it]Epoch 18:  72%|███████▏  | 217/300 [05:44<02:02,  1.47s/it]Epoch 18:  73%|███████▎  | 218/300 [05:46<02:00,  1.47s/it]Epoch 18:  73%|███████▎  | 219/300 [05:47<01:57,  1.44s/it]06/19/2022 17:17:18 - INFO - __main__ - global step: 2810; train loss: 8.186285018920898; dev loss
Epoch 18:  73%|███████▎  | 220/300 [05:49<02:04,  1.55s/it]Epoch 18:  74%|███████▎  | 221/300 [05:51<02:08,  1.62s/it]Epoch 18:  74%|███████▍  | 222/300 [05:52<02:03,  1.58s/it]Epoch 18:  74%|███████▍  | 223/300 [05:54<01:57,  1.52s/it]Epoch 18:  75%|███████▍  | 224/300 [05:55<01:52,  1.48s/it]Epoch 18:  75%|███████▌  | 225/300 [05:56<01:48,  1.44s/it]Epoch 18:  75%|███████▌  | 226/300 [05:58<01:48,  1.46s/it]Epoch 18:  76%|███████▌  | 227/300 [05:59<01:44,  1.43s/it]Epoch 18:  76%|███████▌  | 228/300 [06:01<01:50,  1.54s/it]Epoch 18:  76%|███████▋  | 229/300 [06:02<01:46,  1.50s/it]Epoch 18:  77%|███████▋  | 230/300 [06:04<01:44,  1.50s/it]Epoch 18:  77%|███████▋  | 231/300 [06:06<01:50,  1.60s/it]Epoch 18:  77%|███████▋  | 232/300 [06:08<01:52,  1.66s/it]Epoch 18:  78%|███████▊  | 233/300 [06:09<01:53,  1.70s/it]Epoch 18:  78%|███████▊  | 234/300 [06:11<01:54,  1.73s/it]Epoch 18:  78%|███████▊  | 235/300 [06:13<01:55,  1.78s/it]Epoch 18:  79%|███████▊  | 236/300 [06:15<01:54,  1.79s/it]Epoch 18:  79%|███████▉  | 237/300 [06:17<01:52,  1.79s/it]Epoch 18:  79%|███████▉  | 238/300 [06:19<01:51,  1.79s/it]Epoch 18:  80%|███████▉  | 239/300 [06:20<01:43,  1.70s/it]06/19/2022 17:17:50 - INFO - __main__ - global step: 2820; train loss: 8.33380126953125; dev loss
Epoch 18:  80%|████████  | 240/300 [06:21<01:36,  1.62s/it]Epoch 18:  80%|████████  | 241/300 [06:23<01:38,  1.67s/it]Epoch 18:  81%|████████  | 242/300 [06:25<01:32,  1.59s/it]Epoch 18:  81%|████████  | 243/300 [06:26<01:28,  1.56s/it]Epoch 18:  81%|████████▏ | 244/300 [06:28<01:31,  1.63s/it]Epoch 18:  82%|████████▏ | 245/300 [06:30<01:32,  1.68s/it]Epoch 18:  82%|████████▏ | 246/300 [06:31<01:26,  1.60s/it]Epoch 18:  82%|████████▏ | 247/300 [06:33<01:23,  1.57s/it]Epoch 18:  83%|████████▎ | 248/300 [06:34<01:19,  1.52s/it]Epoch 18:  83%|████████▎ | 249/300 [06:35<01:15,  1.49s/it]Epoch 18:  83%|████████▎ | 250/300 [06:37<01:12,  1.46s/it]Epoch 18:  84%|████████▎ | 251/300 [06:38<01:11,  1.46s/it]Epoch 18:  84%|████████▍ | 252/300 [06:40<01:09,  1.44s/it]Epoch 18:  84%|████████▍ | 253/300 [06:41<01:06,  1.42s/it]Epoch 18:  85%|████████▍ | 254/300 [06:42<01:04,  1.41s/it]Epoch 18:  85%|████████▌ | 255/300 [06:44<01:04,  1.43s/it]Epoch 18:  85%|████████▌ | 256/300 [06:46<01:07,  1.54s/it]Epoch 18:  86%|████████▌ | 257/300 [06:48<01:09,  1.62s/it]Epoch 18:  86%|████████▌ | 258/300 [06:49<01:05,  1.56s/it]Epoch 18:  86%|████████▋ | 259/300 [06:51<01:07,  1.66s/it]06/19/2022 17:18:21 - INFO - __main__ - global step: 2830; train loss: 7.80765438079834; dev loss
Epoch 18:  87%|████████▋ | 260/300 [06:52<01:03,  1.58s/it]Epoch 18:  87%|████████▋ | 261/300 [06:54<00:59,  1.53s/it]Epoch 18:  87%|████████▋ | 262/300 [06:55<01:01,  1.62s/it]Epoch 18:  88%|████████▊ | 263/300 [06:57<01:02,  1.68s/it]Epoch 18:  88%|████████▊ | 264/300 [06:59<01:03,  1.76s/it]Epoch 18:  88%|████████▊ | 265/300 [07:01<01:02,  1.78s/it]Epoch 18:  89%|████████▊ | 266/300 [07:03<00:57,  1.68s/it]Epoch 18:  89%|████████▉ | 267/300 [07:04<00:52,  1.60s/it]Epoch 18:  89%|████████▉ | 268/300 [07:05<00:50,  1.58s/it]Epoch 18:  90%|████████▉ | 269/300 [07:07<00:51,  1.65s/it]Epoch 18:  90%|█████████ | 270/300 [07:09<00:51,  1.71s/it]Epoch 18:  90%|█████████ | 271/300 [07:11<00:50,  1.75s/it]Epoch 18:  91%|█████████ | 272/300 [07:12<00:47,  1.68s/it]Epoch 18:  91%|█████████ | 273/300 [07:14<00:46,  1.73s/it]Epoch 18:  91%|█████████▏| 274/300 [07:16<00:45,  1.76s/it]Epoch 18:  92%|█████████▏| 275/300 [07:18<00:41,  1.65s/it]Epoch 18:  92%|█████████▏| 276/300 [07:19<00:38,  1.61s/it]Epoch 18:  92%|█████████▏| 277/300 [07:21<00:38,  1.68s/it]Epoch 18:  93%|█████████▎| 278/300 [07:22<00:35,  1.60s/it]Epoch 18:  93%|█████████▎| 279/300 [07:24<00:35,  1.68s/it]06/19/2022 17:18:54 - INFO - __main__ - global step: 2840; train loss: 8.152606010437012; dev loss
Epoch 18:  93%|█████████▎| 280/300 [07:26<00:32,  1.63s/it]Epoch 18:  94%|█████████▎| 281/300 [07:28<00:32,  1.69s/it]Epoch 18:  94%|█████████▍| 282/300 [07:29<00:31,  1.73s/it]Epoch 18:  94%|█████████▍| 283/300 [07:31<00:30,  1.77s/it]Epoch 18:  95%|█████████▍| 284/300 [07:33<00:29,  1.82s/it]Epoch 18:  95%|█████████▌| 285/300 [07:35<00:25,  1.70s/it]Epoch 18:  95%|█████████▌| 286/300 [07:36<00:24,  1.74s/it]Epoch 18:  96%|█████████▌| 287/300 [07:38<00:21,  1.64s/it]Epoch 18:  96%|█████████▌| 288/300 [07:40<00:20,  1.70s/it]Epoch 18:  96%|█████████▋| 289/300 [07:42<00:19,  1.77s/it]Epoch 18:  97%|█████████▋| 290/300 [07:43<00:16,  1.67s/it]Epoch 18:  97%|█████████▋| 291/300 [07:44<00:14,  1.60s/it]Epoch 18:  97%|█████████▋| 292/300 [07:46<00:13,  1.66s/it]Epoch 18:  98%|█████████▊| 293/300 [07:48<00:12,  1.74s/it]Epoch 18:  98%|█████████▊| 294/300 [07:50<00:10,  1.77s/it]Epoch 18:  98%|█████████▊| 295/300 [07:52<00:08,  1.79s/it]Epoch 18:  99%|█████████▊| 296/300 [07:53<00:06,  1.68s/it]Epoch 18:  99%|█████████▉| 297/300 [07:55<00:05,  1.76s/it]Epoch 18:  99%|█████████▉| 298/300 [07:57<00:03,  1.66s/it]Epoch 18: 100%|█████████▉| 299/300 [07:59<00:01,  1.72s/it]06/19/2022 17:19:29 - INFO - __main__ - global step: 2850; train loss: 7.536356449127197; dev loss
Epoch 18: 100%|██████████| 300/300 [08:00<00:00,  1.75s/it]Epoch 18: 100%|██████████| 300/300 [08:00<00:00,  1.60s/it]
Epoch 19:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 19:   0%|          | 1/300 [00:01<07:39,  1.54s/it]Epoch 19:   1%|          | 2/300 [00:02<07:19,  1.47s/it]Epoch 19:   1%|          | 3/300 [00:04<07:09,  1.45s/it]Epoch 19:   1%|▏         | 4/300 [00:06<07:53,  1.60s/it]Epoch 19:   2%|▏         | 5/300 [00:07<07:40,  1.56s/it]Epoch 19:   2%|▏         | 6/300 [00:09<07:22,  1.51s/it]Epoch 19:   2%|▏         | 7/300 [00:10<07:50,  1.61s/it]Epoch 19:   3%|▎         | 8/300 [00:12<07:31,  1.55s/it]Epoch 19:   3%|▎         | 9/300 [00:13<07:26,  1.54s/it]Epoch 19:   3%|▎         | 10/300 [00:15<07:50,  1.62s/it]Epoch 19:   4%|▎         | 11/300 [00:17<07:31,  1.56s/it]Epoch 19:   4%|▍         | 12/300 [00:18<07:18,  1.52s/it]Epoch 19:   4%|▍         | 13/300 [00:20<07:16,  1.52s/it]Epoch 19:   5%|▍         | 14/300 [00:21<07:41,  1.61s/it]Epoch 19:   5%|▌         | 15/300 [00:23<07:58,  1.68s/it]Epoch 19:   5%|▌         | 16/300 [00:25<08:09,  1.72s/it]Epoch 19:   6%|▌         | 17/300 [00:27<08:17,  1.76s/it]Epoch 19:   6%|▌         | 18/300 [00:28<07:56,  1.69s/it]Epoch 19:   6%|▋         | 19/300 [00:30<07:29,  1.60s/it]06/19/2022 17:20:01 - INFO - __main__ - global step: 2860; train loss: 7.924173831939697; dev loss
Epoch 19:   7%|▋         | 20/300 [00:31<07:10,  1.54s/it]Epoch 19:   7%|▋         | 21/300 [00:33<07:34,  1.63s/it]Epoch 19:   7%|▋         | 22/300 [00:35<07:56,  1.72s/it]Epoch 19:   8%|▊         | 23/300 [00:37<08:04,  1.75s/it]Epoch 19:   8%|▊         | 24/300 [00:39<08:09,  1.77s/it]Epoch 19:   8%|▊         | 25/300 [00:40<07:37,  1.66s/it]Epoch 19:   9%|▊         | 26/300 [00:42<07:24,  1.62s/it]Epoch 19:   9%|▉         | 27/300 [00:43<07:05,  1.56s/it]Epoch 19:   9%|▉         | 28/300 [00:45<07:26,  1.64s/it]Epoch 19:  10%|▉         | 29/300 [00:46<07:07,  1.58s/it]Epoch 19:  10%|█         | 30/300 [00:48<07:01,  1.56s/it]Epoch 19:  10%|█         | 31/300 [00:50<07:19,  1.63s/it]Epoch 19:  11%|█         | 32/300 [00:51<07:35,  1.70s/it]Epoch 19:  11%|█         | 33/300 [00:53<07:44,  1.74s/it]Epoch 19:  11%|█▏        | 34/300 [00:55<07:57,  1.79s/it]Epoch 19:  12%|█▏        | 35/300 [00:57<07:25,  1.68s/it]Epoch 19:  12%|█▏        | 36/300 [00:58<07:35,  1.72s/it]Epoch 19:  12%|█▏        | 37/300 [01:00<07:40,  1.75s/it]Epoch 19:  13%|█▎        | 38/300 [01:02<07:23,  1.69s/it]Epoch 19:  13%|█▎        | 39/300 [01:04<07:32,  1.73s/it]06/19/2022 17:20:35 - INFO - __main__ - global step: 2870; train loss: 7.719675540924072; dev loss
Epoch 19:  13%|█▎        | 40/300 [01:05<07:07,  1.64s/it]Epoch 19:  14%|█▎        | 41/300 [01:06<06:49,  1.58s/it]Epoch 19:  14%|█▍        | 42/300 [01:08<07:12,  1.68s/it]Epoch 19:  14%|█▍        | 43/300 [01:10<07:04,  1.65s/it]Epoch 19:  15%|█▍        | 44/300 [01:11<06:45,  1.59s/it]Epoch 19:  15%|█▌        | 45/300 [01:13<06:30,  1.53s/it]Epoch 19:  15%|█▌        | 46/300 [01:14<06:21,  1.50s/it]Epoch 19:  16%|█▌        | 47/300 [01:16<06:18,  1.50s/it]Epoch 19:  16%|█▌        | 48/300 [01:17<06:11,  1.47s/it]Epoch 19:  16%|█▋        | 49/300 [01:19<06:34,  1.57s/it]Epoch 19:  17%|█▋        | 50/300 [01:21<06:49,  1.64s/it]Epoch 19:  17%|█▋        | 51/300 [01:22<06:39,  1.61s/it]Epoch 19:  17%|█▋        | 52/300 [01:24<06:23,  1.54s/it]Epoch 19:  18%|█▊        | 53/300 [01:25<06:10,  1.50s/it]Epoch 19:  18%|█▊        | 54/300 [01:27<06:32,  1.59s/it]Epoch 19:  18%|█▊        | 55/300 [01:29<06:55,  1.69s/it]Epoch 19:  19%|█▊        | 56/300 [01:31<07:05,  1.74s/it]Epoch 19:  19%|█▉        | 57/300 [01:32<06:39,  1.64s/it]Epoch 19:  19%|█▉        | 58/300 [01:33<06:20,  1.57s/it]Epoch 19:  20%|█▉        | 59/300 [01:35<06:12,  1.55s/it]06/19/2022 17:21:06 - INFO - __main__ - global step: 2880; train loss: 8.353445053100586; dev loss
Epoch 19:  20%|██        | 60/300 [01:37<06:30,  1.63s/it]Epoch 19:  20%|██        | 61/300 [01:39<06:43,  1.69s/it]Epoch 19:  21%|██        | 62/300 [01:40<06:21,  1.60s/it]Epoch 19:  21%|██        | 63/300 [01:41<06:11,  1.57s/it]Epoch 19:  21%|██▏       | 64/300 [01:43<06:26,  1.64s/it]Epoch 19:  22%|██▏       | 65/300 [01:45<06:35,  1.68s/it]Epoch 19:  22%|██▏       | 66/300 [01:46<06:15,  1.61s/it]Epoch 19:  22%|██▏       | 67/300 [01:48<06:08,  1.58s/it]Epoch 19:  23%|██▎       | 68/300 [01:50<06:21,  1.65s/it]Epoch 19:  23%|██▎       | 69/300 [01:51<06:03,  1.57s/it]Epoch 19:  23%|██▎       | 70/300 [01:53<05:51,  1.53s/it]Epoch 19:  24%|██▎       | 71/300 [01:54<06:09,  1.61s/it]Epoch 19:  24%|██▍       | 72/300 [01:56<06:28,  1.70s/it]Epoch 19:  24%|██▍       | 73/300 [01:58<06:34,  1.74s/it]Epoch 19:  25%|██▍       | 74/300 [02:00<06:10,  1.64s/it]Epoch 19:  25%|██▌       | 75/300 [02:01<05:53,  1.57s/it]Epoch 19:  25%|██▌       | 76/300 [02:02<05:47,  1.55s/it]Epoch 19:  26%|██▌       | 77/300 [02:04<05:36,  1.51s/it]Epoch 19:  26%|██▌       | 78/300 [02:05<05:28,  1.48s/it]Epoch 19:  26%|██▋       | 79/300 [02:07<05:51,  1.59s/it]06/19/2022 17:21:38 - INFO - __main__ - global step: 2890; train loss: 7.970599174499512; dev loss
Epoch 19:  27%|██▋       | 80/300 [02:09<05:44,  1.57s/it]Epoch 19:  27%|██▋       | 81/300 [02:10<05:30,  1.51s/it]Epoch 19:  27%|██▋       | 82/300 [02:11<05:23,  1.48s/it]Epoch 19:  28%|██▊       | 83/300 [02:13<05:43,  1.59s/it]Epoch 19:  28%|██▊       | 84/300 [02:15<05:38,  1.57s/it]Epoch 19:  28%|██▊       | 85/300 [02:16<05:26,  1.52s/it]Epoch 19:  29%|██▊       | 86/300 [02:18<05:44,  1.61s/it]Epoch 19:  29%|██▉       | 87/300 [02:20<05:56,  1.67s/it]Epoch 19:  29%|██▉       | 88/300 [02:22<06:11,  1.75s/it]Epoch 19:  30%|██▉       | 89/300 [02:24<06:15,  1.78s/it]Epoch 19:  30%|███       | 90/300 [02:25<06:16,  1.79s/it]Epoch 19:  30%|███       | 91/300 [02:27<05:49,  1.67s/it]Epoch 19:  31%|███       | 92/300 [02:28<05:37,  1.62s/it]Epoch 19:  31%|███       | 93/300 [02:30<05:49,  1.69s/it]Epoch 19:  31%|███▏      | 94/300 [02:32<05:31,  1.61s/it]Epoch 19:  32%|███▏      | 95/300 [02:33<05:16,  1.55s/it]Epoch 19:  32%|███▏      | 96/300 [02:35<05:32,  1.63s/it]Epoch 19:  32%|███▏      | 97/300 [02:37<05:48,  1.72s/it]Epoch 19:  33%|███▎      | 98/300 [02:39<05:53,  1.75s/it]Epoch 19:  33%|███▎      | 99/300 [02:40<05:30,  1.64s/it]06/19/2022 17:22:11 - INFO - __main__ - global step: 2900; train loss: 7.878134250640869; dev loss
Epoch 19:  33%|███▎      | 100/300 [02:42<05:39,  1.70s/it]Epoch 19:  34%|███▎      | 101/300 [02:43<05:28,  1.65s/it]Epoch 19:  34%|███▍      | 102/300 [02:45<05:36,  1.70s/it]Epoch 19:  34%|███▍      | 103/300 [02:47<05:18,  1.62s/it]Epoch 19:  35%|███▍      | 104/300 [02:48<05:06,  1.56s/it]Epoch 19:  35%|███▌      | 105/300 [02:50<05:26,  1.67s/it]Epoch 19:  35%|███▌      | 106/300 [02:52<05:34,  1.72s/it]Epoch 19:  36%|███▌      | 107/300 [02:54<05:38,  1.75s/it]Epoch 19:  36%|███▌      | 108/300 [02:55<05:17,  1.66s/it]Epoch 19:  36%|███▋      | 109/300 [02:57<05:32,  1.74s/it]Epoch 19:  37%|███▋      | 110/300 [02:58<05:11,  1.64s/it]Epoch 19:  37%|███▋      | 111/300 [03:00<05:21,  1.70s/it]Epoch 19:  37%|███▋      | 112/300 [03:02<05:03,  1.61s/it]Epoch 19:  38%|███▊      | 113/300 [03:03<04:56,  1.58s/it]Epoch 19:  38%|███▊      | 114/300 [03:05<04:45,  1.54s/it]Epoch 19:  38%|███▊      | 115/300 [03:06<04:59,  1.62s/it]Epoch 19:  39%|███▊      | 116/300 [03:08<05:09,  1.68s/it]Epoch 19:  39%|███▉      | 117/300 [03:10<05:21,  1.76s/it]Epoch 19:  39%|███▉      | 118/300 [03:12<05:25,  1.79s/it]Epoch 19:  40%|███▉      | 119/300 [03:13<05:03,  1.68s/it]06/19/2022 17:22:45 - INFO - __main__ - global step: 2910; train loss: 7.894767761230469; dev loss
Epoch 19:  40%|████      | 120/300 [03:15<05:11,  1.73s/it]Epoch 19:  40%|████      | 121/300 [03:17<04:58,  1.67s/it]Epoch 19:  41%|████      | 122/300 [03:18<04:43,  1.59s/it]Epoch 19:  41%|████      | 123/300 [03:20<04:54,  1.67s/it]Epoch 19:  41%|████▏     | 124/300 [03:22<05:02,  1.72s/it]Epoch 19:  42%|████▏     | 125/300 [03:23<04:44,  1.62s/it]Epoch 19:  42%|████▏     | 126/300 [03:25<04:59,  1.72s/it]Epoch 19:  42%|████▏     | 127/300 [03:27<04:42,  1.63s/it]Epoch 19:  43%|████▎     | 128/300 [03:29<04:50,  1.69s/it]Epoch 19:  43%|████▎     | 129/300 [03:30<04:35,  1.61s/it]Epoch 19:  43%|████▎     | 130/300 [03:32<04:51,  1.71s/it]Epoch 19:  44%|████▎     | 131/300 [03:34<04:56,  1.75s/it]Epoch 19:  44%|████▍     | 132/300 [03:35<04:38,  1.66s/it]Epoch 19:  44%|████▍     | 133/300 [03:37<04:46,  1.72s/it]Epoch 19:  45%|████▍     | 134/300 [03:39<04:34,  1.66s/it]Epoch 19:  45%|████▌     | 135/300 [03:40<04:42,  1.71s/it]Epoch 19:  45%|████▌     | 136/300 [03:42<04:27,  1.63s/it]Epoch 19:  46%|████▌     | 137/300 [03:44<04:35,  1.69s/it]Epoch 19:  46%|████▌     | 138/300 [03:45<04:26,  1.64s/it]Epoch 19:  46%|████▋     | 139/300 [03:47<04:13,  1.57s/it]06/19/2022 17:23:18 - INFO - __main__ - global step: 2920; train loss: 7.478678226470947; dev loss
Epoch 19:  47%|████▋     | 140/300 [03:48<04:03,  1.52s/it]Epoch 19:  47%|████▋     | 141/300 [03:49<03:56,  1.49s/it]Epoch 19:  47%|████▋     | 142/300 [03:51<04:15,  1.62s/it]Epoch 19:  48%|████▊     | 143/300 [03:53<04:26,  1.70s/it]Epoch 19:  48%|████▊     | 144/300 [03:55<04:12,  1.62s/it]Epoch 19:  48%|████▊     | 145/300 [03:56<04:18,  1.67s/it]Epoch 19:  49%|████▊     | 146/300 [03:58<04:11,  1.63s/it]Epoch 19:  49%|████▉     | 147/300 [03:59<03:59,  1.57s/it]Epoch 19:  49%|████▉     | 148/300 [04:01<04:09,  1.64s/it]Epoch 19:  50%|████▉     | 149/300 [04:03<04:17,  1.70s/it]Epoch 19:  50%|█████     | 150/300 [04:05<04:04,  1.63s/it]Epoch 19:  50%|█████     | 151/300 [04:06<03:58,  1.60s/it]Epoch 19:  51%|█████     | 152/300 [04:08<04:07,  1.68s/it]Epoch 19:  51%|█████     | 153/300 [04:09<03:53,  1.59s/it]Epoch 19:  51%|█████▏    | 154/300 [04:11<03:44,  1.54s/it]Epoch 19:  52%|█████▏    | 155/300 [04:12<03:40,  1.52s/it]Epoch 19:  52%|█████▏    | 156/300 [04:14<03:34,  1.49s/it]Epoch 19:  52%|█████▏    | 157/300 [04:15<03:30,  1.47s/it]Epoch 19:  53%|█████▎    | 158/300 [04:16<03:26,  1.45s/it]Epoch 19:  53%|█████▎    | 159/300 [04:18<03:44,  1.59s/it]06/19/2022 17:23:49 - INFO - __main__ - global step: 2930; train loss: 7.903016567230225; dev loss
Epoch 19:  53%|█████▎    | 160/300 [04:20<03:36,  1.54s/it]Epoch 19:  54%|█████▎    | 161/300 [04:22<03:46,  1.63s/it]Epoch 19:  54%|█████▍    | 162/300 [04:23<03:36,  1.57s/it]Epoch 19:  54%|█████▍    | 163/300 [04:25<03:49,  1.67s/it]Epoch 19:  55%|█████▍    | 164/300 [04:26<03:38,  1.60s/it]Epoch 19:  55%|█████▌    | 165/300 [04:28<03:46,  1.68s/it]Epoch 19:  55%|█████▌    | 166/300 [04:30<03:34,  1.60s/it]Epoch 19:  56%|█████▌    | 167/300 [04:32<03:46,  1.70s/it]Epoch 19:  56%|█████▌    | 168/300 [04:33<03:50,  1.74s/it]Epoch 19:  56%|█████▋    | 169/300 [04:35<03:51,  1.77s/it]Epoch 19:  57%|█████▋    | 170/300 [04:37<03:52,  1.79s/it]Epoch 19:  57%|█████▋    | 171/300 [04:39<03:55,  1.83s/it]Epoch 19:  57%|█████▋    | 172/300 [04:40<03:38,  1.71s/it]Epoch 19:  58%|█████▊    | 173/300 [04:42<03:41,  1.74s/it]Epoch 19:  58%|█████▊    | 174/300 [04:44<03:27,  1.65s/it]Epoch 19:  58%|█████▊    | 175/300 [04:46<03:36,  1.73s/it]Epoch 19:  59%|█████▊    | 176/300 [04:47<03:38,  1.76s/it]Epoch 19:  59%|█████▉    | 177/300 [04:49<03:24,  1.66s/it]Epoch 19:  59%|█████▉    | 178/300 [04:50<03:13,  1.59s/it]Epoch 19:  60%|█████▉    | 179/300 [04:52<03:21,  1.66s/it]06/19/2022 17:24:24 - INFO - __main__ - global step: 2940; train loss: 7.908848762512207; dev loss
Epoch 19:  60%|██████    | 180/300 [04:54<03:29,  1.75s/it]Epoch 19:  60%|██████    | 181/300 [04:56<03:31,  1.78s/it]Epoch 19:  61%|██████    | 182/300 [04:58<03:31,  1.80s/it]Epoch 19:  61%|██████    | 183/300 [05:00<03:32,  1.82s/it]Epoch 19:  61%|██████▏   | 184/300 [05:02<03:33,  1.84s/it]Epoch 19:  62%|██████▏   | 185/300 [05:03<03:17,  1.72s/it]Epoch 19:  62%|██████▏   | 186/300 [05:04<03:06,  1.63s/it]Epoch 19:  62%|██████▏   | 187/300 [05:06<02:57,  1.57s/it]Epoch 19:  63%|██████▎   | 188/300 [05:08<03:08,  1.68s/it]Epoch 19:  63%|██████▎   | 189/300 [05:10<03:11,  1.72s/it]Epoch 19:  63%|██████▎   | 190/300 [05:11<03:13,  1.76s/it]Epoch 19:  64%|██████▎   | 191/300 [05:13<03:00,  1.66s/it]Epoch 19:  64%|██████▍   | 192/300 [05:14<02:55,  1.62s/it]Epoch 19:  64%|██████▍   | 193/300 [05:16<03:00,  1.68s/it]Epoch 19:  65%|██████▍   | 194/300 [05:18<02:50,  1.61s/it]Epoch 19:  65%|██████▌   | 195/300 [05:19<02:43,  1.55s/it]Epoch 19:  65%|██████▌   | 196/300 [05:21<02:52,  1.66s/it]Epoch 19:  66%|██████▌   | 197/300 [05:23<02:56,  1.72s/it]Epoch 19:  66%|██████▌   | 198/300 [05:24<02:46,  1.63s/it]Epoch 19:  66%|██████▋   | 199/300 [05:26<02:49,  1.68s/it]06/19/2022 17:24:57 - INFO - __main__ - global step: 2950; train loss: 7.64450216293335; dev loss
Epoch 19:  67%|██████▋   | 200/300 [05:28<02:42,  1.63s/it]Epoch 19:  67%|██████▋   | 201/300 [05:29<02:34,  1.56s/it]Epoch 19:  67%|██████▋   | 202/300 [05:30<02:28,  1.52s/it]Epoch 19:  68%|██████▊   | 203/300 [05:32<02:37,  1.62s/it]Epoch 19:  68%|██████▊   | 204/300 [05:34<02:42,  1.69s/it]Epoch 19:  68%|██████▊   | 205/300 [05:36<02:36,  1.64s/it]Epoch 19:  69%|██████▊   | 206/300 [05:37<02:39,  1.70s/it]Epoch 19:  69%|██████▉   | 207/300 [05:39<02:41,  1.74s/it]Epoch 19:  69%|██████▉   | 208/300 [05:41<02:42,  1.77s/it]Epoch 19:  70%|██████▉   | 209/300 [05:43<02:34,  1.70s/it]Epoch 19:  70%|███████   | 210/300 [05:44<02:24,  1.61s/it]Epoch 19:  70%|███████   | 211/300 [05:46<02:28,  1.67s/it]Epoch 19:  71%|███████   | 212/300 [05:48<02:31,  1.72s/it]Epoch 19:  71%|███████   | 213/300 [05:49<02:24,  1.66s/it]Epoch 19:  71%|███████▏  | 214/300 [05:51<02:26,  1.70s/it]Epoch 19:  72%|███████▏  | 215/300 [05:53<02:17,  1.62s/it]Epoch 19:  72%|███████▏  | 216/300 [05:54<02:21,  1.68s/it]Epoch 19:  72%|███████▏  | 217/300 [05:56<02:26,  1.77s/it]Epoch 19:  73%|███████▎  | 218/300 [05:58<02:16,  1.67s/it]Epoch 19:  73%|███████▎  | 219/300 [05:59<02:09,  1.60s/it]06/19/2022 17:25:31 - INFO - __main__ - global step: 2960; train loss: 7.773819923400879; dev loss
Epoch 19:  73%|███████▎  | 220/300 [06:01<02:13,  1.66s/it]Epoch 19:  74%|███████▎  | 221/300 [06:02<02:07,  1.62s/it]Epoch 19:  74%|███████▍  | 222/300 [06:04<02:01,  1.56s/it]Epoch 19:  74%|███████▍  | 223/300 [06:05<01:57,  1.53s/it]Epoch 19:  75%|███████▍  | 224/300 [06:07<01:53,  1.49s/it]Epoch 19:  75%|███████▌  | 225/300 [06:08<01:52,  1.50s/it]Epoch 19:  75%|███████▌  | 226/300 [06:10<01:48,  1.47s/it]Epoch 19:  76%|███████▌  | 227/300 [06:11<01:45,  1.45s/it]Epoch 19:  76%|███████▌  | 228/300 [06:13<01:52,  1.57s/it]Epoch 19:  76%|███████▋  | 229/300 [06:15<01:58,  1.67s/it]Epoch 19:  77%|███████▋  | 230/300 [06:16<01:52,  1.60s/it]Epoch 19:  77%|███████▋  | 231/300 [06:18<01:55,  1.67s/it]Epoch 19:  77%|███████▋  | 232/300 [06:20<01:57,  1.72s/it]Epoch 19:  78%|███████▊  | 233/300 [06:22<01:57,  1.75s/it]Epoch 19:  78%|███████▊  | 234/300 [06:23<01:51,  1.69s/it]Epoch 19:  78%|███████▊  | 235/300 [06:25<01:44,  1.61s/it]Epoch 19:  79%|███████▊  | 236/300 [06:26<01:39,  1.55s/it]Epoch 19:  79%|███████▉  | 237/300 [06:28<01:35,  1.51s/it]Epoch 19:  79%|███████▉  | 238/300 [06:29<01:33,  1.50s/it]Epoch 19:  80%|███████▉  | 239/300 [06:30<01:29,  1.47s/it]06/19/2022 17:26:02 - INFO - __main__ - global step: 2970; train loss: 8.004066467285156; dev loss
Epoch 19:  80%|████████  | 240/300 [06:32<01:35,  1.59s/it]Epoch 19:  80%|████████  | 241/300 [06:34<01:38,  1.67s/it]Epoch 19:  81%|████████  | 242/300 [06:36<01:34,  1.63s/it]Epoch 19:  81%|████████  | 243/300 [06:38<01:36,  1.70s/it]Epoch 19:  81%|████████▏ | 244/300 [06:39<01:30,  1.61s/it]Epoch 19:  82%|████████▏ | 245/300 [06:41<01:32,  1.67s/it]Epoch 19:  82%|████████▏ | 246/300 [06:43<01:34,  1.76s/it]Epoch 19:  82%|████████▏ | 247/300 [06:44<01:27,  1.66s/it]Epoch 19:  83%|████████▎ | 248/300 [06:46<01:28,  1.71s/it]Epoch 19:  83%|████████▎ | 249/300 [06:48<01:29,  1.75s/it]Epoch 19:  83%|████████▎ | 250/300 [06:49<01:24,  1.68s/it]Epoch 19:  84%|████████▎ | 251/300 [06:51<01:24,  1.73s/it]Epoch 19:  84%|████████▍ | 252/300 [06:53<01:18,  1.64s/it]Epoch 19:  84%|████████▍ | 253/300 [06:54<01:19,  1.70s/it]Epoch 19:  85%|████████▍ | 254/300 [06:56<01:15,  1.65s/it]Epoch 19:  85%|████████▌ | 255/300 [06:58<01:16,  1.71s/it]Epoch 19:  85%|████████▌ | 256/300 [07:00<01:16,  1.74s/it]Epoch 19:  86%|████████▌ | 257/300 [07:02<01:16,  1.78s/it]Epoch 19:  86%|████████▌ | 258/300 [07:03<01:10,  1.67s/it]Epoch 19:  86%|████████▋ | 259/300 [07:04<01:06,  1.62s/it]06/19/2022 17:26:35 - INFO - __main__ - global step: 2980; train loss: 7.693065643310547; dev loss
Epoch 19:  87%|████████▋ | 260/300 [07:06<01:02,  1.57s/it]Epoch 19:  87%|████████▋ | 261/300 [07:08<01:04,  1.65s/it]Epoch 19:  87%|████████▋ | 262/300 [07:10<01:04,  1.71s/it]Epoch 19:  88%|████████▊ | 263/300 [07:12<01:05,  1.78s/it]Epoch 19:  88%|████████▊ | 264/300 [07:13<01:04,  1.80s/it]Epoch 19:  88%|████████▊ | 265/300 [07:15<01:03,  1.81s/it]Epoch 19:  89%|████████▊ | 266/300 [07:17<00:57,  1.69s/it]Epoch 19:  89%|████████▉ | 267/300 [07:18<00:54,  1.65s/it]Epoch 19:  89%|████████▉ | 268/300 [07:20<00:54,  1.70s/it]Epoch 19:  90%|████████▉ | 269/300 [07:22<00:54,  1.74s/it]Epoch 19:  90%|█████████ | 270/300 [07:23<00:49,  1.65s/it]Epoch 19:  90%|█████████ | 271/300 [07:25<00:46,  1.60s/it]Epoch 19:  91%|█████████ | 272/300 [07:26<00:43,  1.55s/it]Epoch 19:  91%|█████████ | 273/300 [07:28<00:40,  1.51s/it]Epoch 19:  91%|█████████▏| 274/300 [07:29<00:41,  1.60s/it]Epoch 19:  92%|█████████▏| 275/300 [07:31<00:39,  1.58s/it]Epoch 19:  92%|█████████▏| 276/300 [07:33<00:39,  1.66s/it]Epoch 19:  92%|█████████▏| 277/300 [07:35<00:39,  1.73s/it]Epoch 19:  93%|█████████▎| 278/300 [07:36<00:36,  1.64s/it]Epoch 19:  93%|█████████▎| 279/300 [07:38<00:36,  1.73s/it]06/19/2022 17:27:09 - INFO - __main__ - global step: 2990; train loss: 7.901476860046387; dev loss
Epoch 19:  93%|█████████▎| 280/300 [07:40<00:35,  1.77s/it]Epoch 19:  94%|█████████▎| 281/300 [07:41<00:31,  1.66s/it]Epoch 19:  94%|█████████▍| 282/300 [07:43<00:28,  1.60s/it]Epoch 19:  94%|█████████▍| 283/300 [07:45<00:28,  1.70s/it]Epoch 19:  95%|█████████▍| 284/300 [07:46<00:25,  1.62s/it]Epoch 19:  95%|█████████▌| 285/300 [07:48<00:25,  1.69s/it]Epoch 19:  95%|█████████▌| 286/300 [07:50<00:24,  1.74s/it]Epoch 19:  96%|█████████▌| 287/300 [07:52<00:23,  1.78s/it]Epoch 19:  96%|█████████▌| 288/300 [07:53<00:20,  1.70s/it]Epoch 19:  96%|█████████▋| 289/300 [07:55<00:17,  1.62s/it]Epoch 19:  97%|█████████▋| 290/300 [07:57<00:16,  1.70s/it]Epoch 19:  97%|█████████▋| 291/300 [07:58<00:14,  1.60s/it]Epoch 19:  97%|█████████▋| 292/300 [08:00<00:13,  1.71s/it]Epoch 19:  98%|█████████▊| 293/300 [08:01<00:11,  1.62s/it]Epoch 19:  98%|█████████▊| 294/300 [08:03<00:10,  1.69s/it]Epoch 19:  98%|█████████▊| 295/300 [08:05<00:08,  1.61s/it]Epoch 19:  99%|█████████▊| 296/300 [08:07<00:06,  1.71s/it]Epoch 19:  99%|█████████▉| 297/300 [08:08<00:05,  1.76s/it]Epoch 19:  99%|█████████▉| 298/300 [08:10<00:03,  1.66s/it]Epoch 19: 100%|█████████▉| 299/300 [08:11<00:01,  1.59s/it]06/19/2022 17:27:43 - INFO - __main__ - global step: 3000; train loss: 8.141034126281738; dev loss
Epoch 19: 100%|██████████| 300/300 [08:13<00:00,  1.70s/it]Epoch 19: 100%|██████████| 300/300 [08:13<00:00,  1.65s/it]
Epoch 20:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 20:   0%|          | 1/300 [00:01<07:12,  1.45s/it]Epoch 20:   1%|          | 2/300 [00:02<07:07,  1.43s/it]Epoch 20:   1%|          | 3/300 [00:04<06:59,  1.41s/it]Epoch 20:   1%|▏         | 4/300 [00:06<08:01,  1.63s/it]Epoch 20:   2%|▏         | 5/300 [00:08<08:23,  1.71s/it]Epoch 20:   2%|▏         | 6/300 [00:09<08:31,  1.74s/it]Epoch 20:   2%|▏         | 7/300 [00:11<07:59,  1.64s/it]Epoch 20:   3%|▎         | 8/300 [00:12<07:46,  1.60s/it]Epoch 20:   3%|▎         | 9/300 [00:14<08:05,  1.67s/it]Epoch 20:   3%|▎         | 10/300 [00:16<08:17,  1.71s/it]Epoch 20:   4%|▎         | 11/300 [00:17<07:50,  1.63s/it]Epoch 20:   4%|▍         | 12/300 [00:19<08:05,  1.69s/it]Epoch 20:   4%|▍         | 13/300 [00:21<08:05,  1.69s/it]Epoch 20:   5%|▍         | 14/300 [00:22<07:41,  1.61s/it]Epoch 20:   5%|▌         | 15/300 [00:24<07:20,  1.55s/it]Epoch 20:   5%|▌         | 16/300 [00:25<07:24,  1.56s/it]Epoch 20:   6%|▌         | 17/300 [00:27<07:31,  1.60s/it]Epoch 20:   6%|▌         | 18/300 [00:28<07:12,  1.53s/it]Epoch 20:   6%|▋         | 19/300 [00:30<07:17,  1.56s/it]06/19/2022 17:28:15 - INFO - __main__ - global step: 3010; train loss: 7.657221794128418; dev loss
Epoch 20:   7%|▋         | 20/300 [00:32<07:18,  1.57s/it]Epoch 20:   7%|▋         | 21/300 [00:33<07:25,  1.60s/it]Epoch 20:   7%|▋         | 22/300 [00:35<07:22,  1.59s/it]Epoch 20:   8%|▊         | 23/300 [00:36<07:19,  1.59s/it]Epoch 20:   8%|▊         | 24/300 [00:38<07:03,  1.53s/it]Epoch 20:   8%|▊         | 25/300 [00:40<07:28,  1.63s/it]Epoch 20:   9%|▊         | 26/300 [00:41<07:08,  1.56s/it]Epoch 20:   9%|▉         | 27/300 [00:42<06:51,  1.51s/it]Epoch 20:   9%|▉         | 28/300 [00:44<06:41,  1.48s/it]Epoch 20:  10%|▉         | 29/300 [00:45<06:41,  1.48s/it]Epoch 20:  10%|█         | 30/300 [00:47<06:33,  1.46s/it]Epoch 20:  10%|█         | 31/300 [00:48<06:27,  1.44s/it]Epoch 20:  11%|█         | 32/300 [00:50<06:24,  1.43s/it]Epoch 20:  11%|█         | 33/300 [00:52<07:02,  1.58s/it]Epoch 20:  11%|█▏        | 34/300 [00:53<07:20,  1.65s/it]Epoch 20:  12%|█▏        | 35/300 [00:55<06:59,  1.58s/it]Epoch 20:  12%|█▏        | 36/300 [00:56<06:45,  1.54s/it]Epoch 20:  12%|█▏        | 37/300 [00:58<06:42,  1.53s/it]Epoch 20:  13%|█▎        | 38/300 [00:59<06:30,  1.49s/it]Epoch 20:  13%|█▎        | 39/300 [01:01<06:54,  1.59s/it]06/19/2022 17:28:46 - INFO - __main__ - global step: 3020; train loss: 8.366799354553223; dev loss
Epoch 20:  13%|█▎        | 40/300 [01:03<07:13,  1.67s/it]Epoch 20:  14%|█▎        | 41/300 [01:04<06:51,  1.59s/it]Epoch 20:  14%|█▍        | 42/300 [01:06<06:43,  1.56s/it]Epoch 20:  14%|█▍        | 43/300 [01:07<07:00,  1.64s/it]Epoch 20:  15%|█▍        | 44/300 [01:09<06:40,  1.56s/it]Epoch 20:  15%|█▌        | 45/300 [01:10<06:25,  1.51s/it]Epoch 20:  15%|█▌        | 46/300 [01:12<06:21,  1.50s/it]Epoch 20:  16%|█▌        | 47/300 [01:13<06:11,  1.47s/it]Epoch 20:  16%|█▌        | 48/300 [01:15<06:04,  1.45s/it]Epoch 20:  16%|█▋        | 49/300 [01:16<05:58,  1.43s/it]Epoch 20:  17%|█▋        | 50/300 [01:18<06:31,  1.57s/it]Epoch 20:  17%|█▋        | 51/300 [01:19<06:17,  1.52s/it]Epoch 20:  17%|█▋        | 52/300 [01:21<06:37,  1.60s/it]Epoch 20:  18%|█▊        | 53/300 [01:22<06:19,  1.54s/it]Epoch 20:  18%|█▊        | 54/300 [01:24<06:13,  1.52s/it]Epoch 20:  18%|█▊        | 55/300 [01:25<06:00,  1.47s/it]Epoch 20:  19%|█▊        | 56/300 [01:27<06:25,  1.58s/it]Epoch 20:  19%|█▉        | 57/300 [01:29<06:39,  1.65s/it]Epoch 20:  19%|█▉        | 58/300 [01:30<06:27,  1.60s/it]Epoch 20:  20%|█▉        | 59/300 [01:32<06:38,  1.65s/it]06/19/2022 17:29:17 - INFO - __main__ - global step: 3030; train loss: 8.06469440460205; dev loss
Epoch 20:  20%|██        | 60/300 [01:34<06:18,  1.58s/it]Epoch 20:  20%|██        | 61/300 [01:35<06:04,  1.53s/it]Epoch 20:  21%|██        | 62/300 [01:37<06:30,  1.64s/it]Epoch 20:  21%|██        | 63/300 [01:38<06:11,  1.57s/it]Epoch 20:  21%|██▏       | 64/300 [01:40<06:26,  1.64s/it]Epoch 20:  22%|██▏       | 65/300 [01:42<06:36,  1.69s/it]Epoch 20:  22%|██▏       | 66/300 [01:44<06:45,  1.73s/it]Epoch 20:  22%|██▏       | 67/300 [01:46<06:55,  1.78s/it]Epoch 20:  23%|██▎       | 68/300 [01:47<06:26,  1.67s/it]Epoch 20:  23%|██▎       | 69/300 [01:49<06:34,  1.71s/it]Epoch 20:  23%|██▎       | 70/300 [01:51<06:40,  1.74s/it]Epoch 20:  24%|██▎       | 71/300 [01:52<06:22,  1.67s/it]Epoch 20:  24%|██▍       | 72/300 [01:54<06:02,  1.59s/it]Epoch 20:  24%|██▍       | 73/300 [01:55<06:17,  1.66s/it]Epoch 20:  25%|██▍       | 74/300 [01:57<05:58,  1.59s/it]Epoch 20:  25%|██▌       | 75/300 [01:58<05:51,  1.56s/it]Epoch 20:  25%|██▌       | 76/300 [02:00<06:06,  1.64s/it]Epoch 20:  26%|██▌       | 77/300 [02:02<05:52,  1.58s/it]Epoch 20:  26%|██▌       | 78/300 [02:03<06:08,  1.66s/it]Epoch 20:  26%|██▋       | 79/300 [02:05<06:30,  1.77s/it]06/19/2022 17:29:50 - INFO - __main__ - global step: 3040; train loss: 7.747819423675537; dev loss
Epoch 20:  27%|██▋       | 80/300 [02:07<06:10,  1.68s/it]Epoch 20:  27%|██▋       | 81/300 [02:08<05:58,  1.64s/it]Epoch 20:  27%|██▋       | 82/300 [02:10<06:14,  1.72s/it]Epoch 20:  28%|██▊       | 83/300 [02:12<06:33,  1.81s/it]Epoch 20:  28%|██▊       | 84/300 [02:14<06:10,  1.71s/it]Epoch 20:  28%|██▊       | 85/300 [02:16<06:19,  1.76s/it]Epoch 20:  29%|██▊       | 86/300 [02:17<06:00,  1.69s/it]Epoch 20:  29%|██▉       | 87/300 [02:19<05:53,  1.66s/it]Epoch 20:  29%|██▉       | 88/300 [02:20<05:41,  1.61s/it]Epoch 20:  30%|██▉       | 89/300 [02:22<05:31,  1.57s/it]Epoch 20:  30%|███       | 90/300 [02:24<05:51,  1.67s/it]Epoch 20:  30%|███       | 91/300 [02:25<05:47,  1.66s/it]Epoch 20:  31%|███       | 92/300 [02:27<05:37,  1.62s/it]Epoch 20:  31%|███       | 93/300 [02:28<05:26,  1.58s/it]Epoch 20:  31%|███▏      | 94/300 [02:30<05:20,  1.55s/it]Epoch 20:  32%|███▏      | 95/300 [02:31<05:15,  1.54s/it]Epoch 20:  32%|███▏      | 96/300 [02:33<05:18,  1.56s/it]Epoch 20:  32%|███▏      | 97/300 [02:34<05:14,  1.55s/it]Epoch 20:  33%|███▎      | 98/300 [02:36<05:33,  1.65s/it]Epoch 20:  33%|███▎      | 99/300 [02:38<05:47,  1.73s/it]06/19/2022 17:30:23 - INFO - __main__ - global step: 3050; train loss: 7.866490364074707; dev loss
Epoch 20:  33%|███▎      | 100/300 [02:40<05:37,  1.69s/it]Epoch 20:  34%|███▎      | 101/300 [02:41<05:23,  1.63s/it]Epoch 20:  34%|███▍      | 102/300 [02:43<05:37,  1.70s/it]Epoch 20:  34%|███▍      | 103/300 [02:45<05:47,  1.77s/it]Epoch 20:  35%|███▍      | 104/300 [02:47<05:36,  1.72s/it]Epoch 20:  35%|███▌      | 105/300 [02:49<05:45,  1.77s/it]Epoch 20:  35%|███▌      | 106/300 [02:50<05:45,  1.78s/it]Epoch 20:  36%|███▌      | 107/300 [02:52<05:43,  1.78s/it]Epoch 20:  36%|███▌      | 108/300 [02:54<05:26,  1.70s/it]Epoch 20:  36%|███▋      | 109/300 [02:55<05:05,  1.60s/it]Epoch 20:  37%|███▋      | 110/300 [02:57<05:13,  1.65s/it]Epoch 20:  37%|███▋      | 111/300 [02:59<05:19,  1.69s/it]Epoch 20:  37%|███▋      | 112/300 [03:00<05:06,  1.63s/it]Epoch 20:  38%|███▊      | 113/300 [03:02<05:14,  1.68s/it]Epoch 20:  38%|███▊      | 114/300 [03:03<04:57,  1.60s/it]Epoch 20:  38%|███▊      | 115/300 [03:05<04:42,  1.53s/it]Epoch 20:  39%|███▊      | 116/300 [03:06<04:38,  1.51s/it]Epoch 20:  39%|███▉      | 117/300 [03:08<04:30,  1.48s/it]Epoch 20:  39%|███▉      | 118/300 [03:09<04:45,  1.57s/it]Epoch 20:  40%|███▉      | 119/300 [03:11<04:58,  1.65s/it]06/19/2022 17:30:56 - INFO - __main__ - global step: 3060; train loss: 8.182683944702148; dev loss
Epoch 20:  40%|████      | 120/300 [03:13<05:07,  1.71s/it]Epoch 20:  40%|████      | 121/300 [03:15<05:16,  1.77s/it]Epoch 20:  41%|████      | 122/300 [03:17<05:17,  1.78s/it]Epoch 20:  41%|████      | 123/300 [03:18<04:57,  1.68s/it]Epoch 20:  41%|████▏     | 124/300 [03:20<04:41,  1.60s/it]Epoch 20:  42%|████▏     | 125/300 [03:21<04:34,  1.57s/it]Epoch 20:  42%|████▏     | 126/300 [03:23<04:24,  1.52s/it]Epoch 20:  42%|████▏     | 127/300 [03:24<04:18,  1.49s/it]Epoch 20:  43%|████▎     | 128/300 [03:26<04:32,  1.58s/it]Epoch 20:  43%|████▎     | 129/300 [03:28<04:48,  1.69s/it]Epoch 20:  43%|████▎     | 130/300 [03:29<04:33,  1.61s/it]Epoch 20:  44%|████▎     | 131/300 [03:31<04:21,  1.55s/it]Epoch 20:  44%|████▍     | 132/300 [03:32<04:33,  1.63s/it]Epoch 20:  44%|████▍     | 133/300 [03:34<04:46,  1.72s/it]Epoch 20:  45%|████▍     | 134/300 [03:36<04:48,  1.74s/it]Epoch 20:  45%|████▌     | 135/300 [03:38<04:48,  1.75s/it]Epoch 20:  45%|████▌     | 136/300 [03:39<04:30,  1.65s/it]Epoch 20:  46%|████▌     | 137/300 [03:41<04:38,  1.71s/it]Epoch 20:  46%|████▌     | 138/300 [03:43<04:43,  1.75s/it]Epoch 20:  46%|████▋     | 139/300 [03:45<04:34,  1.70s/it]06/19/2022 17:31:29 - INFO - __main__ - global step: 3070; train loss: 7.897233486175537; dev loss
Epoch 20:  47%|████▋     | 140/300 [03:46<04:18,  1.61s/it]Epoch 20:  47%|████▋     | 141/300 [03:48<04:19,  1.63s/it]Epoch 20:  47%|████▋     | 142/300 [03:49<04:15,  1.62s/it]Epoch 20:  48%|████▊     | 143/300 [03:51<04:11,  1.60s/it]Epoch 20:  48%|████▊     | 144/300 [03:52<04:01,  1.55s/it]Epoch 20:  48%|████▊     | 145/300 [03:54<03:59,  1.55s/it]Epoch 20:  49%|████▊     | 146/300 [03:55<03:51,  1.50s/it]Epoch 20:  49%|████▉     | 147/300 [03:57<03:53,  1.53s/it]Epoch 20:  49%|████▉     | 148/300 [03:58<03:55,  1.55s/it]Epoch 20:  50%|████▉     | 149/300 [04:00<03:54,  1.55s/it]Epoch 20:  50%|█████     | 150/300 [04:01<03:50,  1.54s/it]Epoch 20:  50%|█████     | 151/300 [04:03<03:43,  1.50s/it]Epoch 20:  51%|█████     | 152/300 [04:04<03:45,  1.52s/it]Epoch 20:  51%|█████     | 153/300 [04:06<03:46,  1.54s/it]Epoch 20:  51%|█████▏    | 154/300 [04:08<03:50,  1.58s/it]Epoch 20:  52%|█████▏    | 155/300 [04:09<03:41,  1.53s/it]Epoch 20:  52%|█████▏    | 156/300 [04:10<03:35,  1.49s/it]Epoch 20:  52%|█████▏    | 157/300 [04:12<03:37,  1.52s/it]Epoch 20:  53%|█████▎    | 158/300 [04:14<03:35,  1.52s/it]Epoch 20:  53%|█████▎    | 159/300 [04:15<03:36,  1.53s/it]06/19/2022 17:32:00 - INFO - __main__ - global step: 3080; train loss: 8.040109634399414; dev loss
Epoch 20:  53%|█████▎    | 160/300 [04:17<03:29,  1.50s/it]Epoch 20:  54%|█████▎    | 161/300 [04:18<03:32,  1.53s/it]Epoch 20:  54%|█████▍    | 162/300 [04:20<03:30,  1.53s/it]Epoch 20:  54%|█████▍    | 163/300 [04:21<03:31,  1.54s/it]Epoch 20:  55%|█████▍    | 164/300 [04:23<03:32,  1.56s/it]Epoch 20:  55%|█████▌    | 165/300 [04:24<03:32,  1.57s/it]Epoch 20:  55%|█████▌    | 166/300 [04:26<03:33,  1.59s/it]Epoch 20:  56%|█████▌    | 167/300 [04:28<03:31,  1.59s/it]Epoch 20:  56%|█████▌    | 168/300 [04:29<03:29,  1.59s/it]Epoch 20:  56%|█████▋    | 169/300 [04:31<03:27,  1.59s/it]Epoch 20:  57%|█████▋    | 170/300 [04:33<03:30,  1.62s/it]Epoch 20:  57%|█████▋    | 171/300 [04:34<03:28,  1.62s/it]Epoch 20:  57%|█████▋    | 172/300 [04:36<03:18,  1.55s/it]Epoch 20:  58%|█████▊    | 173/300 [04:37<03:18,  1.56s/it]Epoch 20:  58%|█████▊    | 174/300 [04:39<03:10,  1.52s/it]Epoch 20:  58%|█████▊    | 175/300 [04:40<03:08,  1.51s/it]Epoch 20:  59%|█████▊    | 176/300 [04:41<03:03,  1.48s/it]Epoch 20:  59%|█████▉    | 177/300 [04:43<02:59,  1.46s/it]Epoch 20:  59%|█████▉    | 178/300 [04:44<02:56,  1.45s/it]Epoch 20:  60%|█████▉    | 179/300 [04:46<02:57,  1.46s/it]06/19/2022 17:32:30 - INFO - __main__ - global step: 3090; train loss: 7.561406135559082; dev loss
Epoch 20:  60%|██████    | 180/300 [04:47<02:53,  1.45s/it]Epoch 20:  60%|██████    | 181/300 [04:49<02:50,  1.44s/it]Epoch 20:  61%|██████    | 182/300 [04:50<02:48,  1.43s/it]Epoch 20:  61%|██████    | 183/300 [04:51<02:49,  1.45s/it]Epoch 20:  61%|██████▏   | 184/300 [04:53<02:46,  1.44s/it]Epoch 20:  62%|██████▏   | 185/300 [04:54<02:44,  1.43s/it]Epoch 20:  62%|██████▏   | 186/300 [04:56<02:48,  1.48s/it]Epoch 20:  62%|██████▏   | 187/300 [04:58<02:54,  1.54s/it]Epoch 20:  63%|██████▎   | 188/300 [04:59<02:54,  1.56s/it]Epoch 20:  63%|██████▎   | 189/300 [05:01<02:53,  1.56s/it]Epoch 20:  63%|██████▎   | 190/300 [05:02<02:47,  1.52s/it]Epoch 20:  64%|██████▎   | 191/300 [05:04<02:45,  1.52s/it]Epoch 20:  64%|██████▍   | 192/300 [05:05<02:40,  1.49s/it]Epoch 20:  64%|██████▍   | 193/300 [05:07<02:36,  1.46s/it]Epoch 20:  65%|██████▍   | 194/300 [05:08<02:33,  1.45s/it]Epoch 20:  65%|██████▌   | 195/300 [05:10<02:38,  1.51s/it]Epoch 20:  65%|██████▌   | 196/300 [05:11<02:39,  1.54s/it]Epoch 20:  66%|██████▌   | 197/300 [05:13<02:39,  1.55s/it]Epoch 20:  66%|██████▌   | 198/300 [05:14<02:39,  1.56s/it]Epoch 20:  66%|██████▋   | 199/300 [05:16<02:41,  1.60s/it]06/19/2022 17:33:01 - INFO - __main__ - global step: 3100; train loss: 7.981553077697754; dev loss
Epoch 20:  67%|██████▋   | 200/300 [05:17<02:33,  1.54s/it]Epoch 20:  67%|██████▋   | 201/300 [05:19<02:28,  1.50s/it]Epoch 20:  67%|██████▋   | 202/300 [05:20<02:24,  1.48s/it]Epoch 20:  68%|██████▊   | 203/300 [05:22<02:26,  1.51s/it]Epoch 20:  68%|██████▊   | 204/300 [05:24<02:30,  1.57s/it]Epoch 20:  68%|██████▊   | 205/300 [05:25<02:25,  1.53s/it]Epoch 20:  69%|██████▊   | 206/300 [05:26<02:19,  1.49s/it]Epoch 20:  69%|██████▉   | 207/300 [05:28<02:21,  1.53s/it]Epoch 20:  69%|██████▉   | 208/300 [05:30<02:19,  1.52s/it]Epoch 20:  70%|██████▉   | 209/300 [05:31<02:20,  1.54s/it]Epoch 20:  70%|███████   | 210/300 [05:33<02:20,  1.56s/it]Epoch 20:  70%|███████   | 211/300 [05:34<02:20,  1.58s/it]Epoch 20:  71%|███████   | 212/300 [05:36<02:22,  1.62s/it]Epoch 20:  71%|███████   | 213/300 [05:38<02:20,  1.61s/it]Epoch 20:  71%|███████▏  | 214/300 [05:39<02:13,  1.55s/it]Epoch 20:  72%|███████▏  | 215/300 [05:40<02:08,  1.51s/it]Epoch 20:  72%|███████▏  | 216/300 [05:42<02:10,  1.55s/it]Epoch 20:  72%|███████▏  | 217/300 [05:44<02:04,  1.51s/it]Epoch 20:  73%|███████▎  | 218/300 [05:45<02:01,  1.48s/it]Epoch 20:  73%|███████▎  | 219/300 [05:47<02:02,  1.51s/it]06/19/2022 17:33:32 - INFO - __main__ - global step: 3110; train loss: 7.616931915283203; dev loss
Epoch 20:  73%|███████▎  | 220/300 [05:48<02:04,  1.56s/it]Epoch 20:  74%|███████▎  | 221/300 [05:50<01:59,  1.51s/it]Epoch 20:  74%|███████▍  | 222/300 [05:51<01:55,  1.48s/it]Epoch 20:  74%|███████▍  | 223/300 [05:53<01:56,  1.51s/it]Epoch 20:  75%|███████▍  | 224/300 [05:54<01:58,  1.56s/it]Epoch 20:  75%|███████▌  | 225/300 [05:56<01:58,  1.58s/it]Epoch 20:  75%|███████▌  | 226/300 [05:58<01:57,  1.58s/it]Epoch 20:  76%|███████▌  | 227/300 [05:59<01:52,  1.53s/it]Epoch 20:  76%|███████▌  | 228/300 [06:00<01:48,  1.50s/it]Epoch 20:  76%|███████▋  | 229/300 [06:02<01:50,  1.56s/it]Epoch 20:  77%|███████▋  | 230/300 [06:04<01:51,  1.59s/it]Epoch 20:  77%|███████▋  | 231/300 [06:05<01:47,  1.56s/it]Epoch 20:  77%|███████▋  | 232/300 [06:07<01:44,  1.54s/it]Epoch 20:  78%|███████▊  | 233/300 [06:08<01:43,  1.55s/it]Epoch 20:  78%|███████▊  | 234/300 [06:10<01:40,  1.52s/it]Epoch 20:  78%|███████▊  | 235/300 [06:11<01:42,  1.57s/it]Epoch 20:  79%|███████▊  | 236/300 [06:13<01:38,  1.54s/it]Epoch 20:  79%|███████▉  | 237/300 [06:15<01:41,  1.61s/it]Epoch 20:  79%|███████▉  | 238/300 [06:16<01:40,  1.62s/it]Epoch 20:  80%|███████▉  | 239/300 [06:18<01:36,  1.58s/it]06/19/2022 17:34:03 - INFO - __main__ - global step: 3120; train loss: 7.8277764320373535; dev loss
Epoch 20:  80%|████████  | 240/300 [06:19<01:36,  1.60s/it]Epoch 20:  80%|████████  | 241/300 [06:21<01:37,  1.65s/it]Epoch 20:  81%|████████  | 242/300 [06:23<01:36,  1.66s/it]Epoch 20:  81%|████████  | 243/300 [06:24<01:31,  1.61s/it]Epoch 20:  81%|████████▏ | 244/300 [06:26<01:28,  1.58s/it]Epoch 20:  82%|████████▏ | 245/300 [06:28<01:32,  1.68s/it]Epoch 20:  82%|████████▏ | 246/300 [06:30<01:33,  1.73s/it]Epoch 20:  82%|████████▏ | 247/300 [06:31<01:29,  1.68s/it]Epoch 20:  83%|████████▎ | 248/300 [06:33<01:24,  1.62s/it]Epoch 20:  83%|████████▎ | 249/300 [06:34<01:24,  1.65s/it]Epoch 20:  83%|████████▎ | 250/300 [06:36<01:20,  1.61s/it]Epoch 20:  84%|████████▎ | 251/300 [06:37<01:17,  1.58s/it]Epoch 20:  84%|████████▍ | 252/300 [06:39<01:16,  1.60s/it]Epoch 20:  84%|████████▍ | 253/300 [06:41<01:15,  1.60s/it]Epoch 20:  85%|████████▍ | 254/300 [06:42<01:11,  1.55s/it]Epoch 20:  85%|████████▌ | 255/300 [06:44<01:11,  1.58s/it]Epoch 20:  85%|████████▌ | 256/300 [06:45<01:10,  1.61s/it]Epoch 20:  86%|████████▌ | 257/300 [06:47<01:09,  1.63s/it]Epoch 20:  86%|████████▌ | 258/300 [06:49<01:09,  1.67s/it]Epoch 20:  86%|████████▋ | 259/300 [06:50<01:06,  1.62s/it]06/19/2022 17:34:35 - INFO - __main__ - global step: 3130; train loss: 7.71029806137085; dev loss
Epoch 20:  87%|████████▋ | 260/300 [06:52<01:05,  1.63s/it]Epoch 20:  87%|████████▋ | 261/300 [06:53<01:01,  1.58s/it]Epoch 20:  87%|████████▋ | 262/300 [06:55<00:59,  1.58s/it]Epoch 20:  88%|████████▊ | 263/300 [06:57<01:00,  1.62s/it]Epoch 20:  88%|████████▊ | 264/300 [06:59<01:01,  1.71s/it]Epoch 20:  88%|████████▊ | 265/300 [07:01<01:01,  1.76s/it]Epoch 20:  89%|████████▊ | 266/300 [07:02<00:58,  1.71s/it]Epoch 20:  89%|████████▉ | 267/300 [07:04<00:58,  1.76s/it]Epoch 20:  89%|████████▉ | 268/300 [07:06<00:53,  1.68s/it]Epoch 20:  90%|████████▉ | 269/300 [07:07<00:54,  1.74s/it]Epoch 20:  90%|█████████ | 270/300 [07:09<00:50,  1.70s/it]Epoch 20:  90%|█████████ | 271/300 [07:10<00:47,  1.63s/it]Epoch 20:  91%|█████████ | 272/300 [07:12<00:47,  1.71s/it]Epoch 20:  91%|█████████ | 273/300 [07:14<00:44,  1.66s/it]Epoch 20:  91%|█████████▏| 274/300 [07:16<00:42,  1.65s/it]Epoch 20:  92%|█████████▏| 275/300 [07:17<00:40,  1.61s/it]Epoch 20:  92%|█████████▏| 276/300 [07:19<00:38,  1.59s/it]Epoch 20:  92%|█████████▏| 277/300 [07:21<00:38,  1.69s/it]Epoch 20:  93%|█████████▎| 278/300 [07:23<00:39,  1.79s/it]Epoch 20:  93%|█████████▎| 279/300 [07:24<00:35,  1.70s/it]06/19/2022 17:35:09 - INFO - __main__ - global step: 3140; train loss: 8.32209587097168; dev loss
Epoch 20:  93%|█████████▎| 280/300 [07:26<00:35,  1.77s/it]Epoch 20:  94%|█████████▎| 281/300 [07:27<00:31,  1.68s/it]Epoch 20:  94%|█████████▍| 282/300 [07:29<00:29,  1.62s/it]Epoch 20:  94%|█████████▍| 283/300 [07:31<00:29,  1.73s/it]Epoch 20:  95%|█████████▍| 284/300 [07:32<00:26,  1.66s/it]Epoch 20:  95%|█████████▌| 285/300 [07:34<00:25,  1.72s/it]Epoch 20:  95%|█████████▌| 286/300 [07:36<00:24,  1.77s/it]Epoch 20:  96%|█████████▌| 287/300 [07:38<00:23,  1.84s/it]Epoch 20:  96%|█████████▌| 288/300 [07:40<00:20,  1.75s/it]Epoch 20:  96%|█████████▋| 289/300 [07:42<00:19,  1.81s/it]Epoch 20:  97%|█████████▋| 290/300 [07:43<00:17,  1.71s/it]Epoch 20:  97%|█████████▋| 291/300 [07:45<00:16,  1.79s/it]Epoch 20:  97%|█████████▋| 292/300 [07:47<00:14,  1.82s/it]Epoch 20:  98%|█████████▊| 293/300 [07:48<00:12,  1.72s/it]Epoch 20:  98%|█████████▊| 294/300 [07:50<00:10,  1.76s/it]Epoch 20:  98%|█████████▊| 295/300 [07:52<00:08,  1.72s/it]Epoch 20:  99%|█████████▊| 296/300 [07:53<00:06,  1.64s/it]Epoch 20:  99%|█████████▉| 297/300 [07:55<00:04,  1.59s/it]Epoch 20:  99%|█████████▉| 298/300 [07:56<00:03,  1.56s/it]Epoch 20: 100%|█████████▉| 299/300 [07:58<00:01,  1.57s/it]06/19/2022 17:35:43 - INFO - __main__ - global step: 3150; train loss: 7.7442216873168945; dev loss
Epoch 20: 100%|██████████| 300/300 [07:59<00:00,  1.54s/it]Epoch 20: 100%|██████████| 300/300 [07:59<00:00,  1.60s/it]
Epoch 21:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 21:   0%|          | 1/300 [00:01<07:33,  1.52s/it]Epoch 21:   1%|          | 2/300 [00:03<07:26,  1.50s/it]Epoch 21:   1%|          | 3/300 [00:04<08:32,  1.73s/it]Epoch 21:   1%|▏         | 4/300 [00:06<08:04,  1.64s/it]Epoch 21:   2%|▏         | 5/300 [00:07<07:46,  1.58s/it]Epoch 21:   2%|▏         | 6/300 [00:09<08:15,  1.69s/it]Epoch 21:   2%|▏         | 7/300 [00:11<08:03,  1.65s/it]Epoch 21:   3%|▎         | 8/300 [00:12<07:43,  1.59s/it]Epoch 21:   3%|▎         | 9/300 [00:14<07:31,  1.55s/it]Epoch 21:   3%|▎         | 10/300 [00:15<07:23,  1.53s/it]Epoch 21:   4%|▎         | 11/300 [00:17<07:15,  1.51s/it]Epoch 21:   4%|▍         | 12/300 [00:18<07:18,  1.52s/it]Epoch 21:   4%|▍         | 13/300 [00:20<07:13,  1.51s/it]Epoch 21:   5%|▍         | 14/300 [00:21<07:10,  1.50s/it]Epoch 21:   5%|▌         | 15/300 [00:23<07:41,  1.62s/it]Epoch 21:   5%|▌         | 16/300 [00:25<08:11,  1.73s/it]Epoch 21:   6%|▌         | 17/300 [00:27<07:47,  1.65s/it]Epoch 21:   6%|▌         | 18/300 [00:29<08:07,  1.73s/it]Epoch 21:   6%|▋         | 19/300 [00:30<08:18,  1.78s/it]06/19/2022 17:36:15 - INFO - __main__ - global step: 3160; train loss: 8.196451187133789; dev loss
Epoch 21:   7%|▋         | 20/300 [00:32<07:59,  1.71s/it]Epoch 21:   7%|▋         | 21/300 [00:34<07:37,  1.64s/it]Epoch 21:   7%|▋         | 22/300 [00:35<07:55,  1.71s/it]Epoch 21:   8%|▊         | 23/300 [00:37<08:07,  1.76s/it]Epoch 21:   8%|▊         | 24/300 [00:39<08:24,  1.83s/it]Epoch 21:   8%|▊         | 25/300 [00:41<07:52,  1.72s/it]Epoch 21:   9%|▊         | 26/300 [00:42<07:29,  1.64s/it]Epoch 21:   9%|▉         | 27/300 [00:44<07:48,  1.72s/it]Epoch 21:   9%|▉         | 28/300 [00:46<08:07,  1.79s/it]Epoch 21:  10%|▉         | 29/300 [00:47<07:38,  1.69s/it]Epoch 21:  10%|█         | 30/300 [00:49<07:50,  1.74s/it]Epoch 21:  10%|█         | 31/300 [00:51<07:28,  1.67s/it]Epoch 21:  11%|█         | 32/300 [00:53<07:52,  1.76s/it]Epoch 21:  11%|█         | 33/300 [00:55<08:02,  1.81s/it]Epoch 21:  11%|█▏        | 34/300 [00:56<07:33,  1.70s/it]Epoch 21:  12%|█▏        | 35/300 [00:58<07:14,  1.64s/it]Epoch 21:  12%|█▏        | 36/300 [01:00<07:32,  1.71s/it]Epoch 21:  12%|█▏        | 37/300 [01:02<07:53,  1.80s/it]Epoch 21:  13%|█▎        | 38/300 [01:03<07:57,  1.82s/it]Epoch 21:  13%|█▎        | 39/300 [01:05<08:01,  1.85s/it]06/19/2022 17:36:50 - INFO - __main__ - global step: 3170; train loss: 7.912219047546387; dev loss
Epoch 21:  13%|█▎        | 40/300 [01:07<08:02,  1.86s/it]Epoch 21:  14%|█▎        | 41/300 [01:09<08:11,  1.90s/it]Epoch 21:  14%|█▍        | 42/300 [01:11<08:08,  1.89s/it]Epoch 21:  14%|█▍        | 43/300 [01:13<07:34,  1.77s/it]Epoch 21:  15%|█▍        | 44/300 [01:14<07:41,  1.80s/it]Epoch 21:  15%|█▌        | 45/300 [01:16<07:22,  1.73s/it]Epoch 21:  15%|█▌        | 46/300 [01:18<07:01,  1.66s/it]Epoch 21:  16%|█▌        | 47/300 [01:19<06:46,  1.61s/it]Epoch 21:  16%|█▌        | 48/300 [01:21<07:05,  1.69s/it]Epoch 21:  16%|█▋        | 49/300 [01:22<06:54,  1.65s/it]Epoch 21:  17%|█▋        | 50/300 [01:24<06:40,  1.60s/it]Epoch 21:  17%|█▋        | 51/300 [01:25<06:29,  1.56s/it]Epoch 21:  17%|█▋        | 52/300 [01:27<06:22,  1.54s/it]Epoch 21:  18%|█▊        | 53/300 [01:29<06:51,  1.67s/it]Epoch 21:  18%|█▊        | 54/300 [01:30<06:37,  1.61s/it]Epoch 21:  18%|█▊        | 55/300 [01:32<06:26,  1.58s/it]Epoch 21:  19%|█▊        | 56/300 [01:33<06:17,  1.55s/it]Epoch 21:  19%|█▉        | 57/300 [01:35<06:46,  1.67s/it]Epoch 21:  19%|█▉        | 58/300 [01:37<06:30,  1.61s/it]Epoch 21:  20%|█▉        | 59/300 [01:39<06:47,  1.69s/it]06/19/2022 17:37:23 - INFO - __main__ - global step: 3180; train loss: 8.485219955444336; dev loss
Epoch 21:  20%|██        | 60/300 [01:40<06:31,  1.63s/it]Epoch 21:  20%|██        | 61/300 [01:42<06:54,  1.73s/it]Epoch 21:  21%|██        | 62/300 [01:44<07:02,  1.77s/it]Epoch 21:  21%|██        | 63/300 [01:46<07:10,  1.82s/it]Epoch 21:  21%|██▏       | 64/300 [01:48<07:13,  1.84s/it]Epoch 21:  22%|██▏       | 65/300 [01:50<07:17,  1.86s/it]Epoch 21:  22%|██▏       | 66/300 [01:51<06:55,  1.77s/it]Epoch 21:  22%|██▏       | 67/300 [01:53<07:00,  1.81s/it]Epoch 21:  23%|██▎       | 68/300 [01:55<06:38,  1.72s/it]Epoch 21:  23%|██▎       | 69/300 [01:57<06:47,  1.77s/it]Epoch 21:  23%|██▎       | 70/300 [01:58<06:35,  1.72s/it]Epoch 21:  24%|██▎       | 71/300 [02:00<06:44,  1.77s/it]Epoch 21:  24%|██▍       | 72/300 [02:02<06:27,  1.70s/it]Epoch 21:  24%|██▍       | 73/300 [02:03<06:10,  1.63s/it]Epoch 21:  25%|██▍       | 74/300 [02:05<06:31,  1.73s/it]Epoch 21:  25%|██▌       | 75/300 [02:06<06:13,  1.66s/it]Epoch 21:  25%|██▌       | 76/300 [02:08<06:26,  1.73s/it]Epoch 21:  26%|██▌       | 77/300 [02:10<06:37,  1.78s/it]Epoch 21:  26%|██▌       | 78/300 [02:12<06:48,  1.84s/it]Epoch 21:  26%|██▋       | 79/300 [02:14<06:51,  1.86s/it]06/19/2022 17:37:59 - INFO - __main__ - global step: 3190; train loss: 7.399687767028809; dev loss
Epoch 21:  27%|██▋       | 80/300 [02:16<06:51,  1.87s/it]Epoch 21:  27%|██▋       | 81/300 [02:18<06:49,  1.87s/it]Epoch 21:  27%|██▋       | 82/300 [02:19<06:26,  1.77s/it]Epoch 21:  28%|██▊       | 83/300 [02:21<06:05,  1.69s/it]Epoch 21:  28%|██▊       | 84/300 [02:22<05:52,  1.63s/it]Epoch 21:  28%|██▊       | 85/300 [02:24<06:06,  1.71s/it]Epoch 21:  29%|██▊       | 86/300 [02:26<05:58,  1.67s/it]Epoch 21:  29%|██▉       | 87/300 [02:27<05:45,  1.62s/it]Epoch 21:  29%|██▉       | 88/300 [02:29<05:59,  1.70s/it]Epoch 21:  30%|██▉       | 89/300 [02:31<05:42,  1.62s/it]Epoch 21:  30%|███       | 90/300 [02:32<05:33,  1.59s/it]Epoch 21:  30%|███       | 91/300 [02:34<05:31,  1.59s/it]Epoch 21:  31%|███       | 92/300 [02:36<05:48,  1.68s/it]Epoch 21:  31%|███       | 93/300 [02:37<05:35,  1.62s/it]Epoch 21:  31%|███▏      | 94/300 [02:39<05:24,  1.58s/it]Epoch 21:  32%|███▏      | 95/300 [02:41<05:49,  1.71s/it]Epoch 21:  32%|███▏      | 96/300 [02:43<05:59,  1.76s/it]Epoch 21:  32%|███▏      | 97/300 [02:44<05:42,  1.69s/it]Epoch 21:  33%|███▎      | 98/300 [02:46<05:52,  1.74s/it]Epoch 21:  33%|███▎      | 99/300 [02:48<06:05,  1.82s/it]06/19/2022 17:38:33 - INFO - __main__ - global step: 3200; train loss: 7.787600517272949; dev loss
Epoch 21:  33%|███▎      | 100/300 [02:50<06:08,  1.84s/it]Epoch 21:  34%|███▎      | 101/300 [02:51<05:46,  1.74s/it]Epoch 21:  34%|███▍      | 102/300 [02:53<05:31,  1.67s/it]Epoch 21:  34%|███▍      | 103/300 [02:55<05:24,  1.65s/it]Epoch 21:  35%|███▍      | 104/300 [02:56<05:12,  1.60s/it]Epoch 21:  35%|███▌      | 105/300 [02:57<05:05,  1.57s/it]Epoch 21:  35%|███▌      | 106/300 [02:59<05:23,  1.67s/it]Epoch 21:  36%|███▌      | 107/300 [03:01<05:17,  1.64s/it]Epoch 21:  36%|███▌      | 108/300 [03:02<05:05,  1.59s/it]Epoch 21:  36%|███▋      | 109/300 [03:04<05:20,  1.68s/it]Epoch 21:  37%|███▋      | 110/300 [03:06<05:07,  1.62s/it]Epoch 21:  37%|███▋      | 111/300 [03:07<05:03,  1.61s/it]Epoch 21:  37%|███▋      | 112/300 [03:09<04:55,  1.57s/it]Epoch 21:  38%|███▊      | 113/300 [03:10<04:48,  1.54s/it]Epoch 21:  38%|███▊      | 114/300 [03:12<05:06,  1.65s/it]Epoch 21:  38%|███▊      | 115/300 [03:14<05:02,  1.63s/it]Epoch 21:  39%|███▊      | 116/300 [03:15<04:52,  1.59s/it]Epoch 21:  39%|███▉      | 117/300 [03:17<05:06,  1.68s/it]Epoch 21:  39%|███▉      | 118/300 [03:19<04:56,  1.63s/it]Epoch 21:  40%|███▉      | 119/300 [03:20<04:46,  1.58s/it]06/19/2022 17:39:05 - INFO - __main__ - global step: 3210; train loss: 7.751023769378662; dev loss
Epoch 21:  40%|████      | 120/300 [03:22<05:06,  1.70s/it]Epoch 21:  40%|████      | 121/300 [03:24<05:14,  1.76s/it]Epoch 21:  41%|████      | 122/300 [03:26<04:59,  1.68s/it]Epoch 21:  41%|████      | 123/300 [03:27<05:09,  1.75s/it]Epoch 21:  41%|████▏     | 124/300 [03:29<05:20,  1.82s/it]Epoch 21:  42%|████▏     | 125/300 [03:31<05:00,  1.72s/it]Epoch 21:  42%|████▏     | 126/300 [03:33<05:09,  1.78s/it]Epoch 21:  42%|████▏     | 127/300 [03:34<04:53,  1.69s/it]Epoch 21:  43%|████▎     | 128/300 [03:36<04:46,  1.67s/it]Epoch 21:  43%|████▎     | 129/300 [03:38<04:56,  1.73s/it]Epoch 21:  43%|████▎     | 130/300 [03:40<05:02,  1.78s/it]Epoch 21:  44%|████▎     | 131/300 [03:41<04:46,  1.70s/it]Epoch 21:  44%|████▍     | 132/300 [03:43<04:39,  1.67s/it]Epoch 21:  44%|████▍     | 133/300 [03:44<04:29,  1.62s/it]Epoch 21:  45%|████▍     | 134/300 [03:46<04:43,  1.71s/it]Epoch 21:  45%|████▌     | 135/300 [03:48<04:32,  1.65s/it]Epoch 21:  45%|████▌     | 136/300 [03:49<04:28,  1.64s/it]Epoch 21:  46%|████▌     | 137/300 [03:51<04:38,  1.71s/it]Epoch 21:  46%|████▌     | 138/300 [03:53<04:27,  1.65s/it]Epoch 21:  46%|████▋     | 139/300 [03:55<04:32,  1.69s/it]06/19/2022 17:39:39 - INFO - __main__ - global step: 3220; train loss: 8.168862342834473; dev loss
Epoch 21:  47%|████▋     | 140/300 [03:56<04:26,  1.67s/it]Epoch 21:  47%|████▋     | 141/300 [03:58<04:28,  1.69s/it]Epoch 21:  47%|████▋     | 142/300 [03:59<04:19,  1.64s/it]Epoch 21:  48%|████▊     | 143/300 [04:01<04:24,  1.69s/it]Epoch 21:  48%|████▊     | 144/300 [04:03<04:16,  1.65s/it]Epoch 21:  48%|████▊     | 145/300 [04:04<04:12,  1.63s/it]Epoch 21:  49%|████▊     | 146/300 [04:06<04:04,  1.59s/it]Epoch 21:  49%|████▉     | 147/300 [04:07<03:58,  1.56s/it]Epoch 21:  49%|████▉     | 148/300 [04:09<03:54,  1.54s/it]Epoch 21:  50%|████▉     | 149/300 [04:10<03:54,  1.56s/it]Epoch 21:  50%|█████     | 150/300 [04:12<03:50,  1.54s/it]Epoch 21:  50%|█████     | 151/300 [04:14<04:05,  1.65s/it]Epoch 21:  51%|█████     | 152/300 [04:16<04:15,  1.73s/it]Epoch 21:  51%|█████     | 153/300 [04:18<04:26,  1.81s/it]Epoch 21:  51%|█████▏    | 154/300 [04:19<04:08,  1.70s/it]Epoch 21:  52%|█████▏    | 155/300 [04:21<03:55,  1.63s/it]Epoch 21:  52%|█████▏    | 156/300 [04:22<04:03,  1.69s/it]Epoch 21:  52%|█████▏    | 157/300 [04:24<04:14,  1.78s/it]Epoch 21:  53%|█████▎    | 158/300 [04:26<03:58,  1.68s/it]Epoch 21:  53%|█████▎    | 159/300 [04:28<04:02,  1.72s/it]06/19/2022 17:40:12 - INFO - __main__ - global step: 3230; train loss: 7.387341499328613; dev loss
Epoch 21:  53%|█████▎    | 160/300 [04:29<03:49,  1.64s/it]Epoch 21:  54%|█████▎    | 161/300 [04:31<04:00,  1.73s/it]Epoch 21:  54%|█████▍    | 162/300 [04:33<03:46,  1.64s/it]Epoch 21:  54%|█████▍    | 163/300 [04:34<03:37,  1.59s/it]Epoch 21:  55%|█████▍    | 164/300 [04:35<03:30,  1.54s/it]Epoch 21:  55%|█████▌    | 165/300 [04:37<03:29,  1.55s/it]Epoch 21:  55%|█████▌    | 166/300 [04:39<03:39,  1.64s/it]Epoch 21:  56%|█████▌    | 167/300 [04:41<03:45,  1.69s/it]Epoch 21:  56%|█████▌    | 168/300 [04:43<03:48,  1.73s/it]Epoch 21:  56%|█████▋    | 169/300 [04:44<03:54,  1.79s/it]Epoch 21:  57%|█████▋    | 170/300 [04:46<03:40,  1.70s/it]Epoch 21:  57%|█████▋    | 171/300 [04:47<03:28,  1.62s/it]Epoch 21:  57%|█████▋    | 172/300 [04:49<03:34,  1.68s/it]Epoch 21:  58%|█████▊    | 173/300 [04:51<03:24,  1.61s/it]Epoch 21:  58%|█████▊    | 174/300 [04:53<03:35,  1.71s/it]Epoch 21:  58%|█████▊    | 175/300 [04:54<03:24,  1.64s/it]Epoch 21:  59%|█████▊    | 176/300 [04:56<03:30,  1.69s/it]Epoch 21:  59%|█████▉    | 177/300 [04:57<03:19,  1.62s/it]Epoch 21:  59%|█████▉    | 178/300 [04:59<03:28,  1.71s/it]Epoch 21:  60%|█████▉    | 179/300 [05:01<03:31,  1.75s/it]06/19/2022 17:40:46 - INFO - __main__ - global step: 3240; train loss: 7.9418535232543945; dev loss
Epoch 21:  60%|██████    | 180/300 [05:03<03:33,  1.78s/it]Epoch 21:  60%|██████    | 181/300 [05:04<03:19,  1.68s/it]Epoch 21:  61%|██████    | 182/300 [05:06<03:27,  1.76s/it]Epoch 21:  61%|██████    | 183/300 [05:08<03:15,  1.67s/it]Epoch 21:  61%|██████▏   | 184/300 [05:10<03:18,  1.71s/it]Epoch 21:  62%|██████▏   | 185/300 [05:11<03:20,  1.75s/it]Epoch 21:  62%|██████▏   | 186/300 [05:13<03:25,  1.80s/it]Epoch 21:  62%|██████▏   | 187/300 [05:15<03:24,  1.81s/it]Epoch 21:  63%|██████▎   | 188/300 [05:17<03:10,  1.70s/it]Epoch 21:  63%|██████▎   | 189/300 [05:18<02:59,  1.62s/it]Epoch 21:  63%|██████▎   | 190/300 [05:20<02:57,  1.61s/it]Epoch 21:  64%|██████▎   | 191/300 [05:21<02:50,  1.56s/it]Epoch 21:  64%|██████▍   | 192/300 [05:23<02:56,  1.64s/it]Epoch 21:  64%|██████▍   | 193/300 [05:25<03:03,  1.71s/it]Epoch 21:  65%|██████▍   | 194/300 [05:27<03:08,  1.78s/it]Epoch 21:  65%|██████▌   | 195/300 [05:28<02:55,  1.67s/it]Epoch 21:  65%|██████▌   | 196/300 [05:30<02:46,  1.60s/it]Epoch 21:  66%|██████▌   | 197/300 [05:31<02:51,  1.67s/it]Epoch 21:  66%|██████▌   | 198/300 [05:33<02:54,  1.71s/it]Epoch 21:  66%|██████▋   | 199/300 [05:35<02:48,  1.67s/it]06/19/2022 17:41:20 - INFO - __main__ - global step: 3250; train loss: 8.055076599121094; dev loss
Epoch 21:  67%|██████▋   | 200/300 [05:37<02:51,  1.72s/it]Epoch 21:  67%|██████▋   | 201/300 [05:38<02:52,  1.75s/it]Epoch 21:  67%|██████▋   | 202/300 [05:40<02:53,  1.77s/it]Epoch 21:  68%|██████▊   | 203/300 [05:42<02:57,  1.83s/it]Epoch 21:  68%|██████▊   | 204/300 [05:44<02:44,  1.71s/it]Epoch 21:  68%|██████▊   | 205/300 [05:45<02:35,  1.64s/it]Epoch 21:  69%|██████▊   | 206/300 [05:47<02:39,  1.69s/it]Epoch 21:  69%|██████▉   | 207/300 [05:49<02:44,  1.77s/it]Epoch 21:  69%|██████▉   | 208/300 [05:51<02:44,  1.79s/it]Epoch 21:  70%|██████▉   | 209/300 [05:52<02:33,  1.69s/it]Epoch 21:  70%|███████   | 210/300 [05:54<02:35,  1.73s/it]Epoch 21:  70%|███████   | 211/300 [05:56<02:30,  1.69s/it]Epoch 21:  71%|███████   | 212/300 [05:57<02:31,  1.73s/it]Epoch 21:  71%|███████   | 213/300 [05:59<02:23,  1.64s/it]Epoch 21:  71%|███████▏  | 214/300 [06:00<02:16,  1.59s/it]Epoch 21:  72%|███████▏  | 215/300 [06:02<02:23,  1.69s/it]Epoch 21:  72%|███████▏  | 216/300 [06:04<02:25,  1.73s/it]Epoch 21:  72%|███████▏  | 217/300 [06:06<02:26,  1.76s/it]Epoch 21:  73%|███████▎  | 218/300 [06:08<02:25,  1.78s/it]Epoch 21:  73%|███████▎  | 219/300 [06:09<02:18,  1.71s/it]06/19/2022 17:41:54 - INFO - __main__ - global step: 3260; train loss: 8.334217071533203; dev loss
Epoch 21:  73%|███████▎  | 220/300 [06:11<02:19,  1.74s/it]Epoch 21:  74%|███████▎  | 221/300 [06:13<02:19,  1.77s/it]Epoch 21:  74%|███████▍  | 222/300 [06:14<02:10,  1.67s/it]Epoch 21:  74%|███████▍  | 223/300 [06:16<02:14,  1.75s/it]Epoch 21:  75%|███████▍  | 224/300 [06:18<02:06,  1.66s/it]Epoch 21:  75%|███████▌  | 225/300 [06:20<02:07,  1.71s/it]Epoch 21:  75%|███████▌  | 226/300 [06:21<02:09,  1.74s/it]Epoch 21:  76%|███████▌  | 227/300 [06:23<02:00,  1.65s/it]Epoch 21:  76%|███████▌  | 228/300 [06:25<02:05,  1.74s/it]Epoch 21:  76%|███████▋  | 229/300 [06:26<01:57,  1.66s/it]Epoch 21:  77%|███████▋  | 230/300 [06:28<01:59,  1.71s/it]Epoch 21:  77%|███████▋  | 231/300 [06:30<02:00,  1.75s/it]Epoch 21:  77%|███████▋  | 232/300 [06:32<02:03,  1.82s/it]Epoch 21:  78%|███████▊  | 233/300 [06:34<02:01,  1.82s/it]Epoch 21:  78%|███████▊  | 234/300 [06:35<01:52,  1.71s/it]Epoch 21:  78%|███████▊  | 235/300 [06:37<01:53,  1.74s/it]Epoch 21:  79%|███████▊  | 236/300 [06:39<01:48,  1.70s/it]Epoch 21:  79%|███████▉  | 237/300 [06:40<01:42,  1.63s/it]Epoch 21:  79%|███████▉  | 238/300 [06:42<01:37,  1.57s/it]Epoch 21:  80%|███████▉  | 239/300 [06:43<01:40,  1.64s/it]06/19/2022 17:42:28 - INFO - __main__ - global step: 3270; train loss: 7.9079461097717285; dev loss
Epoch 21:  80%|████████  | 240/300 [06:45<01:37,  1.62s/it]Epoch 21:  80%|████████  | 241/300 [06:46<01:32,  1.57s/it]Epoch 21:  81%|████████  | 242/300 [06:48<01:36,  1.66s/it]Epoch 21:  81%|████████  | 243/300 [06:50<01:37,  1.72s/it]Epoch 21:  81%|████████▏ | 244/300 [06:52<01:40,  1.79s/it]Epoch 21:  82%|████████▏ | 245/300 [06:54<01:33,  1.69s/it]Epoch 21:  82%|████████▏ | 246/300 [06:55<01:27,  1.62s/it]Epoch 21:  82%|████████▏ | 247/300 [06:57<01:29,  1.68s/it]Epoch 21:  83%|████████▎ | 248/300 [06:59<01:31,  1.76s/it]Epoch 21:  83%|████████▎ | 249/300 [07:01<01:30,  1.78s/it]Epoch 21:  83%|████████▎ | 250/300 [07:02<01:24,  1.68s/it]Epoch 21:  84%|████████▎ | 251/300 [07:04<01:24,  1.72s/it]Epoch 21:  84%|████████▍ | 252/300 [07:05<01:18,  1.64s/it]Epoch 21:  84%|████████▍ | 253/300 [07:07<01:15,  1.61s/it]Epoch 21:  85%|████████▍ | 254/300 [07:08<01:11,  1.56s/it]Epoch 21:  85%|████████▌ | 255/300 [07:10<01:13,  1.64s/it]Epoch 21:  85%|████████▌ | 256/300 [07:12<01:14,  1.70s/it]Epoch 21:  86%|████████▌ | 257/300 [07:14<01:16,  1.77s/it]Epoch 21:  86%|████████▌ | 258/300 [07:16<01:15,  1.79s/it]Epoch 21:  86%|████████▋ | 259/300 [07:17<01:09,  1.69s/it]06/19/2022 17:43:02 - INFO - __main__ - global step: 3280; train loss: 7.925244331359863; dev loss
Epoch 21:  87%|████████▋ | 260/300 [07:19<01:09,  1.73s/it]Epoch 21:  87%|████████▋ | 261/300 [07:21<01:05,  1.68s/it]Epoch 21:  87%|████████▋ | 262/300 [07:22<01:05,  1.72s/it]Epoch 21:  88%|████████▊ | 263/300 [07:24<01:00,  1.65s/it]Epoch 21:  88%|████████▊ | 264/300 [07:26<01:01,  1.70s/it]Epoch 21:  88%|████████▊ | 265/300 [07:28<01:02,  1.78s/it]Epoch 21:  89%|████████▊ | 266/300 [07:29<01:00,  1.79s/it]Epoch 21:  89%|████████▉ | 267/300 [07:31<00:59,  1.80s/it]Epoch 21:  89%|████████▉ | 268/300 [07:33<00:54,  1.70s/it]Epoch 21:  90%|████████▉ | 269/300 [07:34<00:51,  1.66s/it]Epoch 21:  90%|█████████ | 270/300 [07:36<00:47,  1.60s/it]Epoch 21:  90%|█████████ | 271/300 [07:37<00:44,  1.55s/it]Epoch 21:  91%|█████████ | 272/300 [07:39<00:42,  1.51s/it]Epoch 21:  91%|█████████ | 273/300 [07:41<00:44,  1.64s/it]Epoch 21:  91%|█████████▏| 274/300 [07:42<00:43,  1.69s/it]Epoch 21:  92%|█████████▏| 275/300 [07:44<00:43,  1.74s/it]Epoch 21:  92%|█████████▏| 276/300 [07:46<00:42,  1.77s/it]Epoch 21:  92%|█████████▏| 277/300 [07:48<00:42,  1.83s/it]Epoch 21:  93%|█████████▎| 278/300 [07:50<00:40,  1.83s/it]Epoch 21:  93%|█████████▎| 279/300 [07:51<00:36,  1.72s/it]06/19/2022 17:43:36 - INFO - __main__ - global step: 3290; train loss: 8.090302467346191; dev loss
Epoch 21:  93%|█████████▎| 280/300 [07:53<00:35,  1.75s/it]Epoch 21:  94%|█████████▎| 281/300 [07:55<00:33,  1.78s/it]Epoch 21:  94%|█████████▍| 282/300 [07:57<00:30,  1.70s/it]Epoch 21:  94%|█████████▍| 283/300 [07:58<00:27,  1.64s/it]Epoch 21:  95%|█████████▍| 284/300 [08:00<00:27,  1.69s/it]Epoch 21:  95%|█████████▌| 285/300 [08:02<00:25,  1.73s/it]Epoch 21:  95%|█████████▌| 286/300 [08:03<00:23,  1.69s/it]Epoch 21:  96%|█████████▌| 287/300 [08:05<00:21,  1.63s/it]Epoch 21:  96%|█████████▌| 288/300 [08:07<00:20,  1.68s/it]Epoch 21:  96%|█████████▋| 289/300 [08:08<00:17,  1.62s/it]Epoch 21:  97%|█████████▋| 290/300 [08:10<00:15,  1.60s/it]Epoch 21:  97%|█████████▋| 291/300 [08:11<00:14,  1.56s/it]Epoch 21:  97%|█████████▋| 292/300 [08:13<00:13,  1.64s/it]Epoch 21:  98%|█████████▊| 293/300 [08:15<00:11,  1.69s/it]Epoch 21:  98%|█████████▊| 294/300 [08:17<00:10,  1.77s/it]Epoch 21:  98%|█████████▊| 295/300 [08:18<00:08,  1.79s/it]Epoch 21:  99%|█████████▊| 296/300 [08:20<00:06,  1.69s/it]Epoch 21:  99%|█████████▉| 297/300 [08:22<00:05,  1.74s/it]Epoch 21:  99%|█████████▉| 298/300 [08:23<00:03,  1.70s/it]Epoch 21: 100%|█████████▉| 299/300 [08:25<00:01,  1.74s/it]06/19/2022 17:44:10 - INFO - __main__ - global step: 3300; train loss: 7.573616981506348; dev loss
Epoch 21: 100%|██████████| 300/300 [08:27<00:00,  1.77s/it]Epoch 21: 100%|██████████| 300/300 [08:27<00:00,  1.69s/it]
Epoch 22:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 22:   0%|          | 1/300 [00:01<09:01,  1.81s/it]Epoch 22:   1%|          | 2/300 [00:03<08:18,  1.67s/it]Epoch 22:   1%|          | 3/300 [00:05<08:36,  1.74s/it]Epoch 22:   1%|▏         | 4/300 [00:07<08:46,  1.78s/it]Epoch 22:   2%|▏         | 5/300 [00:08<08:10,  1.66s/it]Epoch 22:   2%|▏         | 6/300 [00:09<07:49,  1.60s/it]Epoch 22:   2%|▏         | 7/300 [00:11<08:23,  1.72s/it]Epoch 22:   3%|▎         | 8/300 [00:13<08:31,  1.75s/it]Epoch 22:   3%|▎         | 9/300 [00:15<08:38,  1.78s/it]Epoch 22:   3%|▎         | 10/300 [00:17<08:41,  1.80s/it]Epoch 22:   4%|▎         | 11/300 [00:19<08:20,  1.73s/it]Epoch 22:   4%|▍         | 12/300 [00:20<07:54,  1.65s/it]Epoch 22:   4%|▍         | 13/300 [00:22<08:06,  1.70s/it]Epoch 22:   5%|▍         | 14/300 [00:24<08:17,  1.74s/it]Epoch 22:   5%|▌         | 15/300 [00:25<07:59,  1.68s/it]Epoch 22:   5%|▌         | 16/300 [00:27<07:35,  1.60s/it]Epoch 22:   6%|▌         | 17/300 [00:28<07:52,  1.67s/it]Epoch 22:   6%|▌         | 18/300 [00:30<08:05,  1.72s/it]Epoch 22:   6%|▋         | 19/300 [00:32<07:51,  1.68s/it]06/19/2022 17:44:44 - INFO - __main__ - global step: 3310; train loss: 7.683650970458984; dev loss
Epoch 22:   7%|▋         | 20/300 [00:33<07:29,  1.61s/it]Epoch 22:   7%|▋         | 21/300 [00:35<07:46,  1.67s/it]Epoch 22:   7%|▋         | 22/300 [00:37<07:56,  1.71s/it]Epoch 22:   8%|▊         | 23/300 [00:39<08:13,  1.78s/it]Epoch 22:   8%|▊         | 24/300 [00:40<07:44,  1.68s/it]Epoch 22:   8%|▊         | 25/300 [00:42<07:51,  1.71s/it]Epoch 22:   9%|▊         | 26/300 [00:44<07:58,  1.74s/it]Epoch 22:   9%|▉         | 27/300 [00:45<07:42,  1.69s/it]Epoch 22:   9%|▉         | 28/300 [00:47<07:18,  1.61s/it]Epoch 22:  10%|▉         | 29/300 [00:48<07:02,  1.56s/it]Epoch 22:  10%|█         | 30/300 [00:50<07:20,  1.63s/it]Epoch 22:  10%|█         | 31/300 [00:52<07:12,  1.61s/it]Epoch 22:  11%|█         | 32/300 [00:53<06:56,  1.55s/it]Epoch 22:  11%|█         | 33/300 [00:55<07:13,  1.62s/it]Epoch 22:  11%|█▏        | 34/300 [00:56<06:56,  1.57s/it]Epoch 22:  12%|█▏        | 35/300 [00:58<06:47,  1.54s/it]Epoch 22:  12%|█▏        | 36/300 [01:00<07:19,  1.67s/it]Epoch 22:  12%|█▏        | 37/300 [01:02<07:30,  1.71s/it]Epoch 22:  13%|█▎        | 38/300 [01:03<07:11,  1.65s/it]Epoch 22:  13%|█▎        | 39/300 [01:05<06:56,  1.59s/it]06/19/2022 17:45:17 - INFO - __main__ - global step: 3320; train loss: 7.676999092102051; dev loss
Epoch 22:  13%|█▎        | 40/300 [01:07<07:21,  1.70s/it]Epoch 22:  14%|█▎        | 41/300 [01:08<07:30,  1.74s/it]Epoch 22:  14%|█▍        | 42/300 [01:10<07:38,  1.78s/it]Epoch 22:  14%|█▍        | 43/300 [01:12<07:40,  1.79s/it]Epoch 22:  15%|█▍        | 44/300 [01:14<07:22,  1.73s/it]Epoch 22:  15%|█▌        | 45/300 [01:15<07:04,  1.66s/it]Epoch 22:  15%|█▌        | 46/300 [01:17<07:17,  1.72s/it]Epoch 22:  16%|█▌        | 47/300 [01:18<06:56,  1.65s/it]Epoch 22:  16%|█▌        | 48/300 [01:20<06:50,  1.63s/it]Epoch 22:  16%|█▋        | 49/300 [01:22<07:04,  1.69s/it]Epoch 22:  17%|█▋        | 50/300 [01:24<07:14,  1.74s/it]Epoch 22:  17%|█▋        | 51/300 [01:26<07:20,  1.77s/it]Epoch 22:  17%|█▋        | 52/300 [01:28<07:31,  1.82s/it]Epoch 22:  18%|█▊        | 53/300 [01:29<07:31,  1.83s/it]Epoch 22:  18%|█▊        | 54/300 [01:31<07:30,  1.83s/it]Epoch 22:  18%|█▊        | 55/300 [01:33<07:28,  1.83s/it]Epoch 22:  19%|█▊        | 56/300 [01:35<07:07,  1.75s/it]Epoch 22:  19%|█▉        | 57/300 [01:36<07:13,  1.78s/it]Epoch 22:  19%|█▉        | 58/300 [01:38<07:14,  1.80s/it]Epoch 22:  20%|█▉        | 59/300 [01:40<06:49,  1.70s/it]06/19/2022 17:45:52 - INFO - __main__ - global step: 3330; train loss: 7.500203609466553; dev loss
Epoch 22:  20%|██        | 60/300 [01:41<06:31,  1.63s/it]Epoch 22:  20%|██        | 61/300 [01:43<06:55,  1.74s/it]Epoch 22:  21%|██        | 62/300 [01:45<06:33,  1.65s/it]Epoch 22:  21%|██        | 63/300 [01:46<06:18,  1.60s/it]Epoch 22:  21%|██▏       | 64/300 [01:48<06:06,  1.55s/it]Epoch 22:  22%|██▏       | 65/300 [01:49<06:28,  1.65s/it]Epoch 22:  22%|██▏       | 66/300 [01:51<06:38,  1.70s/it]Epoch 22:  22%|██▏       | 67/300 [01:53<06:21,  1.64s/it]Epoch 22:  23%|██▎       | 68/300 [01:54<06:05,  1.58s/it]Epoch 22:  23%|██▎       | 69/300 [01:56<06:02,  1.57s/it]Epoch 22:  23%|██▎       | 70/300 [01:57<05:56,  1.55s/it]Epoch 22:  24%|██▎       | 71/300 [01:59<05:51,  1.53s/it]Epoch 22:  24%|██▍       | 72/300 [02:00<05:44,  1.51s/it]Epoch 22:  24%|██▍       | 73/300 [02:02<05:48,  1.53s/it]Epoch 22:  25%|██▍       | 74/300 [02:03<05:42,  1.52s/it]Epoch 22:  25%|██▌       | 75/300 [02:05<06:03,  1.61s/it]Epoch 22:  25%|██▌       | 76/300 [02:07<06:16,  1.68s/it]Epoch 22:  26%|██▌       | 77/300 [02:09<06:34,  1.77s/it]Epoch 22:  26%|██▌       | 78/300 [02:11<06:36,  1.79s/it]Epoch 22:  26%|██▋       | 79/300 [02:12<06:12,  1.68s/it]06/19/2022 17:46:24 - INFO - __main__ - global step: 3340; train loss: 8.271197319030762; dev loss
Epoch 22:  27%|██▋       | 80/300 [02:14<05:56,  1.62s/it]Epoch 22:  27%|██▋       | 81/300 [02:15<05:52,  1.61s/it]Epoch 22:  27%|██▋       | 82/300 [02:17<05:42,  1.57s/it]Epoch 22:  28%|██▊       | 83/300 [02:18<05:36,  1.55s/it]Epoch 22:  28%|██▊       | 84/300 [02:20<05:31,  1.53s/it]Epoch 22:  28%|██▊       | 85/300 [02:21<05:31,  1.54s/it]Epoch 22:  29%|██▊       | 86/300 [02:23<05:24,  1.52s/it]Epoch 22:  29%|██▉       | 87/300 [02:25<05:43,  1.61s/it]Epoch 22:  29%|██▉       | 88/300 [02:26<05:34,  1.58s/it]Epoch 22:  30%|██▉       | 89/300 [02:28<05:26,  1.55s/it]Epoch 22:  30%|███       | 90/300 [02:29<05:26,  1.55s/it]Epoch 22:  30%|███       | 91/300 [02:31<05:21,  1.54s/it]Epoch 22:  31%|███       | 92/300 [02:32<05:18,  1.53s/it]Epoch 22:  31%|███       | 93/300 [02:34<05:11,  1.50s/it]Epoch 22:  31%|███▏      | 94/300 [02:35<05:15,  1.53s/it]Epoch 22:  32%|███▏      | 95/300 [02:37<05:32,  1.62s/it]Epoch 22:  32%|███▏      | 96/300 [02:38<05:21,  1.57s/it]Epoch 22:  32%|███▏      | 97/300 [02:40<05:35,  1.65s/it]Epoch 22:  33%|███▎      | 98/300 [02:42<05:52,  1.74s/it]Epoch 22:  33%|███▎      | 99/300 [02:44<05:55,  1.77s/it]06/19/2022 17:46:56 - INFO - __main__ - global step: 3350; train loss: 7.9170331954956055; dev loss
Epoch 22:  33%|███▎      | 100/300 [02:46<05:34,  1.67s/it]Epoch 22:  34%|███▎      | 101/300 [02:47<05:20,  1.61s/it]Epoch 22:  34%|███▍      | 102/300 [02:49<05:38,  1.71s/it]Epoch 22:  34%|███▍      | 103/300 [02:50<05:22,  1.64s/it]Epoch 22:  35%|███▍      | 104/300 [02:52<05:32,  1.70s/it]Epoch 22:  35%|███▌      | 105/300 [02:54<05:39,  1.74s/it]Epoch 22:  35%|███▌      | 106/300 [02:56<05:50,  1.80s/it]Epoch 22:  36%|███▌      | 107/300 [02:58<05:51,  1.82s/it]Epoch 22:  36%|███▌      | 108/300 [03:00<05:50,  1.83s/it]Epoch 22:  36%|███▋      | 109/300 [03:02<05:49,  1.83s/it]Epoch 22:  37%|███▋      | 110/300 [03:04<05:53,  1.86s/it]Epoch 22:  37%|███▋      | 111/300 [03:05<05:28,  1.74s/it]Epoch 22:  37%|███▋      | 112/300 [03:06<05:12,  1.66s/it]Epoch 22:  38%|███▊      | 113/300 [03:08<05:20,  1.71s/it]Epoch 22:  38%|███▊      | 114/300 [03:10<05:04,  1.64s/it]Epoch 22:  38%|███▊      | 115/300 [03:12<05:20,  1.73s/it]Epoch 22:  39%|███▊      | 116/300 [03:14<05:25,  1.77s/it]Epoch 22:  39%|███▉      | 117/300 [03:15<05:05,  1.67s/it]Epoch 22:  39%|███▉      | 118/300 [03:17<05:13,  1.72s/it]Epoch 22:  40%|███▉      | 119/300 [03:18<05:03,  1.68s/it]06/19/2022 17:47:31 - INFO - __main__ - global step: 3360; train loss: 7.704329013824463; dev loss
Epoch 22:  40%|████      | 120/300 [03:20<05:13,  1.74s/it]Epoch 22:  40%|████      | 121/300 [03:22<05:00,  1.68s/it]Epoch 22:  41%|████      | 122/300 [03:24<05:10,  1.75s/it]Epoch 22:  41%|████      | 123/300 [03:26<05:24,  1.83s/it]Epoch 22:  41%|████▏     | 124/300 [03:28<05:26,  1.86s/it]Epoch 22:  42%|████▏     | 125/300 [03:30<05:25,  1.86s/it]Epoch 22:  42%|████▏     | 126/300 [03:31<05:25,  1.87s/it]Epoch 22:  42%|████▏     | 127/300 [03:33<05:11,  1.80s/it]Epoch 22:  43%|████▎     | 128/300 [03:35<05:13,  1.82s/it]Epoch 22:  43%|████▎     | 129/300 [03:37<05:12,  1.83s/it]Epoch 22:  43%|████▎     | 130/300 [03:39<05:12,  1.84s/it]Epoch 22:  44%|████▎     | 131/300 [03:41<05:16,  1.88s/it]Epoch 22:  44%|████▍     | 132/300 [03:42<04:53,  1.75s/it]Epoch 22:  44%|████▍     | 133/300 [03:44<04:36,  1.66s/it]Epoch 22:  45%|████▍     | 134/300 [03:45<04:24,  1.59s/it]Epoch 22:  45%|████▌     | 135/300 [03:47<04:39,  1.69s/it]Epoch 22:  45%|████▌     | 136/300 [03:49<04:44,  1.73s/it]Epoch 22:  46%|████▌     | 137/300 [03:50<04:30,  1.66s/it]Epoch 22:  46%|████▌     | 138/300 [03:52<04:18,  1.59s/it]Epoch 22:  46%|████▋     | 139/300 [03:54<04:32,  1.69s/it]06/19/2022 17:48:06 - INFO - __main__ - global step: 3370; train loss: 7.545095920562744; dev loss
Epoch 22:  47%|████▋     | 140/300 [03:55<04:21,  1.63s/it]Epoch 22:  47%|████▋     | 141/300 [03:57<04:11,  1.58s/it]Epoch 22:  47%|████▋     | 142/300 [03:58<04:22,  1.66s/it]Epoch 22:  48%|████▊     | 143/300 [04:00<04:28,  1.71s/it]Epoch 22:  48%|████▊     | 144/300 [04:02<04:19,  1.66s/it]Epoch 22:  48%|████▊     | 145/300 [04:03<04:07,  1.59s/it]Epoch 22:  49%|████▊     | 146/300 [04:05<04:16,  1.67s/it]Epoch 22:  49%|████▉     | 147/300 [04:07<04:23,  1.72s/it]Epoch 22:  49%|████▉     | 148/300 [04:08<04:13,  1.67s/it]Epoch 22:  50%|████▉     | 149/300 [04:10<04:02,  1.61s/it]Epoch 22:  50%|█████     | 150/300 [04:12<04:11,  1.68s/it]Epoch 22:  50%|█████     | 151/300 [04:14<04:16,  1.72s/it]Epoch 22:  51%|█████     | 152/300 [04:15<04:07,  1.67s/it]Epoch 22:  51%|█████     | 153/300 [04:17<04:13,  1.73s/it]Epoch 22:  51%|█████▏    | 154/300 [04:19<04:17,  1.77s/it]Epoch 22:  52%|█████▏    | 155/300 [04:20<04:02,  1.67s/it]Epoch 22:  52%|█████▏    | 156/300 [04:22<03:55,  1.64s/it]Epoch 22:  52%|█████▏    | 157/300 [04:24<04:02,  1.69s/it]Epoch 22:  53%|█████▎    | 158/300 [04:25<04:06,  1.74s/it]Epoch 22:  53%|█████▎    | 159/300 [04:27<04:09,  1.77s/it]06/19/2022 17:48:40 - INFO - __main__ - global step: 3380; train loss: 8.684136390686035; dev loss
Epoch 22:  53%|█████▎    | 160/300 [04:29<03:59,  1.71s/it]Epoch 22:  54%|█████▎    | 161/300 [04:31<04:03,  1.75s/it]Epoch 22:  54%|█████▍    | 162/300 [04:32<03:49,  1.66s/it]Epoch 22:  54%|█████▍    | 163/300 [04:34<03:38,  1.60s/it]Epoch 22:  55%|█████▍    | 164/300 [04:36<03:52,  1.71s/it]Epoch 22:  55%|█████▌    | 165/300 [04:37<03:55,  1.75s/it]Epoch 22:  55%|█████▌    | 166/300 [04:39<03:58,  1.78s/it]Epoch 22:  56%|█████▌    | 167/300 [04:41<03:59,  1.80s/it]Epoch 22:  56%|█████▌    | 168/300 [04:43<03:44,  1.70s/it]Epoch 22:  56%|█████▋    | 169/300 [04:44<03:38,  1.67s/it]Epoch 22:  57%|█████▋    | 170/300 [04:46<03:28,  1.60s/it]Epoch 22:  57%|█████▋    | 171/300 [04:47<03:20,  1.55s/it]Epoch 22:  57%|█████▋    | 172/300 [04:49<03:14,  1.52s/it]Epoch 22:  58%|█████▊    | 173/300 [04:50<03:28,  1.64s/it]Epoch 22:  58%|█████▊    | 174/300 [04:52<03:34,  1.70s/it]Epoch 22:  58%|█████▊    | 175/300 [04:54<03:37,  1.74s/it]Epoch 22:  59%|█████▊    | 176/300 [04:56<03:25,  1.66s/it]Epoch 22:  59%|█████▉    | 177/300 [04:58<03:34,  1.74s/it]Epoch 22:  59%|█████▉    | 178/300 [04:59<03:35,  1.77s/it]Epoch 22:  60%|█████▉    | 179/300 [05:01<03:22,  1.68s/it]06/19/2022 17:49:13 - INFO - __main__ - global step: 3390; train loss: 8.304855346679688; dev loss
Epoch 22:  60%|██████    | 180/300 [05:02<03:12,  1.61s/it]Epoch 22:  60%|██████    | 181/300 [05:04<03:08,  1.59s/it]Epoch 22:  61%|██████    | 182/300 [05:06<03:16,  1.67s/it]Epoch 22:  61%|██████    | 183/300 [05:08<03:22,  1.73s/it]Epoch 22:  61%|██████▏   | 184/300 [05:09<03:26,  1.78s/it]Epoch 22:  62%|██████▏   | 185/300 [05:11<03:20,  1.75s/it]Epoch 22:  62%|██████▏   | 186/300 [05:13<03:11,  1.68s/it]Epoch 22:  62%|██████▏   | 187/300 [05:14<03:03,  1.63s/it]Epoch 22:  63%|██████▎   | 188/300 [05:16<02:57,  1.59s/it]Epoch 22:  63%|██████▎   | 189/300 [05:18<03:09,  1.71s/it]Epoch 22:  63%|██████▎   | 190/300 [05:20<03:14,  1.77s/it]Epoch 22:  64%|██████▎   | 191/300 [05:21<03:03,  1.69s/it]Epoch 22:  64%|██████▍   | 192/300 [05:23<02:55,  1.63s/it]Epoch 22:  64%|██████▍   | 193/300 [05:24<02:52,  1.62s/it]Epoch 22:  65%|██████▍   | 194/300 [05:26<03:00,  1.70s/it]Epoch 22:  65%|██████▌   | 195/300 [05:28<02:51,  1.64s/it]Epoch 22:  65%|██████▌   | 196/300 [05:29<02:46,  1.60s/it]Epoch 22:  66%|██████▌   | 197/300 [05:31<02:41,  1.56s/it]Epoch 22:  66%|██████▌   | 198/300 [05:32<02:40,  1.58s/it]Epoch 22:  66%|██████▋   | 199/300 [05:34<02:48,  1.67s/it]06/19/2022 17:49:46 - INFO - __main__ - global step: 3400; train loss: 7.708182334899902; dev loss
Epoch 22:  67%|██████▋   | 200/300 [05:36<02:42,  1.62s/it]Epoch 22:  67%|██████▋   | 201/300 [05:37<02:49,  1.71s/it]Epoch 22:  67%|██████▋   | 202/300 [05:39<02:57,  1.81s/it]Epoch 22:  68%|██████▊   | 203/300 [05:41<02:58,  1.84s/it]Epoch 22:  68%|██████▊   | 204/300 [05:43<02:58,  1.86s/it]Epoch 22:  68%|██████▊   | 205/300 [05:45<02:46,  1.75s/it]Epoch 22:  69%|██████▊   | 206/300 [05:46<02:41,  1.71s/it]Epoch 22:  69%|██████▉   | 207/300 [05:48<02:32,  1.64s/it]Epoch 22:  69%|██████▉   | 208/300 [05:49<02:26,  1.59s/it]Epoch 22:  70%|██████▉   | 209/300 [05:51<02:33,  1.68s/it]Epoch 22:  70%|███████   | 210/300 [05:53<02:30,  1.67s/it]Epoch 22:  70%|███████   | 211/300 [05:55<02:34,  1.74s/it]Epoch 22:  71%|███████   | 212/300 [05:57<02:37,  1.79s/it]Epoch 22:  71%|███████   | 213/300 [05:58<02:27,  1.70s/it]Epoch 22:  71%|███████▏  | 214/300 [06:00<02:34,  1.79s/it]Epoch 22:  72%|███████▏  | 215/300 [06:02<02:24,  1.70s/it]Epoch 22:  72%|███████▏  | 216/300 [06:04<02:27,  1.76s/it]Epoch 22:  72%|███████▏  | 217/300 [06:05<02:29,  1.80s/it]Epoch 22:  73%|███████▎  | 218/300 [06:07<02:23,  1.75s/it]Epoch 22:  73%|███████▎  | 219/300 [06:09<02:25,  1.79s/it]06/19/2022 17:50:22 - INFO - __main__ - global step: 3410; train loss: 8.071121215820312; dev loss
Epoch 22:  73%|███████▎  | 220/300 [06:11<02:26,  1.83s/it]Epoch 22:  74%|███████▎  | 221/300 [06:13<02:25,  1.85s/it]Epoch 22:  74%|███████▍  | 222/300 [06:15<02:24,  1.86s/it]Epoch 22:  74%|███████▍  | 223/300 [06:17<02:26,  1.90s/it]Epoch 22:  75%|███████▍  | 224/300 [06:18<02:15,  1.78s/it]Epoch 22:  75%|███████▌  | 225/300 [06:20<02:07,  1.70s/it]Epoch 22:  75%|███████▌  | 226/300 [06:22<02:09,  1.75s/it]Epoch 22:  76%|███████▌  | 227/300 [06:24<02:13,  1.82s/it]Epoch 22:  76%|███████▌  | 228/300 [06:25<02:04,  1.72s/it]Epoch 22:  76%|███████▋  | 229/300 [06:27<01:57,  1.66s/it]Epoch 22:  77%|███████▋  | 230/300 [06:28<02:00,  1.73s/it]Epoch 22:  77%|███████▋  | 231/300 [06:30<01:56,  1.69s/it]Epoch 22:  77%|███████▋  | 232/300 [06:32<01:58,  1.75s/it]Epoch 22:  78%|███████▊  | 233/300 [06:33<01:51,  1.67s/it]Epoch 22:  78%|███████▊  | 234/300 [06:35<01:46,  1.62s/it]Epoch 22:  78%|███████▊  | 235/300 [06:36<01:44,  1.61s/it]Epoch 22:  79%|███████▊  | 236/300 [06:38<01:41,  1.58s/it]Epoch 22:  79%|███████▉  | 237/300 [06:39<01:37,  1.55s/it]Epoch 22:  79%|███████▉  | 238/300 [06:41<01:35,  1.54s/it]Epoch 22:  80%|███████▉  | 239/300 [06:43<01:42,  1.68s/it]06/19/2022 17:50:56 - INFO - __main__ - global step: 3420; train loss: 8.385845184326172; dev loss
Epoch 22:  80%|████████  | 240/300 [06:45<01:44,  1.74s/it]Epoch 22:  80%|████████  | 241/300 [06:46<01:38,  1.66s/it]Epoch 22:  81%|████████  | 242/300 [06:48<01:33,  1.61s/it]Epoch 22:  81%|████████  | 243/300 [06:50<01:38,  1.72s/it]Epoch 22:  81%|████████▏ | 244/300 [06:51<01:32,  1.65s/it]Epoch 22:  82%|████████▏ | 245/300 [06:53<01:27,  1.60s/it]Epoch 22:  82%|████████▏ | 246/300 [06:54<01:24,  1.57s/it]Epoch 22:  82%|████████▏ | 247/300 [06:56<01:23,  1.57s/it]Epoch 22:  83%|████████▎ | 248/300 [06:58<01:26,  1.67s/it]Epoch 22:  83%|████████▎ | 249/300 [06:59<01:22,  1.62s/it]Epoch 22:  83%|████████▎ | 250/300 [07:01<01:19,  1.58s/it]Epoch 22:  84%|████████▎ | 251/300 [07:03<01:22,  1.68s/it]Epoch 22:  84%|████████▍ | 252/300 [07:04<01:19,  1.65s/it]Epoch 22:  84%|████████▍ | 253/300 [07:06<01:20,  1.72s/it]Epoch 22:  85%|████████▍ | 254/300 [07:08<01:21,  1.77s/it]Epoch 22:  85%|████████▌ | 255/300 [07:10<01:15,  1.68s/it]Epoch 22:  85%|████████▌ | 256/300 [07:11<01:13,  1.66s/it]Epoch 22:  86%|████████▌ | 257/300 [07:13<01:09,  1.61s/it]Epoch 22:  86%|████████▌ | 258/300 [07:14<01:10,  1.68s/it]Epoch 22:  86%|████████▋ | 259/300 [07:16<01:06,  1.63s/it]06/19/2022 17:51:29 - INFO - __main__ - global step: 3430; train loss: 8.203914642333984; dev loss
Epoch 22:  87%|████████▋ | 260/300 [07:18<01:09,  1.74s/it]Epoch 22:  87%|████████▋ | 261/300 [07:19<01:04,  1.66s/it]Epoch 22:  87%|████████▋ | 262/300 [07:21<01:05,  1.73s/it]Epoch 22:  88%|████████▊ | 263/300 [07:23<01:01,  1.66s/it]Epoch 22:  88%|████████▊ | 264/300 [07:25<01:03,  1.76s/it]Epoch 22:  88%|████████▊ | 265/300 [07:27<01:02,  1.80s/it]Epoch 22:  89%|████████▊ | 266/300 [07:29<01:02,  1.83s/it]Epoch 22:  89%|████████▉ | 267/300 [07:30<00:57,  1.73s/it]Epoch 22:  89%|████████▉ | 268/300 [07:32<00:54,  1.69s/it]Epoch 22:  90%|████████▉ | 269/300 [07:34<00:54,  1.76s/it]Epoch 22:  90%|█████████ | 270/300 [07:35<00:50,  1.68s/it]Epoch 22:  90%|█████████ | 271/300 [07:37<00:47,  1.63s/it]Epoch 22:  91%|█████████ | 272/300 [07:39<00:48,  1.73s/it]Epoch 22:  91%|█████████ | 273/300 [07:40<00:44,  1.66s/it]Epoch 22:  91%|█████████▏| 274/300 [07:42<00:44,  1.73s/it]Epoch 22:  92%|█████████▏| 275/300 [07:43<00:41,  1.66s/it]Epoch 22:  92%|█████████▏| 276/300 [07:45<00:38,  1.61s/it]Epoch 22:  92%|█████████▏| 277/300 [07:47<00:37,  1.62s/it]Epoch 22:  93%|█████████▎| 278/300 [07:48<00:34,  1.58s/it]Epoch 22:  93%|█████████▎| 279/300 [07:50<00:32,  1.56s/it]06/19/2022 17:52:02 - INFO - __main__ - global step: 3440; train loss: 7.720264434814453; dev loss
Epoch 22:  93%|█████████▎| 280/300 [07:52<00:33,  1.66s/it]Epoch 22:  94%|█████████▎| 281/300 [07:53<00:31,  1.65s/it]Epoch 22:  94%|█████████▍| 282/300 [07:55<00:30,  1.72s/it]Epoch 22:  94%|█████████▍| 283/300 [07:57<00:28,  1.66s/it]Epoch 22:  95%|█████████▍| 284/300 [07:58<00:27,  1.73s/it]Epoch 22:  95%|█████████▌| 285/300 [08:00<00:27,  1.82s/it]Epoch 22:  95%|█████████▌| 286/300 [08:02<00:25,  1.84s/it]Epoch 22:  96%|█████████▌| 287/300 [08:04<00:24,  1.86s/it]Epoch 22:  96%|█████████▌| 288/300 [08:06<00:21,  1.75s/it]Epoch 22:  96%|█████████▋| 289/300 [08:08<00:20,  1.83s/it]Epoch 22:  97%|█████████▋| 290/300 [08:10<00:18,  1.85s/it]Epoch 22:  97%|█████████▋| 291/300 [08:11<00:15,  1.74s/it]Epoch 22:  97%|█████████▋| 292/300 [08:13<00:14,  1.79s/it]Epoch 22:  98%|█████████▊| 293/300 [08:15<00:13,  1.86s/it]Epoch 22:  98%|█████████▊| 294/300 [08:17<00:10,  1.76s/it]Epoch 22:  98%|█████████▊| 295/300 [08:18<00:08,  1.67s/it]Epoch 22:  99%|█████████▊| 296/300 [08:20<00:06,  1.62s/it]Epoch 22:  99%|█████████▉| 297/300 [08:22<00:05,  1.73s/it]Epoch 22:  99%|█████████▉| 298/300 [08:23<00:03,  1.78s/it]Epoch 22: 100%|█████████▉| 299/300 [08:25<00:01,  1.70s/it]06/19/2022 17:52:37 - INFO - __main__ - global step: 3450; train loss: 7.676931858062744; dev loss
Epoch 22: 100%|██████████| 300/300 [08:26<00:00,  1.64s/it]Epoch 22: 100%|██████████| 300/300 [08:26<00:00,  1.69s/it]
Epoch 23:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 23:   0%|          | 1/300 [00:01<08:02,  1.61s/it]Epoch 23:   1%|          | 2/300 [00:03<08:48,  1.77s/it]Epoch 23:   1%|          | 3/300 [00:04<08:07,  1.64s/it]Epoch 23:   1%|▏         | 4/300 [00:06<08:34,  1.74s/it]Epoch 23:   2%|▏         | 5/300 [00:08<08:51,  1.80s/it]Epoch 23:   2%|▏         | 6/300 [00:10<08:31,  1.74s/it]Epoch 23:   2%|▏         | 7/300 [00:12<08:46,  1.80s/it]Epoch 23:   3%|▎         | 8/300 [00:13<08:18,  1.71s/it]Epoch 23:   3%|▎         | 9/300 [00:15<08:33,  1.77s/it]Epoch 23:   3%|▎         | 10/300 [00:17<08:20,  1.73s/it]Epoch 23:   4%|▎         | 11/300 [00:18<08:00,  1.66s/it]Epoch 23:   4%|▍         | 12/300 [00:20<07:46,  1.62s/it]Epoch 23:   4%|▍         | 13/300 [00:21<07:33,  1.58s/it]Epoch 23:   5%|▍         | 14/300 [00:23<08:07,  1.70s/it]Epoch 23:   5%|▌         | 15/300 [00:25<07:48,  1.65s/it]Epoch 23:   5%|▌         | 16/300 [00:26<07:35,  1.60s/it]Epoch 23:   6%|▌         | 17/300 [00:28<07:59,  1.69s/it]Epoch 23:   6%|▌         | 18/300 [00:30<08:25,  1.79s/it]Epoch 23:   6%|▋         | 19/300 [00:32<08:00,  1.71s/it]06/19/2022 17:53:11 - INFO - __main__ - global step: 3460; train loss: 8.235722541809082; dev loss
Epoch 23:   7%|▋         | 20/300 [00:34<08:13,  1.76s/it]Epoch 23:   7%|▋         | 21/300 [00:36<08:22,  1.80s/it]Epoch 23:   7%|▋         | 22/300 [00:37<08:03,  1.74s/it]Epoch 23:   8%|▊         | 23/300 [00:39<08:14,  1.78s/it]Epoch 23:   8%|▊         | 24/300 [00:41<07:49,  1.70s/it]Epoch 23:   8%|▊         | 25/300 [00:42<07:31,  1.64s/it]Epoch 23:   9%|▊         | 26/300 [00:44<08:01,  1.76s/it]Epoch 23:   9%|▉         | 27/300 [00:46<07:38,  1.68s/it]Epoch 23:   9%|▉         | 28/300 [00:48<07:53,  1.74s/it]Epoch 23:  10%|▉         | 29/300 [00:49<07:33,  1.67s/it]Epoch 23:  10%|█         | 30/300 [00:51<07:49,  1.74s/it]Epoch 23:  10%|█         | 31/300 [00:53<07:34,  1.69s/it]Epoch 23:  11%|█         | 32/300 [00:54<07:49,  1.75s/it]Epoch 23:  11%|█         | 33/300 [00:56<07:58,  1.79s/it]Epoch 23:  11%|█▏        | 34/300 [00:58<08:05,  1.82s/it]Epoch 23:  12%|█▏        | 35/300 [01:00<07:45,  1.75s/it]Epoch 23:  12%|█▏        | 36/300 [01:02<07:53,  1.79s/it]Epoch 23:  12%|█▏        | 37/300 [01:04<07:58,  1.82s/it]Epoch 23:  13%|█▎        | 38/300 [01:05<08:02,  1.84s/it]Epoch 23:  13%|█▎        | 39/300 [01:07<07:44,  1.78s/it]06/19/2022 17:53:46 - INFO - __main__ - global step: 3470; train loss: 7.996145725250244; dev loss
Epoch 23:  13%|█▎        | 40/300 [01:09<07:18,  1.69s/it]Epoch 23:  14%|█▎        | 41/300 [01:10<07:00,  1.62s/it]Epoch 23:  14%|█▍        | 42/300 [01:12<07:21,  1.71s/it]Epoch 23:  14%|█▍        | 43/300 [01:14<07:12,  1.68s/it]Epoch 23:  15%|█▍        | 44/300 [01:15<07:28,  1.75s/it]Epoch 23:  15%|█▌        | 45/300 [01:17<07:37,  1.79s/it]Epoch 23:  15%|█▌        | 46/300 [01:19<07:42,  1.82s/it]Epoch 23:  16%|█▌        | 47/300 [01:21<07:55,  1.88s/it]Epoch 23:  16%|█▌        | 48/300 [01:23<07:55,  1.89s/it]Epoch 23:  16%|█▋        | 49/300 [01:25<07:53,  1.89s/it]Epoch 23:  17%|█▋        | 50/300 [01:27<07:22,  1.77s/it]Epoch 23:  17%|█▋        | 51/300 [01:29<07:37,  1.84s/it]Epoch 23:  17%|█▋        | 52/300 [01:30<07:40,  1.85s/it]Epoch 23:  18%|█▊        | 53/300 [01:32<07:10,  1.74s/it]Epoch 23:  18%|█▊        | 54/300 [01:34<07:23,  1.80s/it]Epoch 23:  18%|█▊        | 55/300 [01:36<07:38,  1.87s/it]Epoch 23:  19%|█▊        | 56/300 [01:37<07:14,  1.78s/it]Epoch 23:  19%|█▉        | 57/300 [01:40<07:33,  1.87s/it]Epoch 23:  19%|█▉        | 58/300 [01:42<07:45,  1.92s/it]Epoch 23:  20%|█▉        | 59/300 [01:43<07:37,  1.90s/it]06/19/2022 17:54:23 - INFO - __main__ - global step: 3480; train loss: 8.220816612243652; dev loss
Epoch 23:  20%|██        | 60/300 [01:45<07:38,  1.91s/it]Epoch 23:  20%|██        | 61/300 [01:47<07:50,  1.97s/it]Epoch 23:  21%|██        | 62/300 [01:50<07:55,  2.00s/it]Epoch 23:  21%|██        | 63/300 [01:51<07:41,  1.95s/it]Epoch 23:  21%|██▏       | 64/300 [01:53<07:38,  1.94s/it]Epoch 23:  22%|██▏       | 65/300 [01:55<07:43,  1.97s/it]Epoch 23:  22%|██▏       | 66/300 [01:57<07:48,  2.00s/it]Epoch 23:  22%|██▏       | 67/300 [01:59<07:50,  2.02s/it]Epoch 23:  23%|██▎       | 68/300 [02:01<07:39,  1.98s/it]Epoch 23:  23%|██▎       | 69/300 [02:03<07:27,  1.94s/it]Epoch 23:  23%|██▎       | 70/300 [02:05<07:16,  1.90s/it]Epoch 23:  24%|██▎       | 71/300 [02:07<07:25,  1.95s/it]Epoch 23:  24%|██▍       | 72/300 [02:09<07:40,  2.02s/it]Epoch 23:  24%|██▍       | 73/300 [02:11<07:24,  1.96s/it]Epoch 23:  25%|██▍       | 74/300 [02:13<07:21,  1.95s/it]Epoch 23:  25%|██▌       | 75/300 [02:14<06:48,  1.82s/it]Epoch 23:  25%|██▌       | 76/300 [02:16<06:31,  1.75s/it]Epoch 23:  26%|██▌       | 77/300 [02:18<06:12,  1.67s/it]Epoch 23:  26%|██▌       | 78/300 [02:19<06:24,  1.73s/it]Epoch 23:  26%|██▋       | 79/300 [02:21<06:33,  1.78s/it]06/19/2022 17:55:01 - INFO - __main__ - global step: 3490; train loss: 7.825948238372803; dev loss
Epoch 23:  27%|██▋       | 80/300 [02:23<06:20,  1.73s/it]Epoch 23:  27%|██▋       | 81/300 [02:24<06:02,  1.66s/it]Epoch 23:  27%|██▋       | 82/300 [02:26<05:48,  1.60s/it]Epoch 23:  28%|██▊       | 83/300 [02:27<05:38,  1.56s/it]Epoch 23:  28%|██▊       | 84/300 [02:29<05:56,  1.65s/it]Epoch 23:  28%|██▊       | 85/300 [02:31<06:16,  1.75s/it]Epoch 23:  29%|██▊       | 86/300 [02:33<05:58,  1.68s/it]Epoch 23:  29%|██▉       | 87/300 [02:35<06:10,  1.74s/it]Epoch 23:  29%|██▉       | 88/300 [02:36<05:53,  1.67s/it]Epoch 23:  30%|██▉       | 89/300 [02:38<05:46,  1.64s/it]Epoch 23:  30%|███       | 90/300 [02:39<05:35,  1.60s/it]Epoch 23:  30%|███       | 91/300 [02:41<05:51,  1.68s/it]Epoch 23:  31%|███       | 92/300 [02:43<05:39,  1.63s/it]Epoch 23:  31%|███       | 93/300 [02:45<05:59,  1.74s/it]Epoch 23:  31%|███▏      | 94/300 [02:46<05:42,  1.66s/it]Epoch 23:  32%|███▏      | 95/300 [02:48<05:54,  1.73s/it]Epoch 23:  32%|███▏      | 96/300 [02:49<05:37,  1.65s/it]Epoch 23:  32%|███▏      | 97/300 [02:51<05:32,  1.64s/it]Epoch 23:  33%|███▎      | 98/300 [02:53<05:22,  1.59s/it]Epoch 23:  33%|███▎      | 99/300 [02:54<05:14,  1.56s/it]06/19/2022 17:55:34 - INFO - __main__ - global step: 3500; train loss: 8.034594535827637; dev loss
Epoch 23:  33%|███▎      | 100/300 [02:56<05:30,  1.65s/it]Epoch 23:  34%|███▎      | 101/300 [02:57<05:24,  1.63s/it]Epoch 23:  34%|███▍      | 102/300 [02:59<05:14,  1.59s/it]Epoch 23:  34%|███▍      | 103/300 [03:01<05:30,  1.68s/it]Epoch 23:  35%|███▍      | 104/300 [03:03<05:43,  1.75s/it]Epoch 23:  35%|███▌      | 105/300 [03:05<05:54,  1.82s/it]Epoch 23:  35%|███▌      | 106/300 [03:07<05:55,  1.83s/it]Epoch 23:  36%|███▌      | 107/300 [03:08<05:35,  1.74s/it]Epoch 23:  36%|███▌      | 108/300 [03:10<05:19,  1.67s/it]Epoch 23:  36%|███▋      | 109/300 [03:11<05:13,  1.64s/it]Epoch 23:  37%|███▋      | 110/300 [03:13<05:25,  1.71s/it]Epoch 23:  37%|███▋      | 111/300 [03:15<05:11,  1.65s/it]Epoch 23:  37%|███▋      | 112/300 [03:16<05:23,  1.72s/it]Epoch 23:  38%|███▊      | 113/300 [03:18<05:30,  1.77s/it]Epoch 23:  38%|███▊      | 114/300 [03:20<05:41,  1.83s/it]Epoch 23:  38%|███▊      | 115/300 [03:22<05:42,  1.85s/it]Epoch 23:  39%|███▊      | 116/300 [03:24<05:43,  1.87s/it]Epoch 23:  39%|███▉      | 117/300 [03:26<05:42,  1.87s/it]Epoch 23:  39%|███▉      | 118/300 [03:28<05:47,  1.91s/it]Epoch 23:  40%|███▉      | 119/300 [03:30<05:44,  1.90s/it]06/19/2022 17:56:09 - INFO - __main__ - global step: 3510; train loss: 7.779411315917969; dev loss
Epoch 23:  40%|████      | 120/300 [03:32<05:41,  1.90s/it]Epoch 23:  40%|████      | 121/300 [03:33<05:18,  1.78s/it]Epoch 23:  41%|████      | 122/300 [03:35<05:27,  1.84s/it]Epoch 23:  41%|████      | 123/300 [03:37<05:08,  1.74s/it]Epoch 23:  41%|████▏     | 124/300 [03:39<05:14,  1.79s/it]Epoch 23:  42%|████▏     | 125/300 [03:40<04:58,  1.71s/it]Epoch 23:  42%|████▏     | 126/300 [03:42<05:14,  1.81s/it]Epoch 23:  42%|████▏     | 127/300 [03:44<05:19,  1.85s/it]Epoch 23:  43%|████▎     | 128/300 [03:46<05:03,  1.77s/it]Epoch 23:  43%|████▎     | 129/300 [03:48<05:09,  1.81s/it]Epoch 23:  43%|████▎     | 130/300 [03:49<04:59,  1.76s/it]Epoch 23:  44%|████▎     | 131/300 [03:51<04:45,  1.69s/it]Epoch 23:  44%|████▍     | 132/300 [03:52<04:38,  1.66s/it]Epoch 23:  44%|████▍     | 133/300 [03:54<04:31,  1.62s/it]Epoch 23:  45%|████▍     | 134/300 [03:56<04:50,  1.75s/it]Epoch 23:  45%|████▌     | 135/300 [03:58<04:38,  1.69s/it]Epoch 23:  45%|████▌     | 136/300 [03:59<04:29,  1.64s/it]Epoch 23:  46%|████▌     | 137/300 [04:01<04:41,  1.73s/it]Epoch 23:  46%|████▌     | 138/300 [04:03<04:50,  1.79s/it]Epoch 23:  46%|████▋     | 139/300 [04:05<04:45,  1.77s/it]06/19/2022 17:56:44 - INFO - __main__ - global step: 3520; train loss: 8.100126266479492; dev loss
Epoch 23:  47%|████▋     | 140/300 [04:06<04:30,  1.69s/it]Epoch 23:  47%|████▋     | 141/300 [04:08<04:18,  1.63s/it]Epoch 23:  47%|████▋     | 142/300 [04:09<04:11,  1.59s/it]Epoch 23:  48%|████▊     | 143/300 [04:11<04:27,  1.71s/it]Epoch 23:  48%|████▊     | 144/300 [04:13<04:34,  1.76s/it]Epoch 23:  48%|████▊     | 145/300 [04:15<04:22,  1.69s/it]Epoch 23:  49%|████▊     | 146/300 [04:16<04:14,  1.65s/it]Epoch 23:  49%|████▉     | 147/300 [04:18<04:28,  1.75s/it]Epoch 23:  49%|████▉     | 148/300 [04:20<04:14,  1.68s/it]Epoch 23:  50%|████▉     | 149/300 [04:21<04:03,  1.62s/it]Epoch 23:  50%|█████     | 150/300 [04:23<03:56,  1.58s/it]Epoch 23:  50%|█████     | 151/300 [04:25<04:14,  1.70s/it]Epoch 23:  51%|█████     | 152/300 [04:26<04:20,  1.76s/it]Epoch 23:  51%|█████     | 153/300 [04:28<04:07,  1.68s/it]Epoch 23:  51%|█████▏    | 154/300 [04:29<03:57,  1.63s/it]Epoch 23:  52%|█████▏    | 155/300 [04:31<03:56,  1.63s/it]Epoch 23:  52%|█████▏    | 156/300 [04:33<04:05,  1.71s/it]Epoch 23:  52%|█████▏    | 157/300 [04:34<03:55,  1.65s/it]Epoch 23:  53%|█████▎    | 158/300 [04:36<03:47,  1.60s/it]Epoch 23:  53%|█████▎    | 159/300 [04:38<03:45,  1.60s/it]06/19/2022 17:57:17 - INFO - __main__ - global step: 3530; train loss: 7.968113899230957; dev loss
Epoch 23:  53%|█████▎    | 160/300 [04:39<03:56,  1.69s/it]Epoch 23:  54%|█████▎    | 161/300 [04:41<04:03,  1.75s/it]Epoch 23:  54%|█████▍    | 162/300 [04:43<04:07,  1.79s/it]Epoch 23:  54%|█████▍    | 163/300 [04:45<03:58,  1.74s/it]Epoch 23:  55%|█████▍    | 164/300 [04:47<04:03,  1.79s/it]Epoch 23:  55%|█████▌    | 165/300 [04:48<03:50,  1.71s/it]Epoch 23:  55%|█████▌    | 166/300 [04:50<03:56,  1.76s/it]Epoch 23:  56%|█████▌    | 167/300 [04:52<03:59,  1.80s/it]Epoch 23:  56%|█████▌    | 168/300 [04:54<04:06,  1.87s/it]Epoch 23:  56%|█████▋    | 169/300 [04:56<03:50,  1.76s/it]Epoch 23:  57%|█████▋    | 170/300 [04:58<03:54,  1.80s/it]Epoch 23:  57%|█████▋    | 171/300 [04:59<03:55,  1.83s/it]Epoch 23:  57%|█████▋    | 172/300 [05:01<04:00,  1.88s/it]Epoch 23:  58%|█████▊    | 173/300 [05:03<03:59,  1.88s/it]Epoch 23:  58%|█████▊    | 174/300 [05:05<03:57,  1.89s/it]Epoch 23:  58%|█████▊    | 175/300 [05:07<03:56,  1.89s/it]Epoch 23:  59%|█████▊    | 176/300 [05:09<03:43,  1.80s/it]Epoch 23:  59%|█████▉    | 177/300 [05:11<03:45,  1.83s/it]Epoch 23:  59%|█████▉    | 178/300 [05:12<03:31,  1.73s/it]Epoch 23:  60%|█████▉    | 179/300 [05:14<03:22,  1.67s/it]06/19/2022 17:57:53 - INFO - __main__ - global step: 3540; train loss: 7.709570407867432; dev loss
Epoch 23:  60%|██████    | 180/300 [05:15<03:19,  1.66s/it]Epoch 23:  60%|██████    | 181/300 [05:17<03:14,  1.63s/it]Epoch 23:  61%|██████    | 182/300 [05:18<03:09,  1.60s/it]Epoch 23:  61%|██████    | 183/300 [05:20<03:18,  1.70s/it]Epoch 23:  61%|██████▏   | 184/300 [05:22<03:26,  1.78s/it]Epoch 23:  62%|██████▏   | 185/300 [05:24<03:14,  1.69s/it]Epoch 23:  62%|██████▏   | 186/300 [05:25<03:06,  1.64s/it]Epoch 23:  62%|██████▏   | 187/300 [05:27<03:00,  1.60s/it]Epoch 23:  63%|██████▎   | 188/300 [05:29<03:12,  1.72s/it]Epoch 23:  63%|██████▎   | 189/300 [05:31<03:16,  1.77s/it]Epoch 23:  63%|██████▎   | 190/300 [05:32<03:06,  1.69s/it]Epoch 23:  64%|██████▎   | 191/300 [05:34<02:58,  1.64s/it]Epoch 23:  64%|██████▍   | 192/300 [05:36<03:05,  1.71s/it]Epoch 23:  64%|██████▍   | 193/300 [05:38<03:12,  1.80s/it]Epoch 23:  65%|██████▍   | 194/300 [05:39<03:01,  1.71s/it]Epoch 23:  65%|██████▌   | 195/300 [05:41<03:04,  1.76s/it]Epoch 23:  65%|██████▌   | 196/300 [05:42<02:54,  1.68s/it]Epoch 23:  66%|██████▌   | 197/300 [05:44<02:50,  1.66s/it]Epoch 23:  66%|██████▌   | 198/300 [05:46<02:56,  1.73s/it]Epoch 23:  66%|██████▋   | 199/300 [05:47<02:47,  1.66s/it]06/19/2022 17:58:27 - INFO - __main__ - global step: 3550; train loss: 8.18218994140625; dev loss
Epoch 23:  67%|██████▋   | 200/300 [05:49<02:40,  1.60s/it]Epoch 23:  67%|██████▋   | 201/300 [05:51<02:49,  1.71s/it]Epoch 23:  67%|██████▋   | 202/300 [05:53<02:53,  1.77s/it]Epoch 23:  68%|██████▊   | 203/300 [05:55<02:54,  1.80s/it]Epoch 23:  68%|██████▊   | 204/300 [05:56<02:44,  1.71s/it]Epoch 23:  68%|██████▊   | 205/300 [05:58<02:39,  1.68s/it]Epoch 23:  69%|██████▊   | 206/300 [05:59<02:32,  1.62s/it]Epoch 23:  69%|██████▉   | 207/300 [06:01<02:37,  1.70s/it]Epoch 23:  69%|██████▉   | 208/300 [06:03<02:41,  1.75s/it]Epoch 23:  70%|██████▉   | 209/300 [06:05<02:36,  1.72s/it]Epoch 23:  70%|███████   | 210/300 [06:06<02:28,  1.65s/it]Epoch 23:  70%|███████   | 211/300 [06:08<02:22,  1.60s/it]Epoch 23:  71%|███████   | 212/300 [06:09<02:28,  1.68s/it]Epoch 23:  71%|███████   | 213/300 [06:11<02:24,  1.66s/it]Epoch 23:  71%|███████▏  | 214/300 [06:13<02:28,  1.73s/it]Epoch 23:  72%|███████▏  | 215/300 [06:14<02:20,  1.66s/it]Epoch 23:  72%|███████▏  | 216/300 [06:16<02:25,  1.73s/it]Epoch 23:  72%|███████▏  | 217/300 [06:18<02:30,  1.81s/it]Epoch 23:  73%|███████▎  | 218/300 [06:20<02:20,  1.71s/it]Epoch 23:  73%|███████▎  | 219/300 [06:22<02:22,  1.76s/it]06/19/2022 17:59:01 - INFO - __main__ - global step: 3560; train loss: 8.189486503601074; dev loss
Epoch 23:  73%|███████▎  | 220/300 [06:23<02:14,  1.68s/it]Epoch 23:  74%|███████▎  | 221/300 [06:25<02:08,  1.63s/it]Epoch 23:  74%|███████▍  | 222/300 [06:27<02:16,  1.75s/it]Epoch 23:  74%|███████▍  | 223/300 [06:28<02:08,  1.67s/it]Epoch 23:  75%|███████▍  | 224/300 [06:30<02:11,  1.73s/it]Epoch 23:  75%|███████▌  | 225/300 [06:32<02:12,  1.77s/it]Epoch 23:  75%|███████▌  | 226/300 [06:34<02:07,  1.72s/it]Epoch 23:  76%|███████▌  | 227/300 [06:35<02:00,  1.66s/it]Epoch 23:  76%|███████▌  | 228/300 [06:37<02:04,  1.72s/it]Epoch 23:  76%|███████▋  | 229/300 [06:39<02:06,  1.78s/it]Epoch 23:  77%|███████▋  | 230/300 [06:40<02:00,  1.72s/it]Epoch 23:  77%|███████▋  | 231/300 [06:42<01:54,  1.66s/it]Epoch 23:  77%|███████▋  | 232/300 [06:44<01:57,  1.73s/it]Epoch 23:  78%|███████▊  | 233/300 [06:45<01:50,  1.65s/it]Epoch 23:  78%|███████▊  | 234/300 [06:47<01:55,  1.75s/it]Epoch 23:  78%|███████▊  | 235/300 [06:49<01:48,  1.67s/it]Epoch 23:  79%|███████▊  | 236/300 [06:50<01:43,  1.62s/it]Epoch 23:  79%|███████▉  | 237/300 [06:52<01:46,  1.70s/it]Epoch 23:  79%|███████▉  | 238/300 [06:54<01:43,  1.67s/it]Epoch 23:  80%|███████▉  | 239/300 [06:55<01:38,  1.62s/it]06/19/2022 17:59:35 - INFO - __main__ - global step: 3570; train loss: 8.07397747039795; dev loss
Epoch 23:  80%|████████  | 240/300 [06:57<01:41,  1.69s/it]Epoch 23:  80%|████████  | 241/300 [06:59<01:36,  1.63s/it]Epoch 23:  81%|████████  | 242/300 [07:01<01:41,  1.75s/it]Epoch 23:  81%|████████  | 243/300 [07:03<01:41,  1.79s/it]Epoch 23:  81%|████████▏ | 244/300 [07:04<01:41,  1.81s/it]Epoch 23:  82%|████████▏ | 245/300 [07:06<01:40,  1.83s/it]Epoch 23:  82%|████████▏ | 246/300 [07:08<01:39,  1.85s/it]Epoch 23:  82%|████████▏ | 247/300 [07:10<01:34,  1.78s/it]Epoch 23:  83%|████████▎ | 248/300 [07:11<01:28,  1.70s/it]Epoch 23:  83%|████████▎ | 249/300 [07:13<01:23,  1.64s/it]Epoch 23:  83%|████████▎ | 250/300 [07:15<01:25,  1.71s/it]Epoch 23:  84%|████████▎ | 251/300 [07:17<01:27,  1.79s/it]Epoch 23:  84%|████████▍ | 252/300 [07:18<01:26,  1.79s/it]Epoch 23:  84%|████████▍ | 253/300 [07:20<01:25,  1.82s/it]Epoch 23:  85%|████████▍ | 254/300 [07:22<01:24,  1.84s/it]Epoch 23:  85%|████████▌ | 255/300 [07:24<01:20,  1.78s/it]Epoch 23:  85%|████████▌ | 256/300 [07:25<01:14,  1.69s/it]Epoch 23:  86%|████████▌ | 257/300 [07:27<01:15,  1.75s/it]Epoch 23:  86%|████████▌ | 258/300 [07:29<01:10,  1.68s/it]Epoch 23:  86%|████████▋ | 259/300 [07:31<01:12,  1.78s/it]06/19/2022 18:00:10 - INFO - __main__ - global step: 3580; train loss: 7.839766025543213; dev loss
Epoch 23:  87%|████████▋ | 260/300 [07:32<01:07,  1.70s/it]Epoch 23:  87%|████████▋ | 261/300 [07:34<01:08,  1.75s/it]Epoch 23:  87%|████████▋ | 262/300 [07:36<01:03,  1.68s/it]Epoch 23:  88%|████████▊ | 263/300 [07:38<01:05,  1.77s/it]Epoch 23:  88%|████████▊ | 264/300 [07:39<01:01,  1.69s/it]Epoch 23:  88%|████████▊ | 265/300 [07:41<01:01,  1.76s/it]Epoch 23:  89%|████████▊ | 266/300 [07:43<00:56,  1.68s/it]Epoch 23:  89%|████████▉ | 267/300 [07:45<00:58,  1.77s/it]Epoch 23:  89%|████████▉ | 268/300 [07:46<00:57,  1.81s/it]Epoch 23:  90%|████████▉ | 269/300 [07:48<00:53,  1.72s/it]Epoch 23:  90%|█████████ | 270/300 [07:50<00:53,  1.77s/it]Epoch 23:  90%|█████████ | 271/300 [07:51<00:50,  1.73s/it]Epoch 23:  91%|█████████ | 272/300 [07:53<00:49,  1.78s/it]Epoch 23:  91%|█████████ | 273/300 [07:55<00:48,  1.81s/it]Epoch 23:  91%|█████████▏| 274/300 [07:57<00:47,  1.83s/it]Epoch 23:  92%|█████████▏| 275/300 [07:59<00:43,  1.73s/it]Epoch 23:  92%|█████████▏| 276/300 [08:00<00:40,  1.70s/it]Epoch 23:  92%|█████████▏| 277/300 [08:02<00:40,  1.75s/it]Epoch 23:  93%|█████████▎| 278/300 [08:04<00:36,  1.67s/it]Epoch 23:  93%|█████████▎| 279/300 [08:05<00:36,  1.74s/it]06/19/2022 18:00:45 - INFO - __main__ - global step: 3590; train loss: 7.637424468994141; dev loss
Epoch 23:  93%|█████████▎| 280/300 [08:07<00:36,  1.81s/it]Epoch 23:  94%|█████████▎| 281/300 [08:09<00:35,  1.85s/it]Epoch 23:  94%|█████████▍| 282/300 [08:11<00:33,  1.86s/it]Epoch 23:  94%|█████████▍| 283/300 [08:13<00:29,  1.75s/it]Epoch 23:  95%|█████████▍| 284/300 [08:15<00:29,  1.82s/it]Epoch 23:  95%|█████████▌| 285/300 [08:17<00:27,  1.84s/it]Epoch 23:  95%|█████████▌| 286/300 [08:18<00:24,  1.74s/it]Epoch 23:  96%|█████████▌| 287/300 [08:20<00:23,  1.79s/it]Epoch 23:  96%|█████████▌| 288/300 [08:22<00:20,  1.73s/it]Epoch 23:  96%|█████████▋| 289/300 [08:24<00:19,  1.77s/it]Epoch 23:  97%|█████████▋| 290/300 [08:25<00:18,  1.81s/it]Epoch 23:  97%|█████████▋| 291/300 [08:27<00:16,  1.84s/it]Epoch 23:  97%|█████████▋| 292/300 [08:29<00:14,  1.77s/it]Epoch 23:  98%|█████████▊| 293/300 [08:30<00:11,  1.69s/it]Epoch 23:  98%|█████████▊| 294/300 [08:32<00:09,  1.63s/it]Epoch 23:  98%|█████████▊| 295/300 [08:33<00:07,  1.58s/it]Epoch 23:  99%|█████████▊| 296/300 [08:35<00:06,  1.59s/it]Epoch 23:  99%|█████████▉| 297/300 [08:36<00:04,  1.56s/it]Epoch 23:  99%|█████████▉| 298/300 [08:38<00:03,  1.66s/it]Epoch 23: 100%|█████████▉| 299/300 [08:40<00:01,  1.73s/it]06/19/2022 18:01:19 - INFO - __main__ - global step: 3600; train loss: 7.4466142654418945; dev loss
Epoch 23: 100%|██████████| 300/300 [08:42<00:00,  1.65s/it]Epoch 23: 100%|██████████| 300/300 [08:42<00:00,  1.74s/it]
Epoch 24:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 24:   0%|          | 1/300 [00:01<08:04,  1.62s/it]Epoch 24:   1%|          | 2/300 [00:03<07:37,  1.54s/it]Epoch 24:   1%|          | 3/300 [00:04<07:29,  1.51s/it]Epoch 24:   1%|▏         | 4/300 [00:06<08:10,  1.66s/it]Epoch 24:   2%|▏         | 5/300 [00:08<08:42,  1.77s/it]Epoch 24:   2%|▏         | 6/300 [00:10<08:31,  1.74s/it]Epoch 24:   2%|▏         | 7/300 [00:11<08:05,  1.66s/it]Epoch 24:   3%|▎         | 8/300 [00:13<08:04,  1.66s/it]Epoch 24:   3%|▎         | 9/300 [00:15<08:10,  1.69s/it]Epoch 24:   3%|▎         | 10/300 [00:16<07:53,  1.63s/it]Epoch 24:   4%|▎         | 11/300 [00:18<07:41,  1.60s/it]Epoch 24:   4%|▍         | 12/300 [00:19<07:30,  1.57s/it]Epoch 24:   4%|▍         | 13/300 [00:21<07:31,  1.57s/it]Epoch 24:   5%|▍         | 14/300 [00:22<07:24,  1.56s/it]Epoch 24:   5%|▌         | 15/300 [00:24<07:18,  1.54s/it]Epoch 24:   5%|▌         | 16/300 [00:25<07:24,  1.56s/it]Epoch 24:   6%|▌         | 17/300 [00:27<07:40,  1.63s/it]Epoch 24:   6%|▌         | 18/300 [00:29<07:27,  1.59s/it]Epoch 24:   6%|▋         | 19/300 [00:30<07:30,  1.60s/it]06/19/2022 18:01:52 - INFO - __main__ - global step: 3610; train loss: 7.6047773361206055; dev loss
Epoch 24:   7%|▋         | 20/300 [00:32<07:36,  1.63s/it]Epoch 24:   7%|▋         | 21/300 [00:33<07:33,  1.62s/it]Epoch 24:   7%|▋         | 22/300 [00:35<07:22,  1.59s/it]Epoch 24:   8%|▊         | 23/300 [00:37<07:29,  1.62s/it]Epoch 24:   8%|▊         | 24/300 [00:38<07:33,  1.64s/it]Epoch 24:   8%|▊         | 25/300 [00:40<07:25,  1.62s/it]Epoch 24:   9%|▊         | 26/300 [00:42<07:47,  1.71s/it]Epoch 24:   9%|▉         | 27/300 [00:43<07:29,  1.65s/it]Epoch 24:   9%|▉         | 28/300 [00:45<07:15,  1.60s/it]Epoch 24:  10%|▉         | 29/300 [00:46<07:06,  1.57s/it]Epoch 24:  10%|█         | 30/300 [00:48<07:08,  1.59s/it]Epoch 24:  10%|█         | 31/300 [00:49<06:59,  1.56s/it]Epoch 24:  11%|█         | 32/300 [00:51<07:22,  1.65s/it]Epoch 24:  11%|█         | 33/300 [00:53<07:40,  1.72s/it]Epoch 24:  11%|█▏        | 34/300 [00:55<07:29,  1.69s/it]Epoch 24:  12%|█▏        | 35/300 [00:56<07:12,  1.63s/it]Epoch 24:  12%|█▏        | 36/300 [00:58<07:29,  1.70s/it]Epoch 24:  12%|█▏        | 37/300 [01:00<07:41,  1.76s/it]Epoch 24:  13%|█▎        | 38/300 [01:02<08:02,  1.84s/it]Epoch 24:  13%|█▎        | 39/300 [01:04<08:05,  1.86s/it]06/19/2022 18:02:26 - INFO - __main__ - global step: 3620; train loss: 7.873370170593262; dev loss
Epoch 24:  13%|█▎        | 40/300 [01:06<08:05,  1.87s/it]Epoch 24:  14%|█▎        | 41/300 [01:08<08:05,  1.87s/it]Epoch 24:  14%|█▍        | 42/300 [01:10<08:14,  1.92s/it]Epoch 24:  14%|█▍        | 43/300 [01:12<08:09,  1.91s/it]Epoch 24:  15%|█▍        | 44/300 [01:13<07:37,  1.79s/it]Epoch 24:  15%|█▌        | 45/300 [01:15<07:15,  1.71s/it]Epoch 24:  15%|█▌        | 46/300 [01:17<07:37,  1.80s/it]Epoch 24:  16%|█▌        | 47/300 [01:18<07:13,  1.71s/it]Epoch 24:  16%|█▌        | 48/300 [01:20<06:56,  1.65s/it]Epoch 24:  16%|█▋        | 49/300 [01:21<06:42,  1.60s/it]Epoch 24:  17%|█▋        | 50/300 [01:23<06:40,  1.60s/it]Epoch 24:  17%|█▋        | 51/300 [01:24<06:29,  1.57s/it]Epoch 24:  17%|█▋        | 52/300 [01:26<06:51,  1.66s/it]Epoch 24:  18%|█▊        | 53/300 [01:28<06:39,  1.62s/it]Epoch 24:  18%|█▊        | 54/300 [01:30<06:57,  1.70s/it]Epoch 24:  18%|█▊        | 55/300 [01:32<07:16,  1.78s/it]Epoch 24:  19%|█▊        | 56/300 [01:33<06:54,  1.70s/it]Epoch 24:  19%|█▉        | 57/300 [01:35<07:07,  1.76s/it]Epoch 24:  19%|█▉        | 58/300 [01:37<06:47,  1.69s/it]Epoch 24:  20%|█▉        | 59/300 [01:38<06:38,  1.65s/it]06/19/2022 18:03:00 - INFO - __main__ - global step: 3630; train loss: 7.990880012512207; dev loss
Epoch 24:  20%|██        | 60/300 [01:40<06:24,  1.60s/it]Epoch 24:  20%|██        | 61/300 [01:41<06:43,  1.69s/it]Epoch 24:  21%|██        | 62/300 [01:43<06:31,  1.65s/it]Epoch 24:  21%|██        | 63/300 [01:45<06:54,  1.75s/it]Epoch 24:  21%|██▏       | 64/300 [01:47<07:01,  1.79s/it]Epoch 24:  22%|██▏       | 65/300 [01:49<07:06,  1.82s/it]Epoch 24:  22%|██▏       | 66/300 [01:50<06:42,  1.72s/it]Epoch 24:  22%|██▏       | 67/300 [01:52<06:33,  1.69s/it]Epoch 24:  23%|██▎       | 68/300 [01:53<06:16,  1.62s/it]Epoch 24:  23%|██▎       | 69/300 [01:55<06:32,  1.70s/it]Epoch 24:  23%|██▎       | 70/300 [01:57<06:16,  1.64s/it]Epoch 24:  24%|██▎       | 71/300 [01:58<06:11,  1.62s/it]Epoch 24:  24%|██▍       | 72/300 [02:00<05:59,  1.58s/it]Epoch 24:  24%|██▍       | 73/300 [02:01<05:53,  1.56s/it]Epoch 24:  25%|██▍       | 74/300 [02:03<06:13,  1.65s/it]Epoch 24:  25%|██▌       | 75/300 [02:05<06:34,  1.76s/it]Epoch 24:  25%|██▌       | 76/300 [02:07<06:42,  1.79s/it]Epoch 24:  26%|██▌       | 77/300 [02:09<06:21,  1.71s/it]Epoch 24:  26%|██▌       | 78/300 [02:10<06:04,  1.64s/it]Epoch 24:  26%|██▋       | 79/300 [02:12<05:59,  1.63s/it]06/19/2022 18:03:33 - INFO - __main__ - global step: 3640; train loss: 7.807086944580078; dev loss
Epoch 24:  27%|██▋       | 80/300 [02:13<05:48,  1.59s/it]Epoch 24:  27%|██▋       | 81/300 [02:15<05:41,  1.56s/it]Epoch 24:  27%|██▋       | 82/300 [02:16<05:37,  1.55s/it]Epoch 24:  28%|██▊       | 83/300 [02:18<05:57,  1.65s/it]Epoch 24:  28%|██▊       | 84/300 [02:20<06:18,  1.75s/it]Epoch 24:  28%|██▊       | 85/300 [02:21<05:53,  1.64s/it]Epoch 24:  29%|██▊       | 86/300 [02:23<06:04,  1.70s/it]Epoch 24:  29%|██▉       | 87/300 [02:25<05:43,  1.61s/it]Epoch 24:  29%|██▉       | 88/300 [02:27<06:07,  1.73s/it]Epoch 24:  30%|██▉       | 89/300 [02:28<05:44,  1.63s/it]Epoch 24:  30%|███       | 90/300 [02:30<05:58,  1.71s/it]Epoch 24:  30%|███       | 91/300 [02:31<05:38,  1.62s/it]Epoch 24:  31%|███       | 92/300 [02:33<05:57,  1.72s/it]Epoch 24:  31%|███       | 93/300 [02:35<06:04,  1.76s/it]Epoch 24:  31%|███▏      | 94/300 [02:37<05:44,  1.67s/it]Epoch 24:  32%|███▏      | 95/300 [02:38<05:27,  1.60s/it]Epoch 24:  32%|███▏      | 96/300 [02:40<05:51,  1.72s/it]Epoch 24:  32%|███▏      | 97/300 [02:42<05:57,  1.76s/it]Epoch 24:  33%|███▎      | 98/300 [02:43<05:34,  1.66s/it]Epoch 24:  33%|███▎      | 99/300 [02:45<05:18,  1.59s/it]06/19/2022 18:04:07 - INFO - __main__ - global step: 3650; train loss: 7.865597724914551; dev loss
Epoch 24:  33%|███▎      | 100/300 [02:47<05:39,  1.70s/it]Epoch 24:  34%|███▎      | 101/300 [02:48<05:20,  1.61s/it]Epoch 24:  34%|███▍      | 102/300 [02:50<05:35,  1.69s/it]Epoch 24:  34%|███▍      | 103/300 [02:52<05:45,  1.75s/it]Epoch 24:  35%|███▍      | 104/300 [02:53<05:28,  1.68s/it]Epoch 24:  35%|███▌      | 105/300 [02:55<05:10,  1.59s/it]Epoch 24:  35%|███▌      | 106/300 [02:57<05:23,  1.67s/it]Epoch 24:  36%|███▌      | 107/300 [02:58<05:32,  1.73s/it]Epoch 24:  36%|███▌      | 108/300 [03:00<05:13,  1.63s/it]Epoch 24:  36%|███▋      | 109/300 [03:02<05:30,  1.73s/it]Epoch 24:  37%|███▋      | 110/300 [03:03<05:10,  1.64s/it]Epoch 24:  37%|███▋      | 111/300 [03:05<04:56,  1.57s/it]Epoch 24:  37%|███▋      | 112/300 [03:06<04:45,  1.52s/it]Epoch 24:  38%|███▊      | 113/300 [03:08<05:08,  1.65s/it]Epoch 24:  38%|███▊      | 114/300 [03:10<05:18,  1.71s/it]Epoch 24:  38%|███▊      | 115/300 [03:11<04:58,  1.62s/it]Epoch 24:  39%|███▊      | 116/300 [03:13<05:09,  1.68s/it]Epoch 24:  39%|███▉      | 117/300 [03:15<04:59,  1.64s/it]Epoch 24:  39%|███▉      | 118/300 [03:16<05:07,  1.69s/it]Epoch 24:  40%|███▉      | 119/300 [03:18<04:50,  1.61s/it]06/19/2022 18:04:40 - INFO - __main__ - global step: 3660; train loss: 8.26613712310791; dev loss
Epoch 24:  40%|████      | 120/300 [03:20<05:01,  1.67s/it]Epoch 24:  40%|████      | 121/300 [03:21<04:49,  1.62s/it]Epoch 24:  41%|████      | 122/300 [03:23<04:59,  1.68s/it]Epoch 24:  41%|████      | 123/300 [03:25<05:05,  1.73s/it]Epoch 24:  41%|████▏     | 124/300 [03:26<04:46,  1.63s/it]Epoch 24:  42%|████▏     | 125/300 [03:28<05:02,  1.73s/it]Epoch 24:  42%|████▏     | 126/300 [03:30<04:45,  1.64s/it]Epoch 24:  42%|████▏     | 127/300 [03:31<04:53,  1.70s/it]Epoch 24:  43%|████▎     | 128/300 [03:33<05:01,  1.75s/it]Epoch 24:  43%|████▎     | 129/300 [03:35<05:09,  1.81s/it]Epoch 24:  43%|████▎     | 130/300 [03:37<04:46,  1.69s/it]Epoch 24:  44%|████▎     | 131/300 [03:39<04:53,  1.74s/it]Epoch 24:  44%|████▍     | 132/300 [03:40<04:56,  1.77s/it]Epoch 24:  44%|████▍     | 133/300 [03:42<05:03,  1.82s/it]Epoch 24:  45%|████▍     | 134/300 [03:44<04:40,  1.69s/it]Epoch 24:  45%|████▌     | 135/300 [03:46<04:46,  1.73s/it]Epoch 24:  45%|████▌     | 136/300 [03:47<04:49,  1.77s/it]Epoch 24:  46%|████▌     | 137/300 [03:49<04:50,  1.78s/it]Epoch 24:  46%|████▌     | 138/300 [03:51<04:56,  1.83s/it]Epoch 24:  46%|████▋     | 139/300 [03:53<04:34,  1.70s/it]06/19/2022 18:05:14 - INFO - __main__ - global step: 3670; train loss: 7.340244293212891; dev loss
Epoch 24:  47%|████▋     | 140/300 [03:54<04:17,  1.61s/it]Epoch 24:  47%|████▋     | 141/300 [03:55<04:05,  1.55s/it]Epoch 24:  47%|████▋     | 142/300 [03:57<04:22,  1.66s/it]Epoch 24:  48%|████▊     | 143/300 [03:59<04:29,  1.71s/it]Epoch 24:  48%|████▊     | 144/300 [04:01<04:13,  1.63s/it]Epoch 24:  48%|████▊     | 145/300 [04:02<04:01,  1.56s/it]Epoch 24:  49%|████▊     | 146/300 [04:04<04:16,  1.67s/it]Epoch 24:  49%|████▉     | 147/300 [04:06<04:22,  1.72s/it]Epoch 24:  49%|████▉     | 148/300 [04:08<04:27,  1.76s/it]Epoch 24:  50%|████▉     | 149/300 [04:09<04:09,  1.65s/it]Epoch 24:  50%|█████     | 150/300 [04:11<04:19,  1.73s/it]Epoch 24:  50%|█████     | 151/300 [04:13<04:22,  1.76s/it]Epoch 24:  51%|█████     | 152/300 [04:14<04:05,  1.66s/it]Epoch 24:  51%|█████     | 153/300 [04:16<04:12,  1.72s/it]Epoch 24:  51%|█████▏    | 154/300 [04:18<04:02,  1.66s/it]Epoch 24:  52%|█████▏    | 155/300 [04:19<04:07,  1.71s/it]Epoch 24:  52%|█████▏    | 156/300 [04:21<03:54,  1.63s/it]Epoch 24:  52%|█████▏    | 157/300 [04:22<03:47,  1.59s/it]Epoch 24:  53%|█████▎    | 158/300 [04:24<04:03,  1.72s/it]Epoch 24:  53%|█████▎    | 159/300 [04:26<03:53,  1.66s/it]06/19/2022 18:05:47 - INFO - __main__ - global step: 3680; train loss: 8.144596099853516; dev loss
Epoch 24:  53%|█████▎    | 160/300 [04:27<03:44,  1.60s/it]Epoch 24:  54%|█████▎    | 161/300 [04:29<03:54,  1.69s/it]Epoch 24:  54%|█████▍    | 162/300 [04:31<04:01,  1.75s/it]Epoch 24:  54%|█████▍    | 163/300 [04:33<04:09,  1.82s/it]Epoch 24:  55%|█████▍    | 164/300 [04:35<04:11,  1.85s/it]Epoch 24:  55%|█████▌    | 165/300 [04:36<03:55,  1.74s/it]Epoch 24:  55%|█████▌    | 166/300 [04:38<03:58,  1.78s/it]Epoch 24:  56%|█████▌    | 167/300 [04:40<03:51,  1.74s/it]Epoch 24:  56%|█████▌    | 168/300 [04:42<03:54,  1.78s/it]Epoch 24:  56%|█████▋    | 169/300 [04:43<03:40,  1.69s/it]Epoch 24:  57%|█████▋    | 170/300 [04:45<03:46,  1.74s/it]Epoch 24:  57%|█████▋    | 171/300 [04:47<03:39,  1.70s/it]Epoch 24:  57%|█████▋    | 172/300 [04:49<03:45,  1.76s/it]Epoch 24:  58%|█████▊    | 173/300 [04:51<03:48,  1.80s/it]Epoch 24:  58%|█████▊    | 174/300 [04:52<03:34,  1.70s/it]Epoch 24:  58%|█████▊    | 175/300 [04:54<03:28,  1.67s/it]Epoch 24:  59%|█████▊    | 176/300 [04:55<03:19,  1.61s/it]Epoch 24:  59%|█████▉    | 177/300 [04:57<03:14,  1.58s/it]Epoch 24:  59%|█████▉    | 178/300 [04:59<03:23,  1.67s/it]Epoch 24:  60%|█████▉    | 179/300 [05:01<03:33,  1.77s/it]06/19/2022 18:06:22 - INFO - __main__ - global step: 3690; train loss: 7.6130805015563965; dev loss
Epoch 24:  60%|██████    | 180/300 [05:02<03:21,  1.68s/it]Epoch 24:  60%|██████    | 181/300 [05:03<03:13,  1.63s/it]Epoch 24:  61%|██████    | 182/300 [05:05<03:20,  1.70s/it]Epoch 24:  61%|██████    | 183/300 [05:07<03:16,  1.68s/it]Epoch 24:  61%|██████▏   | 184/300 [05:09<03:22,  1.74s/it]Epoch 24:  62%|██████▏   | 185/300 [05:10<03:12,  1.67s/it]Epoch 24:  62%|██████▏   | 186/300 [05:12<03:18,  1.74s/it]Epoch 24:  62%|██████▏   | 187/300 [05:14<03:23,  1.80s/it]Epoch 24:  63%|██████▎   | 188/300 [05:16<03:25,  1.84s/it]Epoch 24:  63%|██████▎   | 189/300 [05:18<03:24,  1.84s/it]Epoch 24:  63%|██████▎   | 190/300 [05:20<03:22,  1.84s/it]Epoch 24:  64%|██████▎   | 191/300 [05:22<03:19,  1.83s/it]Epoch 24:  64%|██████▍   | 192/300 [05:24<03:19,  1.85s/it]Epoch 24:  64%|██████▍   | 193/300 [05:25<03:02,  1.71s/it]Epoch 24:  65%|██████▍   | 194/300 [05:27<03:04,  1.74s/it]Epoch 24:  65%|██████▌   | 195/300 [05:28<02:51,  1.64s/it]Epoch 24:  65%|██████▌   | 196/300 [05:30<02:45,  1.59s/it]Epoch 24:  66%|██████▌   | 197/300 [05:31<02:37,  1.53s/it]Epoch 24:  66%|██████▌   | 198/300 [05:32<02:32,  1.49s/it]Epoch 24:  66%|██████▋   | 199/300 [05:34<02:40,  1.59s/it]06/19/2022 18:06:56 - INFO - __main__ - global step: 3700; train loss: 8.157720565795898; dev loss
Epoch 24:  67%|██████▋   | 200/300 [05:36<02:47,  1.68s/it]Epoch 24:  67%|██████▋   | 201/300 [05:38<02:38,  1.60s/it]Epoch 24:  67%|██████▋   | 202/300 [05:39<02:31,  1.55s/it]Epoch 24:  68%|██████▊   | 203/300 [05:40<02:25,  1.50s/it]Epoch 24:  68%|██████▊   | 204/300 [05:42<02:35,  1.62s/it]Epoch 24:  68%|██████▊   | 205/300 [05:44<02:28,  1.56s/it]Epoch 24:  69%|██████▊   | 206/300 [05:45<02:21,  1.51s/it]Epoch 24:  69%|██████▉   | 207/300 [05:46<02:16,  1.47s/it]Epoch 24:  69%|██████▉   | 208/300 [05:48<02:27,  1.60s/it]Epoch 24:  70%|██████▉   | 209/300 [05:50<02:20,  1.54s/it]Epoch 24:  70%|███████   | 210/300 [05:51<02:14,  1.50s/it]Epoch 24:  70%|███████   | 211/300 [05:53<02:10,  1.46s/it]Epoch 24:  71%|███████   | 212/300 [05:54<02:09,  1.48s/it]Epoch 24:  71%|███████   | 213/300 [05:56<02:17,  1.57s/it]Epoch 24:  71%|███████▏  | 214/300 [05:58<02:20,  1.64s/it]Epoch 24:  72%|███████▏  | 215/300 [05:59<02:24,  1.70s/it]Epoch 24:  72%|███████▏  | 216/300 [06:01<02:15,  1.61s/it]Epoch 24:  72%|███████▏  | 217/300 [06:02<02:10,  1.58s/it]Epoch 24:  73%|███████▎  | 218/300 [06:04<02:14,  1.64s/it]Epoch 24:  73%|███████▎  | 219/300 [06:06<02:06,  1.57s/it]06/19/2022 18:07:27 - INFO - __main__ - global step: 3710; train loss: 7.971188545227051; dev loss
Epoch 24:  73%|███████▎  | 220/300 [06:07<02:01,  1.52s/it]Epoch 24:  74%|███████▎  | 221/300 [06:08<01:59,  1.51s/it]Epoch 24:  74%|███████▍  | 222/300 [06:10<01:54,  1.47s/it]Epoch 24:  74%|███████▍  | 223/300 [06:11<01:51,  1.45s/it]Epoch 24:  75%|███████▍  | 224/300 [06:13<01:58,  1.56s/it]Epoch 24:  75%|███████▌  | 225/300 [06:15<02:04,  1.66s/it]Epoch 24:  75%|███████▌  | 226/300 [06:16<01:56,  1.58s/it]Epoch 24:  76%|███████▌  | 227/300 [06:18<01:51,  1.53s/it]Epoch 24:  76%|███████▌  | 228/300 [06:20<01:56,  1.61s/it]Epoch 24:  76%|███████▋  | 229/300 [06:21<02:00,  1.70s/it]Epoch 24:  77%|███████▋  | 230/300 [06:23<02:01,  1.73s/it]Epoch 24:  77%|███████▋  | 231/300 [06:25<02:00,  1.75s/it]Epoch 24:  77%|███████▋  | 232/300 [06:27<02:00,  1.77s/it]Epoch 24:  78%|███████▊  | 233/300 [06:29<02:00,  1.81s/it]Epoch 24:  78%|███████▊  | 234/300 [06:31<01:59,  1.81s/it]Epoch 24:  78%|███████▊  | 235/300 [06:32<01:57,  1.81s/it]Epoch 24:  79%|███████▊  | 236/300 [06:34<01:48,  1.69s/it]Epoch 24:  79%|███████▉  | 237/300 [06:35<01:42,  1.63s/it]Epoch 24:  79%|███████▉  | 238/300 [06:37<01:36,  1.56s/it]Epoch 24:  80%|███████▉  | 239/300 [06:38<01:32,  1.51s/it]06/19/2022 18:08:00 - INFO - __main__ - global step: 3720; train loss: 7.888741970062256; dev loss
Epoch 24:  80%|████████  | 240/300 [06:40<01:36,  1.60s/it]Epoch 24:  80%|████████  | 241/300 [06:41<01:32,  1.57s/it]Epoch 24:  81%|████████  | 242/300 [06:43<01:28,  1.52s/it]Epoch 24:  81%|████████  | 243/300 [06:44<01:24,  1.48s/it]Epoch 24:  81%|████████▏ | 244/300 [06:46<01:21,  1.45s/it]Epoch 24:  82%|████████▏ | 245/300 [06:47<01:19,  1.44s/it]Epoch 24:  82%|████████▏ | 246/300 [06:48<01:19,  1.46s/it]Epoch 24:  82%|████████▏ | 247/300 [06:50<01:22,  1.56s/it]Epoch 24:  83%|████████▎ | 248/300 [06:52<01:18,  1.51s/it]Epoch 24:  83%|████████▎ | 249/300 [06:53<01:15,  1.48s/it]Epoch 24:  83%|████████▎ | 250/300 [06:55<01:20,  1.61s/it]Epoch 24:  84%|████████▎ | 251/300 [06:57<01:21,  1.67s/it]Epoch 24:  84%|████████▍ | 252/300 [06:58<01:16,  1.59s/it]Epoch 24:  84%|████████▍ | 253/300 [07:00<01:11,  1.53s/it]Epoch 24:  85%|████████▍ | 254/300 [07:01<01:15,  1.64s/it]Epoch 24:  85%|████████▌ | 255/300 [07:03<01:16,  1.69s/it]Epoch 24:  85%|████████▌ | 256/300 [07:05<01:15,  1.73s/it]Epoch 24:  86%|████████▌ | 257/300 [07:07<01:10,  1.63s/it]Epoch 24:  86%|████████▌ | 258/300 [07:08<01:06,  1.59s/it]Epoch 24:  86%|████████▋ | 259/300 [07:09<01:02,  1.53s/it]06/19/2022 18:08:31 - INFO - __main__ - global step: 3730; train loss: 7.754328727722168; dev loss
Epoch 24:  87%|████████▋ | 260/300 [07:11<01:04,  1.62s/it]Epoch 24:  87%|████████▋ | 261/300 [07:13<01:05,  1.68s/it]Epoch 24:  87%|████████▋ | 262/300 [07:15<01:06,  1.75s/it]Epoch 24:  88%|████████▊ | 263/300 [07:16<01:00,  1.64s/it]Epoch 24:  88%|████████▊ | 264/300 [07:18<00:56,  1.57s/it]Epoch 24:  88%|████████▊ | 265/300 [07:20<00:57,  1.64s/it]Epoch 24:  89%|████████▊ | 266/300 [07:21<00:58,  1.72s/it]Epoch 24:  89%|████████▉ | 267/300 [07:23<00:57,  1.74s/it]Epoch 24:  89%|████████▉ | 268/300 [07:25<00:56,  1.76s/it]Epoch 24:  90%|████████▉ | 269/300 [07:27<00:54,  1.77s/it]Epoch 24:  90%|█████████ | 270/300 [07:29<00:53,  1.78s/it]Epoch 24:  90%|█████████ | 271/300 [07:31<00:52,  1.82s/it]Epoch 24:  91%|█████████ | 272/300 [07:32<00:47,  1.70s/it]Epoch 24:  91%|█████████ | 273/300 [07:33<00:43,  1.61s/it]Epoch 24:  91%|█████████▏| 274/300 [07:35<00:40,  1.55s/it]Epoch 24:  92%|█████████▏| 275/300 [07:36<00:38,  1.54s/it]Epoch 24:  92%|█████████▏| 276/300 [07:38<00:35,  1.49s/it]Epoch 24:  92%|█████████▏| 277/300 [07:39<00:36,  1.58s/it]Epoch 24:  93%|█████████▎| 278/300 [07:41<00:36,  1.65s/it]Epoch 24:  93%|█████████▎| 279/300 [07:43<00:33,  1.61s/it]06/19/2022 18:09:04 - INFO - __main__ - global step: 3740; train loss: 8.107604026794434; dev loss
Epoch 24:  93%|█████████▎| 280/300 [07:44<00:31,  1.55s/it]Epoch 24:  94%|█████████▎| 281/300 [07:46<00:28,  1.50s/it]Epoch 24:  94%|█████████▍| 282/300 [07:47<00:28,  1.59s/it]Epoch 24:  94%|█████████▍| 283/300 [07:49<00:26,  1.56s/it]Epoch 24:  95%|█████████▍| 284/300 [07:51<00:26,  1.64s/it]Epoch 24:  95%|█████████▌| 285/300 [07:53<00:25,  1.69s/it]Epoch 24:  95%|█████████▌| 286/300 [07:54<00:24,  1.72s/it]Epoch 24:  96%|█████████▌| 287/300 [07:56<00:23,  1.77s/it]Epoch 24:  96%|█████████▌| 288/300 [07:58<00:21,  1.78s/it]Epoch 24:  96%|█████████▋| 289/300 [07:59<00:18,  1.67s/it]Epoch 24:  97%|█████████▋| 290/300 [08:01<00:17,  1.71s/it]Epoch 24:  97%|█████████▋| 291/300 [08:03<00:16,  1.78s/it]Epoch 24:  97%|█████████▋| 292/300 [08:05<00:14,  1.78s/it]Epoch 24:  98%|█████████▊| 293/300 [08:06<00:11,  1.67s/it]Epoch 24:  98%|█████████▊| 294/300 [08:08<00:10,  1.71s/it]Epoch 24:  98%|█████████▊| 295/300 [08:10<00:08,  1.65s/it]Epoch 24:  99%|█████████▊| 296/300 [08:11<00:06,  1.69s/it]Epoch 24:  99%|█████████▉| 297/300 [08:13<00:04,  1.60s/it]Epoch 24:  99%|█████████▉| 298/300 [08:14<00:03,  1.54s/it]Epoch 24: 100%|█████████▉| 299/300 [08:16<00:01,  1.51s/it]06/19/2022 18:09:37 - INFO - __main__ - global step: 3750; train loss: 8.348506927490234; dev loss
Epoch 24: 100%|██████████| 300/300 [08:17<00:00,  1.51s/it]Epoch 24: 100%|██████████| 300/300 [08:17<00:00,  1.66s/it]
Epoch 25:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 25:   0%|          | 1/300 [00:01<07:01,  1.41s/it]Epoch 25:   1%|          | 2/300 [00:02<07:00,  1.41s/it]Epoch 25:   1%|          | 3/300 [00:04<07:50,  1.59s/it]Epoch 25:   1%|▏         | 4/300 [00:06<07:38,  1.55s/it]Epoch 25:   2%|▏         | 5/300 [00:07<08:03,  1.64s/it]Epoch 25:   2%|▏         | 6/300 [00:09<07:37,  1.56s/it]Epoch 25:   2%|▏         | 7/300 [00:11<08:00,  1.64s/it]Epoch 25:   3%|▎         | 8/300 [00:12<07:46,  1.60s/it]Epoch 25:   3%|▎         | 9/300 [00:14<07:26,  1.54s/it]Epoch 25:   3%|▎         | 10/300 [00:15<07:49,  1.62s/it]Epoch 25:   4%|▎         | 11/300 [00:17<07:30,  1.56s/it]Epoch 25:   4%|▍         | 12/300 [00:18<07:22,  1.54s/it]Epoch 25:   4%|▍         | 13/300 [00:20<07:08,  1.49s/it]Epoch 25:   5%|▍         | 14/300 [00:21<07:32,  1.58s/it]Epoch 25:   5%|▌         | 15/300 [00:23<07:13,  1.52s/it]Epoch 25:   5%|▌         | 16/300 [00:24<07:09,  1.51s/it]Epoch 25:   6%|▌         | 17/300 [00:26<06:57,  1.48s/it]Epoch 25:   6%|▌         | 18/300 [00:27<07:24,  1.58s/it]Epoch 25:   6%|▋         | 19/300 [00:29<07:07,  1.52s/it]06/19/2022 18:10:08 - INFO - __main__ - global step: 3760; train loss: 7.958540916442871; dev loss
Epoch 25:   7%|▋         | 20/300 [00:31<07:37,  1.64s/it]Epoch 25:   7%|▋         | 21/300 [00:32<07:18,  1.57s/it]Epoch 25:   7%|▋         | 22/300 [00:34<07:00,  1.51s/it]Epoch 25:   8%|▊         | 23/300 [00:35<06:49,  1.48s/it]Epoch 25:   8%|▊         | 24/300 [00:36<06:41,  1.46s/it]Epoch 25:   8%|▊         | 25/300 [00:38<06:40,  1.46s/it]Epoch 25:   9%|▊         | 26/300 [00:39<06:34,  1.44s/it]Epoch 25:   9%|▉         | 27/300 [00:41<07:02,  1.55s/it]Epoch 25:   9%|▉         | 28/300 [00:42<06:49,  1.50s/it]Epoch 25:  10%|▉         | 29/300 [00:44<06:46,  1.50s/it]Epoch 25:  10%|█         | 30/300 [00:45<06:37,  1.47s/it]Epoch 25:  10%|█         | 31/300 [00:47<07:03,  1.57s/it]Epoch 25:  11%|█         | 32/300 [00:49<07:20,  1.64s/it]Epoch 25:  11%|█         | 33/300 [00:51<07:39,  1.72s/it]Epoch 25:  11%|█▏        | 34/300 [00:52<07:11,  1.62s/it]Epoch 25:  12%|█▏        | 35/300 [00:54<06:51,  1.55s/it]Epoch 25:  12%|█▏        | 36/300 [00:55<07:10,  1.63s/it]Epoch 25:  12%|█▏        | 37/300 [00:57<07:30,  1.71s/it]Epoch 25:  13%|█▎        | 38/300 [00:59<07:36,  1.74s/it]Epoch 25:  13%|█▎        | 39/300 [01:01<07:07,  1.64s/it]06/19/2022 18:10:40 - INFO - __main__ - global step: 3770; train loss: 8.038679122924805; dev loss
Epoch 25:  13%|█▎        | 40/300 [01:02<06:48,  1.57s/it]Epoch 25:  14%|█▎        | 41/300 [01:04<07:10,  1.66s/it]Epoch 25:  14%|█▍        | 42/300 [01:06<07:19,  1.70s/it]Epoch 25:  14%|█▍        | 43/300 [01:07<06:54,  1.61s/it]Epoch 25:  15%|█▍        | 44/300 [01:09<07:09,  1.68s/it]Epoch 25:  15%|█▌        | 45/300 [01:11<07:27,  1.75s/it]Epoch 25:  15%|█▌        | 46/300 [01:12<06:59,  1.65s/it]Epoch 25:  16%|█▌        | 47/300 [01:14<07:07,  1.69s/it]Epoch 25:  16%|█▌        | 48/300 [01:16<07:15,  1.73s/it]Epoch 25:  16%|█▋        | 49/300 [01:18<07:24,  1.77s/it]Epoch 25:  17%|█▋        | 50/300 [01:19<07:16,  1.75s/it]Epoch 25:  17%|█▋        | 51/300 [01:21<06:53,  1.66s/it]Epoch 25:  17%|█▋        | 52/300 [01:22<06:37,  1.60s/it]Epoch 25:  18%|█▊        | 53/300 [01:24<06:29,  1.58s/it]Epoch 25:  18%|█▊        | 54/300 [01:25<06:23,  1.56s/it]Epoch 25:  18%|█▊        | 55/300 [01:27<06:35,  1.61s/it]Epoch 25:  19%|█▊        | 56/300 [01:29<06:47,  1.67s/it]Epoch 25:  19%|█▉        | 57/300 [01:31<06:55,  1.71s/it]Epoch 25:  19%|█▉        | 58/300 [01:32<06:39,  1.65s/it]Epoch 25:  20%|█▉        | 59/300 [01:34<06:48,  1.70s/it]06/19/2022 18:11:14 - INFO - __main__ - global step: 3780; train loss: 7.943388938903809; dev loss
Epoch 25:  20%|██        | 60/300 [01:36<06:56,  1.73s/it]Epoch 25:  20%|██        | 61/300 [01:38<06:59,  1.76s/it]Epoch 25:  21%|██        | 62/300 [01:39<06:41,  1.69s/it]Epoch 25:  21%|██        | 63/300 [01:41<06:18,  1.60s/it]Epoch 25:  21%|██▏       | 64/300 [01:42<06:02,  1.54s/it]Epoch 25:  22%|██▏       | 65/300 [01:44<06:19,  1.62s/it]Epoch 25:  22%|██▏       | 66/300 [01:46<06:40,  1.71s/it]Epoch 25:  22%|██▏       | 67/300 [01:47<06:45,  1.74s/it]Epoch 25:  23%|██▎       | 68/300 [01:49<06:49,  1.76s/it]Epoch 25:  23%|██▎       | 69/300 [01:51<06:21,  1.65s/it]Epoch 25:  23%|██▎       | 70/300 [01:53<06:38,  1.73s/it]Epoch 25:  24%|██▎       | 71/300 [01:54<06:14,  1.64s/it]Epoch 25:  24%|██▍       | 72/300 [01:55<05:56,  1.57s/it]Epoch 25:  24%|██▍       | 73/300 [01:57<06:11,  1.64s/it]Epoch 25:  25%|██▍       | 74/300 [01:59<06:30,  1.73s/it]Epoch 25:  25%|██▌       | 75/300 [02:01<06:05,  1.63s/it]Epoch 25:  25%|██▌       | 76/300 [02:02<06:16,  1.68s/it]Epoch 25:  26%|██▌       | 77/300 [02:04<05:56,  1.60s/it]Epoch 25:  26%|██▌       | 78/300 [02:05<05:41,  1.54s/it]Epoch 25:  26%|██▋       | 79/300 [02:07<05:37,  1.53s/it]06/19/2022 18:11:46 - INFO - __main__ - global step: 3790; train loss: 8.058881759643555; dev loss
Epoch 25:  27%|██▋       | 80/300 [02:08<05:27,  1.49s/it]Epoch 25:  27%|██▋       | 81/300 [02:10<05:47,  1.59s/it]Epoch 25:  27%|██▋       | 82/300 [02:11<05:35,  1.54s/it]Epoch 25:  28%|██▊       | 83/300 [02:13<05:56,  1.64s/it]Epoch 25:  28%|██▊       | 84/300 [02:15<05:41,  1.58s/it]Epoch 25:  28%|██▊       | 85/300 [02:16<05:53,  1.64s/it]Epoch 25:  29%|██▊       | 86/300 [02:18<06:02,  1.69s/it]Epoch 25:  29%|██▉       | 87/300 [02:20<06:15,  1.76s/it]Epoch 25:  29%|██▉       | 88/300 [02:22<05:50,  1.65s/it]Epoch 25:  30%|██▉       | 89/300 [02:23<05:31,  1.57s/it]Epoch 25:  30%|███       | 90/300 [02:25<05:43,  1.64s/it]Epoch 25:  30%|███       | 91/300 [02:27<05:58,  1.71s/it]Epoch 25:  31%|███       | 92/300 [02:28<05:35,  1.61s/it]Epoch 25:  31%|███       | 93/300 [02:29<05:19,  1.54s/it]Epoch 25:  31%|███▏      | 94/300 [02:31<05:32,  1.61s/it]Epoch 25:  32%|███▏      | 95/300 [02:33<05:24,  1.58s/it]Epoch 25:  32%|███▏      | 96/300 [02:34<05:34,  1.64s/it]Epoch 25:  32%|███▏      | 97/300 [02:36<05:16,  1.56s/it]Epoch 25:  33%|███▎      | 98/300 [02:37<05:05,  1.51s/it]Epoch 25:  33%|███▎      | 99/300 [02:39<05:03,  1.51s/it]06/19/2022 18:12:18 - INFO - __main__ - global step: 3800; train loss: 7.890860557556152; dev loss
Epoch 25:  33%|███▎      | 100/300 [02:40<04:54,  1.47s/it]Epoch 25:  34%|███▎      | 101/300 [02:41<04:47,  1.44s/it]Epoch 25:  34%|███▍      | 102/300 [02:43<05:05,  1.54s/it]Epoch 25:  34%|███▍      | 103/300 [02:45<05:26,  1.66s/it]Epoch 25:  35%|███▍      | 104/300 [02:47<05:10,  1.58s/it]Epoch 25:  35%|███▌      | 105/300 [02:48<04:57,  1.52s/it]Epoch 25:  35%|███▌      | 106/300 [02:49<04:47,  1.48s/it]Epoch 25:  36%|███▌      | 107/300 [02:51<05:02,  1.57s/it]Epoch 25:  36%|███▌      | 108/300 [02:53<05:20,  1.67s/it]Epoch 25:  36%|███▋      | 109/300 [02:54<05:02,  1.58s/it]Epoch 25:  37%|███▋      | 110/300 [02:56<04:50,  1.53s/it]Epoch 25:  37%|███▋      | 111/300 [02:58<05:01,  1.59s/it]Epoch 25:  37%|███▋      | 112/300 [02:59<05:14,  1.67s/it]Epoch 25:  38%|███▊      | 113/300 [03:01<05:02,  1.62s/it]Epoch 25:  38%|███▊      | 114/300 [03:03<04:59,  1.61s/it]Epoch 25:  38%|███▊      | 115/300 [03:04<05:05,  1.65s/it]Epoch 25:  39%|███▊      | 116/300 [03:06<05:02,  1.65s/it]Epoch 25:  39%|███▉      | 117/300 [03:07<04:52,  1.60s/it]Epoch 25:  39%|███▉      | 118/300 [03:09<04:38,  1.53s/it]Epoch 25:  40%|███▉      | 119/300 [03:11<04:49,  1.60s/it]06/19/2022 18:12:50 - INFO - __main__ - global step: 3810; train loss: 7.931266784667969; dev loss
Epoch 25:  40%|████      | 120/300 [03:12<04:57,  1.65s/it]Epoch 25:  40%|████      | 121/300 [03:14<05:00,  1.68s/it]Epoch 25:  41%|████      | 122/300 [03:16<05:04,  1.71s/it]Epoch 25:  41%|████      | 123/300 [03:18<05:09,  1.75s/it]Epoch 25:  41%|████▏     | 124/300 [03:20<05:15,  1.79s/it]Epoch 25:  42%|████▏     | 125/300 [03:21<04:52,  1.67s/it]Epoch 25:  42%|████▏     | 126/300 [03:22<04:35,  1.58s/it]Epoch 25:  42%|████▏     | 127/300 [03:24<04:44,  1.65s/it]Epoch 25:  43%|████▎     | 128/300 [03:26<04:36,  1.61s/it]Epoch 25:  43%|████▎     | 129/300 [03:27<04:23,  1.54s/it]Epoch 25:  43%|████▎     | 130/300 [03:28<04:14,  1.50s/it]Epoch 25:  44%|████▎     | 131/300 [03:30<04:27,  1.58s/it]Epoch 25:  44%|████▍     | 132/300 [03:32<04:16,  1.52s/it]Epoch 25:  44%|████▍     | 133/300 [03:33<04:32,  1.63s/it]Epoch 25:  45%|████▍     | 134/300 [03:35<04:19,  1.56s/it]Epoch 25:  45%|████▌     | 135/300 [03:36<04:09,  1.51s/it]Epoch 25:  45%|████▌     | 136/300 [03:38<04:02,  1.48s/it]Epoch 25:  46%|████▌     | 137/300 [03:40<04:19,  1.59s/it]Epoch 25:  46%|████▌     | 138/300 [03:41<04:27,  1.65s/it]Epoch 25:  46%|████▋     | 139/300 [03:43<04:13,  1.57s/it]06/19/2022 18:13:22 - INFO - __main__ - global step: 3820; train loss: 7.768279075622559; dev loss
Epoch 25:  47%|████▋     | 140/300 [03:44<04:03,  1.52s/it]Epoch 25:  47%|████▋     | 141/300 [03:46<04:00,  1.51s/it]Epoch 25:  47%|████▋     | 142/300 [03:47<04:12,  1.60s/it]Epoch 25:  48%|████▊     | 143/300 [03:49<04:01,  1.54s/it]Epoch 25:  48%|████▊     | 144/300 [03:50<03:53,  1.50s/it]Epoch 25:  48%|████▊     | 145/300 [03:52<04:11,  1.62s/it]Epoch 25:  49%|████▊     | 146/300 [03:54<04:17,  1.67s/it]Epoch 25:  49%|████▉     | 147/300 [03:56<04:21,  1.71s/it]Epoch 25:  49%|████▉     | 148/300 [03:57<04:05,  1.62s/it]Epoch 25:  50%|████▉     | 149/300 [03:59<04:16,  1.70s/it]Epoch 25:  50%|█████     | 150/300 [04:00<04:01,  1.61s/it]Epoch 25:  50%|█████     | 151/300 [04:02<04:07,  1.66s/it]Epoch 25:  51%|█████     | 152/300 [04:04<03:53,  1.58s/it]Epoch 25:  51%|█████     | 153/300 [04:05<04:05,  1.67s/it]Epoch 25:  51%|█████▏    | 154/300 [04:07<04:08,  1.70s/it]Epoch 25:  52%|█████▏    | 155/300 [04:09<03:53,  1.61s/it]Epoch 25:  52%|█████▏    | 156/300 [04:10<03:42,  1.54s/it]Epoch 25:  52%|█████▏    | 157/300 [04:11<03:37,  1.52s/it]Epoch 25:  53%|█████▎    | 158/300 [04:13<03:30,  1.48s/it]Epoch 25:  53%|█████▎    | 159/300 [04:14<03:23,  1.45s/it]06/19/2022 18:13:54 - INFO - __main__ - global step: 3830; train loss: 7.698848724365234; dev loss
Epoch 25:  53%|█████▎    | 160/300 [04:16<03:36,  1.55s/it]Epoch 25:  54%|█████▎    | 161/300 [04:17<03:28,  1.50s/it]Epoch 25:  54%|█████▍    | 162/300 [04:19<03:43,  1.62s/it]Epoch 25:  54%|█████▍    | 163/300 [04:21<03:32,  1.55s/it]Epoch 25:  55%|█████▍    | 164/300 [04:22<03:40,  1.62s/it]Epoch 25:  55%|█████▌    | 165/300 [04:24<03:45,  1.67s/it]Epoch 25:  55%|█████▌    | 166/300 [04:26<03:37,  1.62s/it]Epoch 25:  56%|█████▌    | 167/300 [04:27<03:25,  1.55s/it]Epoch 25:  56%|█████▌    | 168/300 [04:28<03:17,  1.50s/it]Epoch 25:  56%|█████▋    | 169/300 [04:30<03:27,  1.58s/it]Epoch 25:  57%|█████▋    | 170/300 [04:32<03:21,  1.55s/it]Epoch 25:  57%|█████▋    | 171/300 [04:34<03:29,  1.62s/it]Epoch 25:  57%|█████▋    | 172/300 [04:35<03:33,  1.67s/it]Epoch 25:  58%|█████▊    | 173/300 [04:37<03:37,  1.71s/it]Epoch 25:  58%|█████▊    | 174/300 [04:39<03:42,  1.77s/it]Epoch 25:  58%|█████▊    | 175/300 [04:41<03:41,  1.78s/it]Epoch 25:  59%|█████▊    | 176/300 [04:43<03:41,  1.78s/it]Epoch 25:  59%|█████▉    | 177/300 [04:44<03:39,  1.78s/it]Epoch 25:  59%|█████▉    | 178/300 [04:46<03:41,  1.81s/it]Epoch 25:  60%|█████▉    | 179/300 [04:48<03:38,  1.81s/it]06/19/2022 18:14:28 - INFO - __main__ - global step: 3840; train loss: 7.7889838218688965; dev loss
Epoch 25:  60%|██████    | 180/300 [04:50<03:37,  1.81s/it]Epoch 25:  60%|██████    | 181/300 [04:52<03:35,  1.81s/it]Epoch 25:  61%|██████    | 182/300 [04:53<03:22,  1.71s/it]Epoch 25:  61%|██████    | 183/300 [04:55<03:22,  1.73s/it]Epoch 25:  61%|██████▏   | 184/300 [04:57<03:22,  1.75s/it]Epoch 25:  62%|██████▏   | 185/300 [04:59<03:23,  1.77s/it]Epoch 25:  62%|██████▏   | 186/300 [05:00<03:09,  1.66s/it]Epoch 25:  62%|██████▏   | 187/300 [05:01<03:00,  1.60s/it]Epoch 25:  63%|██████▎   | 188/300 [05:03<02:52,  1.54s/it]Epoch 25:  63%|██████▎   | 189/300 [05:05<02:59,  1.62s/it]Epoch 25:  63%|██████▎   | 190/300 [05:06<03:03,  1.67s/it]Epoch 25:  64%|██████▎   | 191/300 [05:08<02:55,  1.61s/it]Epoch 25:  64%|██████▍   | 192/300 [05:10<02:59,  1.66s/it]Epoch 25:  64%|██████▍   | 193/300 [05:11<03:02,  1.70s/it]Epoch 25:  65%|██████▍   | 194/300 [05:13<03:03,  1.73s/it]Epoch 25:  65%|██████▌   | 195/300 [05:15<03:06,  1.78s/it]Epoch 25:  65%|██████▌   | 196/300 [05:17<02:53,  1.66s/it]Epoch 25:  66%|██████▌   | 197/300 [05:18<02:55,  1.70s/it]Epoch 25:  66%|██████▌   | 198/300 [05:20<02:57,  1.74s/it]Epoch 25:  66%|██████▋   | 199/300 [05:22<02:47,  1.66s/it]06/19/2022 18:15:01 - INFO - __main__ - global step: 3850; train loss: 7.993442535400391; dev loss
Epoch 25:  67%|██████▋   | 200/300 [05:23<02:50,  1.70s/it]Epoch 25:  67%|██████▋   | 201/300 [05:25<02:51,  1.74s/it]Epoch 25:  67%|██████▋   | 202/300 [05:27<02:55,  1.79s/it]Epoch 25:  68%|██████▊   | 203/300 [05:29<02:56,  1.82s/it]Epoch 25:  68%|██████▊   | 204/300 [05:31<02:54,  1.82s/it]Epoch 25:  68%|██████▊   | 205/300 [05:32<02:40,  1.69s/it]Epoch 25:  69%|██████▊   | 206/300 [05:34<02:41,  1.72s/it]Epoch 25:  69%|██████▉   | 207/300 [05:36<02:33,  1.65s/it]Epoch 25:  69%|██████▉   | 208/300 [05:37<02:35,  1.69s/it]Epoch 25:  70%|██████▉   | 209/300 [05:39<02:25,  1.60s/it]Epoch 25:  70%|███████   | 210/300 [05:41<02:28,  1.65s/it]Epoch 25:  70%|███████   | 211/300 [05:42<02:22,  1.61s/it]Epoch 25:  71%|███████   | 212/300 [05:43<02:15,  1.54s/it]Epoch 25:  71%|███████   | 213/300 [05:45<02:21,  1.62s/it]Epoch 25:  71%|███████▏  | 214/300 [05:47<02:13,  1.56s/it]Epoch 25:  72%|███████▏  | 215/300 [05:48<02:17,  1.62s/it]Epoch 25:  72%|███████▏  | 216/300 [05:50<02:13,  1.59s/it]Epoch 25:  72%|███████▏  | 217/300 [05:52<02:16,  1.65s/it]Epoch 25:  73%|███████▎  | 218/300 [05:53<02:08,  1.57s/it]Epoch 25:  73%|███████▎  | 219/300 [05:55<02:11,  1.63s/it]06/19/2022 18:15:34 - INFO - __main__ - global step: 3860; train loss: 8.117745399475098; dev loss
Epoch 25:  73%|███████▎  | 220/300 [05:57<02:16,  1.71s/it]Epoch 25:  74%|███████▎  | 221/300 [05:58<02:07,  1.61s/it]Epoch 25:  74%|███████▍  | 222/300 [05:59<01:59,  1.54s/it]Epoch 25:  74%|███████▍  | 223/300 [06:01<01:55,  1.50s/it]Epoch 25:  75%|███████▍  | 224/300 [06:02<01:53,  1.50s/it]Epoch 25:  75%|███████▌  | 225/300 [06:04<01:58,  1.58s/it]Epoch 25:  75%|███████▌  | 226/300 [06:06<01:52,  1.52s/it]Epoch 25:  76%|███████▌  | 227/300 [06:07<01:47,  1.48s/it]Epoch 25:  76%|███████▌  | 228/300 [06:09<01:55,  1.61s/it]Epoch 25:  76%|███████▋  | 229/300 [06:11<01:58,  1.67s/it]Epoch 25:  77%|███████▋  | 230/300 [06:12<01:59,  1.70s/it]Epoch 25:  77%|███████▋  | 231/300 [06:14<01:59,  1.73s/it]Epoch 25:  77%|███████▋  | 232/300 [06:16<02:00,  1.78s/it]Epoch 25:  78%|███████▊  | 233/300 [06:18<02:00,  1.79s/it]Epoch 25:  78%|███████▊  | 234/300 [06:19<01:50,  1.67s/it]Epoch 25:  78%|███████▊  | 235/300 [06:21<01:43,  1.59s/it]Epoch 25:  79%|███████▊  | 236/300 [06:23<01:47,  1.67s/it]Epoch 25:  79%|███████▉  | 237/300 [06:24<01:40,  1.59s/it]Epoch 25:  79%|███████▉  | 238/300 [06:26<01:42,  1.66s/it]Epoch 25:  80%|███████▉  | 239/300 [06:27<01:36,  1.58s/it]06/19/2022 18:16:07 - INFO - __main__ - global step: 3870; train loss: 7.486963748931885; dev loss
Epoch 25:  80%|████████  | 240/300 [06:29<01:38,  1.65s/it]Epoch 25:  80%|████████  | 241/300 [06:30<01:34,  1.60s/it]Epoch 25:  81%|████████  | 242/300 [06:32<01:36,  1.66s/it]Epoch 25:  81%|████████  | 243/300 [06:34<01:29,  1.57s/it]Epoch 25:  81%|████████▏ | 244/300 [06:35<01:31,  1.63s/it]Epoch 25:  82%|████████▏ | 245/300 [06:37<01:33,  1.71s/it]Epoch 25:  82%|████████▏ | 246/300 [06:39<01:27,  1.61s/it]Epoch 25:  82%|████████▏ | 247/300 [06:40<01:22,  1.55s/it]Epoch 25:  83%|████████▎ | 248/300 [06:41<01:17,  1.49s/it]Epoch 25:  83%|████████▎ | 249/300 [06:43<01:15,  1.49s/it]Epoch 25:  83%|████████▎ | 250/300 [06:45<01:19,  1.59s/it]Epoch 25:  84%|████████▎ | 251/300 [06:46<01:15,  1.53s/it]Epoch 25:  84%|████████▍ | 252/300 [06:48<01:11,  1.49s/it]Epoch 25:  84%|████████▍ | 253/300 [06:49<01:15,  1.61s/it]Epoch 25:  85%|████████▍ | 254/300 [06:51<01:16,  1.66s/it]Epoch 25:  85%|████████▌ | 255/300 [06:53<01:16,  1.70s/it]Epoch 25:  85%|████████▌ | 256/300 [06:54<01:10,  1.60s/it]Epoch 25:  86%|████████▌ | 257/300 [06:56<01:12,  1.69s/it]Epoch 25:  86%|████████▌ | 258/300 [06:58<01:12,  1.72s/it]Epoch 25:  86%|████████▋ | 259/300 [07:00<01:11,  1.73s/it]06/19/2022 18:16:39 - INFO - __main__ - global step: 3880; train loss: 8.133416175842285; dev loss
Epoch 25:  87%|████████▋ | 260/300 [07:02<01:09,  1.75s/it]Epoch 25:  87%|████████▋ | 261/300 [07:03<01:04,  1.67s/it]Epoch 25:  87%|████████▋ | 262/300 [07:05<01:04,  1.69s/it]Epoch 25:  88%|████████▊ | 263/300 [07:07<01:03,  1.72s/it]Epoch 25:  88%|████████▊ | 264/300 [07:08<01:02,  1.74s/it]Epoch 25:  88%|████████▊ | 265/300 [07:10<00:58,  1.66s/it]Epoch 25:  89%|████████▊ | 266/300 [07:12<00:57,  1.69s/it]Epoch 25:  89%|████████▉ | 267/300 [07:13<00:56,  1.72s/it]Epoch 25:  89%|████████▉ | 268/300 [07:15<00:51,  1.62s/it]Epoch 25:  90%|████████▉ | 269/300 [07:17<00:51,  1.67s/it]Epoch 25:  90%|█████████ | 270/300 [07:18<00:48,  1.62s/it]Epoch 25:  90%|█████████ | 271/300 [07:20<00:48,  1.67s/it]Epoch 25:  91%|█████████ | 272/300 [07:22<00:47,  1.70s/it]Epoch 25:  91%|█████████ | 273/300 [07:23<00:43,  1.61s/it]Epoch 25:  91%|█████████▏| 274/300 [07:25<00:44,  1.69s/it]Epoch 25:  92%|█████████▏| 275/300 [07:27<00:43,  1.72s/it]Epoch 25:  92%|█████████▏| 276/300 [07:29<00:41,  1.75s/it]Epoch 25:  92%|█████████▏| 277/300 [07:30<00:40,  1.78s/it]Epoch 25:  93%|█████████▎| 278/300 [07:32<00:37,  1.69s/it]Epoch 25:  93%|█████████▎| 279/300 [07:34<00:36,  1.72s/it]06/19/2022 18:17:13 - INFO - __main__ - global step: 3890; train loss: 7.810512542724609; dev loss
Epoch 25:  93%|█████████▎| 280/300 [07:35<00:32,  1.62s/it]Epoch 25:  94%|█████████▎| 281/300 [07:36<00:29,  1.55s/it]Epoch 25:  94%|█████████▍| 282/300 [07:38<00:27,  1.54s/it]Epoch 25:  94%|█████████▍| 283/300 [07:40<00:27,  1.61s/it]Epoch 25:  95%|█████████▍| 284/300 [07:42<00:26,  1.68s/it]Epoch 25:  95%|█████████▌| 285/300 [07:43<00:25,  1.71s/it]Epoch 25:  95%|█████████▌| 286/300 [07:45<00:24,  1.77s/it]Epoch 25:  96%|█████████▌| 287/300 [07:47<00:21,  1.65s/it]Epoch 25:  96%|█████████▌| 288/300 [07:48<00:18,  1.56s/it]Epoch 25:  96%|█████████▋| 289/300 [07:49<00:16,  1.51s/it]Epoch 25:  97%|█████████▋| 290/300 [07:51<00:16,  1.64s/it]Epoch 25:  97%|█████████▋| 291/300 [07:53<00:14,  1.56s/it]Epoch 25:  97%|█████████▋| 292/300 [07:54<00:12,  1.50s/it]Epoch 25:  98%|█████████▊| 293/300 [07:56<00:11,  1.59s/it]Epoch 25:  98%|█████████▊| 294/300 [07:57<00:09,  1.53s/it]Epoch 25:  98%|█████████▊| 295/300 [07:59<00:07,  1.52s/it]Epoch 25:  99%|█████████▊| 296/300 [08:00<00:05,  1.48s/it]Epoch 25:  99%|█████████▉| 297/300 [08:01<00:04,  1.44s/it]Epoch 25:  99%|█████████▉| 298/300 [08:03<00:03,  1.55s/it]Epoch 25: 100%|█████████▉| 299/300 [08:05<00:01,  1.53s/it]06/19/2022 18:17:44 - INFO - __main__ - global step: 3900; train loss: 8.15699291229248; dev loss
Epoch 25: 100%|██████████| 300/300 [08:06<00:00,  1.48s/it]Epoch 25: 100%|██████████| 300/300 [08:06<00:00,  1.62s/it]
Epoch 26:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 26:   0%|          | 1/300 [00:01<06:56,  1.39s/it]Epoch 26:   1%|          | 2/300 [00:03<08:07,  1.63s/it]Epoch 26:   1%|          | 3/300 [00:05<08:41,  1.76s/it]Epoch 26:   1%|▏         | 4/300 [00:06<08:49,  1.79s/it]Epoch 26:   2%|▏         | 5/300 [00:08<08:05,  1.65s/it]Epoch 26:   2%|▏         | 6/300 [00:10<08:18,  1.69s/it]Epoch 26:   2%|▏         | 7/300 [00:12<08:37,  1.77s/it]Epoch 26:   3%|▎         | 8/300 [00:13<08:39,  1.78s/it]Epoch 26:   3%|▎         | 9/300 [00:15<08:02,  1.66s/it]Epoch 26:   3%|▎         | 10/300 [00:16<07:37,  1.58s/it]Epoch 26:   4%|▎         | 11/300 [00:18<07:26,  1.55s/it]Epoch 26:   4%|▍         | 12/300 [00:19<07:46,  1.62s/it]Epoch 26:   4%|▍         | 13/300 [00:21<08:01,  1.68s/it]Epoch 26:   5%|▍         | 14/300 [00:23<08:10,  1.72s/it]Epoch 26:   5%|▌         | 15/300 [00:25<08:28,  1.78s/it]Epoch 26:   5%|▌         | 16/300 [00:26<07:51,  1.66s/it]Epoch 26:   6%|▌         | 17/300 [00:28<07:25,  1.57s/it]Epoch 26:   6%|▌         | 18/300 [00:30<07:43,  1.64s/it]Epoch 26:   6%|▋         | 19/300 [00:31<07:29,  1.60s/it]06/19/2022 18:18:17 - INFO - __main__ - global step: 3910; train loss: 7.790719509124756; dev loss
Epoch 26:   7%|▋         | 20/300 [00:33<07:47,  1.67s/it]Epoch 26:   7%|▋         | 21/300 [00:34<07:23,  1.59s/it]Epoch 26:   7%|▋         | 22/300 [00:36<07:05,  1.53s/it]Epoch 26:   8%|▊         | 23/300 [00:37<06:51,  1.49s/it]Epoch 26:   8%|▊         | 24/300 [00:38<06:50,  1.49s/it]Epoch 26:   8%|▊         | 25/300 [00:40<07:13,  1.58s/it]Epoch 26:   9%|▊         | 26/300 [00:42<07:30,  1.64s/it]Epoch 26:   9%|▉         | 27/300 [00:44<07:42,  1.69s/it]Epoch 26:   9%|▉         | 28/300 [00:45<07:24,  1.63s/it]Epoch 26:  10%|▉         | 29/300 [00:47<07:01,  1.55s/it]Epoch 26:  10%|█         | 30/300 [00:48<06:46,  1.50s/it]Epoch 26:  10%|█         | 31/300 [00:50<06:35,  1.47s/it]Epoch 26:  11%|█         | 32/300 [00:51<07:06,  1.59s/it]Epoch 26:  11%|█         | 33/300 [00:53<07:22,  1.66s/it]Epoch 26:  11%|█▏        | 34/300 [00:55<07:01,  1.58s/it]Epoch 26:  12%|█▏        | 35/300 [00:56<07:15,  1.64s/it]Epoch 26:  12%|█▏        | 36/300 [00:58<07:33,  1.72s/it]Epoch 26:  12%|█▏        | 37/300 [01:00<07:37,  1.74s/it]Epoch 26:  13%|█▎        | 38/300 [01:02<07:44,  1.77s/it]Epoch 26:  13%|█▎        | 39/300 [01:03<07:11,  1.66s/it]06/19/2022 18:18:50 - INFO - __main__ - global step: 3920; train loss: 7.619513511657715; dev loss
Epoch 26:  13%|█▎        | 40/300 [01:05<07:28,  1.72s/it]Epoch 26:  14%|█▎        | 41/300 [01:07<07:33,  1.75s/it]Epoch 26:  14%|█▍        | 42/300 [01:09<07:35,  1.77s/it]Epoch 26:  14%|█▍        | 43/300 [01:11<07:37,  1.78s/it]Epoch 26:  15%|█▍        | 44/300 [01:12<07:13,  1.69s/it]Epoch 26:  15%|█▌        | 45/300 [01:14<06:47,  1.60s/it]Epoch 26:  15%|█▌        | 46/300 [01:15<06:31,  1.54s/it]Epoch 26:  16%|█▌        | 47/300 [01:17<06:48,  1.62s/it]Epoch 26:  16%|█▌        | 48/300 [01:18<06:30,  1.55s/it]Epoch 26:  16%|█▋        | 49/300 [01:20<06:23,  1.53s/it]Epoch 26:  17%|█▋        | 50/300 [01:21<06:42,  1.61s/it]Epoch 26:  17%|█▋        | 51/300 [01:23<06:24,  1.54s/it]Epoch 26:  17%|█▋        | 52/300 [01:24<06:11,  1.50s/it]Epoch 26:  18%|█▊        | 53/300 [01:26<06:08,  1.49s/it]Epoch 26:  18%|█▊        | 54/300 [01:27<05:59,  1.46s/it]Epoch 26:  18%|█▊        | 55/300 [01:29<06:22,  1.56s/it]Epoch 26:  19%|█▊        | 56/300 [01:30<06:09,  1.51s/it]Epoch 26:  19%|█▉        | 57/300 [01:32<06:37,  1.64s/it]Epoch 26:  19%|█▉        | 58/300 [01:34<06:48,  1.69s/it]Epoch 26:  20%|█▉        | 59/300 [01:36<06:54,  1.72s/it]06/19/2022 18:19:22 - INFO - __main__ - global step: 3930; train loss: 7.674231052398682; dev loss
Epoch 26:  20%|██        | 60/300 [01:38<06:58,  1.74s/it]Epoch 26:  20%|██        | 61/300 [01:39<07:09,  1.80s/it]Epoch 26:  21%|██        | 62/300 [01:41<06:39,  1.68s/it]Epoch 26:  21%|██        | 63/300 [01:43<06:48,  1.72s/it]Epoch 26:  21%|██▏       | 64/300 [01:45<06:53,  1.75s/it]Epoch 26:  22%|██▏       | 65/300 [01:46<06:34,  1.68s/it]Epoch 26:  22%|██▏       | 66/300 [01:47<06:10,  1.58s/it]Epoch 26:  22%|██▏       | 67/300 [01:49<05:54,  1.52s/it]Epoch 26:  23%|██▎       | 68/300 [01:51<06:11,  1.60s/it]Epoch 26:  23%|██▎       | 69/300 [01:52<06:02,  1.57s/it]Epoch 26:  23%|██▎       | 70/300 [01:54<06:15,  1.63s/it]Epoch 26:  24%|██▎       | 71/300 [01:56<06:25,  1.68s/it]Epoch 26:  24%|██▍       | 72/300 [01:57<06:04,  1.60s/it]Epoch 26:  24%|██▍       | 73/300 [01:59<06:25,  1.70s/it]Epoch 26:  25%|██▍       | 74/300 [02:01<06:29,  1.72s/it]Epoch 26:  25%|██▌       | 75/300 [02:02<06:07,  1.63s/it]Epoch 26:  25%|██▌       | 76/300 [02:04<06:16,  1.68s/it]Epoch 26:  26%|██▌       | 77/300 [02:05<05:56,  1.60s/it]Epoch 26:  26%|██▌       | 78/300 [02:07<05:47,  1.57s/it]Epoch 26:  26%|██▋       | 79/300 [02:08<05:33,  1.51s/it]06/19/2022 18:19:54 - INFO - __main__ - global step: 3940; train loss: 7.912949562072754; dev loss
Epoch 26:  27%|██▋       | 80/300 [02:10<05:22,  1.47s/it]Epoch 26:  27%|██▋       | 81/300 [02:11<05:42,  1.56s/it]Epoch 26:  27%|██▋       | 82/300 [02:13<05:36,  1.55s/it]Epoch 26:  28%|██▊       | 83/300 [02:15<05:51,  1.62s/it]Epoch 26:  28%|██▊       | 84/300 [02:16<06:01,  1.67s/it]Epoch 26:  28%|██▊       | 85/300 [02:18<06:08,  1.71s/it]Epoch 26:  29%|██▊       | 86/300 [02:20<06:18,  1.77s/it]Epoch 26:  29%|██▉       | 87/300 [02:22<06:18,  1.78s/it]Epoch 26:  29%|██▉       | 88/300 [02:23<05:55,  1.68s/it]Epoch 26:  30%|██▉       | 89/300 [02:25<05:35,  1.59s/it]Epoch 26:  30%|███       | 90/300 [02:27<05:52,  1.68s/it]Epoch 26:  30%|███       | 91/300 [02:28<05:30,  1.58s/it]Epoch 26:  31%|███       | 92/300 [02:30<05:44,  1.66s/it]Epoch 26:  31%|███       | 93/300 [02:32<05:52,  1.70s/it]Epoch 26:  31%|███▏      | 94/300 [02:34<06:05,  1.77s/it]Epoch 26:  32%|███▏      | 95/300 [02:35<06:05,  1.78s/it]Epoch 26:  32%|███▏      | 96/300 [02:37<05:40,  1.67s/it]Epoch 26:  32%|███▏      | 97/300 [02:38<05:21,  1.59s/it]Epoch 26:  33%|███▎      | 98/300 [02:40<05:38,  1.68s/it]Epoch 26:  33%|███▎      | 99/300 [02:42<05:18,  1.59s/it]06/19/2022 18:20:28 - INFO - __main__ - global step: 3950; train loss: 8.062429428100586; dev loss
Epoch 26:  33%|███▎      | 100/300 [02:43<05:28,  1.64s/it]Epoch 26:  34%|███▎      | 101/300 [02:45<05:37,  1.69s/it]Epoch 26:  34%|███▍      | 102/300 [02:47<05:43,  1.73s/it]Epoch 26:  34%|███▍      | 103/300 [02:48<05:26,  1.66s/it]Epoch 26:  35%|███▍      | 104/300 [02:50<05:08,  1.57s/it]Epoch 26:  35%|███▌      | 105/300 [02:52<05:20,  1.64s/it]Epoch 26:  35%|███▌      | 106/300 [02:53<05:05,  1.57s/it]Epoch 26:  36%|███▌      | 107/300 [02:55<05:22,  1.67s/it]Epoch 26:  36%|███▌      | 108/300 [02:56<05:03,  1.58s/it]Epoch 26:  36%|███▋      | 109/300 [02:58<04:50,  1.52s/it]Epoch 26:  37%|███▋      | 110/300 [02:59<04:40,  1.47s/it]Epoch 26:  37%|███▋      | 111/300 [03:00<04:38,  1.47s/it]Epoch 26:  37%|███▋      | 112/300 [03:02<04:31,  1.44s/it]Epoch 26:  38%|███▊      | 113/300 [03:04<04:49,  1.55s/it]Epoch 26:  38%|███▊      | 114/300 [03:05<05:01,  1.62s/it]Epoch 26:  38%|███▊      | 115/300 [03:07<04:52,  1.58s/it]Epoch 26:  39%|███▊      | 116/300 [03:09<05:05,  1.66s/it]Epoch 26:  39%|███▉      | 117/300 [03:10<04:47,  1.57s/it]Epoch 26:  39%|███▉      | 118/300 [03:12<04:57,  1.64s/it]Epoch 26:  40%|███▉      | 119/300 [03:13<04:49,  1.60s/it]06/19/2022 18:20:59 - INFO - __main__ - global step: 3960; train loss: 8.020005226135254; dev loss
Epoch 26:  40%|████      | 120/300 [03:15<04:35,  1.53s/it]Epoch 26:  40%|████      | 121/300 [03:16<04:26,  1.49s/it]Epoch 26:  41%|████      | 122/300 [03:18<04:41,  1.58s/it]Epoch 26:  41%|████      | 123/300 [03:20<04:56,  1.67s/it]Epoch 26:  41%|████▏     | 124/300 [03:21<04:40,  1.60s/it]Epoch 26:  42%|████▏     | 125/300 [03:23<04:50,  1.66s/it]Epoch 26:  42%|████▏     | 126/300 [03:25<04:57,  1.71s/it]Epoch 26:  42%|████▏     | 127/300 [03:26<04:44,  1.64s/it]Epoch 26:  43%|████▎     | 128/300 [03:28<04:29,  1.57s/it]Epoch 26:  43%|████▎     | 129/300 [03:30<04:39,  1.63s/it]Epoch 26:  43%|████▎     | 130/300 [03:31<04:26,  1.57s/it]Epoch 26:  44%|████▎     | 131/300 [03:32<04:16,  1.52s/it]Epoch 26:  44%|████▍     | 132/300 [03:34<04:34,  1.63s/it]Epoch 26:  44%|████▍     | 133/300 [03:36<04:20,  1.56s/it]Epoch 26:  45%|████▍     | 134/300 [03:37<04:09,  1.51s/it]Epoch 26:  45%|████▌     | 135/300 [03:39<04:12,  1.53s/it]Epoch 26:  45%|████▌     | 136/300 [03:40<04:16,  1.56s/it]Epoch 26:  46%|████▌     | 137/300 [03:42<04:21,  1.60s/it]Epoch 26:  46%|████▌     | 138/300 [03:43<04:09,  1.54s/it]Epoch 26:  46%|████▋     | 139/300 [03:45<04:20,  1.62s/it]06/19/2022 18:21:31 - INFO - __main__ - global step: 3970; train loss: 7.752429962158203; dev loss
Epoch 26:  47%|████▋     | 140/300 [03:47<04:12,  1.58s/it]Epoch 26:  47%|████▋     | 141/300 [03:48<04:01,  1.52s/it]Epoch 26:  47%|████▋     | 142/300 [03:49<03:53,  1.48s/it]Epoch 26:  48%|████▊     | 143/300 [03:51<04:10,  1.59s/it]Epoch 26:  48%|████▊     | 144/300 [03:53<04:31,  1.74s/it]Epoch 26:  48%|████▊     | 145/300 [03:55<04:19,  1.67s/it]Epoch 26:  49%|████▊     | 146/300 [03:57<04:28,  1.74s/it]Epoch 26:  49%|████▉     | 147/300 [03:59<04:33,  1.79s/it]Epoch 26:  49%|████▉     | 148/300 [04:00<04:22,  1.73s/it]Epoch 26:  50%|████▉     | 149/300 [04:02<04:11,  1.67s/it]Epoch 26:  50%|█████     | 150/300 [04:04<04:22,  1.75s/it]Epoch 26:  50%|█████     | 151/300 [04:05<04:09,  1.68s/it]Epoch 26:  51%|█████     | 152/300 [04:07<04:02,  1.64s/it]Epoch 26:  51%|█████     | 153/300 [04:08<03:52,  1.58s/it]Epoch 26:  51%|█████▏    | 154/300 [04:10<03:45,  1.54s/it]Epoch 26:  52%|█████▏    | 155/300 [04:11<03:40,  1.52s/it]Epoch 26:  52%|█████▏    | 156/300 [04:13<03:54,  1.63s/it]Epoch 26:  52%|█████▏    | 157/300 [04:15<03:51,  1.62s/it]Epoch 26:  53%|█████▎    | 158/300 [04:16<03:44,  1.58s/it]Epoch 26:  53%|█████▎    | 159/300 [04:18<03:38,  1.55s/it]06/19/2022 18:22:03 - INFO - __main__ - global step: 3980; train loss: 7.520792484283447; dev loss
Epoch 26:  53%|█████▎    | 160/300 [04:19<03:33,  1.52s/it]Epoch 26:  54%|█████▎    | 161/300 [04:21<03:52,  1.67s/it]Epoch 26:  54%|█████▍    | 162/300 [04:23<04:00,  1.75s/it]Epoch 26:  54%|█████▍    | 163/300 [04:25<04:08,  1.81s/it]Epoch 26:  55%|█████▍    | 164/300 [04:27<04:14,  1.87s/it]Epoch 26:  55%|█████▌    | 165/300 [04:29<04:03,  1.80s/it]Epoch 26:  55%|█████▌    | 166/300 [04:31<04:07,  1.85s/it]Epoch 26:  56%|█████▌    | 167/300 [04:32<03:52,  1.75s/it]Epoch 26:  56%|█████▌    | 168/300 [04:34<03:41,  1.68s/it]Epoch 26:  56%|█████▋    | 169/300 [04:35<03:36,  1.65s/it]Epoch 26:  57%|█████▋    | 170/300 [04:37<03:44,  1.73s/it]Epoch 26:  57%|█████▋    | 171/300 [04:39<03:50,  1.79s/it]Epoch 26:  57%|█████▋    | 172/300 [04:41<03:37,  1.70s/it]Epoch 26:  58%|█████▊    | 173/300 [04:42<03:31,  1.66s/it]Epoch 26:  58%|█████▊    | 174/300 [04:44<03:39,  1.74s/it]Epoch 26:  58%|█████▊    | 175/300 [04:46<03:29,  1.68s/it]Epoch 26:  59%|█████▊    | 176/300 [04:47<03:36,  1.75s/it]Epoch 26:  59%|█████▉    | 177/300 [04:49<03:29,  1.70s/it]Epoch 26:  59%|█████▉    | 178/300 [04:51<03:19,  1.64s/it]Epoch 26:  60%|█████▉    | 179/300 [04:52<03:12,  1.59s/it]06/19/2022 18:22:38 - INFO - __main__ - global step: 3990; train loss: 8.325532913208008; dev loss
Epoch 26:  60%|██████    | 180/300 [04:54<03:07,  1.56s/it]Epoch 26:  60%|██████    | 181/300 [04:56<03:21,  1.70s/it]Epoch 26:  61%|██████    | 182/300 [04:58<03:28,  1.77s/it]Epoch 26:  61%|██████    | 183/300 [04:59<03:32,  1.82s/it]Epoch 26:  61%|██████▏   | 184/300 [05:01<03:32,  1.83s/it]Epoch 26:  62%|██████▏   | 185/300 [05:03<03:19,  1.73s/it]Epoch 26:  62%|██████▏   | 186/300 [05:05<03:20,  1.76s/it]Epoch 26:  62%|██████▏   | 187/300 [05:06<03:13,  1.71s/it]Epoch 26:  63%|██████▎   | 188/300 [05:08<03:10,  1.70s/it]Epoch 26:  63%|██████▎   | 189/300 [05:10<03:09,  1.70s/it]Epoch 26:  63%|██████▎   | 190/300 [05:11<03:09,  1.72s/it]Epoch 26:  64%|██████▎   | 191/300 [05:13<03:06,  1.71s/it]Epoch 26:  64%|██████▍   | 192/300 [05:15<02:57,  1.64s/it]Epoch 26:  64%|██████▍   | 193/300 [05:16<02:50,  1.59s/it]Epoch 26:  65%|██████▍   | 194/300 [05:18<02:49,  1.60s/it]Epoch 26:  65%|██████▌   | 195/300 [05:19<02:50,  1.62s/it]Epoch 26:  65%|██████▌   | 196/300 [05:21<02:45,  1.59s/it]Epoch 26:  66%|██████▌   | 197/300 [05:22<02:40,  1.56s/it]Epoch 26:  66%|██████▌   | 198/300 [05:24<02:45,  1.62s/it]Epoch 26:  66%|██████▋   | 199/300 [05:26<02:40,  1.58s/it]06/19/2022 18:23:12 - INFO - __main__ - global step: 4000; train loss: 7.71560001373291; dev loss
Epoch 26:  67%|██████▋   | 200/300 [05:28<02:50,  1.70s/it]Epoch 26:  67%|██████▋   | 201/300 [05:29<02:44,  1.66s/it]Epoch 26:  67%|██████▋   | 202/300 [05:31<02:53,  1.77s/it]Epoch 26:  68%|██████▊   | 203/300 [05:33<02:43,  1.69s/it]Epoch 26:  68%|██████▊   | 204/300 [05:34<02:36,  1.63s/it]Epoch 26:  68%|██████▊   | 205/300 [05:36<02:41,  1.70s/it]Epoch 26:  69%|██████▊   | 206/300 [05:38<02:48,  1.79s/it]Epoch 26:  69%|██████▉   | 207/300 [05:40<02:49,  1.82s/it]Epoch 26:  69%|██████▉   | 208/300 [05:41<02:39,  1.73s/it]Epoch 26:  70%|██████▉   | 209/300 [05:43<02:30,  1.65s/it]Epoch 26:  70%|███████   | 210/300 [05:44<02:24,  1.61s/it]Epoch 26:  70%|███████   | 211/300 [05:46<02:33,  1.72s/it]Epoch 26:  71%|███████   | 212/300 [05:48<02:35,  1.77s/it]Epoch 26:  71%|███████   | 213/300 [05:50<02:26,  1.68s/it]Epoch 26:  71%|███████▏  | 214/300 [05:51<02:19,  1.63s/it]Epoch 26:  72%|███████▏  | 215/300 [05:53<02:17,  1.61s/it]Epoch 26:  72%|███████▏  | 216/300 [05:55<02:22,  1.70s/it]Epoch 26:  72%|███████▏  | 217/300 [05:56<02:16,  1.64s/it]Epoch 26:  73%|███████▎  | 218/300 [05:58<02:20,  1.72s/it]Epoch 26:  73%|███████▎  | 219/300 [06:00<02:16,  1.68s/it]06/19/2022 18:23:46 - INFO - __main__ - global step: 4010; train loss: 8.19566535949707; dev loss
Epoch 26:  73%|███████▎  | 220/300 [06:02<02:19,  1.74s/it]Epoch 26:  74%|███████▎  | 221/300 [06:04<02:21,  1.80s/it]Epoch 26:  74%|███████▍  | 222/300 [06:05<02:22,  1.83s/it]Epoch 26:  74%|███████▍  | 223/300 [06:07<02:14,  1.75s/it]Epoch 26:  75%|███████▍  | 224/300 [06:08<02:07,  1.67s/it]Epoch 26:  75%|███████▌  | 225/300 [06:10<02:01,  1.62s/it]Epoch 26:  75%|███████▌  | 226/300 [06:11<01:56,  1.58s/it]Epoch 26:  76%|███████▌  | 227/300 [06:13<01:55,  1.58s/it]Epoch 26:  76%|███████▌  | 228/300 [06:15<02:00,  1.68s/it]Epoch 26:  76%|███████▋  | 229/300 [06:17<02:04,  1.75s/it]Epoch 26:  77%|███████▋  | 230/300 [06:19<02:05,  1.80s/it]Epoch 26:  77%|███████▋  | 231/300 [06:20<01:59,  1.73s/it]Epoch 26:  77%|███████▋  | 232/300 [06:22<02:01,  1.78s/it]Epoch 26:  78%|███████▊  | 233/300 [06:24<01:53,  1.69s/it]Epoch 26:  78%|███████▊  | 234/300 [06:25<01:48,  1.64s/it]Epoch 26:  78%|███████▊  | 235/300 [06:27<01:53,  1.75s/it]Epoch 26:  79%|███████▊  | 236/300 [06:29<01:47,  1.68s/it]Epoch 26:  79%|███████▉  | 237/300 [06:31<01:49,  1.74s/it]Epoch 26:  79%|███████▉  | 238/300 [06:32<01:42,  1.66s/it]Epoch 26:  80%|███████▉  | 239/300 [06:34<01:37,  1.61s/it]06/19/2022 18:24:20 - INFO - __main__ - global step: 4020; train loss: 7.587890625; dev loss
Epoch 26:  80%|████████  | 240/300 [06:36<01:43,  1.72s/it]Epoch 26:  80%|████████  | 241/300 [06:37<01:44,  1.77s/it]Epoch 26:  81%|████████  | 242/300 [06:39<01:44,  1.81s/it]Epoch 26:  81%|████████  | 243/300 [06:41<01:37,  1.71s/it]Epoch 26:  81%|████████▏ | 244/300 [06:43<01:40,  1.79s/it]Epoch 26:  82%|████████▏ | 245/300 [06:45<01:40,  1.82s/it]Epoch 26:  82%|████████▏ | 246/300 [06:46<01:33,  1.73s/it]Epoch 26:  82%|████████▏ | 247/300 [06:48<01:34,  1.78s/it]Epoch 26:  83%|████████▎ | 248/300 [06:50<01:29,  1.72s/it]Epoch 26:  83%|████████▎ | 249/300 [06:52<01:30,  1.78s/it]Epoch 26:  83%|████████▎ | 250/300 [06:54<01:31,  1.82s/it]Epoch 26:  84%|████████▎ | 251/300 [06:55<01:30,  1.85s/it]Epoch 26:  84%|████████▍ | 252/300 [06:58<01:31,  1.91s/it]Epoch 26:  84%|████████▍ | 253/300 [06:59<01:23,  1.78s/it]Epoch 26:  85%|████████▍ | 254/300 [07:01<01:23,  1.82s/it]Epoch 26:  85%|████████▌ | 255/300 [07:03<01:23,  1.85s/it]Epoch 26:  85%|████████▌ | 256/300 [07:05<01:23,  1.90s/it]Epoch 26:  86%|████████▌ | 257/300 [07:07<01:21,  1.90s/it]Epoch 26:  86%|████████▌ | 258/300 [07:09<01:19,  1.90s/it]Epoch 26:  86%|████████▋ | 259/300 [07:11<01:17,  1.90s/it]06/19/2022 18:24:57 - INFO - __main__ - global step: 4030; train loss: 7.806146144866943; dev loss
Epoch 26:  87%|████████▋ | 260/300 [07:13<01:17,  1.93s/it]Epoch 26:  87%|████████▋ | 261/300 [07:14<01:14,  1.92s/it]Epoch 26:  87%|████████▋ | 262/300 [07:16<01:12,  1.91s/it]Epoch 26:  88%|████████▊ | 263/300 [07:18<01:06,  1.79s/it]Epoch 26:  88%|████████▊ | 264/300 [07:19<01:01,  1.69s/it]Epoch 26:  88%|████████▊ | 265/300 [07:21<00:58,  1.66s/it]Epoch 26:  89%|████████▊ | 266/300 [07:23<00:59,  1.74s/it]Epoch 26:  89%|████████▉ | 267/300 [07:24<00:54,  1.66s/it]Epoch 26:  89%|████████▉ | 268/300 [07:26<00:51,  1.62s/it]Epoch 26:  90%|████████▉ | 269/300 [07:27<00:49,  1.60s/it]Epoch 26:  90%|█████████ | 270/300 [07:29<00:50,  1.69s/it]Epoch 26:  90%|█████████ | 271/300 [07:31<00:50,  1.75s/it]Epoch 26:  91%|█████████ | 272/300 [07:33<00:47,  1.69s/it]Epoch 26:  91%|█████████ | 273/300 [07:34<00:44,  1.67s/it]Epoch 26:  91%|█████████▏| 274/300 [07:36<00:45,  1.74s/it]Epoch 26:  92%|█████████▏| 275/300 [07:38<00:41,  1.67s/it]Epoch 26:  92%|█████████▏| 276/300 [07:40<00:41,  1.74s/it]Epoch 26:  92%|█████████▏| 277/300 [07:42<00:41,  1.82s/it]Epoch 26:  93%|█████████▎| 278/300 [07:44<00:40,  1.85s/it]Epoch 26:  93%|█████████▎| 279/300 [07:45<00:39,  1.87s/it]06/19/2022 18:25:31 - INFO - __main__ - global step: 4040; train loss: 8.282014846801758; dev loss
Epoch 26:  93%|█████████▎| 280/300 [07:47<00:35,  1.76s/it]Epoch 26:  94%|█████████▎| 281/300 [07:49<00:32,  1.71s/it]Epoch 26:  94%|█████████▍| 282/300 [07:50<00:29,  1.64s/it]Epoch 26:  94%|█████████▍| 283/300 [07:52<00:29,  1.72s/it]Epoch 26:  95%|█████████▍| 284/300 [07:54<00:28,  1.78s/it]Epoch 26:  95%|█████████▌| 285/300 [07:55<00:25,  1.73s/it]Epoch 26:  95%|█████████▌| 286/300 [07:57<00:24,  1.78s/it]Epoch 26:  96%|█████████▌| 287/300 [07:59<00:23,  1.81s/it]Epoch 26:  96%|█████████▌| 288/300 [08:01<00:20,  1.72s/it]Epoch 26:  96%|█████████▋| 289/300 [08:02<00:18,  1.68s/it]Epoch 26:  97%|█████████▋| 290/300 [08:04<00:17,  1.75s/it]Epoch 26:  97%|█████████▋| 291/300 [08:06<00:15,  1.68s/it]Epoch 26:  97%|█████████▋| 292/300 [08:07<00:12,  1.62s/it]Epoch 26:  98%|█████████▊| 293/300 [08:09<00:11,  1.59s/it]Epoch 26:  98%|█████████▊| 294/300 [08:10<00:09,  1.59s/it]Epoch 26:  98%|█████████▊| 295/300 [08:12<00:07,  1.56s/it]Epoch 26:  99%|█████████▊| 296/300 [08:14<00:06,  1.66s/it]Epoch 26:  99%|█████████▉| 297/300 [08:16<00:05,  1.72s/it]Epoch 26:  99%|█████████▉| 298/300 [08:17<00:03,  1.68s/it]Epoch 26: 100%|█████████▉| 299/300 [08:19<00:01,  1.74s/it]06/19/2022 18:26:05 - INFO - __main__ - global step: 4050; train loss: 8.177567481994629; dev loss
Epoch 26: 100%|██████████| 300/300 [08:21<00:00,  1.79s/it]Epoch 26: 100%|██████████| 300/300 [08:21<00:00,  1.67s/it]
Epoch 27:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 27:   0%|          | 1/300 [00:01<07:31,  1.51s/it]Epoch 27:   1%|          | 2/300 [00:03<08:55,  1.80s/it]Epoch 27:   1%|          | 3/300 [00:05<08:13,  1.66s/it]Epoch 27:   1%|▏         | 4/300 [00:06<08:40,  1.76s/it]Epoch 27:   2%|▏         | 5/300 [00:08<08:53,  1.81s/it]Epoch 27:   2%|▏         | 6/300 [00:10<09:14,  1.89s/it]Epoch 27:   2%|▏         | 7/300 [00:12<09:19,  1.91s/it]Epoch 27:   3%|▎         | 8/300 [00:14<08:41,  1.78s/it]Epoch 27:   3%|▎         | 9/300 [00:16<08:54,  1.84s/it]Epoch 27:   3%|▎         | 10/300 [00:18<09:19,  1.93s/it]Epoch 27:   4%|▎         | 11/300 [00:20<09:14,  1.92s/it]Epoch 27:   4%|▍         | 12/300 [00:21<08:35,  1.79s/it]Epoch 27:   4%|▍         | 13/300 [00:23<08:44,  1.83s/it]Epoch 27:   5%|▍         | 14/300 [00:25<08:59,  1.89s/it]Epoch 27:   5%|▌         | 15/300 [00:27<08:24,  1.77s/it]Epoch 27:   5%|▌         | 16/300 [00:29<08:35,  1.81s/it]Epoch 27:   6%|▌         | 17/300 [00:30<08:11,  1.74s/it]Epoch 27:   6%|▌         | 18/300 [00:32<08:22,  1.78s/it]Epoch 27:   6%|▋         | 19/300 [00:34<08:03,  1.72s/it]06/19/2022 18:26:41 - INFO - __main__ - global step: 4060; train loss: 7.864622592926025; dev loss
Epoch 27:   7%|▋         | 20/300 [00:36<08:15,  1.77s/it]Epoch 27:   7%|▋         | 21/300 [00:37<07:52,  1.69s/it]Epoch 27:   7%|▋         | 22/300 [00:39<07:34,  1.63s/it]Epoch 27:   8%|▊         | 23/300 [00:41<08:02,  1.74s/it]Epoch 27:   8%|▊         | 24/300 [00:42<07:40,  1.67s/it]Epoch 27:   8%|▊         | 25/300 [00:44<07:59,  1.74s/it]Epoch 27:   9%|▊         | 26/300 [00:45<07:36,  1.67s/it]Epoch 27:   9%|▉         | 27/300 [00:47<08:01,  1.77s/it]Epoch 27:   9%|▉         | 28/300 [00:49<08:10,  1.80s/it]Epoch 27:  10%|▉         | 29/300 [00:51<08:19,  1.84s/it]Epoch 27:  10%|█         | 30/300 [00:53<07:49,  1.74s/it]Epoch 27:  10%|█         | 31/300 [00:54<07:41,  1.72s/it]Epoch 27:  11%|█         | 32/300 [00:56<07:23,  1.66s/it]Epoch 27:  11%|█         | 33/300 [00:58<07:41,  1.73s/it]Epoch 27:  11%|█▏        | 34/300 [01:00<07:53,  1.78s/it]Epoch 27:  12%|█▏        | 35/300 [01:01<07:36,  1.72s/it]Epoch 27:  12%|█▏        | 36/300 [01:03<07:50,  1.78s/it]Epoch 27:  12%|█▏        | 37/300 [01:05<07:25,  1.69s/it]Epoch 27:  13%|█▎        | 38/300 [01:06<07:07,  1.63s/it]Epoch 27:  13%|█▎        | 39/300 [01:08<07:34,  1.74s/it]06/19/2022 18:27:16 - INFO - __main__ - global step: 4070; train loss: 8.068302154541016; dev loss
Epoch 27:  13%|█▎        | 40/300 [01:10<07:13,  1.67s/it]Epoch 27:  14%|█▎        | 41/300 [01:12<07:29,  1.74s/it]Epoch 27:  14%|█▍        | 42/300 [01:13<07:09,  1.66s/it]Epoch 27:  14%|█▍        | 43/300 [01:15<07:05,  1.65s/it]Epoch 27:  15%|█▍        | 44/300 [01:17<07:23,  1.73s/it]Epoch 27:  15%|█▌        | 45/300 [01:18<07:01,  1.65s/it]Epoch 27:  15%|█▌        | 46/300 [01:20<07:17,  1.72s/it]Epoch 27:  16%|█▌        | 47/300 [01:22<06:58,  1.65s/it]Epoch 27:  16%|█▌        | 48/300 [01:24<07:24,  1.76s/it]Epoch 27:  16%|█▋        | 49/300 [01:25<07:32,  1.80s/it]Epoch 27:  17%|█▋        | 50/300 [01:27<07:07,  1.71s/it]Epoch 27:  17%|█▋        | 51/300 [01:28<06:47,  1.64s/it]Epoch 27:  17%|█▋        | 52/300 [01:30<07:14,  1.75s/it]Epoch 27:  18%|█▊        | 53/300 [01:32<07:22,  1.79s/it]Epoch 27:  18%|█▊        | 54/300 [01:34<07:29,  1.83s/it]Epoch 27:  18%|█▊        | 55/300 [01:36<07:33,  1.85s/it]Epoch 27:  19%|█▊        | 56/300 [01:38<07:13,  1.78s/it]Epoch 27:  19%|█▉        | 57/300 [01:39<06:50,  1.69s/it]Epoch 27:  19%|█▉        | 58/300 [01:41<06:34,  1.63s/it]Epoch 27:  20%|█▉        | 59/300 [01:43<06:54,  1.72s/it]06/19/2022 18:27:50 - INFO - __main__ - global step: 4080; train loss: 8.053415298461914; dev loss
Epoch 27:  20%|██        | 60/300 [01:45<07:11,  1.80s/it]Epoch 27:  20%|██        | 61/300 [01:47<07:16,  1.83s/it]Epoch 27:  21%|██        | 62/300 [01:48<06:52,  1.73s/it]Epoch 27:  21%|██        | 63/300 [01:50<06:34,  1.67s/it]Epoch 27:  21%|██▏       | 64/300 [01:52<06:55,  1.76s/it]Epoch 27:  22%|██▏       | 65/300 [01:53<06:35,  1.68s/it]Epoch 27:  22%|██▏       | 66/300 [01:55<06:47,  1.74s/it]Epoch 27:  22%|██▏       | 67/300 [01:57<06:55,  1.79s/it]Epoch 27:  23%|██▎       | 68/300 [01:58<06:39,  1.72s/it]Epoch 27:  23%|██▎       | 69/300 [02:00<06:23,  1.66s/it]Epoch 27:  23%|██▎       | 70/300 [02:01<06:11,  1.61s/it]Epoch 27:  24%|██▎       | 71/300 [02:03<06:29,  1.70s/it]Epoch 27:  24%|██▍       | 72/300 [02:05<06:14,  1.64s/it]Epoch 27:  24%|██▍       | 73/300 [02:07<06:37,  1.75s/it]Epoch 27:  25%|██▍       | 74/300 [02:08<06:17,  1.67s/it]Epoch 27:  25%|██▌       | 75/300 [02:10<06:31,  1.74s/it]Epoch 27:  25%|██▌       | 76/300 [02:12<06:42,  1.80s/it]Epoch 27:  26%|██▌       | 77/300 [02:14<06:26,  1.73s/it]Epoch 27:  26%|██▌       | 78/300 [02:15<06:07,  1.66s/it]Epoch 27:  26%|██▋       | 79/300 [02:17<05:53,  1.60s/it]06/19/2022 18:28:24 - INFO - __main__ - global step: 4090; train loss: 7.513108730316162; dev loss
Epoch 27:  27%|██▋       | 80/300 [02:18<05:43,  1.56s/it]Epoch 27:  27%|██▋       | 81/300 [02:20<05:42,  1.56s/it]Epoch 27:  27%|██▋       | 82/300 [02:21<05:35,  1.54s/it]Epoch 27:  28%|██▊       | 83/300 [02:23<05:31,  1.53s/it]Epoch 27:  28%|██▊       | 84/300 [02:24<05:27,  1.52s/it]Epoch 27:  28%|██▊       | 85/300 [02:26<05:31,  1.54s/it]Epoch 27:  29%|██▊       | 86/300 [02:28<05:52,  1.65s/it]Epoch 27:  29%|██▉       | 87/300 [02:29<05:41,  1.60s/it]Epoch 27:  29%|██▉       | 88/300 [02:31<05:57,  1.69s/it]Epoch 27:  30%|██▉       | 89/300 [02:33<05:48,  1.65s/it]Epoch 27:  30%|███       | 90/300 [02:34<05:35,  1.60s/it]Epoch 27:  30%|███       | 91/300 [02:36<05:51,  1.68s/it]Epoch 27:  31%|███       | 92/300 [02:37<05:37,  1.62s/it]Epoch 27:  31%|███       | 93/300 [02:39<05:58,  1.73s/it]Epoch 27:  31%|███▏      | 94/300 [02:41<06:06,  1.78s/it]Epoch 27:  32%|███▏      | 95/300 [02:43<06:12,  1.82s/it]Epoch 27:  32%|███▏      | 96/300 [02:45<05:49,  1.71s/it]Epoch 27:  32%|███▏      | 97/300 [02:46<05:40,  1.68s/it]Epoch 27:  33%|███▎      | 98/300 [02:48<05:27,  1.62s/it]Epoch 27:  33%|███▎      | 99/300 [02:50<05:39,  1.69s/it]06/19/2022 18:28:57 - INFO - __main__ - global step: 4100; train loss: 8.382360458374023; dev loss
Epoch 27:  33%|███▎      | 100/300 [02:51<05:26,  1.63s/it]Epoch 27:  34%|███▎      | 101/300 [02:53<05:16,  1.59s/it]Epoch 27:  34%|███▍      | 102/300 [02:54<05:14,  1.59s/it]Epoch 27:  34%|███▍      | 103/300 [02:56<05:05,  1.55s/it]Epoch 27:  35%|███▍      | 104/300 [02:57<05:01,  1.54s/it]Epoch 27:  35%|███▌      | 105/300 [02:59<05:21,  1.65s/it]Epoch 27:  35%|███▌      | 106/300 [03:01<05:16,  1.63s/it]Epoch 27:  36%|███▌      | 107/300 [03:02<05:06,  1.59s/it]Epoch 27:  36%|███▌      | 108/300 [03:04<05:23,  1.68s/it]Epoch 27:  36%|███▋      | 109/300 [03:06<05:33,  1.74s/it]Epoch 27:  37%|███▋      | 110/300 [03:08<05:38,  1.78s/it]Epoch 27:  37%|███▋      | 111/300 [03:10<05:36,  1.78s/it]Epoch 27:  37%|███▋      | 112/300 [03:11<05:18,  1.69s/it]Epoch 27:  38%|███▊      | 113/300 [03:13<05:05,  1.63s/it]Epoch 27:  38%|███▊      | 114/300 [03:14<05:01,  1.62s/it]Epoch 27:  38%|███▊      | 115/300 [03:16<05:08,  1.67s/it]Epoch 27:  39%|███▊      | 116/300 [03:17<04:57,  1.61s/it]Epoch 27:  39%|███▉      | 117/300 [03:19<05:04,  1.66s/it]Epoch 27:  39%|███▉      | 118/300 [03:21<05:13,  1.72s/it]Epoch 27:  40%|███▉      | 119/300 [03:23<04:59,  1.65s/it]06/19/2022 18:29:30 - INFO - __main__ - global step: 4110; train loss: 7.574492454528809; dev loss
Epoch 27:  40%|████      | 120/300 [03:24<05:03,  1.69s/it]Epoch 27:  40%|████      | 121/300 [03:26<05:06,  1.71s/it]Epoch 27:  41%|████      | 122/300 [03:28<05:13,  1.76s/it]Epoch 27:  41%|████      | 123/300 [03:29<04:56,  1.67s/it]Epoch 27:  41%|████▏     | 124/300 [03:31<04:43,  1.61s/it]Epoch 27:  42%|████▏     | 125/300 [03:32<04:34,  1.57s/it]Epoch 27:  42%|████▏     | 126/300 [03:34<04:28,  1.54s/it]Epoch 27:  42%|████▏     | 127/300 [03:35<04:27,  1.55s/it]Epoch 27:  43%|████▎     | 128/300 [03:37<04:37,  1.61s/it]Epoch 27:  43%|████▎     | 129/300 [03:39<04:46,  1.67s/it]Epoch 27:  43%|████▎     | 130/300 [03:40<04:35,  1.62s/it]Epoch 27:  44%|████▎     | 131/300 [03:42<04:52,  1.73s/it]Epoch 27:  44%|████▍     | 132/300 [03:44<04:59,  1.78s/it]Epoch 27:  44%|████▍     | 133/300 [03:46<05:04,  1.83s/it]Epoch 27:  45%|████▍     | 134/300 [03:48<05:06,  1.85s/it]Epoch 27:  45%|████▌     | 135/300 [03:50<05:11,  1.89s/it]Epoch 27:  45%|████▌     | 136/300 [03:52<04:50,  1.77s/it]Epoch 27:  46%|████▌     | 137/300 [03:53<04:33,  1.68s/it]Epoch 27:  46%|████▌     | 138/300 [03:55<04:24,  1.63s/it]Epoch 27:  46%|████▋     | 139/300 [03:57<04:39,  1.73s/it]06/19/2022 18:30:04 - INFO - __main__ - global step: 4120; train loss: 8.185580253601074; dev loss
Epoch 27:  47%|████▋     | 140/300 [03:58<04:25,  1.66s/it]Epoch 27:  47%|████▋     | 141/300 [04:00<04:15,  1.61s/it]Epoch 27:  47%|████▋     | 142/300 [04:02<04:27,  1.70s/it]Epoch 27:  48%|████▊     | 143/300 [04:04<04:40,  1.79s/it]Epoch 27:  48%|████▊     | 144/300 [04:05<04:24,  1.70s/it]Epoch 27:  48%|████▊     | 145/300 [04:07<04:13,  1.64s/it]Epoch 27:  49%|████▊     | 146/300 [04:08<04:04,  1.59s/it]Epoch 27:  49%|████▉     | 147/300 [04:10<04:02,  1.58s/it]Epoch 27:  49%|████▉     | 148/300 [04:11<03:55,  1.55s/it]Epoch 27:  50%|████▉     | 149/300 [04:12<03:50,  1.52s/it]Epoch 27:  50%|█████     | 150/300 [04:14<03:47,  1.51s/it]Epoch 27:  50%|█████     | 151/300 [04:16<04:07,  1.66s/it]Epoch 27:  51%|█████     | 152/300 [04:17<03:57,  1.61s/it]Epoch 27:  51%|█████     | 153/300 [04:19<04:08,  1.69s/it]Epoch 27:  51%|█████▏    | 154/300 [04:21<04:17,  1.76s/it]Epoch 27:  52%|█████▏    | 155/300 [04:23<04:03,  1.68s/it]Epoch 27:  52%|█████▏    | 156/300 [04:25<04:15,  1.77s/it]Epoch 27:  52%|█████▏    | 157/300 [04:27<04:20,  1.82s/it]Epoch 27:  53%|█████▎    | 158/300 [04:28<04:05,  1.73s/it]Epoch 27:  53%|█████▎    | 159/300 [04:30<03:52,  1.65s/it]06/19/2022 18:30:37 - INFO - __main__ - global step: 4130; train loss: 8.494965553283691; dev loss
Epoch 27:  53%|█████▎    | 160/300 [04:32<04:05,  1.75s/it]Epoch 27:  54%|█████▎    | 161/300 [04:34<04:09,  1.80s/it]Epoch 27:  54%|█████▍    | 162/300 [04:35<03:54,  1.70s/it]Epoch 27:  54%|█████▍    | 163/300 [04:37<04:01,  1.76s/it]Epoch 27:  55%|█████▍    | 164/300 [04:39<03:52,  1.71s/it]Epoch 27:  55%|█████▌    | 165/300 [04:40<03:58,  1.77s/it]Epoch 27:  55%|█████▌    | 166/300 [04:42<04:02,  1.81s/it]Epoch 27:  56%|█████▌    | 167/300 [04:44<04:03,  1.83s/it]Epoch 27:  56%|█████▌    | 168/300 [04:46<04:08,  1.88s/it]Epoch 27:  56%|█████▋    | 169/300 [04:48<03:51,  1.77s/it]Epoch 27:  57%|█████▋    | 170/300 [04:50<03:55,  1.81s/it]Epoch 27:  57%|█████▋    | 171/300 [04:52<03:57,  1.84s/it]Epoch 27:  57%|█████▋    | 172/300 [04:54<04:03,  1.90s/it]Epoch 27:  58%|█████▊    | 173/300 [04:55<03:46,  1.78s/it]Epoch 27:  58%|█████▊    | 174/300 [04:57<03:48,  1.82s/it]Epoch 27:  58%|█████▊    | 175/300 [04:58<03:34,  1.71s/it]Epoch 27:  59%|█████▊    | 176/300 [05:00<03:43,  1.80s/it]Epoch 27:  59%|█████▉    | 177/300 [05:02<03:45,  1.83s/it]Epoch 27:  59%|█████▉    | 178/300 [05:04<03:31,  1.74s/it]Epoch 27:  60%|█████▉    | 179/300 [05:06<03:35,  1.78s/it]06/19/2022 18:31:13 - INFO - __main__ - global step: 4140; train loss: 7.458360195159912; dev loss
Epoch 27:  60%|██████    | 180/300 [05:07<03:23,  1.69s/it]Epoch 27:  60%|██████    | 181/300 [05:09<03:32,  1.79s/it]Epoch 27:  61%|██████    | 182/300 [05:11<03:35,  1.83s/it]Epoch 27:  61%|██████    | 183/300 [05:13<03:36,  1.85s/it]Epoch 27:  61%|██████▏   | 184/300 [05:15<03:21,  1.74s/it]Epoch 27:  62%|██████▏   | 185/300 [05:16<03:23,  1.77s/it]Epoch 27:  62%|██████▏   | 186/300 [05:18<03:22,  1.78s/it]Epoch 27:  62%|██████▏   | 187/300 [05:20<03:23,  1.80s/it]Epoch 27:  63%|██████▎   | 188/300 [05:22<03:14,  1.74s/it]Epoch 27:  63%|██████▎   | 189/300 [05:23<03:15,  1.76s/it]Epoch 27:  63%|██████▎   | 190/300 [05:25<03:04,  1.68s/it]Epoch 27:  64%|██████▎   | 191/300 [05:26<02:57,  1.62s/it]Epoch 27:  64%|██████▍   | 192/300 [05:28<03:01,  1.68s/it]Epoch 27:  64%|██████▍   | 193/300 [05:30<02:56,  1.65s/it]Epoch 27:  65%|██████▍   | 194/300 [05:31<02:49,  1.60s/it]Epoch 27:  65%|██████▌   | 195/300 [05:33<02:57,  1.69s/it]Epoch 27:  65%|██████▌   | 196/300 [05:35<03:02,  1.75s/it]Epoch 27:  66%|██████▌   | 197/300 [05:37<03:07,  1.82s/it]Epoch 27:  66%|██████▌   | 198/300 [05:39<02:55,  1.72s/it]Epoch 27:  66%|██████▋   | 199/300 [05:40<02:58,  1.76s/it]06/19/2022 18:31:48 - INFO - __main__ - global step: 4150; train loss: 7.410523414611816; dev loss
Epoch 27:  67%|██████▋   | 200/300 [05:42<02:47,  1.68s/it]Epoch 27:  67%|██████▋   | 201/300 [05:44<02:43,  1.65s/it]Epoch 27:  67%|██████▋   | 202/300 [05:45<02:37,  1.60s/it]Epoch 27:  68%|██████▊   | 203/300 [05:46<02:31,  1.56s/it]Epoch 27:  68%|██████▊   | 204/300 [05:48<02:40,  1.67s/it]Epoch 27:  68%|██████▊   | 205/300 [05:50<02:35,  1.64s/it]Epoch 27:  69%|██████▊   | 206/300 [05:51<02:30,  1.60s/it]Epoch 27:  69%|██████▉   | 207/300 [05:53<02:37,  1.69s/it]Epoch 27:  69%|██████▉   | 208/300 [05:55<02:41,  1.75s/it]Epoch 27:  70%|██████▉   | 209/300 [05:57<02:32,  1.68s/it]Epoch 27:  70%|███████   | 210/300 [05:59<02:39,  1.78s/it]Epoch 27:  70%|███████   | 211/300 [06:00<02:30,  1.69s/it]Epoch 27:  71%|███████   | 212/300 [06:02<02:33,  1.75s/it]Epoch 27:  71%|███████   | 213/300 [06:04<02:25,  1.67s/it]Epoch 27:  71%|███████▏  | 214/300 [06:06<02:32,  1.77s/it]Epoch 27:  72%|███████▏  | 215/300 [06:07<02:23,  1.69s/it]Epoch 27:  72%|███████▏  | 216/300 [06:09<02:17,  1.63s/it]Epoch 27:  72%|███████▏  | 217/300 [06:10<02:12,  1.59s/it]Epoch 27:  73%|███████▎  | 218/300 [06:12<02:09,  1.58s/it]Epoch 27:  73%|███████▎  | 219/300 [06:13<02:05,  1.55s/it]06/19/2022 18:32:21 - INFO - __main__ - global step: 4160; train loss: 8.076797485351562; dev loss
Epoch 27:  73%|███████▎  | 220/300 [06:15<02:02,  1.54s/it]Epoch 27:  74%|███████▎  | 221/300 [06:16<02:00,  1.52s/it]Epoch 27:  74%|███████▍  | 222/300 [06:18<02:10,  1.67s/it]Epoch 27:  74%|███████▍  | 223/300 [06:20<02:14,  1.75s/it]Epoch 27:  75%|███████▍  | 224/300 [06:22<02:07,  1.68s/it]Epoch 27:  75%|███████▌  | 225/300 [06:24<02:11,  1.76s/it]Epoch 27:  75%|███████▌  | 226/300 [06:25<02:05,  1.70s/it]Epoch 27:  76%|███████▌  | 227/300 [06:27<02:08,  1.76s/it]Epoch 27:  76%|███████▌  | 228/300 [06:29<02:09,  1.80s/it]Epoch 27:  76%|███████▋  | 229/300 [06:31<02:10,  1.84s/it]Epoch 27:  77%|███████▋  | 230/300 [06:33<02:12,  1.89s/it]Epoch 27:  77%|███████▋  | 231/300 [06:35<02:10,  1.89s/it]Epoch 27:  77%|███████▋  | 232/300 [06:37<02:09,  1.91s/it]Epoch 27:  78%|███████▊  | 233/300 [06:38<01:59,  1.78s/it]Epoch 27:  78%|███████▊  | 234/300 [06:40<02:00,  1.82s/it]Epoch 27:  78%|███████▊  | 235/300 [06:42<02:02,  1.88s/it]Epoch 27:  79%|███████▊  | 236/300 [06:44<01:53,  1.77s/it]Epoch 27:  79%|███████▉  | 237/300 [06:45<01:46,  1.70s/it]Epoch 27:  79%|███████▉  | 238/300 [06:47<01:41,  1.63s/it]Epoch 27:  80%|███████▉  | 239/300 [06:48<01:38,  1.61s/it]06/19/2022 18:32:56 - INFO - __main__ - global step: 4170; train loss: 7.990084648132324; dev loss
Epoch 27:  80%|████████  | 240/300 [06:50<01:34,  1.57s/it]Epoch 27:  80%|████████  | 241/300 [06:52<01:38,  1.67s/it]Epoch 27:  81%|████████  | 242/300 [06:53<01:40,  1.74s/it]Epoch 27:  81%|████████  | 243/300 [06:55<01:36,  1.69s/it]Epoch 27:  81%|████████▏ | 244/300 [06:57<01:38,  1.75s/it]Epoch 27:  82%|████████▏ | 245/300 [06:58<01:31,  1.67s/it]Epoch 27:  82%|████████▏ | 246/300 [07:00<01:33,  1.74s/it]Epoch 27:  82%|████████▏ | 247/300 [07:02<01:36,  1.82s/it]Epoch 27:  83%|████████▎ | 248/300 [07:04<01:36,  1.85s/it]Epoch 27:  83%|████████▎ | 249/300 [07:06<01:28,  1.73s/it]Epoch 27:  83%|████████▎ | 250/300 [07:07<01:22,  1.66s/it]Epoch 27:  84%|████████▎ | 251/300 [07:09<01:20,  1.64s/it]Epoch 27:  84%|████████▍ | 252/300 [07:10<01:16,  1.59s/it]Epoch 27:  84%|████████▍ | 253/300 [07:12<01:18,  1.68s/it]Epoch 27:  85%|████████▍ | 254/300 [07:14<01:20,  1.75s/it]Epoch 27:  85%|████████▌ | 255/300 [07:16<01:21,  1.82s/it]Epoch 27:  85%|████████▌ | 256/300 [07:18<01:21,  1.84s/it]Epoch 27:  86%|████████▌ | 257/300 [07:19<01:14,  1.74s/it]Epoch 27:  86%|████████▌ | 258/300 [07:21<01:14,  1.78s/it]Epoch 27:  86%|████████▋ | 259/300 [07:23<01:10,  1.72s/it]06/19/2022 18:33:31 - INFO - __main__ - global step: 4180; train loss: 7.6133928298950195; dev loss
Epoch 27:  87%|████████▋ | 260/300 [07:25<01:11,  1.78s/it]Epoch 27:  87%|████████▋ | 261/300 [07:26<01:05,  1.69s/it]Epoch 27:  87%|████████▋ | 262/300 [07:28<01:06,  1.75s/it]Epoch 27:  88%|████████▊ | 263/300 [07:30<01:02,  1.68s/it]Epoch 27:  88%|████████▊ | 264/300 [07:32<01:03,  1.77s/it]Epoch 27:  88%|████████▊ | 265/300 [07:33<00:59,  1.69s/it]Epoch 27:  89%|████████▊ | 266/300 [07:35<00:55,  1.63s/it]Epoch 27:  89%|████████▉ | 267/300 [07:37<00:56,  1.71s/it]Epoch 27:  89%|████████▉ | 268/300 [07:39<00:57,  1.80s/it]Epoch 27:  90%|████████▉ | 269/300 [07:40<00:52,  1.71s/it]Epoch 27:  90%|█████████ | 270/300 [07:42<00:49,  1.65s/it]Epoch 27:  90%|█████████ | 271/300 [07:43<00:46,  1.61s/it]Epoch 27:  91%|█████████ | 272/300 [07:45<00:44,  1.60s/it]Epoch 27:  91%|█████████ | 273/300 [07:46<00:42,  1.58s/it]Epoch 27:  91%|█████████▏| 274/300 [07:48<00:40,  1.56s/it]Epoch 27:  92%|█████████▏| 275/300 [07:50<00:41,  1.66s/it]Epoch 27:  92%|█████████▏| 276/300 [07:51<00:39,  1.64s/it]Epoch 27:  92%|█████████▏| 277/300 [07:53<00:36,  1.59s/it]Epoch 27:  93%|█████████▎| 278/300 [07:55<00:37,  1.69s/it]Epoch 27:  93%|█████████▎| 279/300 [07:56<00:34,  1.63s/it]06/19/2022 18:34:04 - INFO - __main__ - global step: 4190; train loss: 7.641493320465088; dev loss
Epoch 27:  93%|█████████▎| 280/300 [07:58<00:32,  1.62s/it]Epoch 27:  94%|█████████▎| 281/300 [07:59<00:30,  1.58s/it]Epoch 27:  94%|█████████▍| 282/300 [08:01<00:30,  1.68s/it]Epoch 27:  94%|█████████▍| 283/300 [08:03<00:29,  1.75s/it]Epoch 27:  95%|█████████▍| 284/300 [08:05<00:27,  1.70s/it]Epoch 27:  95%|█████████▌| 285/300 [08:06<00:26,  1.76s/it]Epoch 27:  95%|█████████▌| 286/300 [08:08<00:23,  1.68s/it]Epoch 27:  96%|█████████▌| 287/300 [08:10<00:22,  1.76s/it]Epoch 27:  96%|█████████▌| 288/300 [08:12<00:21,  1.81s/it]Epoch 27:  96%|█████████▋| 289/300 [08:14<00:20,  1.87s/it]Epoch 27:  97%|█████████▋| 290/300 [08:16<00:18,  1.88s/it]Epoch 27:  97%|█████████▋| 291/300 [08:17<00:15,  1.77s/it]Epoch 27:  97%|█████████▋| 292/300 [08:19<00:14,  1.81s/it]Epoch 27:  98%|█████████▊| 293/300 [08:21<00:13,  1.87s/it]Epoch 27:  98%|█████████▊| 294/300 [08:23<00:11,  1.88s/it]Epoch 27:  98%|█████████▊| 295/300 [08:25<00:08,  1.76s/it]Epoch 27:  99%|█████████▊| 296/300 [08:26<00:07,  1.80s/it]Epoch 27:  99%|█████████▉| 297/300 [08:28<00:05,  1.74s/it]Epoch 27:  99%|█████████▉| 298/300 [08:30<00:03,  1.79s/it]Epoch 27: 100%|█████████▉| 299/300 [08:32<00:01,  1.83s/it]06/19/2022 18:34:40 - INFO - __main__ - global step: 4200; train loss: 7.7879180908203125; dev loss
Epoch 27: 100%|██████████| 300/300 [08:34<00:00,  1.86s/it]Epoch 27: 100%|██████████| 300/300 [08:34<00:00,  1.71s/it]
Epoch 28:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 28:   0%|          | 1/300 [00:01<07:50,  1.57s/it]Epoch 28:   1%|          | 2/300 [00:03<07:32,  1.52s/it]Epoch 28:   1%|          | 3/300 [00:04<08:26,  1.71s/it]Epoch 28:   1%|▏         | 4/300 [00:06<08:51,  1.80s/it]Epoch 28:   2%|▏         | 5/300 [00:08<08:31,  1.73s/it]Epoch 28:   2%|▏         | 6/300 [00:10<08:04,  1.65s/it]Epoch 28:   2%|▏         | 7/300 [00:11<07:48,  1.60s/it]Epoch 28:   3%|▎         | 8/300 [00:13<07:37,  1.57s/it]Epoch 28:   3%|▎         | 9/300 [00:14<07:37,  1.57s/it]Epoch 28:   3%|▎         | 10/300 [00:16<07:27,  1.54s/it]Epoch 28:   4%|▎         | 11/300 [00:17<07:21,  1.53s/it]Epoch 28:   4%|▍         | 12/300 [00:19<07:52,  1.64s/it]Epoch 28:   4%|▍         | 13/300 [00:21<07:45,  1.62s/it]Epoch 28:   5%|▍         | 14/300 [00:22<08:06,  1.70s/it]Epoch 28:   5%|▌         | 15/300 [00:24<07:46,  1.64s/it]Epoch 28:   5%|▌         | 16/300 [00:25<07:32,  1.59s/it]Epoch 28:   6%|▌         | 17/300 [00:27<07:58,  1.69s/it]Epoch 28:   6%|▌         | 18/300 [00:29<07:47,  1.66s/it]Epoch 28:   6%|▋         | 19/300 [00:31<08:07,  1.73s/it]06/19/2022 18:35:13 - INFO - __main__ - global step: 4210; train loss: 7.668292045593262; dev loss
Epoch 28:   7%|▋         | 20/300 [00:33<08:24,  1.80s/it]Epoch 28:   7%|▋         | 21/300 [00:35<08:31,  1.83s/it]Epoch 28:   7%|▋         | 22/300 [00:36<08:11,  1.77s/it]Epoch 28:   8%|▊         | 23/300 [00:38<08:19,  1.80s/it]Epoch 28:   8%|▊         | 24/300 [00:40<07:51,  1.71s/it]Epoch 28:   8%|▊         | 25/300 [00:42<08:05,  1.77s/it]Epoch 28:   9%|▊         | 26/300 [00:44<08:21,  1.83s/it]Epoch 28:   9%|▉         | 27/300 [00:45<07:53,  1.73s/it]Epoch 28:   9%|▉         | 28/300 [00:47<08:04,  1.78s/it]Epoch 28:  10%|▉         | 29/300 [00:48<07:38,  1.69s/it]Epoch 28:  10%|█         | 30/300 [00:50<07:59,  1.78s/it]Epoch 28:  10%|█         | 31/300 [00:52<08:08,  1.81s/it]Epoch 28:  11%|█         | 32/300 [00:54<08:13,  1.84s/it]Epoch 28:  11%|█         | 33/300 [00:56<08:19,  1.87s/it]Epoch 28:  11%|█▏        | 34/300 [00:58<07:55,  1.79s/it]Epoch 28:  12%|█▏        | 35/300 [00:59<07:28,  1.69s/it]Epoch 28:  12%|█▏        | 36/300 [01:01<07:10,  1.63s/it]Epoch 28:  12%|█▏        | 37/300 [01:02<06:59,  1.60s/it]Epoch 28:  13%|█▎        | 38/300 [01:04<07:31,  1.72s/it]Epoch 28:  13%|█▎        | 39/300 [01:06<07:43,  1.78s/it]06/19/2022 18:35:48 - INFO - __main__ - global step: 4220; train loss: 7.823757171630859; dev loss
Epoch 28:  13%|█▎        | 40/300 [01:08<07:50,  1.81s/it]Epoch 28:  14%|█▎        | 41/300 [01:10<07:55,  1.83s/it]Epoch 28:  14%|█▍        | 42/300 [01:11<07:28,  1.74s/it]Epoch 28:  14%|█▍        | 43/300 [01:13<07:48,  1.82s/it]Epoch 28:  15%|█▍        | 44/300 [01:15<07:22,  1.73s/it]Epoch 28:  15%|█▌        | 45/300 [01:16<07:01,  1.65s/it]Epoch 28:  15%|█▌        | 46/300 [01:18<06:47,  1.60s/it]Epoch 28:  16%|█▌        | 47/300 [01:20<06:44,  1.60s/it]Epoch 28:  16%|█▌        | 48/300 [01:21<07:07,  1.70s/it]Epoch 28:  16%|█▋        | 49/300 [01:23<06:49,  1.63s/it]Epoch 28:  17%|█▋        | 50/300 [01:24<06:35,  1.58s/it]Epoch 28:  17%|█▋        | 51/300 [01:26<06:32,  1.58s/it]Epoch 28:  17%|█▋        | 52/300 [01:27<06:23,  1.55s/it]Epoch 28:  18%|█▊        | 53/300 [01:29<06:48,  1.65s/it]Epoch 28:  18%|█▊        | 54/300 [01:31<06:38,  1.62s/it]Epoch 28:  18%|█▊        | 55/300 [01:33<07:03,  1.73s/it]Epoch 28:  19%|█▊        | 56/300 [01:35<07:13,  1.78s/it]Epoch 28:  19%|█▉        | 57/300 [01:36<06:51,  1.69s/it]Epoch 28:  19%|█▉        | 58/300 [01:38<06:34,  1.63s/it]Epoch 28:  20%|█▉        | 59/300 [01:39<06:27,  1.61s/it]06/19/2022 18:36:21 - INFO - __main__ - global step: 4230; train loss: 7.867185115814209; dev loss
Epoch 28:  20%|██        | 60/300 [01:41<06:46,  1.69s/it]Epoch 28:  20%|██        | 61/300 [01:43<06:32,  1.64s/it]Epoch 28:  21%|██        | 62/300 [01:45<06:51,  1.73s/it]Epoch 28:  21%|██        | 63/300 [01:46<06:40,  1.69s/it]Epoch 28:  21%|██▏       | 64/300 [01:48<06:23,  1.62s/it]Epoch 28:  22%|██▏       | 65/300 [01:50<06:41,  1.71s/it]Epoch 28:  22%|██▏       | 66/300 [01:51<06:52,  1.76s/it]Epoch 28:  22%|██▏       | 67/300 [01:53<07:08,  1.84s/it]Epoch 28:  23%|██▎       | 68/300 [01:55<07:11,  1.86s/it]Epoch 28:  23%|██▎       | 69/300 [01:57<07:11,  1.87s/it]Epoch 28:  23%|██▎       | 70/300 [01:59<06:44,  1.76s/it]Epoch 28:  24%|██▎       | 71/300 [02:00<06:24,  1.68s/it]Epoch 28:  24%|██▍       | 72/300 [02:02<06:43,  1.77s/it]Epoch 28:  24%|██▍       | 73/300 [02:04<06:22,  1.68s/it]Epoch 28:  25%|██▍       | 74/300 [02:06<06:34,  1.74s/it]Epoch 28:  25%|██▌       | 75/300 [02:07<06:15,  1.67s/it]Epoch 28:  25%|██▌       | 76/300 [02:09<06:09,  1.65s/it]Epoch 28:  26%|██▌       | 77/300 [02:11<06:25,  1.73s/it]Epoch 28:  26%|██▌       | 78/300 [02:12<06:07,  1.66s/it]Epoch 28:  26%|██▋       | 79/300 [02:14<06:21,  1.73s/it]06/19/2022 18:36:56 - INFO - __main__ - global step: 4240; train loss: 7.911625862121582; dev loss
Epoch 28:  27%|██▋       | 80/300 [02:16<06:10,  1.68s/it]Epoch 28:  27%|██▋       | 81/300 [02:17<05:56,  1.63s/it]Epoch 28:  27%|██▋       | 82/300 [02:19<05:46,  1.59s/it]Epoch 28:  28%|██▊       | 83/300 [02:20<05:39,  1.56s/it]Epoch 28:  28%|██▊       | 84/300 [02:22<06:04,  1.69s/it]Epoch 28:  28%|██▊       | 85/300 [02:24<05:48,  1.62s/it]Epoch 28:  29%|██▊       | 86/300 [02:25<05:38,  1.58s/it]Epoch 28:  29%|██▉       | 87/300 [02:27<05:56,  1.67s/it]Epoch 28:  29%|██▉       | 88/300 [02:28<05:47,  1.64s/it]Epoch 28:  30%|██▉       | 89/300 [02:30<05:34,  1.59s/it]Epoch 28:  30%|███       | 90/300 [02:32<05:52,  1.68s/it]Epoch 28:  30%|███       | 91/300 [02:33<05:39,  1.63s/it]Epoch 28:  31%|███       | 92/300 [02:35<05:34,  1.61s/it]Epoch 28:  31%|███       | 93/300 [02:36<05:25,  1.57s/it]Epoch 28:  31%|███▏      | 94/300 [02:38<05:43,  1.67s/it]Epoch 28:  32%|███▏      | 95/300 [02:40<05:55,  1.74s/it]Epoch 28:  32%|███▏      | 96/300 [02:42<05:40,  1.67s/it]Epoch 28:  32%|███▏      | 97/300 [02:44<05:57,  1.76s/it]Epoch 28:  33%|███▎      | 98/300 [02:45<05:37,  1.67s/it]Epoch 28:  33%|███▎      | 99/300 [02:47<05:22,  1.61s/it]06/19/2022 18:37:28 - INFO - __main__ - global step: 4250; train loss: 8.085260391235352; dev loss
Epoch 28:  33%|███▎      | 100/300 [02:48<05:12,  1.56s/it]Epoch 28:  34%|███▎      | 101/300 [02:50<05:12,  1.57s/it]Epoch 28:  34%|███▍      | 102/300 [02:51<05:04,  1.54s/it]Epoch 28:  34%|███▍      | 103/300 [02:53<05:23,  1.64s/it]Epoch 28:  35%|███▍      | 104/300 [02:54<05:12,  1.59s/it]Epoch 28:  35%|███▌      | 105/300 [02:56<05:33,  1.71s/it]Epoch 28:  35%|███▌      | 106/300 [02:58<05:18,  1.64s/it]Epoch 28:  36%|███▌      | 107/300 [03:00<05:30,  1.71s/it]Epoch 28:  36%|███▌      | 108/300 [03:02<05:38,  1.76s/it]Epoch 28:  36%|███▋      | 109/300 [03:03<05:25,  1.70s/it]Epoch 28:  37%|███▋      | 110/300 [03:05<05:35,  1.76s/it]Epoch 28:  37%|███▋      | 111/300 [03:07<05:40,  1.80s/it]Epoch 28:  37%|███▋      | 112/300 [03:09<05:22,  1.72s/it]Epoch 28:  38%|███▊      | 113/300 [03:10<05:12,  1.67s/it]Epoch 28:  38%|███▊      | 114/300 [03:12<05:26,  1.76s/it]Epoch 28:  38%|███▊      | 115/300 [03:14<05:35,  1.81s/it]Epoch 28:  39%|███▊      | 116/300 [03:16<05:38,  1.84s/it]Epoch 28:  39%|███▉      | 117/300 [03:18<05:22,  1.76s/it]Epoch 28:  39%|███▉      | 118/300 [03:19<05:04,  1.67s/it]Epoch 28:  40%|███▉      | 119/300 [03:21<05:14,  1.74s/it]06/19/2022 18:38:03 - INFO - __main__ - global step: 4260; train loss: 7.680978298187256; dev loss
Epoch 28:  40%|████      | 120/300 [03:23<05:22,  1.79s/it]Epoch 28:  40%|████      | 121/300 [03:25<05:31,  1.85s/it]Epoch 28:  41%|████      | 122/300 [03:26<05:10,  1.74s/it]Epoch 28:  41%|████      | 123/300 [03:28<04:54,  1.66s/it]Epoch 28:  41%|████▏     | 124/300 [03:30<05:05,  1.74s/it]Epoch 28:  42%|████▏     | 125/300 [03:32<05:11,  1.78s/it]Epoch 28:  42%|████▏     | 126/300 [03:34<05:21,  1.85s/it]Epoch 28:  42%|████▏     | 127/300 [03:35<05:21,  1.86s/it]Epoch 28:  43%|████▎     | 128/300 [03:37<05:00,  1.75s/it]Epoch 28:  43%|████▎     | 129/300 [03:38<04:46,  1.68s/it]Epoch 28:  43%|████▎     | 130/300 [03:40<04:59,  1.76s/it]Epoch 28:  44%|████▎     | 131/300 [03:42<05:04,  1.80s/it]Epoch 28:  44%|████▍     | 132/300 [03:44<04:46,  1.71s/it]Epoch 28:  44%|████▍     | 133/300 [03:46<04:53,  1.76s/it]Epoch 28:  45%|████▍     | 134/300 [03:47<04:43,  1.71s/it]Epoch 28:  45%|████▌     | 135/300 [03:49<04:51,  1.76s/it]Epoch 28:  45%|████▌     | 136/300 [03:51<04:36,  1.69s/it]Epoch 28:  46%|████▌     | 137/300 [03:52<04:25,  1.63s/it]Epoch 28:  46%|████▌     | 138/300 [03:54<04:22,  1.62s/it]Epoch 28:  46%|████▋     | 139/300 [03:56<04:33,  1.70s/it]06/19/2022 18:38:38 - INFO - __main__ - global step: 4270; train loss: 7.716147422790527; dev loss
Epoch 28:  47%|████▋     | 140/300 [03:58<04:41,  1.76s/it]Epoch 28:  47%|████▋     | 141/300 [03:59<04:27,  1.68s/it]Epoch 28:  47%|████▋     | 142/300 [04:01<04:21,  1.66s/it]Epoch 28:  48%|████▊     | 143/300 [04:02<04:11,  1.60s/it]Epoch 28:  48%|████▊     | 144/300 [04:04<04:23,  1.69s/it]Epoch 28:  48%|████▊     | 145/300 [04:06<04:32,  1.76s/it]Epoch 28:  49%|████▊     | 146/300 [04:07<04:22,  1.70s/it]Epoch 28:  49%|████▉     | 147/300 [04:09<04:27,  1.75s/it]Epoch 28:  49%|████▉     | 148/300 [04:11<04:32,  1.79s/it]Epoch 28:  50%|████▉     | 149/300 [04:13<04:35,  1.83s/it]Epoch 28:  50%|█████     | 150/300 [04:15<04:19,  1.73s/it]Epoch 28:  50%|█████     | 151/300 [04:17<04:29,  1.81s/it]Epoch 28:  51%|█████     | 152/300 [04:18<04:13,  1.71s/it]Epoch 28:  51%|█████     | 153/300 [04:20<04:18,  1.76s/it]Epoch 28:  51%|█████▏    | 154/300 [04:21<04:03,  1.67s/it]Epoch 28:  52%|█████▏    | 155/300 [04:23<03:57,  1.64s/it]Epoch 28:  52%|█████▏    | 156/300 [04:25<04:06,  1.71s/it]Epoch 28:  52%|█████▏    | 157/300 [04:27<04:10,  1.75s/it]Epoch 28:  53%|█████▎    | 158/300 [04:29<04:15,  1.80s/it]Epoch 28:  53%|█████▎    | 159/300 [04:30<04:04,  1.73s/it]06/19/2022 18:39:12 - INFO - __main__ - global step: 4280; train loss: 8.316914558410645; dev loss
Epoch 28:  53%|█████▎    | 160/300 [04:32<04:08,  1.77s/it]Epoch 28:  54%|█████▎    | 161/300 [04:34<03:54,  1.69s/it]Epoch 28:  54%|█████▍    | 162/300 [04:35<03:43,  1.62s/it]Epoch 28:  54%|█████▍    | 163/300 [04:37<03:55,  1.72s/it]Epoch 28:  55%|█████▍    | 164/300 [04:39<04:00,  1.77s/it]Epoch 28:  55%|█████▌    | 165/300 [04:40<03:47,  1.68s/it]Epoch 28:  55%|█████▌    | 166/300 [04:42<03:52,  1.74s/it]Epoch 28:  56%|█████▌    | 167/300 [04:44<04:00,  1.81s/it]Epoch 28:  56%|█████▌    | 168/300 [04:46<03:44,  1.70s/it]Epoch 28:  56%|█████▋    | 169/300 [04:47<03:34,  1.64s/it]Epoch 28:  57%|█████▋    | 170/300 [04:49<03:42,  1.71s/it]Epoch 28:  57%|█████▋    | 171/300 [04:51<03:36,  1.67s/it]Epoch 28:  57%|█████▋    | 172/300 [04:52<03:26,  1.61s/it]Epoch 28:  58%|█████▊    | 173/300 [04:54<03:34,  1.69s/it]Epoch 28:  58%|█████▊    | 174/300 [04:55<03:25,  1.63s/it]Epoch 28:  58%|█████▊    | 175/300 [04:57<03:20,  1.61s/it]Epoch 28:  59%|█████▊    | 176/300 [04:59<03:28,  1.68s/it]Epoch 28:  59%|█████▉    | 177/300 [05:01<03:34,  1.74s/it]Epoch 28:  59%|█████▉    | 178/300 [05:03<03:37,  1.79s/it]Epoch 28:  60%|█████▉    | 179/300 [05:04<03:25,  1.70s/it]06/19/2022 18:39:46 - INFO - __main__ - global step: 4290; train loss: 7.5588698387146; dev loss
Epoch 28:  60%|██████    | 180/300 [05:06<03:33,  1.78s/it]Epoch 28:  60%|██████    | 181/300 [05:08<03:35,  1.81s/it]Epoch 28:  61%|██████    | 182/300 [05:10<03:36,  1.83s/it]Epoch 28:  61%|██████    | 183/300 [05:11<03:21,  1.73s/it]Epoch 28:  61%|██████▏   | 184/300 [05:13<03:14,  1.68s/it]Epoch 28:  62%|██████▏   | 185/300 [05:15<03:19,  1.73s/it]Epoch 28:  62%|██████▏   | 186/300 [05:17<03:22,  1.77s/it]Epoch 28:  62%|██████▏   | 187/300 [05:18<03:23,  1.80s/it]Epoch 28:  63%|██████▎   | 188/300 [05:20<03:14,  1.73s/it]Epoch 28:  63%|██████▎   | 189/300 [05:22<03:16,  1.77s/it]Epoch 28:  63%|██████▎   | 190/300 [05:24<03:19,  1.81s/it]Epoch 28:  64%|██████▎   | 191/300 [05:26<03:20,  1.84s/it]Epoch 28:  64%|██████▍   | 192/300 [05:28<03:23,  1.88s/it]Epoch 28:  64%|██████▍   | 193/300 [05:29<03:08,  1.76s/it]Epoch 28:  65%|██████▍   | 194/300 [05:31<02:57,  1.67s/it]Epoch 28:  65%|██████▌   | 195/300 [05:33<03:02,  1.74s/it]Epoch 28:  65%|██████▌   | 196/300 [05:34<02:55,  1.69s/it]Epoch 28:  66%|██████▌   | 197/300 [05:36<02:46,  1.62s/it]Epoch 28:  66%|██████▌   | 198/300 [05:37<02:52,  1.69s/it]Epoch 28:  66%|██████▋   | 199/300 [05:39<02:55,  1.74s/it]06/19/2022 18:40:21 - INFO - __main__ - global step: 4300; train loss: 7.790073394775391; dev loss
Epoch 28:  67%|██████▋   | 200/300 [05:41<03:02,  1.82s/it]Epoch 28:  67%|██████▋   | 201/300 [05:43<02:49,  1.71s/it]Epoch 28:  67%|██████▋   | 202/300 [05:45<02:52,  1.76s/it]Epoch 28:  68%|██████▊   | 203/300 [05:46<02:42,  1.68s/it]Epoch 28:  68%|██████▊   | 204/300 [05:48<02:36,  1.63s/it]Epoch 28:  68%|██████▊   | 205/300 [05:50<02:45,  1.74s/it]Epoch 28:  69%|██████▊   | 206/300 [05:51<02:35,  1.65s/it]Epoch 28:  69%|██████▉   | 207/300 [05:53<02:39,  1.71s/it]Epoch 28:  69%|██████▉   | 208/300 [05:55<02:41,  1.76s/it]Epoch 28:  70%|██████▉   | 209/300 [05:56<02:34,  1.70s/it]Epoch 28:  70%|███████   | 210/300 [05:58<02:37,  1.75s/it]Epoch 28:  70%|███████   | 211/300 [06:00<02:27,  1.66s/it]Epoch 28:  71%|███████   | 212/300 [06:01<02:21,  1.61s/it]Epoch 28:  71%|███████   | 213/300 [06:03<02:28,  1.71s/it]Epoch 28:  71%|███████▏  | 214/300 [06:05<02:30,  1.75s/it]Epoch 28:  72%|███████▏  | 215/300 [06:07<02:32,  1.79s/it]Epoch 28:  72%|███████▏  | 216/300 [06:09<02:32,  1.81s/it]Epoch 28:  72%|███████▏  | 217/300 [06:11<02:34,  1.86s/it]Epoch 28:  73%|███████▎  | 218/300 [06:12<02:22,  1.74s/it]Epoch 28:  73%|███████▎  | 219/300 [06:14<02:14,  1.65s/it]06/19/2022 18:40:56 - INFO - __main__ - global step: 4310; train loss: 7.910744667053223; dev loss
Epoch 28:  73%|███████▎  | 220/300 [06:15<02:18,  1.73s/it]Epoch 28:  74%|███████▎  | 221/300 [06:17<02:22,  1.80s/it]Epoch 28:  74%|███████▍  | 222/300 [06:19<02:22,  1.82s/it]Epoch 28:  74%|███████▍  | 223/300 [06:21<02:12,  1.72s/it]Epoch 28:  75%|███████▍  | 224/300 [06:22<02:04,  1.64s/it]Epoch 28:  75%|███████▌  | 225/300 [06:24<02:10,  1.74s/it]Epoch 28:  75%|███████▌  | 226/300 [06:26<02:11,  1.78s/it]Epoch 28:  76%|███████▌  | 227/300 [06:28<02:11,  1.81s/it]Epoch 28:  76%|███████▌  | 228/300 [06:29<02:02,  1.71s/it]Epoch 28:  76%|███████▋  | 229/300 [06:31<02:06,  1.78s/it]Epoch 28:  77%|███████▋  | 230/300 [06:33<01:58,  1.69s/it]Epoch 28:  77%|███████▋  | 231/300 [06:34<01:51,  1.62s/it]Epoch 28:  77%|███████▋  | 232/300 [06:36<01:46,  1.57s/it]Epoch 28:  78%|███████▊  | 233/300 [06:38<01:51,  1.66s/it]Epoch 28:  78%|███████▊  | 234/300 [06:39<01:48,  1.65s/it]Epoch 28:  78%|███████▊  | 235/300 [06:41<01:43,  1.60s/it]Epoch 28:  79%|███████▊  | 236/300 [06:43<01:48,  1.69s/it]Epoch 28:  79%|███████▉  | 237/300 [06:44<01:42,  1.63s/it]Epoch 28:  79%|███████▉  | 238/300 [06:46<01:40,  1.62s/it]Epoch 28:  80%|███████▉  | 239/300 [06:48<01:43,  1.69s/it]06/19/2022 18:41:29 - INFO - __main__ - global step: 4320; train loss: 8.340846061706543; dev loss
Epoch 28:  80%|████████  | 240/300 [06:49<01:37,  1.62s/it]Epoch 28:  80%|████████  | 241/300 [06:51<01:32,  1.58s/it]Epoch 28:  81%|████████  | 242/300 [06:52<01:31,  1.57s/it]Epoch 28:  81%|████████  | 243/300 [06:54<01:27,  1.54s/it]Epoch 28:  81%|████████▏ | 244/300 [06:55<01:31,  1.64s/it]Epoch 28:  82%|████████▏ | 245/300 [06:57<01:33,  1.71s/it]Epoch 28:  82%|████████▏ | 246/300 [06:59<01:36,  1.78s/it]Epoch 28:  82%|████████▏ | 247/300 [07:01<01:29,  1.69s/it]Epoch 28:  83%|████████▎ | 248/300 [07:02<01:24,  1.62s/it]Epoch 28:  83%|████████▎ | 249/300 [07:04<01:26,  1.70s/it]Epoch 28:  83%|████████▎ | 250/300 [07:06<01:23,  1.67s/it]Epoch 28:  84%|████████▎ | 251/300 [07:07<01:18,  1.60s/it]Epoch 28:  84%|████████▍ | 252/300 [07:09<01:20,  1.68s/it]Epoch 28:  84%|████████▍ | 253/300 [07:11<01:21,  1.73s/it]Epoch 28:  85%|████████▍ | 254/300 [07:12<01:17,  1.68s/it]Epoch 28:  85%|████████▌ | 255/300 [07:14<01:18,  1.75s/it]Epoch 28:  85%|████████▌ | 256/300 [07:16<01:13,  1.67s/it]Epoch 28:  86%|████████▌ | 257/300 [07:18<01:14,  1.74s/it]Epoch 28:  86%|████████▌ | 258/300 [07:19<01:09,  1.66s/it]Epoch 28:  86%|████████▋ | 259/300 [07:21<01:06,  1.63s/it]06/19/2022 18:42:02 - INFO - __main__ - global step: 4330; train loss: 7.916140556335449; dev loss
Epoch 28:  87%|████████▋ | 260/300 [07:22<01:02,  1.57s/it]Epoch 28:  87%|████████▋ | 261/300 [07:24<01:00,  1.55s/it]Epoch 28:  87%|████████▋ | 262/300 [07:25<00:57,  1.52s/it]Epoch 28:  88%|████████▊ | 263/300 [07:27<01:01,  1.65s/it]Epoch 28:  88%|████████▊ | 264/300 [07:29<00:57,  1.60s/it]Epoch 28:  88%|████████▊ | 265/300 [07:30<00:58,  1.68s/it]Epoch 28:  89%|████████▊ | 266/300 [07:32<00:59,  1.75s/it]Epoch 28:  89%|████████▉ | 267/300 [07:34<00:59,  1.81s/it]Epoch 28:  89%|████████▉ | 268/300 [07:36<00:54,  1.71s/it]Epoch 28:  90%|████████▉ | 269/300 [07:38<00:54,  1.75s/it]Epoch 28:  90%|█████████ | 270/300 [07:39<00:53,  1.79s/it]Epoch 28:  90%|█████████ | 271/300 [07:41<00:53,  1.85s/it]Epoch 28:  91%|█████████ | 272/300 [07:43<00:48,  1.74s/it]Epoch 28:  91%|█████████ | 273/300 [07:45<00:47,  1.77s/it]Epoch 28:  91%|█████████▏| 274/300 [07:46<00:43,  1.68s/it]Epoch 28:  92%|█████████▏| 275/300 [07:48<00:41,  1.64s/it]Epoch 28:  92%|█████████▏| 276/300 [07:50<00:41,  1.71s/it]Epoch 28:  92%|█████████▏| 277/300 [07:52<00:40,  1.76s/it]Epoch 28:  93%|█████████▎| 278/300 [07:53<00:39,  1.79s/it]Epoch 28:  93%|█████████▎| 279/300 [07:55<00:38,  1.85s/it]06/19/2022 18:42:37 - INFO - __main__ - global step: 4340; train loss: 7.948172092437744; dev loss
Epoch 28:  93%|█████████▎| 280/300 [07:57<00:37,  1.86s/it]Epoch 28:  94%|█████████▎| 281/300 [07:59<00:33,  1.74s/it]Epoch 28:  94%|█████████▍| 282/300 [08:01<00:32,  1.78s/it]Epoch 28:  94%|█████████▍| 283/300 [08:02<00:29,  1.71s/it]Epoch 28:  95%|█████████▍| 284/300 [08:04<00:26,  1.64s/it]Epoch 28:  95%|█████████▌| 285/300 [08:05<00:23,  1.59s/it]Epoch 28:  95%|█████████▌| 286/300 [08:07<00:21,  1.56s/it]Epoch 28:  96%|█████████▌| 287/300 [08:08<00:20,  1.54s/it]Epoch 28:  96%|█████████▌| 288/300 [08:10<00:19,  1.66s/it]Epoch 28:  96%|█████████▋| 289/300 [08:12<00:17,  1.61s/it]Epoch 28:  97%|█████████▋| 290/300 [08:13<00:16,  1.69s/it]Epoch 28:  97%|█████████▋| 291/300 [08:15<00:15,  1.76s/it]Epoch 28:  97%|█████████▋| 292/300 [08:17<00:14,  1.82s/it]Epoch 28:  98%|█████████▊| 293/300 [08:19<00:12,  1.71s/it]Epoch 28:  98%|█████████▊| 294/300 [08:21<00:10,  1.76s/it]Epoch 28:  98%|█████████▊| 295/300 [08:23<00:08,  1.80s/it]Epoch 28:  99%|█████████▊| 296/300 [08:24<00:06,  1.74s/it]Epoch 28:  99%|█████████▉| 297/300 [08:26<00:05,  1.77s/it]Epoch 28:  99%|█████████▉| 298/300 [08:27<00:03,  1.69s/it]Epoch 28: 100%|█████████▉| 299/300 [08:29<00:01,  1.62s/it]06/19/2022 18:43:11 - INFO - __main__ - global step: 4350; train loss: 7.382475852966309; dev loss
Epoch 28: 100%|██████████| 300/300 [08:31<00:00,  1.74s/it]Epoch 28: 100%|██████████| 300/300 [08:31<00:00,  1.70s/it]
Epoch 29:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 29:   0%|          | 1/300 [00:01<07:27,  1.50s/it]Epoch 29:   1%|          | 2/300 [00:03<08:38,  1.74s/it]Epoch 29:   1%|          | 3/300 [00:05<09:03,  1.83s/it]Epoch 29:   1%|▏         | 4/300 [00:07<09:28,  1.92s/it]Epoch 29:   2%|▏         | 5/300 [00:08<08:39,  1.76s/it]Epoch 29:   2%|▏         | 6/300 [00:10<08:07,  1.66s/it]Epoch 29:   2%|▏         | 7/300 [00:12<08:30,  1.74s/it]Epoch 29:   3%|▎         | 8/300 [00:14<08:52,  1.82s/it]Epoch 29:   3%|▎         | 9/300 [00:15<08:19,  1.71s/it]Epoch 29:   3%|▎         | 10/300 [00:17<07:54,  1.64s/it]Epoch 29:   4%|▎         | 11/300 [00:19<08:11,  1.70s/it]Epoch 29:   4%|▍         | 12/300 [00:20<08:24,  1.75s/it]Epoch 29:   4%|▍         | 13/300 [00:22<08:09,  1.70s/it]Epoch 29:   5%|▍         | 14/300 [00:24<08:21,  1.75s/it]Epoch 29:   5%|▌         | 15/300 [00:25<07:54,  1.67s/it]Epoch 29:   5%|▌         | 16/300 [00:27<08:07,  1.72s/it]Epoch 29:   6%|▌         | 17/300 [00:29<08:29,  1.80s/it]Epoch 29:   6%|▌         | 18/300 [00:31<08:30,  1.81s/it]Epoch 29:   6%|▋         | 19/300 [00:33<08:32,  1.82s/it]06/19/2022 18:43:46 - INFO - __main__ - global step: 4360; train loss: 7.962125301361084; dev loss
Epoch 29:   7%|▋         | 20/300 [00:34<08:00,  1.72s/it]Epoch 29:   7%|▋         | 21/300 [00:36<07:48,  1.68s/it]Epoch 29:   7%|▋         | 22/300 [00:38<08:00,  1.73s/it]Epoch 29:   8%|▊         | 23/300 [00:40<08:10,  1.77s/it]Epoch 29:   8%|▊         | 24/300 [00:41<07:43,  1.68s/it]Epoch 29:   8%|▊         | 25/300 [00:43<08:04,  1.76s/it]Epoch 29:   9%|▊         | 26/300 [00:45<07:39,  1.68s/it]Epoch 29:   9%|▉         | 27/300 [00:46<07:53,  1.74s/it]Epoch 29:   9%|▉         | 28/300 [00:48<07:31,  1.66s/it]Epoch 29:  10%|▉         | 29/300 [00:50<07:53,  1.75s/it]Epoch 29:  10%|█         | 30/300 [00:51<07:27,  1.66s/it]Epoch 29:  10%|█         | 31/300 [00:53<07:40,  1.71s/it]Epoch 29:  11%|█         | 32/300 [00:55<07:18,  1.63s/it]Epoch 29:  11%|█         | 33/300 [00:57<07:44,  1.74s/it]Epoch 29:  11%|█▏        | 34/300 [00:58<07:24,  1.67s/it]Epoch 29:  12%|█▏        | 35/300 [01:00<07:07,  1.61s/it]Epoch 29:  12%|█▏        | 36/300 [01:01<06:53,  1.57s/it]Epoch 29:  12%|█▏        | 37/300 [01:03<07:23,  1.69s/it]Epoch 29:  13%|█▎        | 38/300 [01:04<07:04,  1.62s/it]Epoch 29:  13%|█▎        | 39/300 [01:06<07:19,  1.68s/it]06/19/2022 18:44:19 - INFO - __main__ - global step: 4370; train loss: 7.378671169281006; dev loss
Epoch 29:  13%|█▎        | 40/300 [01:08<07:01,  1.62s/it]Epoch 29:  14%|█▎        | 41/300 [01:10<07:18,  1.69s/it]Epoch 29:  14%|█▍        | 42/300 [01:11<07:10,  1.67s/it]Epoch 29:  14%|█▍        | 43/300 [01:13<07:22,  1.72s/it]Epoch 29:  15%|█▍        | 44/300 [01:15<07:34,  1.77s/it]Epoch 29:  15%|█▌        | 45/300 [01:16<07:08,  1.68s/it]Epoch 29:  15%|█▌        | 46/300 [01:18<06:57,  1.64s/it]Epoch 29:  16%|█▌        | 47/300 [01:19<06:41,  1.59s/it]Epoch 29:  16%|█▌        | 48/300 [01:21<07:00,  1.67s/it]Epoch 29:  16%|█▋        | 49/300 [01:23<07:12,  1.72s/it]Epoch 29:  17%|█▋        | 50/300 [01:25<07:30,  1.80s/it]Epoch 29:  17%|█▋        | 51/300 [01:27<07:31,  1.81s/it]Epoch 29:  17%|█▋        | 52/300 [01:29<07:32,  1.83s/it]Epoch 29:  18%|█▊        | 53/300 [01:31<07:33,  1.84s/it]Epoch 29:  18%|█▊        | 54/300 [01:33<07:41,  1.88s/it]Epoch 29:  18%|█▊        | 55/300 [01:35<07:39,  1.88s/it]Epoch 29:  19%|█▊        | 56/300 [01:36<07:36,  1.87s/it]Epoch 29:  19%|█▉        | 57/300 [01:38<07:08,  1.76s/it]Epoch 29:  19%|█▉        | 58/300 [01:39<06:53,  1.71s/it]Epoch 29:  20%|█▉        | 59/300 [01:41<06:37,  1.65s/it]06/19/2022 18:44:54 - INFO - __main__ - global step: 4380; train loss: 7.8878021240234375; dev loss
Epoch 29:  20%|██        | 60/300 [01:43<06:52,  1.72s/it]Epoch 29:  20%|██        | 61/300 [01:44<06:32,  1.64s/it]Epoch 29:  21%|██        | 62/300 [01:46<06:29,  1.64s/it]Epoch 29:  21%|██        | 63/300 [01:47<06:14,  1.58s/it]Epoch 29:  21%|██▏       | 64/300 [01:49<06:03,  1.54s/it]Epoch 29:  22%|██▏       | 65/300 [01:50<05:58,  1.53s/it]Epoch 29:  22%|██▏       | 66/300 [01:52<06:20,  1.62s/it]Epoch 29:  22%|██▏       | 67/300 [01:54<06:43,  1.73s/it]Epoch 29:  23%|██▎       | 68/300 [01:56<06:22,  1.65s/it]Epoch 29:  23%|██▎       | 69/300 [01:57<06:35,  1.71s/it]Epoch 29:  23%|██▎       | 70/300 [01:59<06:45,  1.76s/it]Epoch 29:  24%|██▎       | 71/300 [02:01<06:58,  1.83s/it]Epoch 29:  24%|██▍       | 72/300 [02:03<06:58,  1.83s/it]Epoch 29:  24%|██▍       | 73/300 [02:05<07:00,  1.85s/it]Epoch 29:  25%|██▍       | 74/300 [02:07<06:58,  1.85s/it]Epoch 29:  25%|██▌       | 75/300 [02:09<06:39,  1.77s/it]Epoch 29:  25%|██▌       | 76/300 [02:10<06:43,  1.80s/it]Epoch 29:  26%|██▌       | 77/300 [02:12<06:20,  1.71s/it]Epoch 29:  26%|██▌       | 78/300 [02:13<06:01,  1.63s/it]Epoch 29:  26%|██▋       | 79/300 [02:15<05:56,  1.61s/it]06/19/2022 18:45:28 - INFO - __main__ - global step: 4390; train loss: 8.57630443572998; dev loss
Epoch 29:  27%|██▋       | 80/300 [02:17<06:09,  1.68s/it]Epoch 29:  27%|██▋       | 81/300 [02:19<06:20,  1.74s/it]Epoch 29:  27%|██▋       | 82/300 [02:20<06:25,  1.77s/it]Epoch 29:  28%|██▊       | 83/300 [02:22<06:12,  1.72s/it]Epoch 29:  28%|██▊       | 84/300 [02:24<06:20,  1.76s/it]Epoch 29:  28%|██▊       | 85/300 [02:26<06:29,  1.81s/it]Epoch 29:  29%|██▊       | 86/300 [02:27<06:05,  1.71s/it]Epoch 29:  29%|██▉       | 87/300 [02:29<05:56,  1.67s/it]Epoch 29:  29%|██▉       | 88/300 [02:31<06:06,  1.73s/it]Epoch 29:  30%|██▉       | 89/300 [02:33<06:13,  1.77s/it]Epoch 29:  30%|███       | 90/300 [02:34<05:53,  1.68s/it]Epoch 29:  30%|███       | 91/300 [02:36<06:11,  1.78s/it]Epoch 29:  31%|███       | 92/300 [02:38<06:14,  1.80s/it]Epoch 29:  31%|███       | 93/300 [02:40<06:17,  1.82s/it]Epoch 29:  31%|███▏      | 94/300 [02:41<05:54,  1.72s/it]Epoch 29:  32%|███▏      | 95/300 [02:43<06:00,  1.76s/it]Epoch 29:  32%|███▏      | 96/300 [02:45<06:13,  1.83s/it]Epoch 29:  32%|███▏      | 97/300 [02:47<05:49,  1.72s/it]Epoch 29:  33%|███▎      | 98/300 [02:48<05:33,  1.65s/it]Epoch 29:  33%|███▎      | 99/300 [02:50<05:22,  1.60s/it]06/19/2022 18:46:03 - INFO - __main__ - global step: 4400; train loss: 7.731164455413818; dev loss
Epoch 29:  33%|███▎      | 100/300 [02:51<05:20,  1.60s/it]Epoch 29:  34%|███▎      | 101/300 [02:53<05:10,  1.56s/it]Epoch 29:  34%|███▍      | 102/300 [02:55<05:26,  1.65s/it]Epoch 29:  34%|███▍      | 103/300 [02:56<05:39,  1.72s/it]Epoch 29:  35%|███▍      | 104/300 [02:58<05:52,  1.80s/it]Epoch 29:  35%|███▌      | 105/300 [03:00<05:31,  1.70s/it]Epoch 29:  35%|███▌      | 106/300 [03:01<05:16,  1.63s/it]Epoch 29:  36%|███▌      | 107/300 [03:03<05:05,  1.58s/it]Epoch 29:  36%|███▌      | 108/300 [03:04<05:04,  1.59s/it]Epoch 29:  36%|███▋      | 109/300 [03:06<05:18,  1.67s/it]Epoch 29:  37%|███▋      | 110/300 [03:08<05:29,  1.73s/it]Epoch 29:  37%|███▋      | 111/300 [03:10<05:12,  1.65s/it]Epoch 29:  37%|███▋      | 112/300 [03:11<05:07,  1.64s/it]Epoch 29:  38%|███▊      | 113/300 [03:13<04:58,  1.60s/it]Epoch 29:  38%|███▊      | 114/300 [03:14<04:49,  1.56s/it]Epoch 29:  38%|███▊      | 115/300 [03:16<04:43,  1.53s/it]Epoch 29:  39%|███▊      | 116/300 [03:18<05:05,  1.66s/it]Epoch 29:  39%|███▉      | 117/300 [03:19<04:56,  1.62s/it]Epoch 29:  39%|███▉      | 118/300 [03:21<05:06,  1.69s/it]Epoch 29:  40%|███▉      | 119/300 [03:23<05:14,  1.74s/it]06/19/2022 18:46:36 - INFO - __main__ - global step: 4410; train loss: 7.995409965515137; dev loss
Epoch 29:  40%|████      | 120/300 [03:24<05:00,  1.67s/it]Epoch 29:  40%|████      | 121/300 [03:26<05:16,  1.77s/it]Epoch 29:  41%|████      | 122/300 [03:28<05:20,  1.80s/it]Epoch 29:  41%|████      | 123/300 [03:30<05:00,  1.70s/it]Epoch 29:  41%|████▏     | 124/300 [03:32<05:06,  1.74s/it]Epoch 29:  42%|████▏     | 125/300 [03:33<04:57,  1.70s/it]Epoch 29:  42%|████▏     | 126/300 [03:35<04:44,  1.64s/it]Epoch 29:  42%|████▏     | 127/300 [03:36<04:34,  1.59s/it]Epoch 29:  43%|████▎     | 128/300 [03:38<04:43,  1.65s/it]Epoch 29:  43%|████▎     | 129/300 [03:40<04:58,  1.74s/it]Epoch 29:  43%|████▎     | 130/300 [03:41<04:41,  1.66s/it]Epoch 29:  44%|████▎     | 131/300 [03:43<04:48,  1.71s/it]Epoch 29:  44%|████▍     | 132/300 [03:45<04:34,  1.63s/it]Epoch 29:  44%|████▍     | 133/300 [03:47<04:49,  1.73s/it]Epoch 29:  45%|████▍     | 134/300 [03:48<04:34,  1.66s/it]Epoch 29:  45%|████▌     | 135/300 [03:50<04:41,  1.70s/it]Epoch 29:  45%|████▌     | 136/300 [03:52<04:47,  1.75s/it]Epoch 29:  46%|████▌     | 137/300 [03:54<04:56,  1.82s/it]Epoch 29:  46%|████▌     | 138/300 [03:55<04:39,  1.73s/it]Epoch 29:  46%|████▋     | 139/300 [03:57<04:26,  1.65s/it]06/19/2022 18:47:10 - INFO - __main__ - global step: 4420; train loss: 7.612856388092041; dev loss
Epoch 29:  47%|████▋     | 140/300 [03:58<04:14,  1.59s/it]Epoch 29:  47%|████▋     | 141/300 [04:00<04:30,  1.70s/it]Epoch 29:  47%|████▋     | 142/300 [04:02<04:36,  1.75s/it]Epoch 29:  48%|████▊     | 143/300 [04:04<04:40,  1.78s/it]Epoch 29:  48%|████▊     | 144/300 [04:06<04:41,  1.80s/it]Epoch 29:  48%|████▊     | 145/300 [04:07<04:28,  1.74s/it]Epoch 29:  49%|████▊     | 146/300 [04:09<04:31,  1.76s/it]Epoch 29:  49%|████▉     | 147/300 [04:11<04:15,  1.67s/it]Epoch 29:  49%|████▉     | 148/300 [04:12<04:07,  1.63s/it]Epoch 29:  50%|████▉     | 149/300 [04:14<04:15,  1.69s/it]Epoch 29:  50%|█████     | 150/300 [04:15<04:08,  1.65s/it]Epoch 29:  50%|█████     | 151/300 [04:17<04:14,  1.71s/it]Epoch 29:  51%|█████     | 152/300 [04:19<04:02,  1.64s/it]Epoch 29:  51%|█████     | 153/300 [04:20<03:53,  1.59s/it]Epoch 29:  51%|█████▏    | 154/300 [04:22<04:07,  1.70s/it]Epoch 29:  52%|█████▏    | 155/300 [04:24<03:56,  1.63s/it]Epoch 29:  52%|█████▏    | 156/300 [04:26<04:05,  1.71s/it]Epoch 29:  52%|█████▏    | 157/300 [04:27<03:54,  1.64s/it]Epoch 29:  53%|█████▎    | 158/300 [04:29<03:52,  1.64s/it]Epoch 29:  53%|█████▎    | 159/300 [04:30<03:58,  1.69s/it]06/19/2022 18:47:44 - INFO - __main__ - global step: 4430; train loss: 7.4129319190979; dev loss
Epoch 29:  53%|█████▎    | 160/300 [04:32<04:04,  1.74s/it]Epoch 29:  54%|█████▎    | 161/300 [04:34<04:10,  1.80s/it]Epoch 29:  54%|█████▍    | 162/300 [04:36<04:04,  1.77s/it]Epoch 29:  54%|█████▍    | 163/300 [04:38<03:52,  1.70s/it]Epoch 29:  55%|█████▍    | 164/300 [04:39<03:41,  1.63s/it]Epoch 29:  55%|█████▌    | 165/300 [04:40<03:33,  1.58s/it]Epoch 29:  55%|█████▌    | 166/300 [04:42<03:32,  1.59s/it]Epoch 29:  56%|█████▌    | 167/300 [04:44<03:42,  1.67s/it]Epoch 29:  56%|█████▌    | 168/300 [04:46<03:48,  1.73s/it]Epoch 29:  56%|█████▋    | 169/300 [04:48<03:53,  1.78s/it]Epoch 29:  57%|█████▋    | 170/300 [04:50<04:01,  1.85s/it]Epoch 29:  57%|█████▋    | 171/300 [04:51<03:48,  1.77s/it]Epoch 29:  57%|█████▋    | 172/300 [04:53<03:51,  1.81s/it]Epoch 29:  58%|█████▊    | 173/300 [04:55<03:38,  1.72s/it]Epoch 29:  58%|█████▊    | 174/300 [04:56<03:29,  1.67s/it]Epoch 29:  58%|█████▊    | 175/300 [04:58<03:42,  1.78s/it]Epoch 29:  59%|█████▊    | 176/300 [05:00<03:28,  1.68s/it]Epoch 29:  59%|█████▉    | 177/300 [05:01<03:19,  1.62s/it]Epoch 29:  59%|█████▉    | 178/300 [05:03<03:13,  1.58s/it]Epoch 29:  60%|█████▉    | 179/300 [05:05<03:25,  1.70s/it]06/19/2022 18:48:18 - INFO - __main__ - global step: 4440; train loss: 7.716510772705078; dev loss
Epoch 29:  60%|██████    | 180/300 [05:06<03:16,  1.63s/it]Epoch 29:  60%|██████    | 181/300 [05:08<03:21,  1.70s/it]Epoch 29:  61%|██████    | 182/300 [05:10<03:26,  1.75s/it]Epoch 29:  61%|██████    | 183/300 [05:12<03:31,  1.81s/it]Epoch 29:  61%|██████▏   | 184/300 [05:13<03:18,  1.71s/it]Epoch 29:  62%|██████▏   | 185/300 [05:15<03:08,  1.64s/it]Epoch 29:  62%|██████▏   | 186/300 [05:17<03:14,  1.71s/it]Epoch 29:  62%|██████▏   | 187/300 [05:19<03:22,  1.79s/it]Epoch 29:  63%|██████▎   | 188/300 [05:20<03:10,  1.70s/it]Epoch 29:  63%|██████▎   | 189/300 [05:22<03:13,  1.74s/it]Epoch 29:  63%|██████▎   | 190/300 [05:24<03:14,  1.77s/it]Epoch 29:  64%|██████▎   | 191/300 [05:26<03:19,  1.83s/it]Epoch 29:  64%|██████▍   | 192/300 [05:28<03:18,  1.84s/it]Epoch 29:  64%|██████▍   | 193/300 [05:29<03:05,  1.73s/it]Epoch 29:  65%|██████▍   | 194/300 [05:31<02:54,  1.65s/it]Epoch 29:  65%|██████▌   | 195/300 [05:32<03:01,  1.73s/it]Epoch 29:  65%|██████▌   | 196/300 [05:34<02:51,  1.65s/it]Epoch 29:  66%|██████▌   | 197/300 [05:36<02:56,  1.71s/it]Epoch 29:  66%|██████▌   | 198/300 [05:37<02:48,  1.65s/it]Epoch 29:  66%|██████▋   | 199/300 [05:39<02:46,  1.65s/it]06/19/2022 18:48:52 - INFO - __main__ - global step: 4450; train loss: 8.179664611816406; dev loss
Epoch 29:  67%|██████▋   | 200/300 [05:41<02:52,  1.72s/it]Epoch 29:  67%|██████▋   | 201/300 [05:43<02:54,  1.76s/it]Epoch 29:  67%|██████▋   | 202/300 [05:44<02:45,  1.69s/it]Epoch 29:  68%|██████▊   | 203/300 [05:46<02:37,  1.63s/it]Epoch 29:  68%|██████▊   | 204/300 [05:48<02:47,  1.74s/it]Epoch 29:  68%|██████▊   | 205/300 [05:49<02:38,  1.67s/it]Epoch 29:  69%|██████▊   | 206/300 [05:51<02:42,  1.73s/it]Epoch 29:  69%|██████▉   | 207/300 [05:53<02:45,  1.78s/it]Epoch 29:  69%|██████▉   | 208/300 [05:55<02:38,  1.73s/it]Epoch 29:  70%|██████▉   | 209/300 [05:56<02:31,  1.67s/it]Epoch 29:  70%|███████   | 210/300 [05:58<02:36,  1.74s/it]Epoch 29:  70%|███████   | 211/300 [06:00<02:28,  1.67s/it]Epoch 29:  71%|███████   | 212/300 [06:01<02:25,  1.65s/it]Epoch 29:  71%|███████   | 213/300 [06:03<02:18,  1.59s/it]Epoch 29:  71%|███████▏  | 214/300 [06:04<02:14,  1.56s/it]Epoch 29:  72%|███████▏  | 215/300 [06:06<02:11,  1.55s/it]Epoch 29:  72%|███████▏  | 216/300 [06:07<02:11,  1.56s/it]Epoch 29:  72%|███████▏  | 217/300 [06:09<02:17,  1.66s/it]Epoch 29:  73%|███████▎  | 218/300 [06:11<02:22,  1.74s/it]Epoch 29:  73%|███████▎  | 219/300 [06:12<02:14,  1.66s/it]06/19/2022 18:49:26 - INFO - __main__ - global step: 4460; train loss: 7.706446647644043; dev loss
Epoch 29:  73%|███████▎  | 220/300 [06:14<02:11,  1.65s/it]Epoch 29:  74%|███████▎  | 221/300 [06:16<02:15,  1.72s/it]Epoch 29:  74%|███████▍  | 222/300 [06:17<02:08,  1.65s/it]Epoch 29:  74%|███████▍  | 223/300 [06:19<02:03,  1.61s/it]Epoch 29:  75%|███████▍  | 224/300 [06:21<02:10,  1.72s/it]Epoch 29:  75%|███████▌  | 225/300 [06:23<02:04,  1.67s/it]Epoch 29:  75%|███████▌  | 226/300 [06:24<01:59,  1.61s/it]Epoch 29:  76%|███████▌  | 227/300 [06:26<02:04,  1.70s/it]Epoch 29:  76%|███████▌  | 228/300 [06:28<02:06,  1.76s/it]Epoch 29:  76%|███████▋  | 229/300 [06:29<02:01,  1.72s/it]Epoch 29:  77%|███████▋  | 230/300 [06:31<01:55,  1.65s/it]Epoch 29:  77%|███████▋  | 231/300 [06:33<01:59,  1.73s/it]Epoch 29:  77%|███████▋  | 232/300 [06:35<02:01,  1.78s/it]Epoch 29:  78%|███████▊  | 233/300 [06:36<01:55,  1.73s/it]Epoch 29:  78%|███████▊  | 234/300 [06:38<01:57,  1.78s/it]Epoch 29:  78%|███████▊  | 235/300 [06:40<01:50,  1.70s/it]Epoch 29:  79%|███████▊  | 236/300 [06:42<01:52,  1.76s/it]Epoch 29:  79%|███████▉  | 237/300 [06:44<01:55,  1.83s/it]Epoch 29:  79%|███████▉  | 238/300 [06:46<01:54,  1.85s/it]Epoch 29:  80%|███████▉  | 239/300 [06:47<01:53,  1.87s/it]06/19/2022 18:50:01 - INFO - __main__ - global step: 4470; train loss: 7.8738555908203125; dev loss
Epoch 29:  80%|████████  | 240/300 [06:49<01:45,  1.76s/it]Epoch 29:  80%|████████  | 241/300 [06:51<01:48,  1.83s/it]Epoch 29:  81%|████████  | 242/300 [06:52<01:40,  1.73s/it]Epoch 29:  81%|████████  | 243/300 [06:54<01:34,  1.66s/it]Epoch 29:  81%|████████▏ | 244/300 [06:56<01:36,  1.73s/it]Epoch 29:  82%|████████▏ | 245/300 [06:58<01:39,  1.81s/it]Epoch 29:  82%|████████▏ | 246/300 [07:00<01:38,  1.83s/it]Epoch 29:  82%|████████▏ | 247/300 [07:02<01:37,  1.85s/it]Epoch 29:  83%|████████▎ | 248/300 [07:03<01:30,  1.75s/it]Epoch 29:  83%|████████▎ | 249/300 [07:05<01:27,  1.71s/it]Epoch 29:  83%|████████▎ | 250/300 [07:07<01:28,  1.76s/it]Epoch 29:  84%|████████▎ | 251/300 [07:08<01:22,  1.68s/it]Epoch 29:  84%|████████▍ | 252/300 [07:10<01:17,  1.62s/it]Epoch 29:  84%|████████▍ | 253/300 [07:11<01:15,  1.61s/it]Epoch 29:  85%|████████▍ | 254/300 [07:13<01:12,  1.57s/it]Epoch 29:  85%|████████▌ | 255/300 [07:14<01:09,  1.55s/it]Epoch 29:  85%|████████▌ | 256/300 [07:16<01:07,  1.54s/it]Epoch 29:  86%|████████▌ | 257/300 [07:17<01:06,  1.54s/it]Epoch 29:  86%|████████▌ | 258/300 [07:19<01:05,  1.57s/it]Epoch 29:  86%|████████▋ | 259/300 [07:20<01:03,  1.55s/it]06/19/2022 18:50:34 - INFO - __main__ - global step: 4480; train loss: 8.113719940185547; dev loss
Epoch 29:  87%|████████▋ | 260/300 [07:22<01:06,  1.66s/it]Epoch 29:  87%|████████▋ | 261/300 [07:24<01:07,  1.72s/it]Epoch 29:  87%|████████▋ | 262/300 [07:26<01:08,  1.80s/it]Epoch 29:  88%|████████▊ | 263/300 [07:28<01:03,  1.71s/it]Epoch 29:  88%|████████▊ | 264/300 [07:29<00:59,  1.65s/it]Epoch 29:  88%|████████▊ | 265/300 [07:31<01:00,  1.72s/it]Epoch 29:  89%|████████▊ | 266/300 [07:33<01:01,  1.81s/it]Epoch 29:  89%|████████▉ | 267/300 [07:35<00:56,  1.71s/it]Epoch 29:  89%|████████▉ | 268/300 [07:36<00:56,  1.76s/it]Epoch 29:  90%|████████▉ | 269/300 [07:38<00:52,  1.68s/it]Epoch 29:  90%|█████████ | 270/300 [07:40<00:53,  1.77s/it]Epoch 29:  90%|█████████ | 271/300 [07:41<00:48,  1.68s/it]Epoch 29:  91%|█████████ | 272/300 [07:43<00:45,  1.63s/it]Epoch 29:  91%|█████████ | 273/300 [07:44<00:43,  1.59s/it]Epoch 29:  91%|█████████▏| 274/300 [07:46<00:41,  1.60s/it]Epoch 29:  92%|█████████▏| 275/300 [07:47<00:39,  1.56s/it]Epoch 29:  92%|█████████▏| 276/300 [07:49<00:39,  1.66s/it]Epoch 29:  92%|█████████▏| 277/300 [07:51<00:40,  1.74s/it]Epoch 29:  93%|█████████▎| 278/300 [07:53<00:37,  1.70s/it]Epoch 29:  93%|█████████▎| 279/300 [07:55<00:36,  1.75s/it]06/19/2022 18:51:08 - INFO - __main__ - global step: 4490; train loss: 8.179224014282227; dev loss
Epoch 29:  93%|█████████▎| 280/300 [07:56<00:33,  1.68s/it]Epoch 29:  94%|█████████▎| 281/300 [07:58<00:30,  1.62s/it]Epoch 29:  94%|█████████▍| 282/300 [08:00<00:30,  1.70s/it]Epoch 29:  94%|█████████▍| 283/300 [08:02<00:30,  1.78s/it]Epoch 29:  95%|█████████▍| 284/300 [08:03<00:28,  1.81s/it]Epoch 29:  95%|█████████▌| 285/300 [08:05<00:27,  1.82s/it]Epoch 29:  95%|█████████▌| 286/300 [08:07<00:25,  1.84s/it]Epoch 29:  96%|█████████▌| 287/300 [08:09<00:24,  1.88s/it]Epoch 29:  96%|█████████▌| 288/300 [08:11<00:21,  1.77s/it]Epoch 29:  96%|█████████▋| 289/300 [08:13<00:19,  1.80s/it]Epoch 29:  97%|█████████▋| 290/300 [08:14<00:18,  1.82s/it]Epoch 29:  97%|█████████▋| 291/300 [08:16<00:16,  1.87s/it]Epoch 29:  97%|█████████▋| 292/300 [08:18<00:14,  1.76s/it]Epoch 29:  98%|█████████▊| 293/300 [08:19<00:11,  1.68s/it]Epoch 29:  98%|█████████▊| 294/300 [08:21<00:09,  1.62s/it]Epoch 29:  98%|█████████▊| 295/300 [08:22<00:08,  1.62s/it]Epoch 29:  99%|█████████▊| 296/300 [08:24<00:06,  1.70s/it]Epoch 29:  99%|█████████▉| 297/300 [08:26<00:05,  1.75s/it]Epoch 29:  99%|█████████▉| 298/300 [08:28<00:03,  1.79s/it]Epoch 29: 100%|█████████▉| 299/300 [08:30<00:01,  1.85s/it]06/19/2022 18:51:44 - INFO - __main__ - global step: 4500; train loss: 7.7597784996032715; dev loss
Epoch 29: 100%|██████████| 300/300 [08:32<00:00,  1.87s/it]Epoch 29: 100%|██████████| 300/300 [08:32<00:00,  1.71s/it]
Epoch 30:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 30:   0%|          | 1/300 [00:01<07:22,  1.48s/it]Epoch 30:   1%|          | 2/300 [00:02<07:22,  1.49s/it]Epoch 30:   1%|          | 3/300 [00:04<07:35,  1.53s/it]Epoch 30:   1%|▏         | 4/300 [00:06<07:29,  1.52s/it]Epoch 30:   2%|▏         | 5/300 [00:07<08:05,  1.65s/it]Epoch 30:   2%|▏         | 6/300 [00:09<07:48,  1.59s/it]Epoch 30:   2%|▏         | 7/300 [00:11<07:46,  1.59s/it]Epoch 30:   3%|▎         | 8/300 [00:12<07:35,  1.56s/it]Epoch 30:   3%|▎         | 9/300 [00:14<07:29,  1.54s/it]Epoch 30:   3%|▎         | 10/300 [00:15<07:26,  1.54s/it]Epoch 30:   4%|▎         | 11/300 [00:17<07:55,  1.64s/it]Epoch 30:   4%|▍         | 12/300 [00:19<08:23,  1.75s/it]Epoch 30:   4%|▍         | 13/300 [00:21<08:34,  1.79s/it]Epoch 30:   5%|▍         | 14/300 [00:22<08:09,  1.71s/it]Epoch 30:   5%|▌         | 15/300 [00:24<07:50,  1.65s/it]Epoch 30:   5%|▌         | 16/300 [00:26<08:18,  1.75s/it]Epoch 30:   6%|▌         | 17/300 [00:28<08:26,  1.79s/it]Epoch 30:   6%|▌         | 18/300 [00:29<07:58,  1.70s/it]Epoch 30:   6%|▋         | 19/300 [00:31<08:11,  1.75s/it]06/19/2022 18:52:17 - INFO - __main__ - global step: 4510; train loss: 8.259333610534668; dev loss
Epoch 30:   7%|▋         | 20/300 [00:33<08:29,  1.82s/it]Epoch 30:   7%|▋         | 21/300 [00:35<08:33,  1.84s/it]Epoch 30:   7%|▋         | 22/300 [00:36<08:03,  1.74s/it]Epoch 30:   8%|▊         | 23/300 [00:38<07:42,  1.67s/it]Epoch 30:   8%|▊         | 24/300 [00:40<07:36,  1.65s/it]Epoch 30:   8%|▊         | 25/300 [00:41<07:20,  1.60s/it]Epoch 30:   9%|▊         | 26/300 [00:43<07:37,  1.67s/it]Epoch 30:   9%|▉         | 27/300 [00:44<07:21,  1.62s/it]Epoch 30:   9%|▉         | 28/300 [00:46<07:19,  1.61s/it]Epoch 30:  10%|▉         | 29/300 [00:47<07:07,  1.58s/it]Epoch 30:  10%|█         | 30/300 [00:49<07:30,  1.67s/it]Epoch 30:  10%|█         | 31/300 [00:51<07:14,  1.62s/it]Epoch 30:  11%|█         | 32/300 [00:53<07:43,  1.73s/it]Epoch 30:  11%|█         | 33/300 [00:54<07:25,  1.67s/it]Epoch 30:  11%|█▏        | 34/300 [00:56<07:41,  1.73s/it]Epoch 30:  12%|█▏        | 35/300 [00:58<07:21,  1.67s/it]Epoch 30:  12%|█▏        | 36/300 [00:59<07:07,  1.62s/it]Epoch 30:  12%|█▏        | 37/300 [01:01<07:35,  1.73s/it]Epoch 30:  13%|█▎        | 38/300 [01:03<07:15,  1.66s/it]Epoch 30:  13%|█▎        | 39/300 [01:05<07:31,  1.73s/it]06/19/2022 18:52:50 - INFO - __main__ - global step: 4520; train loss: 8.048821449279785; dev loss
Epoch 30:  13%|█▎        | 40/300 [01:06<07:13,  1.67s/it]Epoch 30:  14%|█▎        | 41/300 [01:08<07:04,  1.64s/it]Epoch 30:  14%|█▍        | 42/300 [01:10<07:21,  1.71s/it]Epoch 30:  14%|█▍        | 43/300 [01:12<07:33,  1.77s/it]Epoch 30:  15%|█▍        | 44/300 [01:13<07:43,  1.81s/it]Epoch 30:  15%|█▌        | 45/300 [01:15<07:26,  1.75s/it]Epoch 30:  15%|█▌        | 46/300 [01:17<07:04,  1.67s/it]Epoch 30:  16%|█▌        | 47/300 [01:18<06:47,  1.61s/it]Epoch 30:  16%|█▌        | 48/300 [01:20<07:06,  1.69s/it]Epoch 30:  16%|█▋        | 49/300 [01:21<06:56,  1.66s/it]Epoch 30:  17%|█▋        | 50/300 [01:23<06:42,  1.61s/it]Epoch 30:  17%|█▋        | 51/300 [01:25<07:02,  1.70s/it]Epoch 30:  17%|█▋        | 52/300 [01:27<07:15,  1.76s/it]Epoch 30:  18%|█▊        | 53/300 [01:29<07:31,  1.83s/it]Epoch 30:  18%|█▊        | 54/300 [01:30<07:04,  1.72s/it]Epoch 30:  18%|█▊        | 55/300 [01:32<07:13,  1.77s/it]Epoch 30:  19%|█▊        | 56/300 [01:34<07:20,  1.80s/it]Epoch 30:  19%|█▉        | 57/300 [01:36<07:31,  1.86s/it]Epoch 30:  19%|█▉        | 58/300 [01:38<07:33,  1.88s/it]Epoch 30:  20%|█▉        | 59/300 [01:40<07:32,  1.88s/it]06/19/2022 18:53:26 - INFO - __main__ - global step: 4530; train loss: 7.760504722595215; dev loss
Epoch 30:  20%|██        | 60/300 [01:42<07:31,  1.88s/it]Epoch 30:  20%|██        | 61/300 [01:43<07:08,  1.79s/it]Epoch 30:  21%|██        | 62/300 [01:45<07:12,  1.82s/it]Epoch 30:  21%|██        | 63/300 [01:47<06:47,  1.72s/it]Epoch 30:  21%|██▏       | 64/300 [01:48<06:30,  1.66s/it]Epoch 30:  22%|██▏       | 65/300 [01:50<06:45,  1.73s/it]Epoch 30:  22%|██▏       | 66/300 [01:52<06:35,  1.69s/it]Epoch 30:  22%|██▏       | 67/300 [01:53<06:47,  1.75s/it]Epoch 30:  23%|██▎       | 68/300 [01:55<06:56,  1.79s/it]Epoch 30:  23%|██▎       | 69/300 [01:57<07:01,  1.82s/it]Epoch 30:  23%|██▎       | 70/300 [01:59<06:43,  1.76s/it]Epoch 30:  24%|██▎       | 71/300 [02:01<06:51,  1.80s/it]Epoch 30:  24%|██▍       | 72/300 [02:02<06:30,  1.71s/it]Epoch 30:  24%|██▍       | 73/300 [02:04<06:40,  1.77s/it]Epoch 30:  25%|██▍       | 74/300 [02:06<06:55,  1.84s/it]Epoch 30:  25%|██▌       | 75/300 [02:08<06:58,  1.86s/it]Epoch 30:  25%|██▌       | 76/300 [02:10<06:59,  1.87s/it]Epoch 30:  26%|██▌       | 77/300 [02:12<07:00,  1.88s/it]Epoch 30:  26%|██▌       | 78/300 [02:14<06:41,  1.81s/it]Epoch 30:  26%|██▋       | 79/300 [02:15<06:19,  1.72s/it]06/19/2022 18:54:01 - INFO - __main__ - global step: 4540; train loss: 7.240423679351807; dev loss
Epoch 30:  27%|██▋       | 80/300 [02:17<06:28,  1.77s/it]Epoch 30:  27%|██▋       | 81/300 [02:18<06:08,  1.68s/it]Epoch 30:  27%|██▋       | 82/300 [02:20<06:01,  1.66s/it]Epoch 30:  28%|██▊       | 83/300 [02:22<05:48,  1.60s/it]Epoch 30:  28%|██▊       | 84/300 [02:23<06:04,  1.69s/it]Epoch 30:  28%|██▊       | 85/300 [02:25<06:16,  1.75s/it]Epoch 30:  29%|██▊       | 86/300 [02:27<06:05,  1.71s/it]Epoch 30:  29%|██▉       | 87/300 [02:29<06:16,  1.77s/it]Epoch 30:  29%|██▉       | 88/300 [02:31<06:23,  1.81s/it]Epoch 30:  30%|██▉       | 89/300 [02:33<06:27,  1.84s/it]Epoch 30:  30%|███       | 90/300 [02:34<06:28,  1.85s/it]Epoch 30:  30%|███       | 91/300 [02:36<06:16,  1.80s/it]Epoch 30:  31%|███       | 92/300 [02:38<06:20,  1.83s/it]Epoch 30:  31%|███       | 93/300 [02:40<05:59,  1.74s/it]Epoch 30:  31%|███▏      | 94/300 [02:41<05:43,  1.67s/it]Epoch 30:  32%|███▏      | 95/300 [02:43<05:37,  1.65s/it]Epoch 30:  32%|███▏      | 96/300 [02:45<05:51,  1.72s/it]Epoch 30:  32%|███▏      | 97/300 [02:47<06:03,  1.79s/it]Epoch 30:  33%|███▎      | 98/300 [02:48<05:46,  1.71s/it]Epoch 30:  33%|███▎      | 99/300 [02:50<06:02,  1.80s/it]06/19/2022 18:54:36 - INFO - __main__ - global step: 4550; train loss: 8.266765594482422; dev loss
Epoch 30:  33%|███▎      | 100/300 [02:52<05:42,  1.71s/it]Epoch 30:  34%|███▎      | 101/300 [02:54<05:53,  1.78s/it]Epoch 30:  34%|███▍      | 102/300 [02:55<05:59,  1.82s/it]Epoch 30:  34%|███▍      | 103/300 [02:57<06:08,  1.87s/it]Epoch 30:  35%|███▍      | 104/300 [02:59<05:44,  1.76s/it]Epoch 30:  35%|███▌      | 105/300 [03:00<05:25,  1.67s/it]Epoch 30:  35%|███▌      | 106/300 [03:02<05:14,  1.62s/it]Epoch 30:  36%|███▌      | 107/300 [03:04<05:36,  1.74s/it]Epoch 30:  36%|███▌      | 108/300 [03:06<05:43,  1.79s/it]Epoch 30:  36%|███▋      | 109/300 [03:08<05:48,  1.82s/it]Epoch 30:  37%|███▋      | 110/300 [03:09<05:28,  1.73s/it]Epoch 30:  37%|███▋      | 111/300 [03:11<05:20,  1.70s/it]Epoch 30:  37%|███▋      | 112/300 [03:13<05:31,  1.76s/it]Epoch 30:  38%|███▊      | 113/300 [03:14<05:15,  1.69s/it]Epoch 30:  38%|███▊      | 114/300 [03:16<05:24,  1.75s/it]Epoch 30:  38%|███▊      | 115/300 [03:18<05:38,  1.83s/it]Epoch 30:  39%|███▊      | 116/300 [03:20<05:19,  1.74s/it]Epoch 30:  39%|███▉      | 117/300 [03:21<05:03,  1.66s/it]Epoch 30:  39%|███▉      | 118/300 [03:23<04:54,  1.62s/it]Epoch 30:  40%|███▉      | 119/300 [03:25<05:07,  1.70s/it]06/19/2022 18:55:10 - INFO - __main__ - global step: 4560; train loss: 7.704905033111572; dev loss
Epoch 30:  40%|████      | 120/300 [03:26<05:02,  1.68s/it]Epoch 30:  40%|████      | 121/300 [03:28<04:50,  1.62s/it]Epoch 30:  41%|████      | 122/300 [03:29<04:40,  1.57s/it]Epoch 30:  41%|████      | 123/300 [03:31<04:36,  1.56s/it]Epoch 30:  41%|████▏     | 124/300 [03:32<04:37,  1.57s/it]Epoch 30:  42%|████▏     | 125/300 [03:34<04:32,  1.56s/it]Epoch 30:  42%|████▏     | 126/300 [03:35<04:27,  1.54s/it]Epoch 30:  42%|████▏     | 127/300 [03:37<04:45,  1.65s/it]Epoch 30:  43%|████▎     | 128/300 [03:39<05:02,  1.76s/it]Epoch 30:  43%|████▎     | 129/300 [03:41<04:48,  1.69s/it]Epoch 30:  43%|████▎     | 130/300 [03:42<04:36,  1.63s/it]Epoch 30:  44%|████▎     | 131/300 [03:44<04:49,  1.71s/it]Epoch 30:  44%|████▍     | 132/300 [03:46<04:43,  1.69s/it]Epoch 30:  44%|████▍     | 133/300 [03:47<04:33,  1.64s/it]Epoch 30:  45%|████▍     | 134/300 [03:49<04:44,  1.71s/it]Epoch 30:  45%|████▌     | 135/300 [03:51<04:51,  1.77s/it]Epoch 30:  45%|████▌     | 136/300 [03:53<05:02,  1.84s/it]Epoch 30:  46%|████▌     | 137/300 [03:55<04:43,  1.74s/it]Epoch 30:  46%|████▌     | 138/300 [03:56<04:34,  1.69s/it]Epoch 30:  46%|████▋     | 139/300 [03:58<04:43,  1.76s/it]06/19/2022 18:55:44 - INFO - __main__ - global step: 4570; train loss: 7.922390937805176; dev loss
Epoch 30:  47%|████▋     | 140/300 [04:00<04:29,  1.68s/it]Epoch 30:  47%|████▋     | 141/300 [04:01<04:14,  1.60s/it]Epoch 30:  47%|████▋     | 142/300 [04:03<04:25,  1.68s/it]Epoch 30:  48%|████▊     | 143/300 [04:04<04:10,  1.59s/it]Epoch 30:  48%|████▊     | 144/300 [04:06<03:58,  1.53s/it]Epoch 30:  48%|████▊     | 145/300 [04:07<03:55,  1.52s/it]Epoch 30:  49%|████▊     | 146/300 [04:09<04:07,  1.61s/it]Epoch 30:  49%|████▉     | 147/300 [04:11<04:13,  1.66s/it]Epoch 30:  49%|████▉     | 148/300 [04:13<04:19,  1.71s/it]Epoch 30:  50%|████▉     | 149/300 [04:14<04:25,  1.76s/it]Epoch 30:  50%|█████     | 150/300 [04:16<04:26,  1.77s/it]Epoch 30:  50%|█████     | 151/300 [04:18<04:25,  1.78s/it]Epoch 30:  51%|█████     | 152/300 [04:19<04:05,  1.66s/it]Epoch 30:  51%|█████     | 153/300 [04:21<03:55,  1.60s/it]Epoch 30:  51%|█████▏    | 154/300 [04:23<04:01,  1.66s/it]Epoch 30:  52%|█████▏    | 155/300 [04:24<03:48,  1.58s/it]Epoch 30:  52%|█████▏    | 156/300 [04:25<03:38,  1.51s/it]Epoch 30:  52%|█████▏    | 157/300 [04:27<03:34,  1.50s/it]Epoch 30:  53%|█████▎    | 158/300 [04:29<03:45,  1.59s/it]Epoch 30:  53%|█████▎    | 159/300 [04:30<03:35,  1.53s/it]06/19/2022 18:56:16 - INFO - __main__ - global step: 4580; train loss: 8.075855255126953; dev loss
Epoch 30:  53%|█████▎    | 160/300 [04:32<03:45,  1.61s/it]Epoch 30:  54%|█████▎    | 161/300 [04:34<03:55,  1.69s/it]Epoch 30:  54%|█████▍    | 162/300 [04:36<03:58,  1.73s/it]Epoch 30:  54%|█████▍    | 163/300 [04:37<03:58,  1.74s/it]Epoch 30:  55%|█████▍    | 164/300 [04:39<03:43,  1.64s/it]Epoch 30:  55%|█████▌    | 165/300 [04:40<03:34,  1.59s/it]Epoch 30:  55%|█████▌    | 166/300 [04:42<03:25,  1.53s/it]Epoch 30:  56%|█████▌    | 167/300 [04:43<03:35,  1.62s/it]Epoch 30:  56%|█████▌    | 168/300 [04:45<03:25,  1.56s/it]Epoch 30:  56%|█████▋    | 169/300 [04:47<03:36,  1.66s/it]Epoch 30:  57%|█████▋    | 170/300 [04:48<03:24,  1.57s/it]Epoch 30:  57%|█████▋    | 171/300 [04:50<03:15,  1.51s/it]Epoch 30:  57%|█████▋    | 172/300 [04:51<03:08,  1.47s/it]Epoch 30:  58%|█████▊    | 173/300 [04:53<03:19,  1.57s/it]Epoch 30:  58%|█████▊    | 174/300 [04:54<03:16,  1.56s/it]Epoch 30:  58%|█████▊    | 175/300 [04:56<03:07,  1.50s/it]Epoch 30:  59%|█████▊    | 176/300 [04:57<03:16,  1.58s/it]Epoch 30:  59%|█████▉    | 177/300 [04:59<03:25,  1.67s/it]Epoch 30:  59%|█████▉    | 178/300 [05:01<03:16,  1.61s/it]Epoch 30:  60%|█████▉    | 179/300 [05:02<03:07,  1.55s/it]06/19/2022 18:56:48 - INFO - __main__ - global step: 4590; train loss: 7.178688049316406; dev loss
Epoch 30:  60%|██████    | 180/300 [05:04<03:13,  1.62s/it]Epoch 30:  60%|██████    | 181/300 [05:05<03:04,  1.55s/it]Epoch 30:  61%|██████    | 182/300 [05:07<03:00,  1.53s/it]Epoch 30:  61%|██████    | 183/300 [05:09<03:07,  1.60s/it]Epoch 30:  61%|██████▏   | 184/300 [05:10<03:12,  1.66s/it]Epoch 30:  62%|██████▏   | 185/300 [05:12<03:01,  1.58s/it]Epoch 30:  62%|██████▏   | 186/300 [05:14<03:10,  1.67s/it]Epoch 30:  62%|██████▏   | 187/300 [05:15<02:58,  1.58s/it]Epoch 30:  63%|██████▎   | 188/300 [05:16<02:50,  1.52s/it]Epoch 30:  63%|██████▎   | 189/300 [05:18<02:44,  1.48s/it]Epoch 30:  63%|██████▎   | 190/300 [05:20<02:56,  1.60s/it]Epoch 30:  64%|██████▎   | 191/300 [05:21<03:00,  1.66s/it]Epoch 30:  64%|██████▍   | 192/300 [05:23<02:49,  1.57s/it]Epoch 30:  64%|██████▍   | 193/300 [05:24<02:42,  1.52s/it]Epoch 30:  65%|██████▍   | 194/300 [05:26<02:39,  1.51s/it]Epoch 30:  65%|██████▌   | 195/300 [05:27<02:34,  1.47s/it]Epoch 30:  65%|██████▌   | 196/300 [05:29<02:42,  1.57s/it]Epoch 30:  66%|██████▌   | 197/300 [05:31<02:49,  1.64s/it]Epoch 30:  66%|██████▌   | 198/300 [05:32<02:52,  1.69s/it]Epoch 30:  66%|██████▋   | 199/300 [05:34<02:43,  1.62s/it]06/19/2022 18:57:20 - INFO - __main__ - global step: 4600; train loss: 8.33065414428711; dev loss
Epoch 30:  67%|██████▋   | 200/300 [05:36<02:46,  1.67s/it]Epoch 30:  67%|██████▋   | 201/300 [05:37<02:48,  1.70s/it]Epoch 30:  67%|██████▋   | 202/300 [05:39<02:37,  1.61s/it]Epoch 30:  68%|██████▊   | 203/300 [05:41<02:43,  1.68s/it]Epoch 30:  68%|██████▊   | 204/300 [05:42<02:32,  1.59s/it]Epoch 30:  68%|██████▊   | 205/300 [05:44<02:25,  1.53s/it]Epoch 30:  69%|██████▊   | 206/300 [05:45<02:31,  1.61s/it]Epoch 30:  69%|██████▉   | 207/300 [05:47<02:37,  1.69s/it]Epoch 30:  69%|██████▉   | 208/300 [05:49<02:39,  1.73s/it]Epoch 30:  70%|██████▉   | 209/300 [05:50<02:27,  1.63s/it]Epoch 30:  70%|███████   | 210/300 [05:52<02:30,  1.67s/it]Epoch 30:  70%|███████   | 211/300 [05:54<02:34,  1.73s/it]Epoch 30:  71%|███████   | 212/300 [05:56<02:33,  1.74s/it]Epoch 30:  71%|███████   | 213/300 [05:58<02:32,  1.76s/it]Epoch 30:  71%|███████▏  | 214/300 [05:59<02:32,  1.77s/it]Epoch 30:  72%|███████▏  | 215/300 [06:01<02:33,  1.80s/it]Epoch 30:  72%|███████▏  | 216/300 [06:03<02:20,  1.67s/it]Epoch 30:  72%|███████▏  | 217/300 [06:04<02:11,  1.59s/it]Epoch 30:  73%|███████▎  | 218/300 [06:05<02:04,  1.52s/it]Epoch 30:  73%|███████▎  | 219/300 [06:07<02:11,  1.63s/it]06/19/2022 18:57:53 - INFO - __main__ - global step: 4610; train loss: 8.153877258300781; dev loss
Epoch 30:  73%|███████▎  | 220/300 [06:09<02:05,  1.56s/it]Epoch 30:  74%|███████▎  | 221/300 [06:10<02:08,  1.63s/it]Epoch 30:  74%|███████▍  | 222/300 [06:12<02:01,  1.55s/it]Epoch 30:  74%|███████▍  | 223/300 [06:14<02:07,  1.65s/it]Epoch 30:  75%|███████▍  | 224/300 [06:15<02:08,  1.69s/it]Epoch 30:  75%|███████▌  | 225/300 [06:17<01:59,  1.60s/it]Epoch 30:  75%|███████▌  | 226/300 [06:19<02:02,  1.66s/it]Epoch 30:  76%|███████▌  | 227/300 [06:20<02:03,  1.69s/it]Epoch 30:  76%|███████▌  | 228/300 [06:22<02:06,  1.75s/it]Epoch 30:  76%|███████▋  | 229/300 [06:24<01:57,  1.65s/it]Epoch 30:  77%|███████▋  | 230/300 [06:26<01:58,  1.70s/it]Epoch 30:  77%|███████▋  | 231/300 [06:27<01:58,  1.72s/it]Epoch 30:  77%|███████▋  | 232/300 [06:29<01:52,  1.65s/it]Epoch 30:  78%|███████▊  | 233/300 [06:31<01:53,  1.70s/it]Epoch 30:  78%|███████▊  | 234/300 [06:32<01:53,  1.72s/it]Epoch 30:  78%|███████▊  | 235/300 [06:34<01:45,  1.63s/it]Epoch 30:  79%|███████▊  | 236/300 [06:35<01:41,  1.58s/it]Epoch 30:  79%|███████▉  | 237/300 [06:37<01:36,  1.53s/it]Epoch 30:  79%|███████▉  | 238/300 [06:38<01:32,  1.49s/it]Epoch 30:  80%|███████▉  | 239/300 [06:39<01:28,  1.45s/it]06/19/2022 18:58:25 - INFO - __main__ - global step: 4620; train loss: 8.094596862792969; dev loss
Epoch 30:  80%|████████  | 240/300 [06:41<01:34,  1.58s/it]Epoch 30:  80%|████████  | 241/300 [06:43<01:29,  1.52s/it]Epoch 30:  81%|████████  | 242/300 [06:44<01:32,  1.60s/it]Epoch 30:  81%|████████  | 243/300 [06:46<01:28,  1.55s/it]Epoch 30:  81%|████████▏ | 244/300 [06:48<01:32,  1.65s/it]Epoch 30:  82%|████████▏ | 245/300 [06:49<01:26,  1.57s/it]Epoch 30:  82%|████████▏ | 246/300 [06:51<01:28,  1.63s/it]Epoch 30:  82%|████████▏ | 247/300 [06:53<01:29,  1.68s/it]Epoch 30:  83%|████████▎ | 248/300 [06:55<01:30,  1.75s/it]Epoch 30:  83%|████████▎ | 249/300 [06:56<01:23,  1.63s/it]Epoch 30:  83%|████████▎ | 250/300 [06:58<01:23,  1.68s/it]Epoch 30:  84%|████████▎ | 251/300 [07:00<01:23,  1.71s/it]Epoch 30:  84%|████████▍ | 252/300 [07:01<01:17,  1.61s/it]Epoch 30:  84%|████████▍ | 253/300 [07:03<01:19,  1.70s/it]Epoch 30:  85%|████████▍ | 254/300 [07:05<01:19,  1.72s/it]Epoch 30:  85%|████████▌ | 255/300 [07:06<01:18,  1.74s/it]Epoch 30:  85%|████████▌ | 256/300 [07:08<01:17,  1.76s/it]Epoch 30:  86%|████████▌ | 257/300 [07:10<01:12,  1.68s/it]Epoch 30:  86%|████████▌ | 258/300 [07:11<01:06,  1.59s/it]Epoch 30:  86%|████████▋ | 259/300 [07:13<01:02,  1.53s/it]06/19/2022 18:58:58 - INFO - __main__ - global step: 4630; train loss: 7.762862205505371; dev loss
Epoch 30:  87%|████████▋ | 260/300 [07:14<01:04,  1.60s/it]Epoch 30:  87%|████████▋ | 261/300 [07:16<01:05,  1.68s/it]Epoch 30:  87%|████████▋ | 262/300 [07:18<01:00,  1.59s/it]Epoch 30:  88%|████████▊ | 263/300 [07:19<00:56,  1.54s/it]Epoch 30:  88%|████████▊ | 264/300 [07:20<00:53,  1.49s/it]Epoch 30:  88%|████████▊ | 265/300 [07:22<00:52,  1.49s/it]Epoch 30:  89%|████████▊ | 266/300 [07:24<00:53,  1.57s/it]Epoch 30:  89%|████████▉ | 267/300 [07:25<00:50,  1.52s/it]Epoch 30:  89%|████████▉ | 268/300 [07:27<00:51,  1.61s/it]Epoch 30:  90%|████████▉ | 269/300 [07:28<00:48,  1.57s/it]Epoch 30:  90%|█████████ | 270/300 [07:30<00:45,  1.52s/it]Epoch 30:  90%|█████████ | 271/300 [07:31<00:42,  1.47s/it]Epoch 30:  91%|█████████ | 272/300 [07:32<00:40,  1.45s/it]Epoch 30:  91%|█████████ | 273/300 [07:34<00:39,  1.46s/it]Epoch 30:  91%|█████████▏| 274/300 [07:35<00:37,  1.44s/it]Epoch 30:  92%|█████████▏| 275/300 [07:37<00:38,  1.54s/it]Epoch 30:  92%|█████████▏| 276/300 [07:39<00:38,  1.61s/it]Epoch 30:  92%|█████████▏| 277/300 [07:40<00:36,  1.58s/it]Epoch 30:  93%|█████████▎| 278/300 [07:42<00:36,  1.64s/it]Epoch 30:  93%|█████████▎| 279/300 [07:44<00:35,  1.68s/it]06/19/2022 18:59:30 - INFO - __main__ - global step: 4640; train loss: 7.712197303771973; dev loss
Epoch 30:  93%|█████████▎| 280/300 [07:45<00:32,  1.60s/it]Epoch 30:  94%|█████████▎| 281/300 [07:47<00:29,  1.54s/it]Epoch 30:  94%|█████████▍| 282/300 [07:48<00:27,  1.52s/it]Epoch 30:  94%|█████████▍| 283/300 [07:50<00:25,  1.48s/it]Epoch 30:  95%|█████████▍| 284/300 [07:51<00:25,  1.58s/it]Epoch 30:  95%|█████████▌| 285/300 [07:53<00:24,  1.65s/it]Epoch 30:  95%|█████████▌| 286/300 [07:55<00:24,  1.73s/it]Epoch 30:  96%|█████████▌| 287/300 [07:57<00:21,  1.64s/it]Epoch 30:  96%|█████████▌| 288/300 [07:58<00:20,  1.69s/it]Epoch 30:  96%|█████████▋| 289/300 [08:00<00:18,  1.72s/it]Epoch 30:  97%|█████████▋| 290/300 [08:02<00:16,  1.65s/it]Epoch 30:  97%|█████████▋| 291/300 [08:03<00:14,  1.57s/it]Epoch 30:  97%|█████████▋| 292/300 [08:05<00:13,  1.63s/it]Epoch 30:  98%|█████████▊| 293/300 [08:06<00:10,  1.56s/it]Epoch 30:  98%|█████████▊| 294/300 [08:08<00:09,  1.65s/it]Epoch 30:  98%|█████████▊| 295/300 [08:10<00:08,  1.70s/it]Epoch 30:  99%|█████████▊| 296/300 [08:11<00:06,  1.61s/it]Epoch 30:  99%|█████████▉| 297/300 [08:13<00:04,  1.54s/it]Epoch 30:  99%|█████████▉| 298/300 [08:15<00:03,  1.63s/it]Epoch 30: 100%|█████████▉| 299/300 [08:16<00:01,  1.56s/it]06/19/2022 19:00:02 - INFO - __main__ - global step: 4650; train loss: 7.509484767913818; dev loss
Epoch 30: 100%|██████████| 300/300 [08:18<00:00,  1.63s/it]Epoch 30: 100%|██████████| 300/300 [08:18<00:00,  1.66s/it]
Epoch 31:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 31:   0%|          | 1/300 [00:01<08:48,  1.77s/it]Epoch 31:   1%|          | 2/300 [00:03<09:09,  1.84s/it]Epoch 31:   1%|          | 3/300 [00:05<08:04,  1.63s/it]Epoch 31:   1%|▏         | 4/300 [00:06<08:20,  1.69s/it]Epoch 31:   2%|▏         | 5/300 [00:08<08:27,  1.72s/it]Epoch 31:   2%|▏         | 6/300 [00:09<07:52,  1.61s/it]Epoch 31:   2%|▏         | 7/300 [00:11<07:38,  1.57s/it]Epoch 31:   3%|▎         | 8/300 [00:12<07:19,  1.51s/it]Epoch 31:   3%|▎         | 9/300 [00:14<07:09,  1.48s/it]Epoch 31:   3%|▎         | 10/300 [00:15<07:00,  1.45s/it]Epoch 31:   4%|▎         | 11/300 [00:17<07:34,  1.57s/it]Epoch 31:   4%|▍         | 12/300 [00:19<07:51,  1.64s/it]Epoch 31:   4%|▍         | 13/300 [00:21<08:04,  1.69s/it]Epoch 31:   5%|▍         | 14/300 [00:22<08:11,  1.72s/it]Epoch 31:   5%|▌         | 15/300 [00:24<07:49,  1.65s/it]Epoch 31:   5%|▌         | 16/300 [00:26<07:58,  1.68s/it]Epoch 31:   6%|▌         | 17/300 [00:27<07:29,  1.59s/it]Epoch 31:   6%|▌         | 18/300 [00:28<07:10,  1.53s/it]Epoch 31:   6%|▋         | 19/300 [00:30<07:05,  1.52s/it]06/19/2022 19:00:34 - INFO - __main__ - global step: 4660; train loss: 7.773749351501465; dev loss
Epoch 31:   7%|▋         | 20/300 [00:32<07:26,  1.59s/it]Epoch 31:   7%|▋         | 21/300 [00:33<07:05,  1.53s/it]Epoch 31:   7%|▋         | 22/300 [00:35<07:25,  1.60s/it]Epoch 31:   8%|▊         | 23/300 [00:36<07:14,  1.57s/it]Epoch 31:   8%|▊         | 24/300 [00:38<07:31,  1.64s/it]Epoch 31:   8%|▊         | 25/300 [00:39<07:07,  1.56s/it]Epoch 31:   9%|▊         | 26/300 [00:41<07:25,  1.63s/it]Epoch 31:   9%|▉         | 27/300 [00:43<07:12,  1.58s/it]Epoch 31:   9%|▉         | 28/300 [00:45<07:26,  1.64s/it]Epoch 31:  10%|▉         | 29/300 [00:46<07:36,  1.69s/it]Epoch 31:  10%|█         | 30/300 [00:48<07:45,  1.72s/it]Epoch 31:  10%|█         | 31/300 [00:50<07:57,  1.78s/it]Epoch 31:  11%|█         | 32/300 [00:51<07:23,  1.66s/it]Epoch 31:  11%|█         | 33/300 [00:53<07:32,  1.69s/it]Epoch 31:  11%|█▏        | 34/300 [00:55<07:06,  1.60s/it]Epoch 31:  12%|█▏        | 35/300 [00:56<07:19,  1.66s/it]Epoch 31:  12%|█▏        | 36/300 [00:58<07:05,  1.61s/it]Epoch 31:  12%|█▏        | 37/300 [00:59<06:45,  1.54s/it]Epoch 31:  13%|█▎        | 38/300 [01:01<07:05,  1.62s/it]Epoch 31:  13%|█▎        | 39/300 [01:03<07:17,  1.68s/it]06/19/2022 19:01:07 - INFO - __main__ - global step: 4670; train loss: 8.001001358032227; dev loss
Epoch 31:  13%|█▎        | 40/300 [01:04<06:58,  1.61s/it]Epoch 31:  14%|█▎        | 41/300 [01:06<07:09,  1.66s/it]Epoch 31:  14%|█▍        | 42/300 [01:08<07:21,  1.71s/it]Epoch 31:  14%|█▍        | 43/300 [01:09<06:55,  1.62s/it]Epoch 31:  15%|█▍        | 44/300 [01:11<06:45,  1.58s/it]Epoch 31:  15%|█▌        | 45/300 [01:13<07:01,  1.65s/it]Epoch 31:  15%|█▌        | 46/300 [01:14<07:13,  1.71s/it]Epoch 31:  16%|█▌        | 47/300 [01:16<07:17,  1.73s/it]Epoch 31:  16%|█▌        | 48/300 [01:18<06:58,  1.66s/it]Epoch 31:  16%|█▋        | 49/300 [01:20<07:05,  1.70s/it]Epoch 31:  17%|█▋        | 50/300 [01:21<07:10,  1.72s/it]Epoch 31:  17%|█▋        | 51/300 [01:23<07:13,  1.74s/it]Epoch 31:  17%|█▋        | 52/300 [01:25<07:22,  1.78s/it]Epoch 31:  18%|█▊        | 53/300 [01:27<07:21,  1.79s/it]Epoch 31:  18%|█▊        | 54/300 [01:28<06:51,  1.67s/it]Epoch 31:  18%|█▊        | 55/300 [01:30<06:58,  1.71s/it]Epoch 31:  19%|█▊        | 56/300 [01:31<06:40,  1.64s/it]Epoch 31:  19%|█▉        | 57/300 [01:33<06:48,  1.68s/it]Epoch 31:  19%|█▉        | 58/300 [01:35<06:25,  1.59s/it]Epoch 31:  20%|█▉        | 59/300 [01:36<06:08,  1.53s/it]06/19/2022 19:01:40 - INFO - __main__ - global step: 4680; train loss: 7.743000030517578; dev loss
Epoch 31:  20%|██        | 60/300 [01:38<06:23,  1.60s/it]Epoch 31:  20%|██        | 61/300 [01:40<06:41,  1.68s/it]Epoch 31:  21%|██        | 62/300 [01:41<06:18,  1.59s/it]Epoch 31:  21%|██        | 63/300 [01:43<06:30,  1.65s/it]Epoch 31:  21%|██▏       | 64/300 [01:45<06:38,  1.69s/it]Epoch 31:  22%|██▏       | 65/300 [01:46<06:21,  1.62s/it]Epoch 31:  22%|██▏       | 66/300 [01:48<06:30,  1.67s/it]Epoch 31:  22%|██▏       | 67/300 [01:49<06:07,  1.58s/it]Epoch 31:  23%|██▎       | 68/300 [01:51<06:20,  1.64s/it]Epoch 31:  23%|██▎       | 69/300 [01:53<06:37,  1.72s/it]Epoch 31:  23%|██▎       | 70/300 [01:55<06:39,  1.74s/it]Epoch 31:  24%|██▎       | 71/300 [01:56<06:42,  1.76s/it]Epoch 31:  24%|██▍       | 72/300 [01:58<06:15,  1.65s/it]Epoch 31:  24%|██▍       | 73/300 [01:59<06:02,  1.60s/it]Epoch 31:  25%|██▍       | 74/300 [02:01<06:16,  1.66s/it]Epoch 31:  25%|██▌       | 75/300 [02:03<05:56,  1.59s/it]Epoch 31:  25%|██▌       | 76/300 [02:04<06:07,  1.64s/it]Epoch 31:  26%|██▌       | 77/300 [02:06<05:54,  1.59s/it]Epoch 31:  26%|██▌       | 78/300 [02:07<05:39,  1.53s/it]Epoch 31:  26%|██▋       | 79/300 [02:09<05:53,  1.60s/it]06/19/2022 19:02:13 - INFO - __main__ - global step: 4690; train loss: 7.823904514312744; dev loss
Epoch 31:  27%|██▋       | 80/300 [02:11<06:06,  1.66s/it]Epoch 31:  27%|██▋       | 81/300 [02:13<06:19,  1.73s/it]Epoch 31:  27%|██▋       | 82/300 [02:14<05:55,  1.63s/it]Epoch 31:  28%|██▊       | 83/300 [02:16<06:03,  1.67s/it]Epoch 31:  28%|██▊       | 84/300 [02:18<06:08,  1.71s/it]Epoch 31:  28%|██▊       | 85/300 [02:19<05:53,  1.64s/it]Epoch 31:  29%|██▊       | 86/300 [02:20<05:35,  1.57s/it]Epoch 31:  29%|██▉       | 87/300 [02:22<05:22,  1.51s/it]Epoch 31:  29%|██▉       | 88/300 [02:24<05:37,  1.59s/it]Epoch 31:  30%|██▉       | 89/300 [02:25<05:24,  1.54s/it]Epoch 31:  30%|███       | 90/300 [02:27<05:18,  1.52s/it]Epoch 31:  30%|███       | 91/300 [02:28<05:35,  1.60s/it]Epoch 31:  31%|███       | 92/300 [02:30<05:21,  1.54s/it]Epoch 31:  31%|███       | 93/300 [02:32<05:35,  1.62s/it]Epoch 31:  31%|███▏      | 94/300 [02:33<05:49,  1.70s/it]Epoch 31:  32%|███▏      | 95/300 [02:35<05:54,  1.73s/it]Epoch 31:  32%|███▏      | 96/300 [02:37<05:56,  1.75s/it]Epoch 31:  32%|███▏      | 97/300 [02:39<05:59,  1.77s/it]Epoch 31:  33%|███▎      | 98/300 [02:41<06:05,  1.81s/it]Epoch 31:  33%|███▎      | 99/300 [02:42<05:36,  1.68s/it]06/19/2022 19:02:46 - INFO - __main__ - global step: 4700; train loss: 7.9441046714782715; dev loss
Epoch 31:  33%|███▎      | 100/300 [02:43<05:16,  1.58s/it]Epoch 31:  34%|███▎      | 101/300 [02:45<05:02,  1.52s/it]Epoch 31:  34%|███▍      | 102/300 [02:47<05:22,  1.63s/it]Epoch 31:  34%|███▍      | 103/300 [02:48<05:07,  1.56s/it]Epoch 31:  35%|███▍      | 104/300 [02:50<05:18,  1.62s/it]Epoch 31:  35%|███▌      | 105/300 [02:52<05:27,  1.68s/it]Epoch 31:  35%|███▌      | 106/300 [02:53<05:14,  1.62s/it]Epoch 31:  36%|███▌      | 107/300 [02:55<05:21,  1.66s/it]Epoch 31:  36%|███▌      | 108/300 [02:57<05:26,  1.70s/it]Epoch 31:  36%|███▋      | 109/300 [02:58<05:06,  1.61s/it]Epoch 31:  37%|███▋      | 110/300 [03:00<05:19,  1.68s/it]Epoch 31:  37%|███▋      | 111/300 [03:01<05:01,  1.59s/it]Epoch 31:  37%|███▋      | 112/300 [03:03<04:47,  1.53s/it]Epoch 31:  38%|███▊      | 113/300 [03:04<04:38,  1.49s/it]Epoch 31:  38%|███▊      | 114/300 [03:06<04:54,  1.58s/it]Epoch 31:  38%|███▊      | 115/300 [03:07<04:46,  1.55s/it]Epoch 31:  39%|███▊      | 116/300 [03:09<04:35,  1.50s/it]Epoch 31:  39%|███▉      | 117/300 [03:10<04:26,  1.46s/it]Epoch 31:  39%|███▉      | 118/300 [03:12<04:43,  1.56s/it]Epoch 31:  40%|███▉      | 119/300 [03:13<04:38,  1.54s/it]06/19/2022 19:03:17 - INFO - __main__ - global step: 4710; train loss: 8.148040771484375; dev loss
Epoch 31:  40%|████      | 120/300 [03:15<04:27,  1.49s/it]Epoch 31:  40%|████      | 121/300 [03:16<04:20,  1.46s/it]Epoch 31:  41%|████      | 122/300 [03:18<04:37,  1.56s/it]Epoch 31:  41%|████      | 123/300 [03:20<04:52,  1.65s/it]Epoch 31:  41%|████▏     | 124/300 [03:22<05:00,  1.71s/it]Epoch 31:  42%|████▏     | 125/300 [03:23<04:41,  1.61s/it]Epoch 31:  42%|████▏     | 126/300 [03:25<04:48,  1.66s/it]Epoch 31:  42%|████▏     | 127/300 [03:27<04:59,  1.73s/it]Epoch 31:  43%|████▎     | 128/300 [03:28<04:39,  1.62s/it]Epoch 31:  43%|████▎     | 129/300 [03:30<04:26,  1.56s/it]Epoch 31:  43%|████▎     | 130/300 [03:31<04:15,  1.50s/it]Epoch 31:  44%|████▎     | 131/300 [03:33<04:34,  1.63s/it]Epoch 31:  44%|████▍     | 132/300 [03:34<04:21,  1.56s/it]Epoch 31:  44%|████▍     | 133/300 [03:36<04:11,  1.51s/it]Epoch 31:  45%|████▍     | 134/300 [03:37<04:24,  1.59s/it]Epoch 31:  45%|████▌     | 135/300 [03:39<04:17,  1.56s/it]Epoch 31:  45%|████▌     | 136/300 [03:41<04:28,  1.64s/it]Epoch 31:  46%|████▌     | 137/300 [03:42<04:14,  1.56s/it]Epoch 31:  46%|████▌     | 138/300 [03:44<04:24,  1.63s/it]Epoch 31:  46%|████▋     | 139/300 [03:46<04:35,  1.71s/it]06/19/2022 19:03:50 - INFO - __main__ - global step: 4720; train loss: 7.646971702575684; dev loss
Epoch 31:  47%|████▋     | 140/300 [03:48<04:37,  1.74s/it]Epoch 31:  47%|████▋     | 141/300 [03:49<04:20,  1.64s/it]Epoch 31:  47%|████▋     | 142/300 [03:51<04:26,  1.68s/it]Epoch 31:  48%|████▊     | 143/300 [03:52<04:11,  1.60s/it]Epoch 31:  48%|████▊     | 144/300 [03:54<04:24,  1.70s/it]Epoch 31:  48%|████▊     | 145/300 [03:56<04:27,  1.73s/it]Epoch 31:  49%|████▊     | 146/300 [03:57<04:10,  1.63s/it]Epoch 31:  49%|████▉     | 147/300 [03:59<04:16,  1.68s/it]Epoch 31:  49%|████▉     | 148/300 [04:01<04:24,  1.74s/it]Epoch 31:  50%|████▉     | 149/300 [04:02<04:08,  1.64s/it]Epoch 31:  50%|█████     | 150/300 [04:04<03:55,  1.57s/it]Epoch 31:  50%|█████     | 151/300 [04:05<03:46,  1.52s/it]Epoch 31:  51%|█████     | 152/300 [04:07<03:43,  1.51s/it]Epoch 31:  51%|█████     | 153/300 [04:08<03:36,  1.47s/it]Epoch 31:  51%|█████▏    | 154/300 [04:10<03:54,  1.61s/it]Epoch 31:  52%|█████▏    | 155/300 [04:11<03:48,  1.57s/it]Epoch 31:  52%|█████▏    | 156/300 [04:13<04:05,  1.71s/it]Epoch 31:  52%|█████▏    | 157/300 [04:15<04:13,  1.78s/it]Epoch 31:  53%|█████▎    | 158/300 [04:17<04:19,  1.83s/it]Epoch 31:  53%|█████▎    | 159/300 [04:19<04:19,  1.84s/it]06/19/2022 19:04:23 - INFO - __main__ - global step: 4730; train loss: 8.100200653076172; dev loss
Epoch 31:  53%|█████▎    | 160/300 [04:21<04:19,  1.86s/it]Epoch 31:  54%|█████▎    | 161/300 [04:23<03:58,  1.72s/it]Epoch 31:  54%|█████▍    | 162/300 [04:24<03:59,  1.74s/it]Epoch 31:  54%|█████▍    | 163/300 [04:26<03:43,  1.63s/it]Epoch 31:  55%|█████▍    | 164/300 [04:28<03:51,  1.70s/it]Epoch 31:  55%|█████▌    | 165/300 [04:29<03:54,  1.74s/it]Epoch 31:  55%|█████▌    | 166/300 [04:31<03:39,  1.64s/it]Epoch 31:  56%|█████▌    | 167/300 [04:32<03:38,  1.65s/it]Epoch 31:  56%|█████▌    | 168/300 [04:34<03:40,  1.67s/it]Epoch 31:  56%|█████▋    | 169/300 [04:36<03:31,  1.61s/it]Epoch 31:  57%|█████▋    | 170/300 [04:37<03:36,  1.67s/it]Epoch 31:  57%|█████▋    | 171/300 [04:39<03:24,  1.58s/it]Epoch 31:  57%|█████▋    | 172/300 [04:40<03:14,  1.52s/it]Epoch 31:  58%|█████▊    | 173/300 [04:42<03:26,  1.63s/it]Epoch 31:  58%|█████▊    | 174/300 [04:43<03:15,  1.55s/it]Epoch 31:  58%|█████▊    | 175/300 [04:45<03:22,  1.62s/it]Epoch 31:  59%|█████▊    | 176/300 [04:47<03:27,  1.68s/it]Epoch 31:  59%|█████▉    | 177/300 [04:49<03:19,  1.62s/it]Epoch 31:  59%|█████▉    | 178/300 [04:50<03:08,  1.54s/it]Epoch 31:  60%|█████▉    | 179/300 [04:52<03:14,  1.61s/it]06/19/2022 19:04:55 - INFO - __main__ - global step: 4740; train loss: 7.7300920486450195; dev loss
Epoch 31:  60%|██████    | 180/300 [04:53<03:05,  1.55s/it]Epoch 31:  60%|██████    | 181/300 [04:55<03:01,  1.53s/it]Epoch 31:  61%|██████    | 182/300 [04:56<02:55,  1.49s/it]Epoch 31:  61%|██████    | 183/300 [04:57<02:50,  1.45s/it]Epoch 31:  61%|██████▏   | 184/300 [04:59<02:46,  1.44s/it]Epoch 31:  62%|██████▏   | 185/300 [05:01<03:00,  1.57s/it]Epoch 31:  62%|██████▏   | 186/300 [05:02<02:53,  1.52s/it]Epoch 31:  62%|██████▏   | 187/300 [05:03<02:46,  1.47s/it]Epoch 31:  63%|██████▎   | 188/300 [05:05<02:57,  1.58s/it]Epoch 31:  63%|██████▎   | 189/300 [05:07<02:53,  1.56s/it]Epoch 31:  63%|██████▎   | 190/300 [05:08<02:45,  1.51s/it]Epoch 31:  64%|██████▎   | 191/300 [05:09<02:39,  1.47s/it]Epoch 31:  64%|██████▍   | 192/300 [05:11<02:35,  1.44s/it]Epoch 31:  64%|██████▍   | 193/300 [05:13<02:49,  1.58s/it]Epoch 31:  65%|██████▍   | 194/300 [05:15<02:58,  1.68s/it]Epoch 31:  65%|██████▌   | 195/300 [05:17<03:04,  1.76s/it]Epoch 31:  65%|██████▌   | 196/300 [05:18<02:52,  1.66s/it]Epoch 31:  66%|██████▌   | 197/300 [05:20<02:45,  1.60s/it]Epoch 31:  66%|██████▌   | 198/300 [05:21<02:43,  1.61s/it]Epoch 31:  66%|██████▋   | 199/300 [05:23<02:40,  1.59s/it]06/19/2022 19:05:27 - INFO - __main__ - global step: 4750; train loss: 7.771980285644531; dev loss
Epoch 31:  67%|██████▋   | 200/300 [05:24<02:39,  1.59s/it]Epoch 31:  67%|██████▋   | 201/300 [05:26<02:35,  1.57s/it]Epoch 31:  67%|██████▋   | 202/300 [05:28<02:47,  1.71s/it]Epoch 31:  68%|██████▊   | 203/300 [05:30<02:52,  1.78s/it]Epoch 31:  68%|██████▊   | 204/300 [05:32<02:50,  1.77s/it]Epoch 31:  68%|██████▊   | 205/300 [05:33<02:48,  1.77s/it]Epoch 31:  69%|██████▊   | 206/300 [05:35<02:54,  1.86s/it]Epoch 31:  69%|██████▉   | 207/300 [05:37<02:55,  1.89s/it]Epoch 31:  69%|██████▉   | 208/300 [05:39<02:55,  1.91s/it]Epoch 31:  70%|██████▉   | 209/300 [05:41<02:54,  1.92s/it]Epoch 31:  70%|███████   | 210/300 [05:43<02:54,  1.94s/it]Epoch 31:  70%|███████   | 211/300 [05:45<02:40,  1.80s/it]Epoch 31:  71%|███████   | 212/300 [05:46<02:30,  1.71s/it]Epoch 31:  71%|███████   | 213/300 [05:48<02:22,  1.64s/it]Epoch 31:  71%|███████▏  | 214/300 [05:50<02:29,  1.74s/it]Epoch 31:  72%|███████▏  | 215/300 [05:51<02:22,  1.67s/it]Epoch 31:  72%|███████▏  | 216/300 [05:53<02:25,  1.74s/it]Epoch 31:  72%|███████▏  | 217/300 [05:55<02:18,  1.66s/it]Epoch 31:  73%|███████▎  | 218/300 [05:57<02:24,  1.76s/it]Epoch 31:  73%|███████▎  | 219/300 [05:58<02:25,  1.80s/it]06/19/2022 19:06:03 - INFO - __main__ - global step: 4760; train loss: 7.927903652191162; dev loss
Epoch 31:  73%|███████▎  | 220/300 [06:00<02:26,  1.83s/it]Epoch 31:  74%|███████▎  | 221/300 [06:02<02:26,  1.85s/it]Epoch 31:  74%|███████▍  | 222/300 [06:04<02:15,  1.74s/it]Epoch 31:  74%|███████▍  | 223/300 [06:06<02:19,  1.82s/it]Epoch 31:  75%|███████▍  | 224/300 [06:08<02:20,  1.84s/it]Epoch 31:  75%|███████▌  | 225/300 [06:10<02:19,  1.86s/it]Epoch 31:  75%|███████▌  | 226/300 [06:11<02:18,  1.88s/it]Epoch 31:  76%|███████▌  | 227/300 [06:13<02:10,  1.79s/it]Epoch 31:  76%|███████▌  | 228/300 [06:15<02:11,  1.82s/it]Epoch 31:  76%|███████▋  | 229/300 [06:16<02:02,  1.73s/it]Epoch 31:  77%|███████▋  | 230/300 [06:18<02:04,  1.77s/it]Epoch 31:  77%|███████▋  | 231/300 [06:20<01:58,  1.72s/it]Epoch 31:  77%|███████▋  | 232/300 [06:22<02:00,  1.77s/it]Epoch 31:  78%|███████▊  | 233/300 [06:24<02:01,  1.81s/it]Epoch 31:  78%|███████▊  | 234/300 [06:26<02:01,  1.85s/it]Epoch 31:  78%|███████▊  | 235/300 [06:27<01:55,  1.77s/it]Epoch 31:  79%|███████▊  | 236/300 [06:29<01:47,  1.68s/it]Epoch 31:  79%|███████▉  | 237/300 [06:30<01:42,  1.62s/it]Epoch 31:  79%|███████▉  | 238/300 [06:32<01:45,  1.70s/it]Epoch 31:  80%|███████▉  | 239/300 [06:34<01:49,  1.79s/it]06/19/2022 19:06:38 - INFO - __main__ - global step: 4770; train loss: 7.954856872558594; dev loss
Epoch 31:  80%|████████  | 240/300 [06:36<01:49,  1.82s/it]Epoch 31:  80%|████████  | 241/300 [06:38<01:49,  1.85s/it]Epoch 31:  81%|████████  | 242/300 [06:39<01:41,  1.74s/it]Epoch 31:  81%|████████  | 243/300 [06:41<01:43,  1.81s/it]Epoch 31:  81%|████████▏ | 244/300 [06:43<01:36,  1.72s/it]Epoch 31:  82%|████████▏ | 245/300 [06:45<01:37,  1.77s/it]Epoch 31:  82%|████████▏ | 246/300 [06:46<01:31,  1.69s/it]Epoch 31:  82%|████████▏ | 247/300 [06:48<01:27,  1.65s/it]Epoch 31:  83%|████████▎ | 248/300 [06:49<01:23,  1.60s/it]Epoch 31:  83%|████████▎ | 249/300 [06:51<01:26,  1.69s/it]Epoch 31:  83%|████████▎ | 250/300 [06:53<01:21,  1.64s/it]Epoch 31:  84%|████████▎ | 251/300 [06:55<01:24,  1.72s/it]Epoch 31:  84%|████████▍ | 252/300 [06:57<01:26,  1.80s/it]Epoch 31:  84%|████████▍ | 253/300 [06:58<01:20,  1.72s/it]Epoch 31:  85%|████████▍ | 254/300 [07:00<01:21,  1.76s/it]Epoch 31:  85%|████████▌ | 255/300 [07:01<01:15,  1.68s/it]Epoch 31:  85%|████████▌ | 256/300 [07:03<01:12,  1.65s/it]Epoch 31:  86%|████████▌ | 257/300 [07:05<01:08,  1.60s/it]Epoch 31:  86%|████████▌ | 258/300 [07:06<01:05,  1.57s/it]Epoch 31:  86%|████████▋ | 259/300 [07:08<01:03,  1.55s/it]06/19/2022 19:07:11 - INFO - __main__ - global step: 4780; train loss: 7.835997581481934; dev loss
Epoch 31:  87%|████████▋ | 260/300 [07:09<01:01,  1.55s/it]Epoch 31:  87%|████████▋ | 261/300 [07:11<01:04,  1.64s/it]Epoch 31:  87%|████████▋ | 262/300 [07:12<01:00,  1.60s/it]Epoch 31:  88%|████████▊ | 263/300 [07:14<00:57,  1.57s/it]Epoch 31:  88%|████████▊ | 264/300 [07:16<01:01,  1.70s/it]Epoch 31:  88%|████████▊ | 265/300 [07:17<00:57,  1.63s/it]Epoch 31:  89%|████████▊ | 266/300 [07:19<00:54,  1.61s/it]Epoch 31:  89%|████████▉ | 267/300 [07:20<00:51,  1.57s/it]Epoch 31:  89%|████████▉ | 268/300 [07:22<00:50,  1.57s/it]Epoch 31:  90%|████████▉ | 269/300 [07:24<00:51,  1.67s/it]Epoch 31:  90%|█████████ | 270/300 [07:25<00:48,  1.61s/it]Epoch 31:  90%|█████████ | 271/300 [07:27<00:49,  1.70s/it]Epoch 31:  91%|█████████ | 272/300 [07:29<00:50,  1.79s/it]Epoch 31:  91%|█████████ | 273/300 [07:31<00:49,  1.82s/it]Epoch 31:  91%|█████████▏| 274/300 [07:33<00:45,  1.73s/it]Epoch 31:  92%|█████████▏| 275/300 [07:35<00:44,  1.79s/it]Epoch 31:  92%|█████████▏| 276/300 [07:37<00:43,  1.82s/it]Epoch 31:  92%|█████████▏| 277/300 [07:38<00:40,  1.74s/it]Epoch 31:  93%|█████████▎| 278/300 [07:40<00:36,  1.66s/it]Epoch 31:  93%|█████████▎| 279/300 [07:41<00:33,  1.60s/it]06/19/2022 19:07:45 - INFO - __main__ - global step: 4790; train loss: 7.813999176025391; dev loss
Epoch 31:  93%|█████████▎| 280/300 [07:43<00:33,  1.68s/it]Epoch 31:  94%|█████████▎| 281/300 [07:44<00:31,  1.65s/it]Epoch 31:  94%|█████████▍| 282/300 [07:46<00:31,  1.73s/it]Epoch 31:  94%|█████████▍| 283/300 [07:48<00:28,  1.66s/it]Epoch 31:  95%|█████████▍| 284/300 [07:49<00:25,  1.61s/it]Epoch 31:  95%|█████████▌| 285/300 [07:51<00:24,  1.60s/it]Epoch 31:  95%|█████████▌| 286/300 [07:53<00:23,  1.69s/it]Epoch 31:  96%|█████████▌| 287/300 [07:54<00:21,  1.63s/it]Epoch 31:  96%|█████████▌| 288/300 [07:56<00:20,  1.71s/it]Epoch 31:  96%|█████████▋| 289/300 [07:58<00:18,  1.68s/it]Epoch 31:  97%|█████████▋| 290/300 [07:59<00:16,  1.62s/it]Epoch 31:  97%|█████████▋| 291/300 [08:01<00:14,  1.58s/it]Epoch 31:  97%|█████████▋| 292/300 [08:02<00:12,  1.55s/it]Epoch 31:  98%|█████████▊| 293/300 [08:04<00:11,  1.68s/it]Epoch 31:  98%|█████████▊| 294/300 [08:06<00:09,  1.62s/it]Epoch 31:  98%|█████████▊| 295/300 [08:08<00:08,  1.71s/it]Epoch 31:  99%|█████████▊| 296/300 [08:09<00:06,  1.64s/it]Epoch 31:  99%|█████████▉| 297/300 [08:11<00:04,  1.62s/it]Epoch 31:  99%|█████████▉| 298/300 [08:12<00:03,  1.58s/it]Epoch 31: 100%|█████████▉| 299/300 [08:14<00:01,  1.54s/it]06/19/2022 19:08:18 - INFO - __main__ - global step: 4800; train loss: 7.854630470275879; dev loss
Epoch 31: 100%|██████████| 300/300 [08:16<00:00,  1.65s/it]Epoch 31: 100%|██████████| 300/300 [08:16<00:00,  1.65s/it]
Epoch 32:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 32:   0%|          | 1/300 [00:01<09:56,  2.00s/it]Epoch 32:   1%|          | 2/300 [00:03<08:31,  1.72s/it]Epoch 32:   1%|          | 3/300 [00:05<08:53,  1.80s/it]Epoch 32:   1%|▏         | 4/300 [00:07<09:02,  1.83s/it]Epoch 32:   2%|▏         | 5/300 [00:08<08:22,  1.70s/it]Epoch 32:   2%|▏         | 6/300 [00:10<08:52,  1.81s/it]Epoch 32:   2%|▏         | 7/300 [00:12<08:59,  1.84s/it]Epoch 32:   3%|▎         | 8/300 [00:14<09:05,  1.87s/it]Epoch 32:   3%|▎         | 9/300 [00:16<08:28,  1.75s/it]Epoch 32:   3%|▎         | 10/300 [00:17<08:12,  1.70s/it]Epoch 32:   4%|▎         | 11/300 [00:19<07:53,  1.64s/it]Epoch 32:   4%|▍         | 12/300 [00:20<07:38,  1.59s/it]Epoch 32:   4%|▍         | 13/300 [00:22<08:02,  1.68s/it]Epoch 32:   5%|▍         | 14/300 [00:24<07:51,  1.65s/it]Epoch 32:   5%|▌         | 15/300 [00:25<07:34,  1.60s/it]Epoch 32:   5%|▌         | 16/300 [00:27<07:23,  1.56s/it]Epoch 32:   6%|▌         | 17/300 [00:28<07:50,  1.66s/it]Epoch 32:   6%|▌         | 18/300 [00:30<07:42,  1.64s/it]Epoch 32:   6%|▋         | 19/300 [00:32<08:01,  1.71s/it]06/19/2022 19:08:52 - INFO - __main__ - global step: 4810; train loss: 7.977551460266113; dev loss
Epoch 32:   7%|▋         | 20/300 [00:34<08:14,  1.77s/it]Epoch 32:   7%|▋         | 21/300 [00:35<07:50,  1.69s/it]Epoch 32:   7%|▋         | 22/300 [00:37<07:38,  1.65s/it]Epoch 32:   8%|▊         | 23/300 [00:39<07:58,  1.73s/it]Epoch 32:   8%|▊         | 24/300 [00:41<08:10,  1.78s/it]Epoch 32:   8%|▊         | 25/300 [00:43<08:20,  1.82s/it]Epoch 32:   9%|▊         | 26/300 [00:44<07:57,  1.74s/it]Epoch 32:   9%|▉         | 27/300 [00:46<07:35,  1.67s/it]Epoch 32:   9%|▉         | 28/300 [00:48<07:51,  1.73s/it]Epoch 32:  10%|▉         | 29/300 [00:49<07:29,  1.66s/it]Epoch 32:  10%|█         | 30/300 [00:51<07:15,  1.61s/it]Epoch 32:  10%|█         | 31/300 [00:52<07:11,  1.61s/it]Epoch 32:  11%|█         | 32/300 [00:54<07:02,  1.58s/it]Epoch 32:  11%|█         | 33/300 [00:56<07:29,  1.68s/it]Epoch 32:  11%|█▏        | 34/300 [00:57<07:44,  1.75s/it]Epoch 32:  12%|█▏        | 35/300 [00:59<07:29,  1.70s/it]Epoch 32:  12%|█▏        | 36/300 [01:01<07:42,  1.75s/it]Epoch 32:  12%|█▏        | 37/300 [01:02<07:20,  1.68s/it]Epoch 32:  13%|█▎        | 38/300 [01:04<07:36,  1.74s/it]Epoch 32:  13%|█▎        | 39/300 [01:06<07:22,  1.70s/it]06/19/2022 19:09:26 - INFO - __main__ - global step: 4820; train loss: 8.051369667053223; dev loss
Epoch 32:  13%|█▎        | 40/300 [01:08<07:35,  1.75s/it]Epoch 32:  14%|█▎        | 41/300 [01:09<07:24,  1.72s/it]Epoch 32:  14%|█▍        | 42/300 [01:11<07:20,  1.71s/it]Epoch 32:  14%|█▍        | 43/300 [01:13<07:38,  1.78s/it]Epoch 32:  15%|█▍        | 44/300 [01:15<07:23,  1.73s/it]Epoch 32:  15%|█▌        | 45/300 [01:16<07:13,  1.70s/it]Epoch 32:  15%|█▌        | 46/300 [01:18<07:04,  1.67s/it]Epoch 32:  16%|█▌        | 47/300 [01:20<07:16,  1.73s/it]Epoch 32:  16%|█▌        | 48/300 [01:22<07:20,  1.75s/it]Epoch 32:  16%|█▋        | 49/300 [01:23<07:12,  1.72s/it]Epoch 32:  17%|█▋        | 50/300 [01:25<07:00,  1.68s/it]Epoch 32:  17%|█▋        | 51/300 [01:27<07:11,  1.73s/it]Epoch 32:  17%|█▋        | 52/300 [01:28<07:08,  1.73s/it]Epoch 32:  18%|█▊        | 53/300 [01:30<07:17,  1.77s/it]Epoch 32:  18%|█▊        | 54/300 [01:32<06:54,  1.69s/it]Epoch 32:  18%|█▊        | 55/300 [01:33<06:46,  1.66s/it]Epoch 32:  19%|█▊        | 56/300 [01:35<07:01,  1.73s/it]Epoch 32:  19%|█▉        | 57/300 [01:37<07:11,  1.78s/it]Epoch 32:  19%|█▉        | 58/300 [01:39<07:19,  1.81s/it]Epoch 32:  20%|█▉        | 59/300 [01:41<07:23,  1.84s/it]06/19/2022 19:10:01 - INFO - __main__ - global step: 4830; train loss: 7.609455108642578; dev loss
Epoch 32:  20%|██        | 60/300 [01:43<07:31,  1.88s/it]Epoch 32:  20%|██        | 61/300 [01:45<07:30,  1.89s/it]Epoch 32:  21%|██        | 62/300 [01:46<07:02,  1.77s/it]Epoch 32:  21%|██        | 63/300 [01:48<06:39,  1.68s/it]Epoch 32:  21%|██▏       | 64/300 [01:50<06:59,  1.78s/it]Epoch 32:  22%|██▏       | 65/300 [01:51<06:35,  1.68s/it]Epoch 32:  22%|██▏       | 66/300 [01:53<06:49,  1.75s/it]Epoch 32:  22%|██▏       | 67/300 [01:55<07:00,  1.80s/it]Epoch 32:  23%|██▎       | 68/300 [01:57<06:44,  1.74s/it]Epoch 32:  23%|██▎       | 69/300 [01:58<06:23,  1.66s/it]Epoch 32:  23%|██▎       | 70/300 [02:00<06:11,  1.61s/it]Epoch 32:  24%|██▎       | 71/300 [02:02<06:30,  1.71s/it]Epoch 32:  24%|██▍       | 72/300 [02:03<06:22,  1.68s/it]Epoch 32:  24%|██▍       | 73/300 [02:05<06:36,  1.75s/it]Epoch 32:  25%|██▍       | 74/300 [02:07<06:45,  1.79s/it]Epoch 32:  25%|██▌       | 75/300 [02:09<06:51,  1.83s/it]Epoch 32:  25%|██▌       | 76/300 [02:11<06:34,  1.76s/it]Epoch 32:  26%|██▌       | 77/300 [02:12<06:14,  1.68s/it]Epoch 32:  26%|██▌       | 78/300 [02:14<06:00,  1.62s/it]Epoch 32:  26%|██▋       | 79/300 [02:15<05:49,  1.58s/it]06/19/2022 19:10:35 - INFO - __main__ - global step: 4840; train loss: 7.973087310791016; dev loss
Epoch 32:  27%|██▋       | 80/300 [02:17<05:49,  1.59s/it]Epoch 32:  27%|██▋       | 81/300 [02:19<06:10,  1.69s/it]Epoch 32:  27%|██▋       | 82/300 [02:20<05:56,  1.64s/it]Epoch 32:  28%|██▊       | 83/300 [02:22<06:12,  1.72s/it]Epoch 32:  28%|██▊       | 84/300 [02:24<06:23,  1.78s/it]Epoch 32:  28%|██▊       | 85/300 [02:26<06:14,  1.74s/it]Epoch 32:  29%|██▊       | 86/300 [02:27<06:25,  1.80s/it]Epoch 32:  29%|██▉       | 87/300 [02:29<06:03,  1.71s/it]Epoch 32:  29%|██▉       | 88/300 [02:30<05:48,  1.64s/it]Epoch 32:  30%|██▉       | 89/300 [02:32<05:43,  1.63s/it]Epoch 32:  30%|███       | 90/300 [02:34<06:00,  1.71s/it]Epoch 32:  30%|███       | 91/300 [02:36<06:09,  1.77s/it]Epoch 32:  31%|███       | 92/300 [02:37<05:53,  1.70s/it]Epoch 32:  31%|███       | 93/300 [02:39<06:10,  1.79s/it]Epoch 32:  31%|███▏      | 94/300 [02:41<06:15,  1.82s/it]Epoch 32:  32%|███▏      | 95/300 [02:43<06:19,  1.85s/it]Epoch 32:  32%|███▏      | 96/300 [02:45<05:56,  1.75s/it]Epoch 32:  32%|███▏      | 97/300 [02:47<06:08,  1.82s/it]Epoch 32:  33%|███▎      | 98/300 [02:48<05:47,  1.72s/it]Epoch 32:  33%|███▎      | 99/300 [02:50<05:32,  1.65s/it]06/19/2022 19:11:10 - INFO - __main__ - global step: 4850; train loss: 7.655292510986328; dev loss
Epoch 32:  33%|███▎      | 100/300 [02:51<05:22,  1.61s/it]Epoch 32:  34%|███▎      | 101/300 [02:53<05:19,  1.60s/it]Epoch 32:  34%|███▍      | 102/300 [02:54<05:10,  1.57s/it]Epoch 32:  34%|███▍      | 103/300 [02:56<05:27,  1.66s/it]Epoch 32:  35%|███▍      | 104/300 [02:58<05:17,  1.62s/it]Epoch 32:  35%|███▌      | 105/300 [03:00<05:39,  1.74s/it]Epoch 32:  35%|███▌      | 106/300 [03:02<05:43,  1.77s/it]Epoch 32:  36%|███▌      | 107/300 [03:03<05:48,  1.81s/it]Epoch 32:  36%|███▌      | 108/300 [03:05<05:28,  1.71s/it]Epoch 32:  36%|███▋      | 109/300 [03:07<05:20,  1.68s/it]Epoch 32:  37%|███▋      | 110/300 [03:08<05:07,  1.62s/it]Epoch 32:  37%|███▋      | 111/300 [03:10<05:21,  1.70s/it]Epoch 32:  37%|███▋      | 112/300 [03:12<05:30,  1.76s/it]Epoch 32:  38%|███▊      | 113/300 [03:13<05:14,  1.68s/it]Epoch 32:  38%|███▊      | 114/300 [03:15<05:08,  1.66s/it]Epoch 32:  38%|███▊      | 115/300 [03:17<05:19,  1.73s/it]Epoch 32:  39%|███▊      | 116/300 [03:18<05:05,  1.66s/it]Epoch 32:  39%|███▉      | 117/300 [03:20<04:55,  1.62s/it]Epoch 32:  39%|███▉      | 118/300 [03:22<05:13,  1.72s/it]Epoch 32:  40%|███▉      | 119/300 [03:24<05:21,  1.77s/it]06/19/2022 19:11:44 - INFO - __main__ - global step: 4860; train loss: 7.784174919128418; dev loss
Epoch 32:  40%|████      | 120/300 [03:26<05:27,  1.82s/it]Epoch 32:  40%|████      | 121/300 [03:27<05:08,  1.72s/it]Epoch 32:  41%|████      | 122/300 [03:29<04:59,  1.68s/it]Epoch 32:  41%|████      | 123/300 [03:30<04:49,  1.64s/it]Epoch 32:  41%|████▏     | 124/300 [03:32<05:01,  1.71s/it]Epoch 32:  42%|████▏     | 125/300 [03:34<05:09,  1.77s/it]Epoch 32:  42%|████▏     | 126/300 [03:36<04:58,  1.72s/it]Epoch 32:  42%|████▏     | 127/300 [03:37<05:06,  1.77s/it]Epoch 32:  43%|████▎     | 128/300 [03:39<04:50,  1.69s/it]Epoch 32:  43%|████▎     | 129/300 [03:41<04:58,  1.75s/it]Epoch 32:  43%|████▎     | 130/300 [03:43<05:11,  1.84s/it]Epoch 32:  44%|████▎     | 131/300 [03:44<04:55,  1.75s/it]Epoch 32:  44%|████▍     | 132/300 [03:46<04:58,  1.78s/it]Epoch 32:  44%|████▍     | 133/300 [03:48<05:06,  1.83s/it]Epoch 32:  45%|████▍     | 134/300 [03:50<05:14,  1.89s/it]Epoch 32:  45%|████▌     | 135/300 [03:52<04:55,  1.79s/it]Epoch 32:  45%|████▌     | 136/300 [03:54<04:54,  1.80s/it]Epoch 32:  46%|████▌     | 137/300 [03:55<04:37,  1.70s/it]Epoch 32:  46%|████▌     | 138/300 [03:57<04:24,  1.63s/it]Epoch 32:  46%|████▋     | 139/300 [03:58<04:20,  1.62s/it]06/19/2022 19:12:18 - INFO - __main__ - global step: 4870; train loss: 7.557478904724121; dev loss
Epoch 32:  47%|████▋     | 140/300 [04:00<04:31,  1.70s/it]Epoch 32:  47%|████▋     | 141/300 [04:02<04:39,  1.76s/it]Epoch 32:  47%|████▋     | 142/300 [04:04<04:46,  1.81s/it]Epoch 32:  48%|████▊     | 143/300 [04:06<04:43,  1.81s/it]Epoch 32:  48%|████▊     | 144/300 [04:08<04:47,  1.84s/it]Epoch 32:  48%|████▊     | 145/300 [04:09<04:30,  1.74s/it]Epoch 32:  49%|████▊     | 146/300 [04:11<04:35,  1.79s/it]Epoch 32:  49%|████▉     | 147/300 [04:13<04:42,  1.85s/it]Epoch 32:  49%|████▉     | 148/300 [04:15<04:24,  1.74s/it]Epoch 32:  50%|████▉     | 149/300 [04:16<04:10,  1.66s/it]Epoch 32:  50%|█████     | 150/300 [04:18<04:19,  1.73s/it]Epoch 32:  50%|█████     | 151/300 [04:20<04:29,  1.81s/it]Epoch 32:  51%|█████     | 152/300 [04:21<04:14,  1.72s/it]Epoch 32:  51%|█████     | 153/300 [04:23<04:20,  1.77s/it]Epoch 32:  51%|█████▏    | 154/300 [04:25<04:06,  1.69s/it]Epoch 32:  52%|█████▏    | 155/300 [04:26<04:01,  1.67s/it]Epoch 32:  52%|█████▏    | 156/300 [04:28<04:09,  1.73s/it]Epoch 32:  52%|█████▏    | 157/300 [04:30<04:17,  1.80s/it]Epoch 32:  53%|█████▎    | 158/300 [04:32<04:03,  1.72s/it]Epoch 32:  53%|█████▎    | 159/300 [04:34<04:13,  1.80s/it]06/19/2022 19:12:54 - INFO - __main__ - global step: 4880; train loss: 7.676144599914551; dev loss
Epoch 32:  53%|█████▎    | 160/300 [04:36<04:15,  1.83s/it]Epoch 32:  54%|█████▎    | 161/300 [04:38<04:18,  1.86s/it]Epoch 32:  54%|█████▍    | 162/300 [04:39<04:18,  1.87s/it]Epoch 32:  54%|█████▍    | 163/300 [04:41<04:04,  1.78s/it]Epoch 32:  55%|█████▍    | 164/300 [04:42<03:48,  1.68s/it]Epoch 32:  55%|█████▌    | 165/300 [04:44<03:38,  1.62s/it]Epoch 32:  55%|█████▌    | 166/300 [04:45<03:32,  1.59s/it]Epoch 32:  56%|█████▌    | 167/300 [04:47<03:43,  1.68s/it]Epoch 32:  56%|█████▌    | 168/300 [04:49<03:38,  1.65s/it]Epoch 32:  56%|█████▋    | 169/300 [04:50<03:30,  1.61s/it]Epoch 32:  57%|█████▋    | 170/300 [04:52<03:34,  1.65s/it]Epoch 32:  57%|█████▋    | 171/300 [04:54<03:26,  1.60s/it]Epoch 32:  57%|█████▋    | 172/300 [04:56<03:35,  1.68s/it]Epoch 32:  58%|█████▊    | 173/300 [04:57<03:38,  1.72s/it]Epoch 32:  58%|█████▊    | 174/300 [04:59<03:39,  1.74s/it]Epoch 32:  58%|█████▊    | 175/300 [05:01<03:31,  1.69s/it]Epoch 32:  59%|█████▊    | 176/300 [05:02<03:26,  1.67s/it]Epoch 32:  59%|█████▉    | 177/300 [05:04<03:19,  1.62s/it]Epoch 32:  59%|█████▉    | 178/300 [05:06<03:23,  1.67s/it]Epoch 32:  60%|█████▉    | 179/300 [05:07<03:17,  1.63s/it]06/19/2022 19:13:27 - INFO - __main__ - global step: 4890; train loss: 8.333295822143555; dev loss
Epoch 32:  60%|██████    | 180/300 [05:09<03:25,  1.71s/it]Epoch 32:  60%|██████    | 181/300 [05:11<03:16,  1.65s/it]Epoch 32:  61%|██████    | 182/300 [05:12<03:18,  1.68s/it]Epoch 32:  61%|██████    | 183/300 [05:14<03:10,  1.62s/it]Epoch 32:  61%|██████▏   | 184/300 [05:16<03:15,  1.69s/it]Epoch 32:  62%|██████▏   | 185/300 [05:17<03:07,  1.63s/it]Epoch 32:  62%|██████▏   | 186/300 [05:19<03:00,  1.58s/it]Epoch 32:  62%|██████▏   | 187/300 [05:20<02:56,  1.56s/it]Epoch 32:  63%|██████▎   | 188/300 [05:22<02:57,  1.58s/it]Epoch 32:  63%|██████▎   | 189/300 [05:24<03:02,  1.64s/it]Epoch 32:  63%|██████▎   | 190/300 [05:25<02:55,  1.60s/it]Epoch 32:  64%|██████▎   | 191/300 [05:27<02:59,  1.65s/it]Epoch 32:  64%|██████▍   | 192/300 [05:29<03:03,  1.69s/it]Epoch 32:  64%|██████▍   | 193/300 [05:30<03:06,  1.74s/it]Epoch 32:  65%|██████▍   | 194/300 [05:32<03:05,  1.75s/it]Epoch 32:  65%|██████▌   | 195/300 [05:34<03:04,  1.76s/it]Epoch 32:  65%|██████▌   | 196/300 [05:36<02:54,  1.68s/it]Epoch 32:  66%|██████▌   | 197/300 [05:37<02:58,  1.74s/it]Epoch 32:  66%|██████▌   | 198/300 [05:39<02:49,  1.67s/it]Epoch 32:  66%|██████▋   | 199/300 [05:40<02:42,  1.61s/it]06/19/2022 19:14:00 - INFO - __main__ - global step: 4900; train loss: 7.874001502990723; dev loss
Epoch 32:  67%|██████▋   | 200/300 [05:42<02:37,  1.58s/it]Epoch 32:  67%|██████▋   | 201/300 [05:44<02:44,  1.67s/it]Epoch 32:  67%|██████▋   | 202/300 [05:45<02:37,  1.60s/it]Epoch 32:  68%|██████▊   | 203/300 [05:47<02:32,  1.57s/it]Epoch 32:  68%|██████▊   | 204/300 [05:48<02:28,  1.55s/it]Epoch 32:  68%|██████▊   | 205/300 [05:50<02:36,  1.65s/it]Epoch 32:  69%|██████▊   | 206/300 [05:52<02:39,  1.70s/it]Epoch 32:  69%|██████▉   | 207/300 [05:54<02:40,  1.72s/it]Epoch 32:  69%|██████▉   | 208/300 [05:55<02:36,  1.70s/it]Epoch 32:  70%|██████▉   | 209/300 [05:57<02:37,  1.73s/it]Epoch 32:  70%|███████   | 210/300 [05:59<02:40,  1.78s/it]Epoch 32:  70%|███████   | 211/300 [06:01<02:41,  1.82s/it]Epoch 32:  71%|███████   | 212/300 [06:03<02:41,  1.84s/it]Epoch 32:  71%|███████   | 213/300 [06:04<02:34,  1.77s/it]Epoch 32:  71%|███████▏  | 214/300 [06:06<02:24,  1.68s/it]Epoch 32:  72%|███████▏  | 215/300 [06:08<02:28,  1.75s/it]Epoch 32:  72%|███████▏  | 216/300 [06:09<02:20,  1.67s/it]Epoch 32:  72%|███████▏  | 217/300 [06:11<02:27,  1.77s/it]Epoch 32:  73%|███████▎  | 218/300 [06:13<02:27,  1.80s/it]Epoch 32:  73%|███████▎  | 219/300 [06:15<02:28,  1.83s/it]06/19/2022 19:14:35 - INFO - __main__ - global step: 4910; train loss: 8.222368240356445; dev loss
Epoch 32:  73%|███████▎  | 220/300 [06:17<02:18,  1.73s/it]Epoch 32:  74%|███████▎  | 221/300 [06:18<02:19,  1.77s/it]Epoch 32:  74%|███████▍  | 222/300 [06:20<02:22,  1.83s/it]Epoch 32:  74%|███████▍  | 223/300 [06:22<02:14,  1.74s/it]Epoch 32:  75%|███████▍  | 224/300 [06:23<02:06,  1.67s/it]Epoch 32:  75%|███████▌  | 225/300 [06:25<02:00,  1.61s/it]Epoch 32:  75%|███████▌  | 226/300 [06:27<02:07,  1.72s/it]Epoch 32:  76%|███████▌  | 227/300 [06:29<02:09,  1.77s/it]Epoch 32:  76%|███████▌  | 228/300 [06:30<02:01,  1.69s/it]Epoch 32:  76%|███████▋  | 229/300 [06:32<02:03,  1.74s/it]Epoch 32:  77%|███████▋  | 230/300 [06:34<01:58,  1.70s/it]Epoch 32:  77%|███████▋  | 231/300 [06:36<02:01,  1.75s/it]Epoch 32:  77%|███████▋  | 232/300 [06:37<01:54,  1.68s/it]Epoch 32:  78%|███████▊  | 233/300 [06:39<01:48,  1.62s/it]Epoch 32:  78%|███████▊  | 234/300 [06:41<01:54,  1.73s/it]Epoch 32:  78%|███████▊  | 235/300 [06:42<01:55,  1.78s/it]Epoch 32:  79%|███████▊  | 236/300 [06:44<01:55,  1.81s/it]Epoch 32:  79%|███████▉  | 237/300 [06:46<01:48,  1.71s/it]Epoch 32:  79%|███████▉  | 238/300 [06:48<01:51,  1.79s/it]Epoch 32:  80%|███████▉  | 239/300 [06:49<01:43,  1.70s/it]06/19/2022 19:15:10 - INFO - __main__ - global step: 4920; train loss: 8.263306617736816; dev loss
Epoch 32:  80%|████████  | 240/300 [06:51<01:47,  1.79s/it]Epoch 32:  80%|████████  | 241/300 [06:53<01:42,  1.73s/it]Epoch 32:  81%|████████  | 242/300 [06:55<01:44,  1.80s/it]Epoch 32:  81%|████████  | 243/300 [06:57<01:44,  1.83s/it]Epoch 32:  81%|████████▏ | 244/300 [06:59<01:43,  1.85s/it]Epoch 32:  82%|████████▏ | 245/300 [07:01<01:42,  1.86s/it]Epoch 32:  82%|████████▏ | 246/300 [07:02<01:34,  1.75s/it]Epoch 32:  82%|████████▏ | 247/300 [07:04<01:30,  1.71s/it]Epoch 32:  83%|████████▎ | 248/300 [07:05<01:25,  1.64s/it]Epoch 32:  83%|████████▎ | 249/300 [07:07<01:21,  1.59s/it]Epoch 32:  83%|████████▎ | 250/300 [07:09<01:24,  1.69s/it]Epoch 32:  84%|████████▎ | 251/300 [07:10<01:21,  1.67s/it]Epoch 32:  84%|████████▍ | 252/300 [07:12<01:17,  1.62s/it]Epoch 32:  84%|████████▍ | 253/300 [07:14<01:20,  1.71s/it]Epoch 32:  85%|████████▍ | 254/300 [07:15<01:15,  1.65s/it]Epoch 32:  85%|████████▌ | 255/300 [07:17<01:13,  1.63s/it]Epoch 32:  85%|████████▌ | 256/300 [07:19<01:15,  1.72s/it]Epoch 32:  86%|████████▌ | 257/300 [07:20<01:11,  1.65s/it]Epoch 32:  86%|████████▌ | 258/300 [07:22<01:07,  1.60s/it]Epoch 32:  86%|████████▋ | 259/300 [07:23<01:05,  1.60s/it]06/19/2022 19:15:43 - INFO - __main__ - global step: 4930; train loss: 7.556174278259277; dev loss
Epoch 32:  87%|████████▋ | 260/300 [07:25<01:07,  1.69s/it]Epoch 32:  87%|████████▋ | 261/300 [07:27<01:04,  1.64s/it]Epoch 32:  87%|████████▋ | 262/300 [07:29<01:05,  1.73s/it]Epoch 32:  88%|████████▊ | 263/300 [07:31<01:07,  1.82s/it]Epoch 32:  88%|████████▊ | 264/300 [07:32<01:06,  1.84s/it]Epoch 32:  88%|████████▊ | 265/300 [07:34<01:05,  1.86s/it]Epoch 32:  89%|████████▊ | 266/300 [07:36<00:59,  1.74s/it]Epoch 32:  89%|████████▉ | 267/300 [07:37<00:56,  1.71s/it]Epoch 32:  89%|████████▉ | 268/300 [07:39<00:56,  1.76s/it]Epoch 32:  90%|████████▉ | 269/300 [07:41<00:53,  1.74s/it]Epoch 32:  90%|█████████ | 270/300 [07:43<00:53,  1.77s/it]Epoch 32:  90%|█████████ | 271/300 [07:45<00:51,  1.77s/it]Epoch 32:  91%|█████████ | 272/300 [07:46<00:49,  1.77s/it]Epoch 32:  91%|█████████ | 273/300 [07:48<00:47,  1.76s/it]Epoch 32:  91%|█████████▏| 274/300 [07:50<00:47,  1.81s/it]Epoch 32:  92%|█████████▏| 275/300 [07:52<00:43,  1.73s/it]Epoch 32:  92%|█████████▏| 276/300 [07:53<00:40,  1.69s/it]Epoch 32:  92%|█████████▏| 277/300 [07:55<00:40,  1.75s/it]Epoch 32:  93%|█████████▎| 278/300 [07:57<00:39,  1.78s/it]Epoch 32:  93%|█████████▎| 279/300 [07:58<00:35,  1.70s/it]06/19/2022 19:16:18 - INFO - __main__ - global step: 4940; train loss: 7.794103145599365; dev loss
Epoch 32:  93%|█████████▎| 280/300 [08:00<00:33,  1.68s/it]Epoch 32:  94%|█████████▎| 281/300 [08:02<00:33,  1.74s/it]Epoch 32:  94%|█████████▍| 282/300 [08:04<00:32,  1.82s/it]Epoch 32:  94%|█████████▍| 283/300 [08:06<00:31,  1.87s/it]Epoch 32:  95%|█████████▍| 284/300 [08:08<00:28,  1.79s/it]Epoch 32:  95%|█████████▌| 285/300 [08:09<00:27,  1.82s/it]Epoch 32:  95%|█████████▌| 286/300 [08:12<00:26,  1.89s/it]Epoch 32:  96%|█████████▌| 287/300 [08:13<00:23,  1.78s/it]Epoch 32:  96%|█████████▌| 288/300 [08:15<00:20,  1.72s/it]Epoch 32:  96%|█████████▋| 289/300 [08:17<00:19,  1.77s/it]Epoch 32:  97%|█████████▋| 290/300 [08:18<00:16,  1.68s/it]Epoch 32:  97%|█████████▋| 291/300 [08:20<00:15,  1.71s/it]Epoch 32:  97%|█████████▋| 292/300 [08:22<00:14,  1.81s/it]Epoch 32:  98%|█████████▊| 293/300 [08:23<00:12,  1.75s/it]Epoch 32:  98%|█████████▊| 294/300 [08:25<00:09,  1.66s/it]Epoch 32:  98%|█████████▊| 295/300 [08:27<00:08,  1.71s/it]Epoch 32:  99%|█████████▊| 296/300 [08:29<00:07,  1.78s/it]Epoch 32:  99%|█████████▉| 297/300 [08:30<00:05,  1.70s/it]Epoch 32:  99%|█████████▉| 298/300 [08:32<00:03,  1.63s/it]Epoch 32: 100%|█████████▉| 299/300 [08:33<00:01,  1.58s/it]06/19/2022 19:16:53 - INFO - __main__ - global step: 4950; train loss: 7.8608808517456055; dev loss
Epoch 32: 100%|██████████| 300/300 [08:35<00:00,  1.66s/it]Epoch 32: 100%|██████████| 300/300 [08:35<00:00,  1.72s/it]
Epoch 33:   0%|          | 0/300 [00:00<?, ?it/s]Epoch 33:   0%|          | 1/300 [00:01<07:54,  1.59s/it]Epoch 33:   1%|          | 2/300 [00:03<07:29,  1.51s/it]Epoch 33:   1%|          | 3/300 [00:04<07:23,  1.49s/it]Epoch 33:   1%|▏         | 4/300 [00:05<07:18,  1.48s/it]Epoch 33:   2%|▏         | 5/300 [00:07<07:28,  1.52s/it]Epoch 33:   2%|▏         | 6/300 [00:09<07:58,  1.63s/it]Epoch 33:   2%|▏         | 7/300 [00:11<08:18,  1.70s/it]Epoch 33:   3%|▎         | 8/300 [00:13<08:31,  1.75s/it]Epoch 33:   3%|▎         | 9/300 [00:14<08:13,  1.70s/it]Epoch 33:   3%|▎         | 10/300 [00:16<07:50,  1.62s/it]Epoch 33:   4%|▎         | 11/300 [00:17<07:34,  1.57s/it]Epoch 33:   4%|▍         | 12/300 [00:19<07:24,  1.54s/it]Epoch 33:   4%|▍         | 13/300 [00:20<07:26,  1.56s/it]Epoch 33:   5%|▍         | 14/300 [00:22<07:17,  1.53s/it]Epoch 33:   5%|▌         | 15/300 [00:23<07:42,  1.62s/it]Epoch 33:   5%|▌         | 16/300 [00:25<07:29,  1.58s/it]Epoch 33:   6%|▌         | 17/300 [00:27<07:59,  1.70s/it]Epoch 33:   6%|▌         | 18/300 [00:28<07:39,  1.63s/it]Epoch 33:   6%|▋         | 19/300 [00:30<07:24,  1.58s/it]06/19/2022 19:17:25 - INFO - __main__ - global step: 4960; train loss: 7.561003684997559; dev loss
Epoch 33:   7%|▋         | 20/300 [00:32<07:40,  1.64s/it]Epoch 33:   7%|▋         | 21/300 [00:33<07:54,  1.70s/it]Epoch 33:   7%|▋         | 22/300 [00:35<08:10,  1.76s/it]Epoch 33:   8%|▊         | 23/300 [00:37<08:04,  1.75s/it]Epoch 33:   8%|▊         | 24/300 [00:39<08:18,  1.81s/it]Epoch 33:   8%|▊         | 25/300 [00:41<08:18,  1.81s/it]Epoch 33:   9%|▊         | 26/300 [00:43<08:11,  1.79s/it]Epoch 33:   9%|▉         | 27/300 [00:44<08:01,  1.76s/it]Epoch 33:   9%|▉         | 28/300 [00:46<07:56,  1.75s/it]Epoch 33:  10%|▉         | 29/300 [00:48<08:06,  1.80s/it]Epoch 33:  10%|█         | 30/300 [00:50<08:28,  1.88s/it]Epoch 33:  10%|█         | 31/300 [00:52<08:10,  1.82s/it]Epoch 33:  11%|█         | 32/300 [00:54<08:13,  1.84s/it]Epoch 33:  11%|█         | 33/300 [00:55<08:03,  1.81s/it]Epoch 33:  11%|█▏        | 34/300 [00:57<08:18,  1.87s/it]Epoch 33:  12%|█▏        | 35/300 [00:59<07:53,  1.79s/it]Epoch 33:  12%|█▏        | 36/300 [01:01<07:54,  1.80s/it]Epoch 33:  12%|█▏        | 37/300 [01:03<07:58,  1.82s/it]Epoch 33:  13%|█▎        | 38/300 [01:05<08:10,  1.87s/it]Epoch 33:  13%|█▎        | 39/300 [01:07<08:17,  1.91s/it]06/19/2022 19:18:02 - INFO - __main__ - global step: 4970; train loss: 8.149993896484375; dev loss
Epoch 33:  13%|█▎        | 40/300 [01:09<08:13,  1.90s/it]Epoch 33:  14%|█▎        | 41/300 [01:10<08:08,  1.89s/it]Epoch 33:  14%|█▍        | 42/300 [01:12<08:17,  1.93s/it]Epoch 33:  14%|█▍        | 43/300 [01:14<08:05,  1.89s/it]Epoch 33:  15%|█▍        | 44/300 [01:16<07:56,  1.86s/it]Epoch 33:  15%|█▌        | 45/300 [01:18<07:47,  1.83s/it]Epoch 33:  15%|█▌        | 46/300 [01:20<07:42,  1.82s/it]Epoch 33:  16%|█▌        | 47/300 [01:21<07:46,  1.84s/it]Epoch 33:  16%|█▌        | 48/300 [01:23<07:42,  1.83s/it]Epoch 33:  16%|█▋        | 49/300 [01:25<07:32,  1.80s/it]Epoch 33:  17%|█▋        | 50/300 [01:27<07:43,  1.85s/it]Epoch 33:  17%|█▋        | 51/300 [01:29<07:43,  1.86s/it]Epoch 33:  17%|█▋        | 52/300 [01:31<07:45,  1.88s/it]Epoch 33:  18%|█▊        | 53/300 [01:33<07:47,  1.89s/it]Epoch 33:  18%|█▊        | 54/300 [01:34<07:33,  1.85s/it]Epoch 33:  18%|█▊        | 55/300 [01:36<07:44,  1.89s/it]Epoch 33:  19%|█▊        | 56/300 [01:38<07:40,  1.89s/it]Epoch 33:  19%|█▉        | 57/300 [01:40<07:36,  1.88s/it]Epoch 33:  19%|█▉        | 58/300 [01:42<07:05,  1.76s/it]Epoch 33:  20%|█▉        | 59/300 [01:44<07:15,  1.81s/it]06/19/2022 19:18:39 - INFO - __main__ - global step: 4980; train loss: 7.913637638092041; dev loss
Epoch 33:  20%|██        | 60/300 [01:45<06:49,  1.71s/it]Epoch 33:  20%|██        | 61/300 [01:46<06:30,  1.63s/it]Epoch 33:  21%|██        | 62/300 [01:48<06:37,  1.67s/it]Epoch 33:  21%|██        | 63/300 [01:50<06:49,  1.73s/it]Epoch 33:  21%|██▏       | 64/300 [01:52<06:49,  1.73s/it]Epoch 33:  22%|██▏       | 65/300 [01:54<06:48,  1.74s/it]Epoch 33:  22%|██▏       | 66/300 [01:55<06:48,  1.75s/it]Epoch 33:  22%|██▏       | 67/300 [01:57<06:53,  1.78s/it]Epoch 33:  23%|██▎       | 68/300 [01:59<06:30,  1.68s/it]Epoch 33:  23%|██▎       | 69/300 [02:00<06:13,  1.62s/it]Epoch 33:  23%|██▎       | 70/300 [02:02<06:20,  1.66s/it]Epoch 33:  24%|██▎       | 71/300 [02:04<06:31,  1.71s/it]Epoch 33:  24%|██▍       | 72/300 [02:05<06:31,  1.72s/it]Epoch 33:  24%|██▍       | 73/300 [02:07<06:14,  1.65s/it]Epoch 33:  25%|██▍       | 74/300 [02:08<06:01,  1.60s/it]Epoch 33:  25%|██▌       | 75/300 [02:10<06:15,  1.67s/it]Epoch 33:  25%|██▌       | 76/300 [02:12<06:20,  1.70s/it]Epoch 33:  26%|██▌       | 77/300 [02:14<06:25,  1.73s/it]Epoch 33:  26%|██▌       | 78/300 [02:15<06:16,  1.70s/it]Epoch 33:  26%|██▋       | 79/300 [02:17<06:15,  1.70s/it]06/19/2022 19:19:13 - INFO - __main__ - global step: 4990; train loss: 7.828399658203125; dev loss
Epoch 33:  27%|██▋       | 80/300 [02:19<06:07,  1.67s/it]Epoch 33:  27%|██▋       | 81/300 [02:21<06:17,  1.72s/it]Epoch 33:  27%|██▋       | 82/300 [02:22<06:23,  1.76s/it]Epoch 33:  28%|██▊       | 83/300 [02:24<06:04,  1.68s/it]Epoch 33:  28%|██▊       | 84/300 [02:26<06:19,  1.76s/it]Epoch 33:  28%|██▊       | 85/300 [02:27<05:59,  1.67s/it]Epoch 33:  29%|██▊       | 86/300 [02:29<05:44,  1.61s/it]Epoch 33:  29%|██▉       | 87/300 [02:31<05:56,  1.68s/it]Epoch 33:  29%|██▉       | 88/300 [02:33<06:13,  1.76s/it]Epoch 33:  30%|██▉       | 89/300 [02:34<06:16,  1.78s/it]Epoch 33:  30%|███       | 90/300 [02:36<05:57,  1.70s/it]Epoch 33:  30%|███       | 91/300 [02:37<05:40,  1.63s/it]Epoch 33:  31%|███       | 92/300 [02:39<05:36,  1.62s/it]Epoch 33:  31%|███       | 93/300 [02:41<05:48,  1.68s/it]Epoch 33:  31%|███▏      | 94/300 [02:42<05:34,  1.62s/it]Epoch 33:  32%|███▏      | 95/300 [02:44<05:21,  1.57s/it]Epoch 33:  32%|███▏      | 96/300 [02:45<05:21,  1.58s/it]Epoch 33:  32%|███▏      | 97/300 [02:47<05:36,  1.66s/it]Epoch 33:  33%|███▎      | 98/300 [02:49<05:46,  1.72s/it]Epoch 33:  33%|███▎      | 99/300 [02:51<05:55,  1.77s/it]06/19/2022 19:19:46 - INFO - __main__ - global step: 5000; train loss: 7.539137363433838; dev loss
Epoch 33:  33%|███▎      | 99/300 [02:53<05:51,  1.75s/it]
06/19/2022 19:19:46 - INFO - __main__ - save model!
t5base para reptile downstream
Task: glue-mrpc, Checkpoint: models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt, Identifier: T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10
06/19/2022 19:19:50 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/19/2022 19:19:50 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc
06/19/2022 19:19:50 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/19/2022 19:19:50 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc
06/19/2022 19:19:51 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/19/2022 19:19:51 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/19/2022 19:19:51 - INFO - __main__ - args.device: cuda:0
06/19/2022 19:19:51 - INFO - __main__ - Using 2 gpus
06/19/2022 19:19:51 - INFO - __main__ - args.device: cuda:1
06/19/2022 19:19:51 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/19/2022 19:19:51 - INFO - __main__ - Using 2 gpus
06/19/2022 19:19:51 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/19/2022 19:19:55 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.5, bsz=8 ...
06/19/2022 19:19:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:19:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:19:56 - INFO - __main__ - Printing 3 examples
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 19:19:56 - INFO - __main__ - Printing 3 examples
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 19:19:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:19:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:19:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:19:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:19:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:19:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:19:56 - INFO - __main__ - Printing 3 examples
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:19:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:19:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:19:56 - INFO - __main__ - Printing 3 examples
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 19:19:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:19:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:19:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:19:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:19:56 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:19:56 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:20:02 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:20:02 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:20:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:20:02 - INFO - __main__ - Starting training!
06/19/2022 19:20:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:20:08 - INFO - __main__ - Starting training!
06/19/2022 19:20:10 - INFO - __main__ - Step 10 Global step 10 Train loss 6.53 on epoch=4
06/19/2022 19:20:11 - INFO - __main__ - Step 20 Global step 20 Train loss 6.53 on epoch=9
06/19/2022 19:20:12 - INFO - __main__ - Step 30 Global step 30 Train loss 6.58 on epoch=14
06/19/2022 19:20:13 - INFO - __main__ - Step 40 Global step 40 Train loss 6.47 on epoch=19
06/19/2022 19:20:15 - INFO - __main__ - Step 50 Global step 50 Train loss 6.45 on epoch=24
06/19/2022 19:20:17 - INFO - __main__ - Global step 50 Train loss 6.51 ACC 0.0 on epoch=24
06/19/2022 19:20:18 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 19:20:19 - INFO - __main__ - Step 60 Global step 60 Train loss 6.49 on epoch=29
06/19/2022 19:20:20 - INFO - __main__ - Step 70 Global step 70 Train loss 6.43 on epoch=34
06/19/2022 19:20:21 - INFO - __main__ - Step 80 Global step 80 Train loss 6.40 on epoch=39
06/19/2022 19:20:22 - INFO - __main__ - Step 90 Global step 90 Train loss 6.34 on epoch=44
06/19/2022 19:20:23 - INFO - __main__ - Step 100 Global step 100 Train loss 6.27 on epoch=49
06/19/2022 19:20:27 - INFO - __main__ - Global step 100 Train loss 6.39 ACC 0.0 on epoch=49
06/19/2022 19:20:28 - INFO - __main__ - Step 110 Global step 110 Train loss 6.19 on epoch=54
06/19/2022 19:20:29 - INFO - __main__ - Step 120 Global step 120 Train loss 6.05 on epoch=59
06/19/2022 19:20:30 - INFO - __main__ - Step 130 Global step 130 Train loss 6.05 on epoch=64
06/19/2022 19:20:32 - INFO - __main__ - Step 140 Global step 140 Train loss 5.93 on epoch=69
06/19/2022 19:20:33 - INFO - __main__ - Step 150 Global step 150 Train loss 5.93 on epoch=74
06/19/2022 19:20:35 - INFO - __main__ - Global step 150 Train loss 6.03 ACC 0.0 on epoch=74
06/19/2022 19:20:36 - INFO - __main__ - Step 160 Global step 160 Train loss 5.86 on epoch=79
06/19/2022 19:20:38 - INFO - __main__ - Step 170 Global step 170 Train loss 5.85 on epoch=84
06/19/2022 19:20:39 - INFO - __main__ - Step 180 Global step 180 Train loss 5.81 on epoch=89
06/19/2022 19:20:40 - INFO - __main__ - Step 190 Global step 190 Train loss 5.77 on epoch=94
06/19/2022 19:20:41 - INFO - __main__ - Step 200 Global step 200 Train loss 5.70 on epoch=99
06/19/2022 19:20:47 - INFO - __main__ - Global step 200 Train loss 5.80 ACC 0.0 on epoch=99
06/19/2022 19:20:48 - INFO - __main__ - Step 210 Global step 210 Train loss 5.75 on epoch=104
06/19/2022 19:20:50 - INFO - __main__ - Step 220 Global step 220 Train loss 5.73 on epoch=109
06/19/2022 19:20:51 - INFO - __main__ - Step 230 Global step 230 Train loss 5.78 on epoch=114
06/19/2022 19:20:52 - INFO - __main__ - Step 240 Global step 240 Train loss 5.71 on epoch=119
06/19/2022 19:20:53 - INFO - __main__ - Step 250 Global step 250 Train loss 5.61 on epoch=124
06/19/2022 19:20:58 - INFO - __main__ - Global step 250 Train loss 5.72 ACC 0.0 on epoch=124
06/19/2022 19:21:00 - INFO - __main__ - Step 260 Global step 260 Train loss 5.52 on epoch=129
06/19/2022 19:21:01 - INFO - __main__ - Step 270 Global step 270 Train loss 5.49 on epoch=134
06/19/2022 19:21:02 - INFO - __main__ - Step 280 Global step 280 Train loss 5.44 on epoch=139
06/19/2022 19:21:03 - INFO - __main__ - Step 290 Global step 290 Train loss 5.43 on epoch=144
06/19/2022 19:21:04 - INFO - __main__ - Step 300 Global step 300 Train loss 5.39 on epoch=149
06/19/2022 19:21:14 - INFO - __main__ - Global step 300 Train loss 5.45 ACC 0.0 on epoch=149
06/19/2022 19:21:16 - INFO - __main__ - Step 310 Global step 310 Train loss 5.48 on epoch=154
06/19/2022 19:21:17 - INFO - __main__ - Step 320 Global step 320 Train loss 5.49 on epoch=159
06/19/2022 19:21:18 - INFO - __main__ - Step 330 Global step 330 Train loss 5.51 on epoch=164
06/19/2022 19:21:19 - INFO - __main__ - Step 340 Global step 340 Train loss 5.48 on epoch=169
06/19/2022 19:21:21 - INFO - __main__ - Step 350 Global step 350 Train loss 5.37 on epoch=174
06/19/2022 19:21:24 - INFO - __main__ - Global step 350 Train loss 5.46 ACC 0.0 on epoch=174
06/19/2022 19:21:25 - INFO - __main__ - Step 360 Global step 360 Train loss 5.32 on epoch=179
06/19/2022 19:21:26 - INFO - __main__ - Step 370 Global step 370 Train loss 5.22 on epoch=184
06/19/2022 19:21:27 - INFO - __main__ - Step 380 Global step 380 Train loss 5.24 on epoch=189
06/19/2022 19:21:29 - INFO - __main__ - Step 390 Global step 390 Train loss 5.26 on epoch=194
06/19/2022 19:21:30 - INFO - __main__ - Step 400 Global step 400 Train loss 5.16 on epoch=199
06/19/2022 19:21:36 - INFO - __main__ - Global step 400 Train loss 5.24 ACC 0.0 on epoch=199
06/19/2022 19:21:37 - INFO - __main__ - Step 410 Global step 410 Train loss 5.10 on epoch=204
06/19/2022 19:21:38 - INFO - __main__ - Step 420 Global step 420 Train loss 4.96 on epoch=209
06/19/2022 19:21:40 - INFO - __main__ - Step 430 Global step 430 Train loss 4.89 on epoch=214
06/19/2022 19:21:41 - INFO - __main__ - Step 440 Global step 440 Train loss 5.08 on epoch=219
06/19/2022 19:21:42 - INFO - __main__ - Step 450 Global step 450 Train loss 4.93 on epoch=224
06/19/2022 19:21:48 - INFO - __main__ - Global step 450 Train loss 4.99 ACC 0.0 on epoch=224
06/19/2022 19:21:49 - INFO - __main__ - Step 460 Global step 460 Train loss 4.89 on epoch=229
06/19/2022 19:21:51 - INFO - __main__ - Step 470 Global step 470 Train loss 4.81 on epoch=234
06/19/2022 19:21:52 - INFO - __main__ - Step 480 Global step 480 Train loss 4.82 on epoch=239
06/19/2022 19:21:53 - INFO - __main__ - Step 490 Global step 490 Train loss 4.75 on epoch=244
06/19/2022 19:21:54 - INFO - __main__ - Step 500 Global step 500 Train loss 4.66 on epoch=249
06/19/2022 19:22:04 - INFO - __main__ - Global step 500 Train loss 4.79 ACC 0.0 on epoch=249
06/19/2022 19:22:06 - INFO - __main__ - Step 510 Global step 510 Train loss 4.59 on epoch=254
06/19/2022 19:22:07 - INFO - __main__ - Step 520 Global step 520 Train loss 4.57 on epoch=259
06/19/2022 19:22:08 - INFO - __main__ - Step 530 Global step 530 Train loss 4.52 on epoch=264
06/19/2022 19:22:09 - INFO - __main__ - Step 540 Global step 540 Train loss 4.41 on epoch=269
06/19/2022 19:22:11 - INFO - __main__ - Step 550 Global step 550 Train loss 4.27 on epoch=274
06/19/2022 19:22:12 - INFO - __main__ - Global step 550 Train loss 4.47 ACC 0.0 on epoch=274
06/19/2022 19:22:13 - INFO - __main__ - Step 560 Global step 560 Train loss 4.27 on epoch=279
06/19/2022 19:22:14 - INFO - __main__ - Step 570 Global step 570 Train loss 4.19 on epoch=284
06/19/2022 19:22:15 - INFO - __main__ - Step 580 Global step 580 Train loss 4.23 on epoch=289
06/19/2022 19:22:17 - INFO - __main__ - Step 590 Global step 590 Train loss 4.02 on epoch=294
06/19/2022 19:22:18 - INFO - __main__ - Step 600 Global step 600 Train loss 3.96 on epoch=299
06/19/2022 19:22:22 - INFO - __main__ - Global step 600 Train loss 4.13 ACC 0.0 on epoch=299
06/19/2022 19:22:23 - INFO - __main__ - Step 610 Global step 610 Train loss 3.83 on epoch=304
06/19/2022 19:22:24 - INFO - __main__ - Step 620 Global step 620 Train loss 3.77 on epoch=309
06/19/2022 19:22:25 - INFO - __main__ - Step 630 Global step 630 Train loss 3.57 on epoch=314
06/19/2022 19:22:27 - INFO - __main__ - Step 640 Global step 640 Train loss 3.51 on epoch=319
06/19/2022 19:22:28 - INFO - __main__ - Step 650 Global step 650 Train loss 3.43 on epoch=324
06/19/2022 19:22:31 - INFO - __main__ - Global step 650 Train loss 3.62 ACC 0.1875 on epoch=324
06/19/2022 19:22:31 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.1875 on epoch=324, global_step=650
06/19/2022 19:22:32 - INFO - __main__ - Step 660 Global step 660 Train loss 3.43 on epoch=329
06/19/2022 19:22:33 - INFO - __main__ - Step 670 Global step 670 Train loss 3.28 on epoch=334
06/19/2022 19:22:34 - INFO - __main__ - Step 680 Global step 680 Train loss 3.26 on epoch=339
06/19/2022 19:22:35 - INFO - __main__ - Step 690 Global step 690 Train loss 3.09 on epoch=344
06/19/2022 19:22:37 - INFO - __main__ - Step 700 Global step 700 Train loss 2.99 on epoch=349
06/19/2022 19:22:39 - INFO - __main__ - Global step 700 Train loss 3.21 ACC 0.5 on epoch=349
06/19/2022 19:22:39 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.5 on epoch=349, global_step=700
06/19/2022 19:22:41 - INFO - __main__ - Step 710 Global step 710 Train loss 2.89 on epoch=354
06/19/2022 19:22:42 - INFO - __main__ - Step 720 Global step 720 Train loss 2.90 on epoch=359
06/19/2022 19:22:43 - INFO - __main__ - Step 730 Global step 730 Train loss 2.76 on epoch=364
06/19/2022 19:22:44 - INFO - __main__ - Step 740 Global step 740 Train loss 2.71 on epoch=369
06/19/2022 19:22:46 - INFO - __main__ - Step 750 Global step 750 Train loss 2.65 on epoch=374
06/19/2022 19:22:48 - INFO - __main__ - Global step 750 Train loss 2.78 ACC 0.5 on epoch=374
06/19/2022 19:22:49 - INFO - __main__ - Step 760 Global step 760 Train loss 2.47 on epoch=379
06/19/2022 19:22:50 - INFO - __main__ - Step 770 Global step 770 Train loss 2.47 on epoch=384
06/19/2022 19:22:52 - INFO - __main__ - Step 780 Global step 780 Train loss 2.30 on epoch=389
06/19/2022 19:22:53 - INFO - __main__ - Step 790 Global step 790 Train loss 2.22 on epoch=394
06/19/2022 19:22:54 - INFO - __main__ - Step 800 Global step 800 Train loss 2.20 on epoch=399
06/19/2022 19:23:03 - INFO - __main__ - Global step 800 Train loss 2.33 ACC 0.46875 on epoch=399
06/19/2022 19:23:04 - INFO - __main__ - Step 810 Global step 810 Train loss 2.16 on epoch=404
06/19/2022 19:23:06 - INFO - __main__ - Step 820 Global step 820 Train loss 1.96 on epoch=409
06/19/2022 19:23:07 - INFO - __main__ - Step 830 Global step 830 Train loss 1.85 on epoch=414
06/19/2022 19:23:08 - INFO - __main__ - Step 840 Global step 840 Train loss 1.84 on epoch=419
06/19/2022 19:23:09 - INFO - __main__ - Step 850 Global step 850 Train loss 1.82 on epoch=424
06/19/2022 19:23:12 - INFO - __main__ - Global step 850 Train loss 1.93 ACC 0.46875 on epoch=424
06/19/2022 19:23:13 - INFO - __main__ - Step 860 Global step 860 Train loss 1.66 on epoch=429
06/19/2022 19:23:15 - INFO - __main__ - Step 870 Global step 870 Train loss 1.60 on epoch=434
06/19/2022 19:23:16 - INFO - __main__ - Step 880 Global step 880 Train loss 1.60 on epoch=439
06/19/2022 19:23:17 - INFO - __main__ - Step 890 Global step 890 Train loss 1.44 on epoch=444
06/19/2022 19:23:18 - INFO - __main__ - Step 900 Global step 900 Train loss 1.43 on epoch=449
06/19/2022 19:23:21 - INFO - __main__ - Global step 900 Train loss 1.55 ACC 0.46875 on epoch=449
06/19/2022 19:23:22 - INFO - __main__ - Step 910 Global step 910 Train loss 1.33 on epoch=454
06/19/2022 19:23:23 - INFO - __main__ - Step 920 Global step 920 Train loss 1.35 on epoch=459
06/19/2022 19:23:25 - INFO - __main__ - Step 930 Global step 930 Train loss 1.31 on epoch=464
06/19/2022 19:23:26 - INFO - __main__ - Step 940 Global step 940 Train loss 1.19 on epoch=469
06/19/2022 19:23:27 - INFO - __main__ - Step 950 Global step 950 Train loss 1.19 on epoch=474
06/19/2022 19:23:32 - INFO - __main__ - Global step 950 Train loss 1.27 ACC 0.40625 on epoch=474
06/19/2022 19:23:33 - INFO - __main__ - Step 960 Global step 960 Train loss 1.16 on epoch=479
06/19/2022 19:23:35 - INFO - __main__ - Step 970 Global step 970 Train loss 1.11 on epoch=484
06/19/2022 19:23:36 - INFO - __main__ - Step 980 Global step 980 Train loss 1.04 on epoch=489
06/19/2022 19:23:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.96 on epoch=494
06/19/2022 19:23:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.93 on epoch=499
06/19/2022 19:23:40 - INFO - __main__ - Global step 1000 Train loss 1.04 ACC 0.4375 on epoch=499
06/19/2022 19:23:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.90 on epoch=504
06/19/2022 19:23:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.85 on epoch=509
06/19/2022 19:23:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.86 on epoch=514
06/19/2022 19:23:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.86 on epoch=519
06/19/2022 19:23:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.83 on epoch=524
06/19/2022 19:23:48 - INFO - __main__ - Global step 1050 Train loss 0.86 ACC 0.5 on epoch=524
06/19/2022 19:23:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.75 on epoch=529
06/19/2022 19:23:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.79 on epoch=534
06/19/2022 19:23:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=539
06/19/2022 19:23:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.74 on epoch=544
06/19/2022 19:23:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.74 on epoch=549
06/19/2022 19:23:56 - INFO - __main__ - Global step 1100 Train loss 0.76 ACC 0.5625 on epoch=549
06/19/2022 19:23:56 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=549, global_step=1100
06/19/2022 19:23:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.67 on epoch=554
06/19/2022 19:23:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=559
06/19/2022 19:24:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.68 on epoch=564
06/19/2022 19:24:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.65 on epoch=569
06/19/2022 19:24:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.66 on epoch=574
06/19/2022 19:24:04 - INFO - __main__ - Global step 1150 Train loss 0.66 ACC 0.5 on epoch=574
06/19/2022 19:24:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=579
06/19/2022 19:24:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.59 on epoch=584
06/19/2022 19:24:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.61 on epoch=589
06/19/2022 19:24:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.61 on epoch=594
06/19/2022 19:24:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.54 on epoch=599
06/19/2022 19:24:12 - INFO - __main__ - Global step 1200 Train loss 0.58 ACC 0.40625 on epoch=599
06/19/2022 19:24:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.58 on epoch=604
06/19/2022 19:24:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
06/19/2022 19:24:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
06/19/2022 19:24:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.60 on epoch=619
06/19/2022 19:24:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.60 on epoch=624
06/19/2022 19:24:20 - INFO - __main__ - Global step 1250 Train loss 0.56 ACC 0.5 on epoch=624
06/19/2022 19:24:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=629
06/19/2022 19:24:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=634
06/19/2022 19:24:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
06/19/2022 19:24:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.49 on epoch=644
06/19/2022 19:24:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=649
06/19/2022 19:24:28 - INFO - __main__ - Global step 1300 Train loss 0.48 ACC 0.375 on epoch=649
06/19/2022 19:24:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
06/19/2022 19:24:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.53 on epoch=659
06/19/2022 19:24:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.52 on epoch=664
06/19/2022 19:24:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
06/19/2022 19:24:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=674
06/19/2022 19:24:36 - INFO - __main__ - Global step 1350 Train loss 0.49 ACC 0.5 on epoch=674
06/19/2022 19:24:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=679
06/19/2022 19:24:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
06/19/2022 19:24:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=689
06/19/2022 19:24:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=694
06/19/2022 19:24:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=699
06/19/2022 19:24:44 - INFO - __main__ - Global step 1400 Train loss 0.46 ACC 0.375 on epoch=699
06/19/2022 19:24:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=704
06/19/2022 19:24:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=709
06/19/2022 19:24:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=714
06/19/2022 19:24:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=719
06/19/2022 19:24:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=724
06/19/2022 19:24:51 - INFO - __main__ - Global step 1450 Train loss 0.41 ACC 0.46875 on epoch=724
06/19/2022 19:24:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
06/19/2022 19:24:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.47 on epoch=734
06/19/2022 19:24:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=739
06/19/2022 19:24:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=744
06/19/2022 19:24:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=749
06/19/2022 19:24:59 - INFO - __main__ - Global step 1500 Train loss 0.41 ACC 0.53125 on epoch=749
06/19/2022 19:25:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=754
06/19/2022 19:25:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=759
06/19/2022 19:25:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=764
06/19/2022 19:25:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.39 on epoch=769
06/19/2022 19:25:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=774
06/19/2022 19:25:05 - INFO - __main__ - Global step 1550 Train loss 0.40 ACC 0.25 on epoch=774
06/19/2022 19:25:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=779
06/19/2022 19:25:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=784
06/19/2022 19:25:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
06/19/2022 19:25:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
06/19/2022 19:25:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=799
06/19/2022 19:25:12 - INFO - __main__ - Global step 1600 Train loss 0.41 ACC 0.46875 on epoch=799
06/19/2022 19:25:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=804
06/19/2022 19:25:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=809
06/19/2022 19:25:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=814
06/19/2022 19:25:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=819
06/19/2022 19:25:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=824
06/19/2022 19:25:19 - INFO - __main__ - Global step 1650 Train loss 0.44 ACC 0.5 on epoch=824
06/19/2022 19:25:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=829
06/19/2022 19:25:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
06/19/2022 19:25:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
06/19/2022 19:25:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=844
06/19/2022 19:25:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=849
06/19/2022 19:25:26 - INFO - __main__ - Global step 1700 Train loss 0.40 ACC 0.5 on epoch=849
06/19/2022 19:25:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
06/19/2022 19:25:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=859
06/19/2022 19:25:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=864
06/19/2022 19:25:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=869
06/19/2022 19:25:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
06/19/2022 19:25:32 - INFO - __main__ - Global step 1750 Train loss 0.38 ACC 0.5 on epoch=874
06/19/2022 19:25:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=879
06/19/2022 19:25:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
06/19/2022 19:25:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=889
06/19/2022 19:25:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=894
06/19/2022 19:25:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
06/19/2022 19:25:39 - INFO - __main__ - Global step 1800 Train loss 0.38 ACC 0.5 on epoch=899
06/19/2022 19:25:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=904
06/19/2022 19:25:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=909
06/19/2022 19:25:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
06/19/2022 19:25:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=919
06/19/2022 19:25:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
06/19/2022 19:25:46 - INFO - __main__ - Global step 1850 Train loss 0.35 ACC 0.4375 on epoch=924
06/19/2022 19:25:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/19/2022 19:25:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=934
06/19/2022 19:25:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=939
06/19/2022 19:25:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
06/19/2022 19:25:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
06/19/2022 19:25:53 - INFO - __main__ - Global step 1900 Train loss 0.36 ACC 0.59375 on epoch=949
06/19/2022 19:25:53 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=949, global_step=1900
06/19/2022 19:25:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=954
06/19/2022 19:25:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
06/19/2022 19:25:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=964
06/19/2022 19:25:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=969
06/19/2022 19:25:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=974
06/19/2022 19:25:59 - INFO - __main__ - Global step 1950 Train loss 0.34 ACC 0.4375 on epoch=974
06/19/2022 19:26:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=979
06/19/2022 19:26:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=984
06/19/2022 19:26:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=989
06/19/2022 19:26:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
06/19/2022 19:26:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/19/2022 19:26:06 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.4375 on epoch=999
06/19/2022 19:26:06 - INFO - __main__ - save last model!
06/19/2022 19:26:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:26:06 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:26:06 - INFO - __main__ - Printing 3 examples
06/19/2022 19:26:06 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:26:06 - INFO - __main__ - ['equivalent']
06/19/2022 19:26:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:26:06 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:26:06 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:06 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:26:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:26:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:26:07 - INFO - __main__ - Printing 3 examples
06/19/2022 19:26:07 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 19:26:07 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 19:26:07 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:07 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 19:26:07 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:26:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:26:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:26:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:26:07 - INFO - __main__ - Printing 3 examples
06/19/2022 19:26:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 19:26:07 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:07 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 19:26:07 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 19:26:07 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:07 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:26:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:26:07 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:26:07 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:26:12 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:26:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:26:13 - INFO - __main__ - Starting training!
06/19/2022 19:26:15 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.5_8_predictions.txt
06/19/2022 19:26:15 - INFO - __main__ - ACC on test data: 0.3382
06/19/2022 19:26:15 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.5, bsz=8, dev_performance=0.59375, test_performance=0.3382352941176471
06/19/2022 19:26:15 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.4, bsz=8 ...
06/19/2022 19:26:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:26:16 - INFO - __main__ - Printing 3 examples
06/19/2022 19:26:16 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 19:26:16 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:16 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 19:26:16 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:16 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 19:26:16 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:26:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:26:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:26:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:26:16 - INFO - __main__ - Printing 3 examples
06/19/2022 19:26:16 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 19:26:16 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:16 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 19:26:16 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:16 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 19:26:16 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:26:16 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:26:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:26:16 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:26:22 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:26:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:26:23 - INFO - __main__ - Starting training!
06/19/2022 19:26:24 - INFO - __main__ - Step 10 Global step 10 Train loss 6.50 on epoch=4
06/19/2022 19:26:25 - INFO - __main__ - Step 20 Global step 20 Train loss 6.55 on epoch=9
06/19/2022 19:26:27 - INFO - __main__ - Step 30 Global step 30 Train loss 6.53 on epoch=14
06/19/2022 19:26:28 - INFO - __main__ - Step 40 Global step 40 Train loss 6.52 on epoch=19
06/19/2022 19:26:29 - INFO - __main__ - Step 50 Global step 50 Train loss 6.47 on epoch=24
06/19/2022 19:26:33 - INFO - __main__ - Global step 50 Train loss 6.51 ACC 0.0 on epoch=24
06/19/2022 19:26:33 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 19:26:34 - INFO - __main__ - Step 60 Global step 60 Train loss 6.50 on epoch=29
06/19/2022 19:26:35 - INFO - __main__ - Step 70 Global step 70 Train loss 6.41 on epoch=34
06/19/2022 19:26:37 - INFO - __main__ - Step 80 Global step 80 Train loss 6.38 on epoch=39
06/19/2022 19:26:38 - INFO - __main__ - Step 90 Global step 90 Train loss 6.37 on epoch=44
06/19/2022 19:26:39 - INFO - __main__ - Step 100 Global step 100 Train loss 6.32 on epoch=49
06/19/2022 19:26:42 - INFO - __main__ - Global step 100 Train loss 6.40 ACC 0.0 on epoch=49
06/19/2022 19:26:43 - INFO - __main__ - Step 110 Global step 110 Train loss 6.19 on epoch=54
06/19/2022 19:26:44 - INFO - __main__ - Step 120 Global step 120 Train loss 6.16 on epoch=59
06/19/2022 19:26:46 - INFO - __main__ - Step 130 Global step 130 Train loss 6.18 on epoch=64
06/19/2022 19:26:47 - INFO - __main__ - Step 140 Global step 140 Train loss 6.09 on epoch=69
06/19/2022 19:26:48 - INFO - __main__ - Step 150 Global step 150 Train loss 6.06 on epoch=74
06/19/2022 19:26:55 - INFO - __main__ - Global step 150 Train loss 6.14 ACC 0.0 on epoch=74
06/19/2022 19:26:56 - INFO - __main__ - Step 160 Global step 160 Train loss 6.02 on epoch=79
06/19/2022 19:26:57 - INFO - __main__ - Step 170 Global step 170 Train loss 6.09 on epoch=84
06/19/2022 19:26:59 - INFO - __main__ - Step 180 Global step 180 Train loss 5.93 on epoch=89
06/19/2022 19:27:00 - INFO - __main__ - Step 190 Global step 190 Train loss 5.92 on epoch=94
06/19/2022 19:27:01 - INFO - __main__ - Step 200 Global step 200 Train loss 5.84 on epoch=99
06/19/2022 19:27:09 - INFO - __main__ - Global step 200 Train loss 5.96 ACC 0.0 on epoch=99
06/19/2022 19:27:11 - INFO - __main__ - Step 210 Global step 210 Train loss 5.75 on epoch=104
06/19/2022 19:27:12 - INFO - __main__ - Step 220 Global step 220 Train loss 5.74 on epoch=109
06/19/2022 19:27:13 - INFO - __main__ - Step 230 Global step 230 Train loss 5.83 on epoch=114
06/19/2022 19:27:14 - INFO - __main__ - Step 240 Global step 240 Train loss 5.75 on epoch=119
06/19/2022 19:27:15 - INFO - __main__ - Step 250 Global step 250 Train loss 5.71 on epoch=124
06/19/2022 19:27:23 - INFO - __main__ - Global step 250 Train loss 5.76 ACC 0.0 on epoch=124
06/19/2022 19:27:24 - INFO - __main__ - Step 260 Global step 260 Train loss 5.63 on epoch=129
06/19/2022 19:27:25 - INFO - __main__ - Step 270 Global step 270 Train loss 5.55 on epoch=134
06/19/2022 19:27:27 - INFO - __main__ - Step 280 Global step 280 Train loss 5.64 on epoch=139
06/19/2022 19:27:28 - INFO - __main__ - Step 290 Global step 290 Train loss 5.48 on epoch=144
06/19/2022 19:27:29 - INFO - __main__ - Step 300 Global step 300 Train loss 5.44 on epoch=149
06/19/2022 19:27:31 - INFO - __main__ - Global step 300 Train loss 5.55 ACC 0.0 on epoch=149
06/19/2022 19:27:32 - INFO - __main__ - Step 310 Global step 310 Train loss 5.44 on epoch=154
06/19/2022 19:27:33 - INFO - __main__ - Step 320 Global step 320 Train loss 5.38 on epoch=159
06/19/2022 19:27:34 - INFO - __main__ - Step 330 Global step 330 Train loss 5.34 on epoch=164
06/19/2022 19:27:35 - INFO - __main__ - Step 340 Global step 340 Train loss 5.27 on epoch=169
06/19/2022 19:27:37 - INFO - __main__ - Step 350 Global step 350 Train loss 5.18 on epoch=174
06/19/2022 19:27:38 - INFO - __main__ - Global step 350 Train loss 5.32 ACC 0.0 on epoch=174
06/19/2022 19:27:40 - INFO - __main__ - Step 360 Global step 360 Train loss 5.11 on epoch=179
06/19/2022 19:27:41 - INFO - __main__ - Step 370 Global step 370 Train loss 5.07 on epoch=184
06/19/2022 19:27:42 - INFO - __main__ - Step 380 Global step 380 Train loss 5.06 on epoch=189
06/19/2022 19:27:43 - INFO - __main__ - Step 390 Global step 390 Train loss 5.09 on epoch=194
06/19/2022 19:27:45 - INFO - __main__ - Step 400 Global step 400 Train loss 5.02 on epoch=199
06/19/2022 19:27:46 - INFO - __main__ - Global step 400 Train loss 5.07 ACC 0.0 on epoch=199
06/19/2022 19:27:47 - INFO - __main__ - Step 410 Global step 410 Train loss 4.87 on epoch=204
06/19/2022 19:27:48 - INFO - __main__ - Step 420 Global step 420 Train loss 4.97 on epoch=209
06/19/2022 19:27:50 - INFO - __main__ - Step 430 Global step 430 Train loss 4.81 on epoch=214
06/19/2022 19:27:51 - INFO - __main__ - Step 440 Global step 440 Train loss 4.75 on epoch=219
06/19/2022 19:27:52 - INFO - __main__ - Step 450 Global step 450 Train loss 4.75 on epoch=224
06/19/2022 19:27:53 - INFO - __main__ - Global step 450 Train loss 4.83 ACC 0.0 on epoch=224
06/19/2022 19:27:54 - INFO - __main__ - Step 460 Global step 460 Train loss 4.81 on epoch=229
06/19/2022 19:27:56 - INFO - __main__ - Step 470 Global step 470 Train loss 4.64 on epoch=234
06/19/2022 19:27:57 - INFO - __main__ - Step 480 Global step 480 Train loss 4.57 on epoch=239
06/19/2022 19:27:58 - INFO - __main__ - Step 490 Global step 490 Train loss 4.56 on epoch=244
06/19/2022 19:27:59 - INFO - __main__ - Step 500 Global step 500 Train loss 4.48 on epoch=249
06/19/2022 19:28:01 - INFO - __main__ - Global step 500 Train loss 4.61 ACC 0.0 on epoch=249
06/19/2022 19:28:02 - INFO - __main__ - Step 510 Global step 510 Train loss 4.55 on epoch=254
06/19/2022 19:28:03 - INFO - __main__ - Step 520 Global step 520 Train loss 4.45 on epoch=259
06/19/2022 19:28:04 - INFO - __main__ - Step 530 Global step 530 Train loss 4.32 on epoch=264
06/19/2022 19:28:05 - INFO - __main__ - Step 540 Global step 540 Train loss 4.30 on epoch=269
06/19/2022 19:28:07 - INFO - __main__ - Step 550 Global step 550 Train loss 4.28 on epoch=274
06/19/2022 19:28:08 - INFO - __main__ - Global step 550 Train loss 4.38 ACC 0.0 on epoch=274
06/19/2022 19:28:09 - INFO - __main__ - Step 560 Global step 560 Train loss 4.25 on epoch=279
06/19/2022 19:28:10 - INFO - __main__ - Step 570 Global step 570 Train loss 4.09 on epoch=284
06/19/2022 19:28:12 - INFO - __main__ - Step 580 Global step 580 Train loss 4.14 on epoch=289
06/19/2022 19:28:13 - INFO - __main__ - Step 590 Global step 590 Train loss 3.95 on epoch=294
06/19/2022 19:28:14 - INFO - __main__ - Step 600 Global step 600 Train loss 3.86 on epoch=299
06/19/2022 19:28:16 - INFO - __main__ - Global step 600 Train loss 4.06 ACC 0.0 on epoch=299
06/19/2022 19:28:17 - INFO - __main__ - Step 610 Global step 610 Train loss 3.86 on epoch=304
06/19/2022 19:28:18 - INFO - __main__ - Step 620 Global step 620 Train loss 3.77 on epoch=309
06/19/2022 19:28:20 - INFO - __main__ - Step 630 Global step 630 Train loss 3.69 on epoch=314
06/19/2022 19:28:21 - INFO - __main__ - Step 640 Global step 640 Train loss 3.61 on epoch=319
06/19/2022 19:28:22 - INFO - __main__ - Step 650 Global step 650 Train loss 3.49 on epoch=324
06/19/2022 19:28:25 - INFO - __main__ - Global step 650 Train loss 3.68 ACC 0.0 on epoch=324
06/19/2022 19:28:26 - INFO - __main__ - Step 660 Global step 660 Train loss 3.48 on epoch=329
06/19/2022 19:28:27 - INFO - __main__ - Step 670 Global step 670 Train loss 3.48 on epoch=334
06/19/2022 19:28:28 - INFO - __main__ - Step 680 Global step 680 Train loss 3.33 on epoch=339
06/19/2022 19:28:29 - INFO - __main__ - Step 690 Global step 690 Train loss 3.23 on epoch=344
06/19/2022 19:28:31 - INFO - __main__ - Step 700 Global step 700 Train loss 3.19 on epoch=349
06/19/2022 19:28:33 - INFO - __main__ - Global step 700 Train loss 3.34 ACC 0.0 on epoch=349
06/19/2022 19:28:35 - INFO - __main__ - Step 710 Global step 710 Train loss 3.15 on epoch=354
06/19/2022 19:28:36 - INFO - __main__ - Step 720 Global step 720 Train loss 2.98 on epoch=359
06/19/2022 19:28:37 - INFO - __main__ - Step 730 Global step 730 Train loss 3.02 on epoch=364
06/19/2022 19:28:38 - INFO - __main__ - Step 740 Global step 740 Train loss 2.96 on epoch=369
06/19/2022 19:28:40 - INFO - __main__ - Step 750 Global step 750 Train loss 2.93 on epoch=374
06/19/2022 19:28:42 - INFO - __main__ - Global step 750 Train loss 3.01 ACC 0.03125 on epoch=374
06/19/2022 19:28:42 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=374, global_step=750
06/19/2022 19:28:43 - INFO - __main__ - Step 760 Global step 760 Train loss 2.77 on epoch=379
06/19/2022 19:28:44 - INFO - __main__ - Step 770 Global step 770 Train loss 2.76 on epoch=384
06/19/2022 19:28:46 - INFO - __main__ - Step 780 Global step 780 Train loss 2.64 on epoch=389
06/19/2022 19:28:47 - INFO - __main__ - Step 790 Global step 790 Train loss 2.67 on epoch=394
06/19/2022 19:28:48 - INFO - __main__ - Step 800 Global step 800 Train loss 2.55 on epoch=399
06/19/2022 19:28:54 - INFO - __main__ - Global step 800 Train loss 2.68 ACC 0.25 on epoch=399
06/19/2022 19:28:54 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.25 on epoch=399, global_step=800
06/19/2022 19:28:56 - INFO - __main__ - Step 810 Global step 810 Train loss 2.46 on epoch=404
06/19/2022 19:28:57 - INFO - __main__ - Step 820 Global step 820 Train loss 2.47 on epoch=409
06/19/2022 19:28:58 - INFO - __main__ - Step 830 Global step 830 Train loss 2.31 on epoch=414
06/19/2022 19:28:59 - INFO - __main__ - Step 840 Global step 840 Train loss 2.40 on epoch=419
06/19/2022 19:29:00 - INFO - __main__ - Step 850 Global step 850 Train loss 2.28 on epoch=424
06/19/2022 19:29:02 - INFO - __main__ - Global step 850 Train loss 2.38 ACC 0.40625 on epoch=424
06/19/2022 19:29:02 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.40625 on epoch=424, global_step=850
06/19/2022 19:29:03 - INFO - __main__ - Step 860 Global step 860 Train loss 2.16 on epoch=429
06/19/2022 19:29:05 - INFO - __main__ - Step 870 Global step 870 Train loss 2.18 on epoch=434
06/19/2022 19:29:06 - INFO - __main__ - Step 880 Global step 880 Train loss 2.12 on epoch=439
06/19/2022 19:29:07 - INFO - __main__ - Step 890 Global step 890 Train loss 2.06 on epoch=444
06/19/2022 19:29:08 - INFO - __main__ - Step 900 Global step 900 Train loss 1.99 on epoch=449
06/19/2022 19:29:10 - INFO - __main__ - Global step 900 Train loss 2.10 ACC 0.5625 on epoch=449
06/19/2022 19:29:10 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.5625 on epoch=449, global_step=900
06/19/2022 19:29:11 - INFO - __main__ - Step 910 Global step 910 Train loss 1.84 on epoch=454
06/19/2022 19:29:12 - INFO - __main__ - Step 920 Global step 920 Train loss 1.91 on epoch=459
06/19/2022 19:29:13 - INFO - __main__ - Step 930 Global step 930 Train loss 1.90 on epoch=464
06/19/2022 19:29:14 - INFO - __main__ - Step 940 Global step 940 Train loss 1.80 on epoch=469
06/19/2022 19:29:16 - INFO - __main__ - Step 950 Global step 950 Train loss 1.72 on epoch=474
06/19/2022 19:29:16 - INFO - __main__ - Global step 950 Train loss 1.84 ACC 0.46875 on epoch=474
06/19/2022 19:29:18 - INFO - __main__ - Step 960 Global step 960 Train loss 1.64 on epoch=479
06/19/2022 19:29:19 - INFO - __main__ - Step 970 Global step 970 Train loss 1.61 on epoch=484
06/19/2022 19:29:20 - INFO - __main__ - Step 980 Global step 980 Train loss 1.49 on epoch=489
06/19/2022 19:29:21 - INFO - __main__ - Step 990 Global step 990 Train loss 1.56 on epoch=494
06/19/2022 19:29:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.44 on epoch=499
06/19/2022 19:29:23 - INFO - __main__ - Global step 1000 Train loss 1.55 ACC 0.53125 on epoch=499
06/19/2022 19:29:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.32 on epoch=504
06/19/2022 19:29:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.25 on epoch=509
06/19/2022 19:29:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.24 on epoch=514
06/19/2022 19:29:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.26 on epoch=519
06/19/2022 19:29:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.15 on epoch=524
06/19/2022 19:29:30 - INFO - __main__ - Global step 1050 Train loss 1.24 ACC 0.5 on epoch=524
06/19/2022 19:29:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.15 on epoch=529
06/19/2022 19:29:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.15 on epoch=534
06/19/2022 19:29:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.03 on epoch=539
06/19/2022 19:29:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.01 on epoch=544
06/19/2022 19:29:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.04 on epoch=549
06/19/2022 19:29:37 - INFO - __main__ - Global step 1100 Train loss 1.07 ACC 0.5 on epoch=549
06/19/2022 19:29:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.95 on epoch=554
06/19/2022 19:29:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.86 on epoch=559
06/19/2022 19:29:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.86 on epoch=564
06/19/2022 19:29:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.94 on epoch=569
06/19/2022 19:29:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.91 on epoch=574
06/19/2022 19:29:44 - INFO - __main__ - Global step 1150 Train loss 0.90 ACC 0.5 on epoch=574
06/19/2022 19:29:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.83 on epoch=579
06/19/2022 19:29:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=584
06/19/2022 19:29:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.82 on epoch=589
06/19/2022 19:29:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=594
06/19/2022 19:29:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.80 on epoch=599
06/19/2022 19:29:51 - INFO - __main__ - Global step 1200 Train loss 0.83 ACC 0.5 on epoch=599
06/19/2022 19:29:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.82 on epoch=604
06/19/2022 19:29:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.76 on epoch=609
06/19/2022 19:29:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.79 on epoch=614
06/19/2022 19:29:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.79 on epoch=619
06/19/2022 19:29:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.71 on epoch=624
06/19/2022 19:29:58 - INFO - __main__ - Global step 1250 Train loss 0.77 ACC 0.5 on epoch=624
06/19/2022 19:29:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.78 on epoch=629
06/19/2022 19:30:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=634
06/19/2022 19:30:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.75 on epoch=639
06/19/2022 19:30:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.69 on epoch=644
06/19/2022 19:30:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.72 on epoch=649
06/19/2022 19:30:04 - INFO - __main__ - Global step 1300 Train loss 0.74 ACC 0.5 on epoch=649
06/19/2022 19:30:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.74 on epoch=654
06/19/2022 19:30:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.62 on epoch=659
06/19/2022 19:30:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.69 on epoch=664
06/19/2022 19:30:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.71 on epoch=669
06/19/2022 19:30:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.68 on epoch=674
06/19/2022 19:30:11 - INFO - __main__ - Global step 1350 Train loss 0.69 ACC 0.5 on epoch=674
06/19/2022 19:30:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.65 on epoch=679
06/19/2022 19:30:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.68 on epoch=684
06/19/2022 19:30:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.59 on epoch=689
06/19/2022 19:30:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.56 on epoch=694
06/19/2022 19:30:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.65 on epoch=699
06/19/2022 19:30:18 - INFO - __main__ - Global step 1400 Train loss 0.63 ACC 0.5 on epoch=699
06/19/2022 19:30:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.53 on epoch=704
06/19/2022 19:30:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.58 on epoch=709
06/19/2022 19:30:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.59 on epoch=714
06/19/2022 19:30:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.59 on epoch=719
06/19/2022 19:30:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.57 on epoch=724
06/19/2022 19:30:24 - INFO - __main__ - Global step 1450 Train loss 0.57 ACC 0.5 on epoch=724
06/19/2022 19:30:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/19/2022 19:30:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.53 on epoch=734
06/19/2022 19:30:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.55 on epoch=739
06/19/2022 19:30:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.54 on epoch=744
06/19/2022 19:30:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.53 on epoch=749
06/19/2022 19:30:31 - INFO - __main__ - Global step 1500 Train loss 0.54 ACC 0.5 on epoch=749
06/19/2022 19:30:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.52 on epoch=754
06/19/2022 19:30:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.53 on epoch=759
06/19/2022 19:30:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.51 on epoch=764
06/19/2022 19:30:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=769
06/19/2022 19:30:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.54 on epoch=774
06/19/2022 19:30:38 - INFO - __main__ - Global step 1550 Train loss 0.51 ACC 0.53125 on epoch=774
06/19/2022 19:30:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.49 on epoch=779
06/19/2022 19:30:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=784
06/19/2022 19:30:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=789
06/19/2022 19:30:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.51 on epoch=794
06/19/2022 19:30:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=799
06/19/2022 19:30:45 - INFO - __main__ - Global step 1600 Train loss 0.49 ACC 0.5 on epoch=799
06/19/2022 19:30:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.47 on epoch=804
06/19/2022 19:30:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=809
06/19/2022 19:30:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
06/19/2022 19:30:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=819
06/19/2022 19:30:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.51 on epoch=824
06/19/2022 19:30:51 - INFO - __main__ - Global step 1650 Train loss 0.49 ACC 0.53125 on epoch=824
06/19/2022 19:30:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=829
06/19/2022 19:30:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=834
06/19/2022 19:30:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=839
06/19/2022 19:30:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
06/19/2022 19:30:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=849
06/19/2022 19:30:58 - INFO - __main__ - Global step 1700 Train loss 0.48 ACC 0.5 on epoch=849
06/19/2022 19:30:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=854
06/19/2022 19:31:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.49 on epoch=859
06/19/2022 19:31:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.49 on epoch=864
06/19/2022 19:31:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=869
06/19/2022 19:31:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=874
06/19/2022 19:31:05 - INFO - __main__ - Global step 1750 Train loss 0.48 ACC 0.5 on epoch=874
06/19/2022 19:31:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=879
06/19/2022 19:31:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=884
06/19/2022 19:31:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
06/19/2022 19:31:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=894
06/19/2022 19:31:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=899
06/19/2022 19:31:12 - INFO - __main__ - Global step 1800 Train loss 0.46 ACC 0.5 on epoch=899
06/19/2022 19:31:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.48 on epoch=904
06/19/2022 19:31:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.46 on epoch=909
06/19/2022 19:31:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
06/19/2022 19:31:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
06/19/2022 19:31:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=924
06/19/2022 19:31:18 - INFO - __main__ - Global step 1850 Train loss 0.43 ACC 0.5 on epoch=924
06/19/2022 19:31:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/19/2022 19:31:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=934
06/19/2022 19:31:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=939
06/19/2022 19:31:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.47 on epoch=944
06/19/2022 19:31:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=949
06/19/2022 19:31:25 - INFO - __main__ - Global step 1900 Train loss 0.44 ACC 0.5 on epoch=949
06/19/2022 19:31:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=954
06/19/2022 19:31:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/19/2022 19:31:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=964
06/19/2022 19:31:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
06/19/2022 19:31:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.47 on epoch=974
06/19/2022 19:31:32 - INFO - __main__ - Global step 1950 Train loss 0.42 ACC 0.5 on epoch=974
06/19/2022 19:31:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=979
06/19/2022 19:31:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=984
06/19/2022 19:31:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=989
06/19/2022 19:31:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=994
06/19/2022 19:31:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/19/2022 19:31:38 - INFO - __main__ - Global step 2000 Train loss 0.41 ACC 0.5 on epoch=999
06/19/2022 19:31:38 - INFO - __main__ - save last model!
06/19/2022 19:31:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:31:38 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:31:38 - INFO - __main__ - Printing 3 examples
06/19/2022 19:31:38 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:31:38 - INFO - __main__ - ['equivalent']
06/19/2022 19:31:38 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:31:38 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:38 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:31:38 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:38 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:31:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:31:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:31:39 - INFO - __main__ - Printing 3 examples
06/19/2022 19:31:39 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 19:31:39 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 19:31:39 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:39 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 19:31:39 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:31:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:31:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:31:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:31:39 - INFO - __main__ - Printing 3 examples
06/19/2022 19:31:39 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 19:31:39 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:39 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 19:31:39 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:39 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 19:31:39 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:39 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:31:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:31:39 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:31:39 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:31:44 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:31:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:31:45 - INFO - __main__ - Starting training!
06/19/2022 19:31:47 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.4_8_predictions.txt
06/19/2022 19:31:47 - INFO - __main__ - ACC on test data: 0.6814
06/19/2022 19:31:47 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.4, bsz=8, dev_performance=0.5625, test_performance=0.6813725490196079
06/19/2022 19:31:47 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.3, bsz=8 ...
06/19/2022 19:31:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:31:48 - INFO - __main__ - Printing 3 examples
06/19/2022 19:31:48 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 19:31:48 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 19:31:48 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:48 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 19:31:48 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:31:48 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:31:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:31:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:31:48 - INFO - __main__ - Printing 3 examples
06/19/2022 19:31:48 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 19:31:48 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:48 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 19:31:48 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:48 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 19:31:48 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:31:48 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:31:48 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:31:48 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:31:53 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:31:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:31:54 - INFO - __main__ - Starting training!
06/19/2022 19:31:55 - INFO - __main__ - Step 10 Global step 10 Train loss 6.58 on epoch=4
06/19/2022 19:31:57 - INFO - __main__ - Step 20 Global step 20 Train loss 6.60 on epoch=9
06/19/2022 19:31:58 - INFO - __main__ - Step 30 Global step 30 Train loss 6.55 on epoch=14
06/19/2022 19:31:59 - INFO - __main__ - Step 40 Global step 40 Train loss 6.55 on epoch=19
06/19/2022 19:32:00 - INFO - __main__ - Step 50 Global step 50 Train loss 6.43 on epoch=24
06/19/2022 19:32:02 - INFO - __main__ - Global step 50 Train loss 6.54 ACC 0.0 on epoch=24
06/19/2022 19:32:02 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 19:32:03 - INFO - __main__ - Step 60 Global step 60 Train loss 6.56 on epoch=29
06/19/2022 19:32:04 - INFO - __main__ - Step 70 Global step 70 Train loss 6.51 on epoch=34
06/19/2022 19:32:05 - INFO - __main__ - Step 80 Global step 80 Train loss 6.48 on epoch=39
06/19/2022 19:32:07 - INFO - __main__ - Step 90 Global step 90 Train loss 6.41 on epoch=44
06/19/2022 19:32:08 - INFO - __main__ - Step 100 Global step 100 Train loss 6.43 on epoch=49
06/19/2022 19:32:12 - INFO - __main__ - Global step 100 Train loss 6.48 ACC 0.0 on epoch=49
06/19/2022 19:32:14 - INFO - __main__ - Step 110 Global step 110 Train loss 6.42 on epoch=54
06/19/2022 19:32:15 - INFO - __main__ - Step 120 Global step 120 Train loss 6.41 on epoch=59
06/19/2022 19:32:16 - INFO - __main__ - Step 130 Global step 130 Train loss 6.36 on epoch=64
06/19/2022 19:32:17 - INFO - __main__ - Step 140 Global step 140 Train loss 6.28 on epoch=69
06/19/2022 19:32:18 - INFO - __main__ - Step 150 Global step 150 Train loss 6.26 on epoch=74
06/19/2022 19:32:19 - INFO - __main__ - Global step 150 Train loss 6.35 ACC 0.0 on epoch=74
06/19/2022 19:32:20 - INFO - __main__ - Step 160 Global step 160 Train loss 6.26 on epoch=79
06/19/2022 19:32:22 - INFO - __main__ - Step 170 Global step 170 Train loss 6.22 on epoch=84
06/19/2022 19:32:23 - INFO - __main__ - Step 180 Global step 180 Train loss 6.19 on epoch=89
06/19/2022 19:32:24 - INFO - __main__ - Step 190 Global step 190 Train loss 6.19 on epoch=94
06/19/2022 19:32:25 - INFO - __main__ - Step 200 Global step 200 Train loss 6.24 on epoch=99
06/19/2022 19:32:27 - INFO - __main__ - Global step 200 Train loss 6.22 ACC 0.0 on epoch=99
06/19/2022 19:32:28 - INFO - __main__ - Step 210 Global step 210 Train loss 6.14 on epoch=104
06/19/2022 19:32:30 - INFO - __main__ - Step 220 Global step 220 Train loss 6.07 on epoch=109
06/19/2022 19:32:31 - INFO - __main__ - Step 230 Global step 230 Train loss 6.07 on epoch=114
06/19/2022 19:32:32 - INFO - __main__ - Step 240 Global step 240 Train loss 5.98 on epoch=119
06/19/2022 19:32:33 - INFO - __main__ - Step 250 Global step 250 Train loss 5.93 on epoch=124
06/19/2022 19:32:36 - INFO - __main__ - Global step 250 Train loss 6.04 ACC 0.0 on epoch=124
06/19/2022 19:32:37 - INFO - __main__ - Step 260 Global step 260 Train loss 5.93 on epoch=129
06/19/2022 19:32:39 - INFO - __main__ - Step 270 Global step 270 Train loss 5.91 on epoch=134
06/19/2022 19:32:40 - INFO - __main__ - Step 280 Global step 280 Train loss 5.85 on epoch=139
06/19/2022 19:32:41 - INFO - __main__ - Step 290 Global step 290 Train loss 5.87 on epoch=144
06/19/2022 19:32:42 - INFO - __main__ - Step 300 Global step 300 Train loss 5.72 on epoch=149
06/19/2022 19:32:48 - INFO - __main__ - Global step 300 Train loss 5.86 ACC 0.0 on epoch=149
06/19/2022 19:32:49 - INFO - __main__ - Step 310 Global step 310 Train loss 5.76 on epoch=154
06/19/2022 19:32:51 - INFO - __main__ - Step 320 Global step 320 Train loss 5.75 on epoch=159
06/19/2022 19:32:52 - INFO - __main__ - Step 330 Global step 330 Train loss 5.63 on epoch=164
06/19/2022 19:32:53 - INFO - __main__ - Step 340 Global step 340 Train loss 5.65 on epoch=169
06/19/2022 19:32:54 - INFO - __main__ - Step 350 Global step 350 Train loss 5.44 on epoch=174
06/19/2022 19:32:56 - INFO - __main__ - Global step 350 Train loss 5.65 ACC 0.0 on epoch=174
06/19/2022 19:32:57 - INFO - __main__ - Step 360 Global step 360 Train loss 5.55 on epoch=179
06/19/2022 19:32:58 - INFO - __main__ - Step 370 Global step 370 Train loss 5.40 on epoch=184
06/19/2022 19:33:00 - INFO - __main__ - Step 380 Global step 380 Train loss 5.42 on epoch=189
06/19/2022 19:33:01 - INFO - __main__ - Step 390 Global step 390 Train loss 5.30 on epoch=194
06/19/2022 19:33:02 - INFO - __main__ - Step 400 Global step 400 Train loss 5.28 on epoch=199
06/19/2022 19:33:04 - INFO - __main__ - Global step 400 Train loss 5.39 ACC 0.0 on epoch=199
06/19/2022 19:33:05 - INFO - __main__ - Step 410 Global step 410 Train loss 5.31 on epoch=204
06/19/2022 19:33:06 - INFO - __main__ - Step 420 Global step 420 Train loss 5.18 on epoch=209
06/19/2022 19:33:08 - INFO - __main__ - Step 430 Global step 430 Train loss 5.27 on epoch=214
06/19/2022 19:33:09 - INFO - __main__ - Step 440 Global step 440 Train loss 5.11 on epoch=219
06/19/2022 19:33:10 - INFO - __main__ - Step 450 Global step 450 Train loss 5.10 on epoch=224
06/19/2022 19:33:15 - INFO - __main__ - Global step 450 Train loss 5.19 ACC 0.0 on epoch=224
06/19/2022 19:33:16 - INFO - __main__ - Step 460 Global step 460 Train loss 5.10 on epoch=229
06/19/2022 19:33:17 - INFO - __main__ - Step 470 Global step 470 Train loss 5.01 on epoch=234
06/19/2022 19:33:19 - INFO - __main__ - Step 480 Global step 480 Train loss 5.08 on epoch=239
06/19/2022 19:33:20 - INFO - __main__ - Step 490 Global step 490 Train loss 4.88 on epoch=244
06/19/2022 19:33:21 - INFO - __main__ - Step 500 Global step 500 Train loss 4.85 on epoch=249
06/19/2022 19:33:24 - INFO - __main__ - Global step 500 Train loss 4.99 ACC 0.0 on epoch=249
06/19/2022 19:33:25 - INFO - __main__ - Step 510 Global step 510 Train loss 4.97 on epoch=254
06/19/2022 19:33:26 - INFO - __main__ - Step 520 Global step 520 Train loss 4.85 on epoch=259
06/19/2022 19:33:28 - INFO - __main__ - Step 530 Global step 530 Train loss 4.74 on epoch=264
06/19/2022 19:33:29 - INFO - __main__ - Step 540 Global step 540 Train loss 4.76 on epoch=269
06/19/2022 19:33:30 - INFO - __main__ - Step 550 Global step 550 Train loss 4.77 on epoch=274
06/19/2022 19:33:36 - INFO - __main__ - Global step 550 Train loss 4.82 ACC 0.0 on epoch=274
06/19/2022 19:33:38 - INFO - __main__ - Step 560 Global step 560 Train loss 4.69 on epoch=279
06/19/2022 19:33:39 - INFO - __main__ - Step 570 Global step 570 Train loss 4.64 on epoch=284
06/19/2022 19:33:40 - INFO - __main__ - Step 580 Global step 580 Train loss 4.59 on epoch=289
06/19/2022 19:33:41 - INFO - __main__ - Step 590 Global step 590 Train loss 4.61 on epoch=294
06/19/2022 19:33:43 - INFO - __main__ - Step 600 Global step 600 Train loss 4.56 on epoch=299
06/19/2022 19:33:46 - INFO - __main__ - Global step 600 Train loss 4.62 ACC 0.0 on epoch=299
06/19/2022 19:33:47 - INFO - __main__ - Step 610 Global step 610 Train loss 4.59 on epoch=304
06/19/2022 19:33:48 - INFO - __main__ - Step 620 Global step 620 Train loss 4.40 on epoch=309
06/19/2022 19:33:49 - INFO - __main__ - Step 630 Global step 630 Train loss 4.55 on epoch=314
06/19/2022 19:33:51 - INFO - __main__ - Step 640 Global step 640 Train loss 4.44 on epoch=319
06/19/2022 19:33:52 - INFO - __main__ - Step 650 Global step 650 Train loss 4.34 on epoch=324
06/19/2022 19:33:59 - INFO - __main__ - Global step 650 Train loss 4.46 ACC 0.0 on epoch=324
06/19/2022 19:34:01 - INFO - __main__ - Step 660 Global step 660 Train loss 4.30 on epoch=329
06/19/2022 19:34:02 - INFO - __main__ - Step 670 Global step 670 Train loss 4.32 on epoch=334
06/19/2022 19:34:03 - INFO - __main__ - Step 680 Global step 680 Train loss 4.14 on epoch=339
06/19/2022 19:34:04 - INFO - __main__ - Step 690 Global step 690 Train loss 4.20 on epoch=344
06/19/2022 19:34:06 - INFO - __main__ - Step 700 Global step 700 Train loss 4.12 on epoch=349
06/19/2022 19:34:13 - INFO - __main__ - Global step 700 Train loss 4.22 ACC 0.0 on epoch=349
06/19/2022 19:34:14 - INFO - __main__ - Step 710 Global step 710 Train loss 4.10 on epoch=354
06/19/2022 19:34:15 - INFO - __main__ - Step 720 Global step 720 Train loss 4.09 on epoch=359
06/19/2022 19:34:17 - INFO - __main__ - Step 730 Global step 730 Train loss 4.03 on epoch=364
06/19/2022 19:34:18 - INFO - __main__ - Step 740 Global step 740 Train loss 3.86 on epoch=369
06/19/2022 19:34:19 - INFO - __main__ - Step 750 Global step 750 Train loss 3.89 on epoch=374
06/19/2022 19:34:27 - INFO - __main__ - Global step 750 Train loss 3.99 ACC 0.0 on epoch=374
06/19/2022 19:34:28 - INFO - __main__ - Step 760 Global step 760 Train loss 3.89 on epoch=379
06/19/2022 19:34:29 - INFO - __main__ - Step 770 Global step 770 Train loss 3.69 on epoch=384
06/19/2022 19:34:30 - INFO - __main__ - Step 780 Global step 780 Train loss 3.65 on epoch=389
06/19/2022 19:34:32 - INFO - __main__ - Step 790 Global step 790 Train loss 3.65 on epoch=394
06/19/2022 19:34:33 - INFO - __main__ - Step 800 Global step 800 Train loss 3.47 on epoch=399
06/19/2022 19:34:41 - INFO - __main__ - Global step 800 Train loss 3.67 ACC 0.0 on epoch=399
06/19/2022 19:34:42 - INFO - __main__ - Step 810 Global step 810 Train loss 3.48 on epoch=404
06/19/2022 19:34:44 - INFO - __main__ - Step 820 Global step 820 Train loss 3.37 on epoch=409
06/19/2022 19:34:45 - INFO - __main__ - Step 830 Global step 830 Train loss 3.25 on epoch=414
06/19/2022 19:34:46 - INFO - __main__ - Step 840 Global step 840 Train loss 3.09 on epoch=419
06/19/2022 19:34:47 - INFO - __main__ - Step 850 Global step 850 Train loss 3.18 on epoch=424
06/19/2022 19:34:58 - INFO - __main__ - Global step 850 Train loss 3.27 ACC 0.0 on epoch=424
06/19/2022 19:34:59 - INFO - __main__ - Step 860 Global step 860 Train loss 3.15 on epoch=429
06/19/2022 19:35:00 - INFO - __main__ - Step 870 Global step 870 Train loss 2.92 on epoch=434
06/19/2022 19:35:01 - INFO - __main__ - Step 880 Global step 880 Train loss 3.06 on epoch=439
06/19/2022 19:35:02 - INFO - __main__ - Step 890 Global step 890 Train loss 2.97 on epoch=444
06/19/2022 19:35:04 - INFO - __main__ - Step 900 Global step 900 Train loss 2.94 on epoch=449
06/19/2022 19:35:15 - INFO - __main__ - Global step 900 Train loss 3.01 ACC 0.0 on epoch=449
06/19/2022 19:35:16 - INFO - __main__ - Step 910 Global step 910 Train loss 2.84 on epoch=454
06/19/2022 19:35:17 - INFO - __main__ - Step 920 Global step 920 Train loss 2.78 on epoch=459
06/19/2022 19:35:19 - INFO - __main__ - Step 930 Global step 930 Train loss 2.65 on epoch=464
06/19/2022 19:35:20 - INFO - __main__ - Step 940 Global step 940 Train loss 2.70 on epoch=469
06/19/2022 19:35:21 - INFO - __main__ - Step 950 Global step 950 Train loss 2.53 on epoch=474
06/19/2022 19:35:31 - INFO - __main__ - Global step 950 Train loss 2.70 ACC 0.0625 on epoch=474
06/19/2022 19:35:31 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=474, global_step=950
06/19/2022 19:35:32 - INFO - __main__ - Step 960 Global step 960 Train loss 2.48 on epoch=479
06/19/2022 19:35:33 - INFO - __main__ - Step 970 Global step 970 Train loss 2.56 on epoch=484
06/19/2022 19:35:34 - INFO - __main__ - Step 980 Global step 980 Train loss 2.38 on epoch=489
06/19/2022 19:35:36 - INFO - __main__ - Step 990 Global step 990 Train loss 2.35 on epoch=494
06/19/2022 19:35:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.42 on epoch=499
06/19/2022 19:35:43 - INFO - __main__ - Global step 1000 Train loss 2.44 ACC 0.3125 on epoch=499
06/19/2022 19:35:43 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.3125 on epoch=499, global_step=1000
06/19/2022 19:35:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.47 on epoch=504
06/19/2022 19:35:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.34 on epoch=509
06/19/2022 19:35:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.16 on epoch=514
06/19/2022 19:35:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.14 on epoch=519
06/19/2022 19:35:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.18 on epoch=524
06/19/2022 19:35:50 - INFO - __main__ - Global step 1050 Train loss 2.26 ACC 0.40625 on epoch=524
06/19/2022 19:35:50 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.40625 on epoch=524, global_step=1050
06/19/2022 19:35:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.07 on epoch=529
06/19/2022 19:35:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.01 on epoch=534
06/19/2022 19:35:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.92 on epoch=539
06/19/2022 19:35:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.07 on epoch=544
06/19/2022 19:35:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.06 on epoch=549
06/19/2022 19:35:57 - INFO - __main__ - Global step 1100 Train loss 2.03 ACC 0.5 on epoch=549
06/19/2022 19:35:57 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.5 on epoch=549, global_step=1100
06/19/2022 19:35:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.02 on epoch=554
06/19/2022 19:36:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.94 on epoch=559
06/19/2022 19:36:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.83 on epoch=564
06/19/2022 19:36:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.93 on epoch=569
06/19/2022 19:36:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.89 on epoch=574
06/19/2022 19:36:04 - INFO - __main__ - Global step 1150 Train loss 1.92 ACC 0.5 on epoch=574
06/19/2022 19:36:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.98 on epoch=579
06/19/2022 19:36:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.83 on epoch=584
06/19/2022 19:36:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.78 on epoch=589
06/19/2022 19:36:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.67 on epoch=594
06/19/2022 19:36:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.88 on epoch=599
06/19/2022 19:36:11 - INFO - __main__ - Global step 1200 Train loss 1.83 ACC 0.5 on epoch=599
06/19/2022 19:36:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.77 on epoch=604
06/19/2022 19:36:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.76 on epoch=609
06/19/2022 19:36:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.70 on epoch=614
06/19/2022 19:36:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.79 on epoch=619
06/19/2022 19:36:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.48 on epoch=624
06/19/2022 19:36:18 - INFO - __main__ - Global step 1250 Train loss 1.70 ACC 0.5 on epoch=624
06/19/2022 19:36:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.68 on epoch=629
06/19/2022 19:36:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.47 on epoch=634
06/19/2022 19:36:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.62 on epoch=639
06/19/2022 19:36:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.51 on epoch=644
06/19/2022 19:36:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.47 on epoch=649
06/19/2022 19:36:25 - INFO - __main__ - Global step 1300 Train loss 1.55 ACC 0.5 on epoch=649
06/19/2022 19:36:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.43 on epoch=654
06/19/2022 19:36:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.43 on epoch=659
06/19/2022 19:36:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.48 on epoch=664
06/19/2022 19:36:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.42 on epoch=669
06/19/2022 19:36:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.44 on epoch=674
06/19/2022 19:36:32 - INFO - __main__ - Global step 1350 Train loss 1.44 ACC 0.5 on epoch=674
06/19/2022 19:36:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.42 on epoch=679
06/19/2022 19:36:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.43 on epoch=684
06/19/2022 19:36:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.47 on epoch=689
06/19/2022 19:36:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.24 on epoch=694
06/19/2022 19:36:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.32 on epoch=699
06/19/2022 19:36:39 - INFO - __main__ - Global step 1400 Train loss 1.38 ACC 0.5 on epoch=699
06/19/2022 19:36:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.36 on epoch=704
06/19/2022 19:36:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.24 on epoch=709
06/19/2022 19:36:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.29 on epoch=714
06/19/2022 19:36:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.31 on epoch=719
06/19/2022 19:36:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.20 on epoch=724
06/19/2022 19:36:46 - INFO - __main__ - Global step 1450 Train loss 1.28 ACC 0.5 on epoch=724
06/19/2022 19:36:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.16 on epoch=729
06/19/2022 19:36:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.18 on epoch=734
06/19/2022 19:36:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.14 on epoch=739
06/19/2022 19:36:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.17 on epoch=744
06/19/2022 19:36:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.14 on epoch=749
06/19/2022 19:36:53 - INFO - __main__ - Global step 1500 Train loss 1.16 ACC 0.5 on epoch=749
06/19/2022 19:36:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.13 on epoch=754
06/19/2022 19:36:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.17 on epoch=759
06/19/2022 19:36:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.11 on epoch=764
06/19/2022 19:36:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.13 on epoch=769
06/19/2022 19:36:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.05 on epoch=774
06/19/2022 19:37:00 - INFO - __main__ - Global step 1550 Train loss 1.12 ACC 0.5 on epoch=774
06/19/2022 19:37:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.01 on epoch=779
06/19/2022 19:37:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.95 on epoch=784
06/19/2022 19:37:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.04 on epoch=789
06/19/2022 19:37:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.95 on epoch=794
06/19/2022 19:37:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.95 on epoch=799
06/19/2022 19:37:07 - INFO - __main__ - Global step 1600 Train loss 0.98 ACC 0.5 on epoch=799
06/19/2022 19:37:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.98 on epoch=804
06/19/2022 19:37:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.92 on epoch=809
06/19/2022 19:37:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.88 on epoch=814
06/19/2022 19:37:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.98 on epoch=819
06/19/2022 19:37:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.86 on epoch=824
06/19/2022 19:37:14 - INFO - __main__ - Global step 1650 Train loss 0.92 ACC 0.5 on epoch=824
06/19/2022 19:37:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.95 on epoch=829
06/19/2022 19:37:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.90 on epoch=834
06/19/2022 19:37:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.88 on epoch=839
06/19/2022 19:37:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.83 on epoch=844
06/19/2022 19:37:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.83 on epoch=849
06/19/2022 19:37:21 - INFO - __main__ - Global step 1700 Train loss 0.88 ACC 0.5 on epoch=849
06/19/2022 19:37:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.85 on epoch=854
06/19/2022 19:37:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.85 on epoch=859
06/19/2022 19:37:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.82 on epoch=864
06/19/2022 19:37:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.86 on epoch=869
06/19/2022 19:37:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.82 on epoch=874
06/19/2022 19:37:27 - INFO - __main__ - Global step 1750 Train loss 0.84 ACC 0.5 on epoch=874
06/19/2022 19:37:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.79 on epoch=879
06/19/2022 19:37:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.76 on epoch=884
06/19/2022 19:37:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.76 on epoch=889
06/19/2022 19:37:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.83 on epoch=894
06/19/2022 19:37:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.75 on epoch=899
06/19/2022 19:37:34 - INFO - __main__ - Global step 1800 Train loss 0.78 ACC 0.5 on epoch=899
06/19/2022 19:37:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.77 on epoch=904
06/19/2022 19:37:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.75 on epoch=909
06/19/2022 19:37:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.72 on epoch=914
06/19/2022 19:37:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.77 on epoch=919
06/19/2022 19:37:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.80 on epoch=924
06/19/2022 19:37:41 - INFO - __main__ - Global step 1850 Train loss 0.76 ACC 0.5 on epoch=924
06/19/2022 19:37:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.73 on epoch=929
06/19/2022 19:37:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.75 on epoch=934
06/19/2022 19:37:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.77 on epoch=939
06/19/2022 19:37:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.73 on epoch=944
06/19/2022 19:37:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.80 on epoch=949
06/19/2022 19:37:48 - INFO - __main__ - Global step 1900 Train loss 0.75 ACC 0.5 on epoch=949
06/19/2022 19:37:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.73 on epoch=954
06/19/2022 19:37:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.57 on epoch=959
06/19/2022 19:37:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.65 on epoch=964
06/19/2022 19:37:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.71 on epoch=969
06/19/2022 19:37:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.61 on epoch=974
06/19/2022 19:37:54 - INFO - __main__ - Global step 1950 Train loss 0.65 ACC 0.5 on epoch=974
06/19/2022 19:37:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.56 on epoch=979
06/19/2022 19:37:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.62 on epoch=984
06/19/2022 19:37:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.68 on epoch=989
06/19/2022 19:37:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.58 on epoch=994
06/19/2022 19:38:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.62 on epoch=999
06/19/2022 19:38:01 - INFO - __main__ - Global step 2000 Train loss 0.61 ACC 0.5 on epoch=999
06/19/2022 19:38:01 - INFO - __main__ - save last model!
06/19/2022 19:38:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:38:01 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:38:01 - INFO - __main__ - Printing 3 examples
06/19/2022 19:38:01 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:38:01 - INFO - __main__ - ['equivalent']
06/19/2022 19:38:01 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:38:01 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:01 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:38:01 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:01 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:38:01 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:38:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:38:02 - INFO - __main__ - Printing 3 examples
06/19/2022 19:38:02 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 19:38:02 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:02 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 19:38:02 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:02 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 19:38:02 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:38:02 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:38:02 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:38:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:38:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:38:02 - INFO - __main__ - Printing 3 examples
06/19/2022 19:38:02 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 19:38:02 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:02 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 19:38:02 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:02 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 19:38:02 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:02 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:38:02 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:38:02 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:38:07 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:38:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:38:07 - INFO - __main__ - Starting training!
06/19/2022 19:38:09 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.3_8_predictions.txt
06/19/2022 19:38:09 - INFO - __main__ - ACC on test data: 0.6838
06/19/2022 19:38:09 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.3, bsz=8, dev_performance=0.5, test_performance=0.6838235294117647
06/19/2022 19:38:09 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.2, bsz=8 ...
06/19/2022 19:38:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:38:10 - INFO - __main__ - Printing 3 examples
06/19/2022 19:38:10 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/19/2022 19:38:10 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/19/2022 19:38:10 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:10 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/19/2022 19:38:10 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:38:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:38:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:38:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:38:10 - INFO - __main__ - Printing 3 examples
06/19/2022 19:38:10 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/19/2022 19:38:10 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:10 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/19/2022 19:38:10 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/19/2022 19:38:10 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:38:10 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:38:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:38:10 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:38:16 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:38:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:38:16 - INFO - __main__ - Starting training!
06/19/2022 19:38:18 - INFO - __main__ - Step 10 Global step 10 Train loss 6.58 on epoch=4
06/19/2022 19:38:19 - INFO - __main__ - Step 20 Global step 20 Train loss 6.51 on epoch=9
06/19/2022 19:38:20 - INFO - __main__ - Step 30 Global step 30 Train loss 6.44 on epoch=14
06/19/2022 19:38:21 - INFO - __main__ - Step 40 Global step 40 Train loss 6.53 on epoch=19
06/19/2022 19:38:23 - INFO - __main__ - Step 50 Global step 50 Train loss 6.63 on epoch=24
06/19/2022 19:38:24 - INFO - __main__ - Global step 50 Train loss 6.54 ACC 0.0 on epoch=24
06/19/2022 19:38:24 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 19:38:25 - INFO - __main__ - Step 60 Global step 60 Train loss 6.55 on epoch=29
06/19/2022 19:38:26 - INFO - __main__ - Step 70 Global step 70 Train loss 6.51 on epoch=34
06/19/2022 19:38:27 - INFO - __main__ - Step 80 Global step 80 Train loss 6.57 on epoch=39
06/19/2022 19:38:29 - INFO - __main__ - Step 90 Global step 90 Train loss 6.51 on epoch=44
06/19/2022 19:38:30 - INFO - __main__ - Step 100 Global step 100 Train loss 6.51 on epoch=49
06/19/2022 19:38:31 - INFO - __main__ - Global step 100 Train loss 6.53 ACC 0.0 on epoch=49
06/19/2022 19:38:32 - INFO - __main__ - Step 110 Global step 110 Train loss 6.48 on epoch=54
06/19/2022 19:38:33 - INFO - __main__ - Step 120 Global step 120 Train loss 6.46 on epoch=59
06/19/2022 19:38:34 - INFO - __main__ - Step 130 Global step 130 Train loss 6.49 on epoch=64
06/19/2022 19:38:36 - INFO - __main__ - Step 140 Global step 140 Train loss 6.46 on epoch=69
06/19/2022 19:38:37 - INFO - __main__ - Step 150 Global step 150 Train loss 6.46 on epoch=74
06/19/2022 19:38:38 - INFO - __main__ - Global step 150 Train loss 6.47 ACC 0.0 on epoch=74
06/19/2022 19:38:39 - INFO - __main__ - Step 160 Global step 160 Train loss 6.44 on epoch=79
06/19/2022 19:38:41 - INFO - __main__ - Step 170 Global step 170 Train loss 6.40 on epoch=84
06/19/2022 19:38:42 - INFO - __main__ - Step 180 Global step 180 Train loss 6.44 on epoch=89
06/19/2022 19:38:43 - INFO - __main__ - Step 190 Global step 190 Train loss 6.35 on epoch=94
06/19/2022 19:38:44 - INFO - __main__ - Step 200 Global step 200 Train loss 6.35 on epoch=99
06/19/2022 19:38:45 - INFO - __main__ - Global step 200 Train loss 6.39 ACC 0.0 on epoch=99
06/19/2022 19:38:46 - INFO - __main__ - Step 210 Global step 210 Train loss 6.37 on epoch=104
06/19/2022 19:38:47 - INFO - __main__ - Step 220 Global step 220 Train loss 6.38 on epoch=109
06/19/2022 19:38:49 - INFO - __main__ - Step 230 Global step 230 Train loss 6.37 on epoch=114
06/19/2022 19:38:50 - INFO - __main__ - Step 240 Global step 240 Train loss 6.32 on epoch=119
06/19/2022 19:38:51 - INFO - __main__ - Step 250 Global step 250 Train loss 6.29 on epoch=124
06/19/2022 19:39:01 - INFO - __main__ - Global step 250 Train loss 6.35 ACC 0.0 on epoch=124
06/19/2022 19:39:02 - INFO - __main__ - Step 260 Global step 260 Train loss 6.23 on epoch=129
06/19/2022 19:39:04 - INFO - __main__ - Step 270 Global step 270 Train loss 6.26 on epoch=134
06/19/2022 19:39:05 - INFO - __main__ - Step 280 Global step 280 Train loss 6.14 on epoch=139
06/19/2022 19:39:06 - INFO - __main__ - Step 290 Global step 290 Train loss 6.19 on epoch=144
06/19/2022 19:39:08 - INFO - __main__ - Step 300 Global step 300 Train loss 6.09 on epoch=149
06/19/2022 19:39:10 - INFO - __main__ - Global step 300 Train loss 6.18 ACC 0.0 on epoch=149
06/19/2022 19:39:11 - INFO - __main__ - Step 310 Global step 310 Train loss 6.16 on epoch=154
06/19/2022 19:39:12 - INFO - __main__ - Step 320 Global step 320 Train loss 6.10 on epoch=159
06/19/2022 19:39:14 - INFO - __main__ - Step 330 Global step 330 Train loss 6.04 on epoch=164
06/19/2022 19:39:15 - INFO - __main__ - Step 340 Global step 340 Train loss 5.99 on epoch=169
06/19/2022 19:39:16 - INFO - __main__ - Step 350 Global step 350 Train loss 5.96 on epoch=174
06/19/2022 19:39:19 - INFO - __main__ - Global step 350 Train loss 6.05 ACC 0.0 on epoch=174
06/19/2022 19:39:20 - INFO - __main__ - Step 360 Global step 360 Train loss 5.96 on epoch=179
06/19/2022 19:39:21 - INFO - __main__ - Step 370 Global step 370 Train loss 5.99 on epoch=184
06/19/2022 19:39:23 - INFO - __main__ - Step 380 Global step 380 Train loss 5.95 on epoch=189
06/19/2022 19:39:24 - INFO - __main__ - Step 390 Global step 390 Train loss 5.85 on epoch=194
06/19/2022 19:39:25 - INFO - __main__ - Step 400 Global step 400 Train loss 5.78 on epoch=199
06/19/2022 19:39:31 - INFO - __main__ - Global step 400 Train loss 5.91 ACC 0.0 on epoch=199
06/19/2022 19:39:32 - INFO - __main__ - Step 410 Global step 410 Train loss 5.77 on epoch=204
06/19/2022 19:39:34 - INFO - __main__ - Step 420 Global step 420 Train loss 5.86 on epoch=209
06/19/2022 19:39:35 - INFO - __main__ - Step 430 Global step 430 Train loss 5.78 on epoch=214
06/19/2022 19:39:36 - INFO - __main__ - Step 440 Global step 440 Train loss 5.87 on epoch=219
06/19/2022 19:39:37 - INFO - __main__ - Step 450 Global step 450 Train loss 5.79 on epoch=224
06/19/2022 19:39:39 - INFO - __main__ - Global step 450 Train loss 5.81 ACC 0.0 on epoch=224
06/19/2022 19:39:41 - INFO - __main__ - Step 460 Global step 460 Train loss 5.69 on epoch=229
06/19/2022 19:39:42 - INFO - __main__ - Step 470 Global step 470 Train loss 5.64 on epoch=234
06/19/2022 19:39:43 - INFO - __main__ - Step 480 Global step 480 Train loss 5.69 on epoch=239
06/19/2022 19:39:44 - INFO - __main__ - Step 490 Global step 490 Train loss 5.61 on epoch=244
06/19/2022 19:39:46 - INFO - __main__ - Step 500 Global step 500 Train loss 5.66 on epoch=249
06/19/2022 19:39:52 - INFO - __main__ - Global step 500 Train loss 5.66 ACC 0.0 on epoch=249
06/19/2022 19:39:54 - INFO - __main__ - Step 510 Global step 510 Train loss 5.59 on epoch=254
06/19/2022 19:39:55 - INFO - __main__ - Step 520 Global step 520 Train loss 5.60 on epoch=259
06/19/2022 19:39:56 - INFO - __main__ - Step 530 Global step 530 Train loss 5.63 on epoch=264
06/19/2022 19:39:57 - INFO - __main__ - Step 540 Global step 540 Train loss 5.72 on epoch=269
06/19/2022 19:39:59 - INFO - __main__ - Step 550 Global step 550 Train loss 5.79 on epoch=274
06/19/2022 19:40:08 - INFO - __main__ - Global step 550 Train loss 5.67 ACC 0.0 on epoch=274
06/19/2022 19:40:10 - INFO - __main__ - Step 560 Global step 560 Train loss 5.70 on epoch=279
06/19/2022 19:40:11 - INFO - __main__ - Step 570 Global step 570 Train loss 5.68 on epoch=284
06/19/2022 19:40:12 - INFO - __main__ - Step 580 Global step 580 Train loss 5.70 on epoch=289
06/19/2022 19:40:13 - INFO - __main__ - Step 590 Global step 590 Train loss 5.64 on epoch=294
06/19/2022 19:40:15 - INFO - __main__ - Step 600 Global step 600 Train loss 5.52 on epoch=299
06/19/2022 19:40:18 - INFO - __main__ - Global step 600 Train loss 5.65 ACC 0.0 on epoch=299
06/19/2022 19:40:19 - INFO - __main__ - Step 610 Global step 610 Train loss 5.69 on epoch=304
06/19/2022 19:40:20 - INFO - __main__ - Step 620 Global step 620 Train loss 5.72 on epoch=309
06/19/2022 19:40:22 - INFO - __main__ - Step 630 Global step 630 Train loss 5.66 on epoch=314
06/19/2022 19:40:23 - INFO - __main__ - Step 640 Global step 640 Train loss 5.64 on epoch=319
06/19/2022 19:40:24 - INFO - __main__ - Step 650 Global step 650 Train loss 5.48 on epoch=324
06/19/2022 19:40:26 - INFO - __main__ - Global step 650 Train loss 5.64 ACC 0.0 on epoch=324
06/19/2022 19:40:27 - INFO - __main__ - Step 660 Global step 660 Train loss 5.52 on epoch=329
06/19/2022 19:40:28 - INFO - __main__ - Step 670 Global step 670 Train loss 5.61 on epoch=334
06/19/2022 19:40:29 - INFO - __main__ - Step 680 Global step 680 Train loss 5.59 on epoch=339
06/19/2022 19:40:31 - INFO - __main__ - Step 690 Global step 690 Train loss 5.54 on epoch=344
06/19/2022 19:40:32 - INFO - __main__ - Step 700 Global step 700 Train loss 5.49 on epoch=349
06/19/2022 19:40:35 - INFO - __main__ - Global step 700 Train loss 5.55 ACC 0.0 on epoch=349
06/19/2022 19:40:36 - INFO - __main__ - Step 710 Global step 710 Train loss 5.43 on epoch=354
06/19/2022 19:40:37 - INFO - __main__ - Step 720 Global step 720 Train loss 5.49 on epoch=359
06/19/2022 19:40:39 - INFO - __main__ - Step 730 Global step 730 Train loss 5.49 on epoch=364
06/19/2022 19:40:40 - INFO - __main__ - Step 740 Global step 740 Train loss 5.43 on epoch=369
06/19/2022 19:40:41 - INFO - __main__ - Step 750 Global step 750 Train loss 5.40 on epoch=374
06/19/2022 19:40:47 - INFO - __main__ - Global step 750 Train loss 5.45 ACC 0.0 on epoch=374
06/19/2022 19:40:48 - INFO - __main__ - Step 760 Global step 760 Train loss 5.37 on epoch=379
06/19/2022 19:40:50 - INFO - __main__ - Step 770 Global step 770 Train loss 5.38 on epoch=384
06/19/2022 19:40:51 - INFO - __main__ - Step 780 Global step 780 Train loss 5.40 on epoch=389
06/19/2022 19:40:52 - INFO - __main__ - Step 790 Global step 790 Train loss 5.36 on epoch=394
06/19/2022 19:40:53 - INFO - __main__ - Step 800 Global step 800 Train loss 5.34 on epoch=399
06/19/2022 19:40:59 - INFO - __main__ - Global step 800 Train loss 5.37 ACC 0.0 on epoch=399
06/19/2022 19:41:00 - INFO - __main__ - Step 810 Global step 810 Train loss 5.29 on epoch=404
06/19/2022 19:41:01 - INFO - __main__ - Step 820 Global step 820 Train loss 5.30 on epoch=409
06/19/2022 19:41:03 - INFO - __main__ - Step 830 Global step 830 Train loss 5.28 on epoch=414
06/19/2022 19:41:04 - INFO - __main__ - Step 840 Global step 840 Train loss 5.33 on epoch=419
06/19/2022 19:41:05 - INFO - __main__ - Step 850 Global step 850 Train loss 5.33 on epoch=424
06/19/2022 19:41:07 - INFO - __main__ - Global step 850 Train loss 5.31 ACC 0.0 on epoch=424
06/19/2022 19:41:08 - INFO - __main__ - Step 860 Global step 860 Train loss 5.31 on epoch=429
06/19/2022 19:41:09 - INFO - __main__ - Step 870 Global step 870 Train loss 5.41 on epoch=434
06/19/2022 19:41:11 - INFO - __main__ - Step 880 Global step 880 Train loss 5.26 on epoch=439
06/19/2022 19:41:12 - INFO - __main__ - Step 890 Global step 890 Train loss 5.29 on epoch=444
06/19/2022 19:41:13 - INFO - __main__ - Step 900 Global step 900 Train loss 5.20 on epoch=449
06/19/2022 19:41:18 - INFO - __main__ - Global step 900 Train loss 5.29 ACC 0.0 on epoch=449
06/19/2022 19:41:20 - INFO - __main__ - Step 910 Global step 910 Train loss 5.31 on epoch=454
06/19/2022 19:41:21 - INFO - __main__ - Step 920 Global step 920 Train loss 5.25 on epoch=459
06/19/2022 19:41:22 - INFO - __main__ - Step 930 Global step 930 Train loss 5.12 on epoch=464
06/19/2022 19:41:23 - INFO - __main__ - Step 940 Global step 940 Train loss 5.26 on epoch=469
06/19/2022 19:41:25 - INFO - __main__ - Step 950 Global step 950 Train loss 5.15 on epoch=474
06/19/2022 19:41:29 - INFO - __main__ - Global step 950 Train loss 5.22 ACC 0.0 on epoch=474
06/19/2022 19:41:30 - INFO - __main__ - Step 960 Global step 960 Train loss 5.25 on epoch=479
06/19/2022 19:41:31 - INFO - __main__ - Step 970 Global step 970 Train loss 5.15 on epoch=484
06/19/2022 19:41:32 - INFO - __main__ - Step 980 Global step 980 Train loss 5.20 on epoch=489
06/19/2022 19:41:34 - INFO - __main__ - Step 990 Global step 990 Train loss 5.04 on epoch=494
06/19/2022 19:41:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 5.17 on epoch=499
06/19/2022 19:41:40 - INFO - __main__ - Global step 1000 Train loss 5.16 ACC 0.0 on epoch=499
06/19/2022 19:41:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 5.12 on epoch=504
06/19/2022 19:41:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 5.10 on epoch=509
06/19/2022 19:41:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 5.03 on epoch=514
06/19/2022 19:41:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 5.06 on epoch=519
06/19/2022 19:41:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 5.05 on epoch=524
06/19/2022 19:41:53 - INFO - __main__ - Global step 1050 Train loss 5.07 ACC 0.0 on epoch=524
06/19/2022 19:41:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.94 on epoch=529
06/19/2022 19:41:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.92 on epoch=534
06/19/2022 19:41:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.82 on epoch=539
06/19/2022 19:41:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 5.06 on epoch=544
06/19/2022 19:42:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.96 on epoch=549
06/19/2022 19:42:05 - INFO - __main__ - Global step 1100 Train loss 4.94 ACC 0.0 on epoch=549
06/19/2022 19:42:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 5.01 on epoch=554
06/19/2022 19:42:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 4.94 on epoch=559
06/19/2022 19:42:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 4.84 on epoch=564
06/19/2022 19:42:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 4.85 on epoch=569
06/19/2022 19:42:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 4.81 on epoch=574
06/19/2022 19:42:17 - INFO - __main__ - Global step 1150 Train loss 4.89 ACC 0.0 on epoch=574
06/19/2022 19:42:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 4.88 on epoch=579
06/19/2022 19:42:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 4.80 on epoch=584
06/19/2022 19:42:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 4.81 on epoch=589
06/19/2022 19:42:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 4.80 on epoch=594
06/19/2022 19:42:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 4.69 on epoch=599
06/19/2022 19:42:29 - INFO - __main__ - Global step 1200 Train loss 4.80 ACC 0.0 on epoch=599
06/19/2022 19:42:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 4.61 on epoch=604
06/19/2022 19:42:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 4.63 on epoch=609
06/19/2022 19:42:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 4.61 on epoch=614
06/19/2022 19:42:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 4.67 on epoch=619
06/19/2022 19:42:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 4.56 on epoch=624
06/19/2022 19:42:37 - INFO - __main__ - Global step 1250 Train loss 4.62 ACC 0.0 on epoch=624
06/19/2022 19:42:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 4.55 on epoch=629
06/19/2022 19:42:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 4.38 on epoch=634
06/19/2022 19:42:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 4.34 on epoch=639
06/19/2022 19:42:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 4.35 on epoch=644
06/19/2022 19:42:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 4.22 on epoch=649
06/19/2022 19:42:45 - INFO - __main__ - Global step 1300 Train loss 4.37 ACC 0.0 on epoch=649
06/19/2022 19:42:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 4.40 on epoch=654
06/19/2022 19:42:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 4.22 on epoch=659
06/19/2022 19:42:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 4.20 on epoch=664
06/19/2022 19:42:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 4.17 on epoch=669
06/19/2022 19:42:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 4.09 on epoch=674
06/19/2022 19:42:59 - INFO - __main__ - Global step 1350 Train loss 4.22 ACC 0.0 on epoch=674
06/19/2022 19:43:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 4.10 on epoch=679
06/19/2022 19:43:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 4.13 on epoch=684
06/19/2022 19:43:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 4.02 on epoch=689
06/19/2022 19:43:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 4.06 on epoch=694
06/19/2022 19:43:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 3.98 on epoch=699
06/19/2022 19:43:07 - INFO - __main__ - Global step 1400 Train loss 4.06 ACC 0.0 on epoch=699
06/19/2022 19:43:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 3.88 on epoch=704
06/19/2022 19:43:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 3.89 on epoch=709
06/19/2022 19:43:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 3.95 on epoch=714
06/19/2022 19:43:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 3.84 on epoch=719
06/19/2022 19:43:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 3.83 on epoch=724
06/19/2022 19:43:15 - INFO - __main__ - Global step 1450 Train loss 3.88 ACC 0.0 on epoch=724
06/19/2022 19:43:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 3.83 on epoch=729
06/19/2022 19:43:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 3.70 on epoch=734
06/19/2022 19:43:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 3.78 on epoch=739
06/19/2022 19:43:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 3.74 on epoch=744
06/19/2022 19:43:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 3.67 on epoch=749
06/19/2022 19:43:22 - INFO - __main__ - Global step 1500 Train loss 3.74 ACC 0.0 on epoch=749
06/19/2022 19:43:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 3.63 on epoch=754
06/19/2022 19:43:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 3.75 on epoch=759
06/19/2022 19:43:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 3.54 on epoch=764
06/19/2022 19:43:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 3.47 on epoch=769
06/19/2022 19:43:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 3.60 on epoch=774
06/19/2022 19:43:37 - INFO - __main__ - Global step 1550 Train loss 3.60 ACC 0.03125 on epoch=774
06/19/2022 19:43:37 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=774, global_step=1550
06/19/2022 19:43:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 3.60 on epoch=779
06/19/2022 19:43:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 3.53 on epoch=784
06/19/2022 19:43:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 3.57 on epoch=789
06/19/2022 19:43:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 3.55 on epoch=794
06/19/2022 19:43:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 3.48 on epoch=799
06/19/2022 19:43:52 - INFO - __main__ - Global step 1600 Train loss 3.55 ACC 0.03125 on epoch=799
06/19/2022 19:43:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 3.44 on epoch=804
06/19/2022 19:43:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 3.44 on epoch=809
06/19/2022 19:43:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 3.46 on epoch=814
06/19/2022 19:43:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 3.39 on epoch=819
06/19/2022 19:44:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 3.31 on epoch=824
06/19/2022 19:44:06 - INFO - __main__ - Global step 1650 Train loss 3.41 ACC 0.0 on epoch=824
06/19/2022 19:44:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 3.27 on epoch=829
06/19/2022 19:44:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 3.24 on epoch=834
06/19/2022 19:44:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 3.20 on epoch=839
06/19/2022 19:44:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 3.18 on epoch=844
06/19/2022 19:44:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 3.24 on epoch=849
06/19/2022 19:44:22 - INFO - __main__ - Global step 1700 Train loss 3.23 ACC 0.28125 on epoch=849
06/19/2022 19:44:22 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.28125 on epoch=849, global_step=1700
06/19/2022 19:44:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 3.16 on epoch=854
06/19/2022 19:44:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 3.12 on epoch=859
06/19/2022 19:44:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 3.12 on epoch=864
06/19/2022 19:44:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 3.05 on epoch=869
06/19/2022 19:44:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 3.01 on epoch=874
06/19/2022 19:44:32 - INFO - __main__ - Global step 1750 Train loss 3.09 ACC 0.4375 on epoch=874
06/19/2022 19:44:32 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.4375 on epoch=874, global_step=1750
06/19/2022 19:44:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 2.92 on epoch=879
06/19/2022 19:44:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 2.88 on epoch=884
06/19/2022 19:44:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 2.85 on epoch=889
06/19/2022 19:44:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 2.85 on epoch=894
06/19/2022 19:44:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 2.74 on epoch=899
06/19/2022 19:44:40 - INFO - __main__ - Global step 1800 Train loss 2.85 ACC 0.5 on epoch=899
06/19/2022 19:44:41 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=899, global_step=1800
06/19/2022 19:44:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 2.67 on epoch=904
06/19/2022 19:44:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 2.77 on epoch=909
06/19/2022 19:44:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 2.70 on epoch=914
06/19/2022 19:44:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 2.63 on epoch=919
06/19/2022 19:44:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 2.57 on epoch=924
06/19/2022 19:44:57 - INFO - __main__ - Global step 1850 Train loss 2.67 ACC 0.4375 on epoch=924
06/19/2022 19:44:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 2.54 on epoch=929
06/19/2022 19:44:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 2.48 on epoch=934
06/19/2022 19:45:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 2.48 on epoch=939
06/19/2022 19:45:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 2.47 on epoch=944
06/19/2022 19:45:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 2.26 on epoch=949
06/19/2022 19:45:05 - INFO - __main__ - Global step 1900 Train loss 2.45 ACC 0.5 on epoch=949
06/19/2022 19:45:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 2.44 on epoch=954
06/19/2022 19:45:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 2.34 on epoch=959
06/19/2022 19:45:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 2.24 on epoch=964
06/19/2022 19:45:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 2.29 on epoch=969
06/19/2022 19:45:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 2.13 on epoch=974
06/19/2022 19:45:13 - INFO - __main__ - Global step 1950 Train loss 2.29 ACC 0.5 on epoch=974
06/19/2022 19:45:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 2.15 on epoch=979
06/19/2022 19:45:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 2.18 on epoch=984
06/19/2022 19:45:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 2.09 on epoch=989
06/19/2022 19:45:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 2.11 on epoch=994
06/19/2022 19:45:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 2.06 on epoch=999
06/19/2022 19:45:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:45:21 - INFO - __main__ - Printing 3 examples
06/19/2022 19:45:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 19:45:21 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 19:45:21 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 19:45:21 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:45:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:45:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:45:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:45:21 - INFO - __main__ - Printing 3 examples
06/19/2022 19:45:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 19:45:21 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 19:45:21 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:21 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 19:45:21 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:21 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:45:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:45:21 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:45:21 - INFO - __main__ - Global step 2000 Train loss 2.12 ACC 0.5 on epoch=999
06/19/2022 19:45:21 - INFO - __main__ - save last model!
06/19/2022 19:45:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:45:21 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:45:21 - INFO - __main__ - Printing 3 examples
06/19/2022 19:45:21 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:45:21 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:45:21 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:45:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:45:21 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:45:21 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:45:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:45:22 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:45:26 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:45:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:45:26 - INFO - __main__ - Starting training!
06/19/2022 19:45:51 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.2_8_predictions.txt
06/19/2022 19:45:51 - INFO - __main__ - ACC on test data: 0.6667
06/19/2022 19:45:51 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.6666666666666666
06/19/2022 19:45:51 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.5, bsz=8 ...
06/19/2022 19:45:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:45:52 - INFO - __main__ - Printing 3 examples
06/19/2022 19:45:52 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 19:45:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:52 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 19:45:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:52 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 19:45:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:45:52 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:45:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:45:52 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:45:52 - INFO - __main__ - Printing 3 examples
06/19/2022 19:45:52 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 19:45:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:52 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 19:45:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:52 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 19:45:52 - INFO - __main__ - ['equivalent']
06/19/2022 19:45:52 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:45:52 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:45:52 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:45:57 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:45:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:45:57 - INFO - __main__ - Starting training!
06/19/2022 19:45:59 - INFO - __main__ - Step 10 Global step 10 Train loss 6.92 on epoch=4
06/19/2022 19:46:00 - INFO - __main__ - Step 20 Global step 20 Train loss 6.90 on epoch=9
06/19/2022 19:46:01 - INFO - __main__ - Step 30 Global step 30 Train loss 6.86 on epoch=14
06/19/2022 19:46:03 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/19/2022 19:46:04 - INFO - __main__ - Step 50 Global step 50 Train loss 6.72 on epoch=24
06/19/2022 19:46:06 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/19/2022 19:46:06 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 19:46:07 - INFO - __main__ - Step 60 Global step 60 Train loss 6.85 on epoch=29
06/19/2022 19:46:09 - INFO - __main__ - Step 70 Global step 70 Train loss 6.74 on epoch=34
06/19/2022 19:46:10 - INFO - __main__ - Step 80 Global step 80 Train loss 6.75 on epoch=39
06/19/2022 19:46:11 - INFO - __main__ - Step 90 Global step 90 Train loss 6.66 on epoch=44
06/19/2022 19:46:12 - INFO - __main__ - Step 100 Global step 100 Train loss 6.73 on epoch=49
06/19/2022 19:46:15 - INFO - __main__ - Global step 100 Train loss 6.75 ACC 0.0 on epoch=49
06/19/2022 19:46:17 - INFO - __main__ - Step 110 Global step 110 Train loss 6.64 on epoch=54
06/19/2022 19:46:18 - INFO - __main__ - Step 120 Global step 120 Train loss 6.63 on epoch=59
06/19/2022 19:46:19 - INFO - __main__ - Step 130 Global step 130 Train loss 6.62 on epoch=64
06/19/2022 19:46:20 - INFO - __main__ - Step 140 Global step 140 Train loss 6.53 on epoch=69
06/19/2022 19:46:22 - INFO - __main__ - Step 150 Global step 150 Train loss 6.67 on epoch=74
06/19/2022 19:46:23 - INFO - __main__ - Global step 150 Train loss 6.62 ACC 0.0 on epoch=74
06/19/2022 19:46:24 - INFO - __main__ - Step 160 Global step 160 Train loss 6.60 on epoch=79
06/19/2022 19:46:26 - INFO - __main__ - Step 170 Global step 170 Train loss 6.50 on epoch=84
06/19/2022 19:46:27 - INFO - __main__ - Step 180 Global step 180 Train loss 6.51 on epoch=89
06/19/2022 19:46:28 - INFO - __main__ - Step 190 Global step 190 Train loss 6.48 on epoch=94
06/19/2022 19:46:29 - INFO - __main__ - Step 200 Global step 200 Train loss 6.40 on epoch=99
06/19/2022 19:46:39 - INFO - __main__ - Global step 200 Train loss 6.50 ACC 0.0 on epoch=99
06/19/2022 19:46:41 - INFO - __main__ - Step 210 Global step 210 Train loss 6.30 on epoch=104
06/19/2022 19:46:42 - INFO - __main__ - Step 220 Global step 220 Train loss 6.32 on epoch=109
06/19/2022 19:46:43 - INFO - __main__ - Step 230 Global step 230 Train loss 6.22 on epoch=114
06/19/2022 19:46:44 - INFO - __main__ - Step 240 Global step 240 Train loss 6.15 on epoch=119
06/19/2022 19:46:46 - INFO - __main__ - Step 250 Global step 250 Train loss 6.07 on epoch=124
06/19/2022 19:46:52 - INFO - __main__ - Global step 250 Train loss 6.21 ACC 0.0 on epoch=124
06/19/2022 19:46:53 - INFO - __main__ - Step 260 Global step 260 Train loss 5.94 on epoch=129
06/19/2022 19:46:54 - INFO - __main__ - Step 270 Global step 270 Train loss 5.86 on epoch=134
06/19/2022 19:46:55 - INFO - __main__ - Step 280 Global step 280 Train loss 5.80 on epoch=139
06/19/2022 19:46:57 - INFO - __main__ - Step 290 Global step 290 Train loss 5.60 on epoch=144
06/19/2022 19:46:58 - INFO - __main__ - Step 300 Global step 300 Train loss 5.36 on epoch=149
06/19/2022 19:47:03 - INFO - __main__ - Global step 300 Train loss 5.71 ACC 0.0 on epoch=149
06/19/2022 19:47:04 - INFO - __main__ - Step 310 Global step 310 Train loss 5.34 on epoch=154
06/19/2022 19:47:06 - INFO - __main__ - Step 320 Global step 320 Train loss 5.25 on epoch=159
06/19/2022 19:47:07 - INFO - __main__ - Step 330 Global step 330 Train loss 5.16 on epoch=164
06/19/2022 19:47:08 - INFO - __main__ - Step 340 Global step 340 Train loss 5.10 on epoch=169
06/19/2022 19:47:09 - INFO - __main__ - Step 350 Global step 350 Train loss 4.91 on epoch=174
06/19/2022 19:47:11 - INFO - __main__ - Global step 350 Train loss 5.15 ACC 0.0 on epoch=174
06/19/2022 19:47:12 - INFO - __main__ - Step 360 Global step 360 Train loss 4.84 on epoch=179
06/19/2022 19:47:13 - INFO - __main__ - Step 370 Global step 370 Train loss 4.69 on epoch=184
06/19/2022 19:47:14 - INFO - __main__ - Step 380 Global step 380 Train loss 4.67 on epoch=189
06/19/2022 19:47:16 - INFO - __main__ - Step 390 Global step 390 Train loss 4.62 on epoch=194
06/19/2022 19:47:17 - INFO - __main__ - Step 400 Global step 400 Train loss 4.53 on epoch=199
06/19/2022 19:47:18 - INFO - __main__ - Global step 400 Train loss 4.67 ACC 0.0 on epoch=199
06/19/2022 19:47:19 - INFO - __main__ - Step 410 Global step 410 Train loss 4.48 on epoch=204
06/19/2022 19:47:20 - INFO - __main__ - Step 420 Global step 420 Train loss 4.41 on epoch=209
06/19/2022 19:47:22 - INFO - __main__ - Step 430 Global step 430 Train loss 4.31 on epoch=214
06/19/2022 19:47:23 - INFO - __main__ - Step 440 Global step 440 Train loss 4.31 on epoch=219
06/19/2022 19:47:24 - INFO - __main__ - Step 450 Global step 450 Train loss 4.13 on epoch=224
06/19/2022 19:47:25 - INFO - __main__ - Global step 450 Train loss 4.33 ACC 0.0 on epoch=224
06/19/2022 19:47:27 - INFO - __main__ - Step 460 Global step 460 Train loss 4.23 on epoch=229
06/19/2022 19:47:28 - INFO - __main__ - Step 470 Global step 470 Train loss 4.15 on epoch=234
06/19/2022 19:47:29 - INFO - __main__ - Step 480 Global step 480 Train loss 3.93 on epoch=239
06/19/2022 19:47:31 - INFO - __main__ - Step 490 Global step 490 Train loss 3.83 on epoch=244
06/19/2022 19:47:32 - INFO - __main__ - Step 500 Global step 500 Train loss 3.91 on epoch=249
06/19/2022 19:47:33 - INFO - __main__ - Global step 500 Train loss 4.01 ACC 0.0 on epoch=249
06/19/2022 19:47:34 - INFO - __main__ - Step 510 Global step 510 Train loss 3.77 on epoch=254
06/19/2022 19:47:35 - INFO - __main__ - Step 520 Global step 520 Train loss 3.62 on epoch=259
06/19/2022 19:47:37 - INFO - __main__ - Step 530 Global step 530 Train loss 3.67 on epoch=264
06/19/2022 19:47:38 - INFO - __main__ - Step 540 Global step 540 Train loss 3.43 on epoch=269
06/19/2022 19:47:39 - INFO - __main__ - Step 550 Global step 550 Train loss 3.33 on epoch=274
06/19/2022 19:47:41 - INFO - __main__ - Global step 550 Train loss 3.56 ACC 0.0 on epoch=274
06/19/2022 19:47:42 - INFO - __main__ - Step 560 Global step 560 Train loss 3.38 on epoch=279
06/19/2022 19:47:44 - INFO - __main__ - Step 570 Global step 570 Train loss 3.47 on epoch=284
06/19/2022 19:47:45 - INFO - __main__ - Step 580 Global step 580 Train loss 3.27 on epoch=289
06/19/2022 19:47:46 - INFO - __main__ - Step 590 Global step 590 Train loss 3.20 on epoch=294
06/19/2022 19:47:47 - INFO - __main__ - Step 600 Global step 600 Train loss 3.15 on epoch=299
06/19/2022 19:47:50 - INFO - __main__ - Global step 600 Train loss 3.29 ACC 0.0 on epoch=299
06/19/2022 19:47:51 - INFO - __main__ - Step 610 Global step 610 Train loss 3.08 on epoch=304
06/19/2022 19:47:52 - INFO - __main__ - Step 620 Global step 620 Train loss 3.01 on epoch=309
06/19/2022 19:47:54 - INFO - __main__ - Step 630 Global step 630 Train loss 3.09 on epoch=314
06/19/2022 19:47:55 - INFO - __main__ - Step 640 Global step 640 Train loss 3.04 on epoch=319
06/19/2022 19:47:56 - INFO - __main__ - Step 650 Global step 650 Train loss 3.01 on epoch=324
06/19/2022 19:47:58 - INFO - __main__ - Global step 650 Train loss 3.05 ACC 0.0 on epoch=324
06/19/2022 19:47:59 - INFO - __main__ - Step 660 Global step 660 Train loss 2.91 on epoch=329
06/19/2022 19:48:00 - INFO - __main__ - Step 670 Global step 670 Train loss 2.77 on epoch=334
06/19/2022 19:48:02 - INFO - __main__ - Step 680 Global step 680 Train loss 2.74 on epoch=339
06/19/2022 19:48:03 - INFO - __main__ - Step 690 Global step 690 Train loss 2.78 on epoch=344
06/19/2022 19:48:04 - INFO - __main__ - Step 700 Global step 700 Train loss 2.71 on epoch=349
06/19/2022 19:48:08 - INFO - __main__ - Global step 700 Train loss 2.78 ACC 0.0 on epoch=349
06/19/2022 19:48:09 - INFO - __main__ - Step 710 Global step 710 Train loss 2.59 on epoch=354
06/19/2022 19:48:10 - INFO - __main__ - Step 720 Global step 720 Train loss 2.54 on epoch=359
06/19/2022 19:48:12 - INFO - __main__ - Step 730 Global step 730 Train loss 2.53 on epoch=364
06/19/2022 19:48:13 - INFO - __main__ - Step 740 Global step 740 Train loss 2.53 on epoch=369
06/19/2022 19:48:14 - INFO - __main__ - Step 750 Global step 750 Train loss 2.46 on epoch=374
06/19/2022 19:48:16 - INFO - __main__ - Global step 750 Train loss 2.53 ACC 0.0 on epoch=374
06/19/2022 19:48:18 - INFO - __main__ - Step 760 Global step 760 Train loss 2.39 on epoch=379
06/19/2022 19:48:19 - INFO - __main__ - Step 770 Global step 770 Train loss 2.27 on epoch=384
06/19/2022 19:48:20 - INFO - __main__ - Step 780 Global step 780 Train loss 2.23 on epoch=389
06/19/2022 19:48:22 - INFO - __main__ - Step 790 Global step 790 Train loss 2.30 on epoch=394
06/19/2022 19:48:23 - INFO - __main__ - Step 800 Global step 800 Train loss 2.15 on epoch=399
06/19/2022 19:48:24 - INFO - __main__ - Global step 800 Train loss 2.27 ACC 0.0 on epoch=399
06/19/2022 19:48:25 - INFO - __main__ - Step 810 Global step 810 Train loss 2.17 on epoch=404
06/19/2022 19:48:27 - INFO - __main__ - Step 820 Global step 820 Train loss 2.14 on epoch=409
06/19/2022 19:48:28 - INFO - __main__ - Step 830 Global step 830 Train loss 2.01 on epoch=414
06/19/2022 19:48:29 - INFO - __main__ - Step 840 Global step 840 Train loss 1.91 on epoch=419
06/19/2022 19:48:30 - INFO - __main__ - Step 850 Global step 850 Train loss 1.87 on epoch=424
06/19/2022 19:48:32 - INFO - __main__ - Global step 850 Train loss 2.02 ACC 0.28125 on epoch=424
06/19/2022 19:48:32 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.28125 on epoch=424, global_step=850
06/19/2022 19:48:34 - INFO - __main__ - Step 860 Global step 860 Train loss 1.85 on epoch=429
06/19/2022 19:48:35 - INFO - __main__ - Step 870 Global step 870 Train loss 1.96 on epoch=434
06/19/2022 19:48:36 - INFO - __main__ - Step 880 Global step 880 Train loss 1.79 on epoch=439
06/19/2022 19:48:37 - INFO - __main__ - Step 890 Global step 890 Train loss 1.84 on epoch=444
06/19/2022 19:48:39 - INFO - __main__ - Step 900 Global step 900 Train loss 1.78 on epoch=449
06/19/2022 19:48:41 - INFO - __main__ - Global step 900 Train loss 1.84 ACC 0.46875 on epoch=449
06/19/2022 19:48:41 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.46875 on epoch=449, global_step=900
06/19/2022 19:48:42 - INFO - __main__ - Step 910 Global step 910 Train loss 1.72 on epoch=454
06/19/2022 19:48:44 - INFO - __main__ - Step 920 Global step 920 Train loss 1.71 on epoch=459
06/19/2022 19:48:45 - INFO - __main__ - Step 930 Global step 930 Train loss 1.62 on epoch=464
06/19/2022 19:48:46 - INFO - __main__ - Step 940 Global step 940 Train loss 1.72 on epoch=469
06/19/2022 19:48:47 - INFO - __main__ - Step 950 Global step 950 Train loss 1.57 on epoch=474
06/19/2022 19:48:49 - INFO - __main__ - Global step 950 Train loss 1.67 ACC 0.5 on epoch=474
06/19/2022 19:48:49 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=474, global_step=950
06/19/2022 19:48:51 - INFO - __main__ - Step 960 Global step 960 Train loss 1.53 on epoch=479
06/19/2022 19:48:52 - INFO - __main__ - Step 970 Global step 970 Train loss 1.46 on epoch=484
06/19/2022 19:48:53 - INFO - __main__ - Step 980 Global step 980 Train loss 1.46 on epoch=489
06/19/2022 19:48:54 - INFO - __main__ - Step 990 Global step 990 Train loss 1.50 on epoch=494
06/19/2022 19:48:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.46 on epoch=499
06/19/2022 19:48:58 - INFO - __main__ - Global step 1000 Train loss 1.48 ACC 0.65625 on epoch=499
06/19/2022 19:48:58 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.65625 on epoch=499, global_step=1000
06/19/2022 19:48:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.37 on epoch=504
06/19/2022 19:49:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.40 on epoch=509
06/19/2022 19:49:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.38 on epoch=514
06/19/2022 19:49:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.46 on epoch=519
06/19/2022 19:49:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.28 on epoch=524
06/19/2022 19:49:08 - INFO - __main__ - Global step 1050 Train loss 1.38 ACC 0.4375 on epoch=524
06/19/2022 19:49:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.27 on epoch=529
06/19/2022 19:49:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.26 on epoch=534
06/19/2022 19:49:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.20 on epoch=539
06/19/2022 19:49:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.16 on epoch=544
06/19/2022 19:49:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.12 on epoch=549
06/19/2022 19:49:17 - INFO - __main__ - Global step 1100 Train loss 1.20 ACC 0.5 on epoch=549
06/19/2022 19:49:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.03 on epoch=554
06/19/2022 19:49:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.04 on epoch=559
06/19/2022 19:49:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.00 on epoch=564
06/19/2022 19:49:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.98 on epoch=569
06/19/2022 19:49:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.04 on epoch=574
06/19/2022 19:49:25 - INFO - __main__ - Global step 1150 Train loss 1.02 ACC 0.5 on epoch=574
06/19/2022 19:49:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.96 on epoch=579
06/19/2022 19:49:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.89 on epoch=584
06/19/2022 19:49:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.89 on epoch=589
06/19/2022 19:49:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.89 on epoch=594
06/19/2022 19:49:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.86 on epoch=599
06/19/2022 19:49:33 - INFO - __main__ - Global step 1200 Train loss 0.90 ACC 0.5 on epoch=599
06/19/2022 19:49:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.73 on epoch=604
06/19/2022 19:49:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.71 on epoch=609
06/19/2022 19:49:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.78 on epoch=614
06/19/2022 19:49:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=619
06/19/2022 19:49:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.76 on epoch=624
06/19/2022 19:49:41 - INFO - __main__ - Global step 1250 Train loss 0.76 ACC 0.5 on epoch=624
06/19/2022 19:49:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.77 on epoch=629
06/19/2022 19:49:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.70 on epoch=634
06/19/2022 19:49:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.69 on epoch=639
06/19/2022 19:49:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.68 on epoch=644
06/19/2022 19:49:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.65 on epoch=649
06/19/2022 19:49:48 - INFO - __main__ - Global step 1300 Train loss 0.70 ACC 0.5 on epoch=649
06/19/2022 19:49:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.52 on epoch=654
06/19/2022 19:49:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.59 on epoch=659
06/19/2022 19:49:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.64 on epoch=664
06/19/2022 19:49:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.57 on epoch=669
06/19/2022 19:49:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.60 on epoch=674
06/19/2022 19:49:56 - INFO - __main__ - Global step 1350 Train loss 0.58 ACC 0.5 on epoch=674
06/19/2022 19:49:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.69 on epoch=679
06/19/2022 19:49:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.54 on epoch=684
06/19/2022 19:50:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=689
06/19/2022 19:50:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.54 on epoch=694
06/19/2022 19:50:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.54 on epoch=699
06/19/2022 19:50:03 - INFO - __main__ - Global step 1400 Train loss 0.56 ACC 0.5 on epoch=699
06/19/2022 19:50:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.52 on epoch=704
06/19/2022 19:50:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
06/19/2022 19:50:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.53 on epoch=714
06/19/2022 19:50:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
06/19/2022 19:50:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.49 on epoch=724
06/19/2022 19:50:11 - INFO - __main__ - Global step 1450 Train loss 0.51 ACC 0.5 on epoch=724
06/19/2022 19:50:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/19/2022 19:50:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=734
06/19/2022 19:50:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
06/19/2022 19:50:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.50 on epoch=744
06/19/2022 19:50:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=749
06/19/2022 19:50:18 - INFO - __main__ - Global step 1500 Train loss 0.48 ACC 0.5 on epoch=749
06/19/2022 19:50:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
06/19/2022 19:50:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=759
06/19/2022 19:50:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
06/19/2022 19:50:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=769
06/19/2022 19:50:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=774
06/19/2022 19:50:25 - INFO - __main__ - Global step 1550 Train loss 0.43 ACC 0.5 on epoch=774
06/19/2022 19:50:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.47 on epoch=779
06/19/2022 19:50:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=784
06/19/2022 19:50:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.47 on epoch=789
06/19/2022 19:50:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=794
06/19/2022 19:50:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=799
06/19/2022 19:50:32 - INFO - __main__ - Global step 1600 Train loss 0.44 ACC 0.5 on epoch=799
06/19/2022 19:50:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=804
06/19/2022 19:50:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=809
06/19/2022 19:50:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=814
06/19/2022 19:50:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=819
06/19/2022 19:50:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.51 on epoch=824
06/19/2022 19:50:38 - INFO - __main__ - Global step 1650 Train loss 0.45 ACC 0.53125 on epoch=824
06/19/2022 19:50:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
06/19/2022 19:50:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=834
06/19/2022 19:50:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=839
06/19/2022 19:50:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=844
06/19/2022 19:50:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=849
06/19/2022 19:50:45 - INFO - __main__ - Global step 1700 Train loss 0.43 ACC 0.46875 on epoch=849
06/19/2022 19:50:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
06/19/2022 19:50:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=859
06/19/2022 19:50:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=864
06/19/2022 19:50:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
06/19/2022 19:50:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
06/19/2022 19:50:51 - INFO - __main__ - Global step 1750 Train loss 0.40 ACC 0.5 on epoch=874
06/19/2022 19:50:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
06/19/2022 19:50:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=884
06/19/2022 19:50:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=889
06/19/2022 19:50:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=894
06/19/2022 19:50:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
06/19/2022 19:50:58 - INFO - __main__ - Global step 1800 Train loss 0.39 ACC 0.5 on epoch=899
06/19/2022 19:50:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
06/19/2022 19:51:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=909
06/19/2022 19:51:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=914
06/19/2022 19:51:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=919
06/19/2022 19:51:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
06/19/2022 19:51:05 - INFO - __main__ - Global step 1850 Train loss 0.40 ACC 0.5 on epoch=924
06/19/2022 19:51:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
06/19/2022 19:51:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
06/19/2022 19:51:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
06/19/2022 19:51:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=944
06/19/2022 19:51:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
06/19/2022 19:51:11 - INFO - __main__ - Global step 1900 Train loss 0.40 ACC 0.5 on epoch=949
06/19/2022 19:51:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=954
06/19/2022 19:51:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=959
06/19/2022 19:51:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=964
06/19/2022 19:51:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
06/19/2022 19:51:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=974
06/19/2022 19:51:18 - INFO - __main__ - Global step 1950 Train loss 0.35 ACC 0.5 on epoch=974
06/19/2022 19:51:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=979
06/19/2022 19:51:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.31 on epoch=984
06/19/2022 19:51:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=989
06/19/2022 19:51:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=994
06/19/2022 19:51:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=999
06/19/2022 19:51:24 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.5 on epoch=999
06/19/2022 19:51:24 - INFO - __main__ - save last model!
06/19/2022 19:51:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:51:24 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:51:24 - INFO - __main__ - Printing 3 examples
06/19/2022 19:51:24 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:51:24 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:24 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:51:24 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:51:24 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:51:24 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:51:24 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:51:25 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:51:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:51:25 - INFO - __main__ - Printing 3 examples
06/19/2022 19:51:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 19:51:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 19:51:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 19:51:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:51:25 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:51:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:51:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:51:25 - INFO - __main__ - Printing 3 examples
06/19/2022 19:51:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 19:51:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 19:51:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:25 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 19:51:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:25 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:51:25 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:51:25 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:51:25 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:51:30 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:51:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:51:31 - INFO - __main__ - Starting training!
06/19/2022 19:51:33 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.5_8_predictions.txt
06/19/2022 19:51:33 - INFO - __main__ - ACC on test data: 0.6863
06/19/2022 19:51:33 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.6862745098039216
06/19/2022 19:51:33 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.4, bsz=8 ...
06/19/2022 19:51:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:51:34 - INFO - __main__ - Printing 3 examples
06/19/2022 19:51:34 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 19:51:34 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 19:51:34 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 19:51:34 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:51:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:51:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:51:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:51:34 - INFO - __main__ - Printing 3 examples
06/19/2022 19:51:34 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 19:51:34 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:34 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 19:51:34 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:34 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 19:51:34 - INFO - __main__ - ['equivalent']
06/19/2022 19:51:34 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:51:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:51:34 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:51:40 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:51:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:51:40 - INFO - __main__ - Starting training!
06/19/2022 19:51:42 - INFO - __main__ - Step 10 Global step 10 Train loss 6.89 on epoch=4
06/19/2022 19:51:43 - INFO - __main__ - Step 20 Global step 20 Train loss 6.90 on epoch=9
06/19/2022 19:51:44 - INFO - __main__ - Step 30 Global step 30 Train loss 6.76 on epoch=14
06/19/2022 19:51:45 - INFO - __main__ - Step 40 Global step 40 Train loss 6.79 on epoch=19
06/19/2022 19:51:47 - INFO - __main__ - Step 50 Global step 50 Train loss 6.89 on epoch=24
06/19/2022 19:51:50 - INFO - __main__ - Global step 50 Train loss 6.84 ACC 0.0 on epoch=24
06/19/2022 19:51:50 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 19:51:51 - INFO - __main__ - Step 60 Global step 60 Train loss 6.83 on epoch=29
06/19/2022 19:51:53 - INFO - __main__ - Step 70 Global step 70 Train loss 6.82 on epoch=34
06/19/2022 19:51:54 - INFO - __main__ - Step 80 Global step 80 Train loss 6.85 on epoch=39
06/19/2022 19:51:55 - INFO - __main__ - Step 90 Global step 90 Train loss 6.81 on epoch=44
06/19/2022 19:51:57 - INFO - __main__ - Step 100 Global step 100 Train loss 6.68 on epoch=49
06/19/2022 19:51:59 - INFO - __main__ - Global step 100 Train loss 6.80 ACC 0.0 on epoch=49
06/19/2022 19:52:00 - INFO - __main__ - Step 110 Global step 110 Train loss 6.84 on epoch=54
06/19/2022 19:52:02 - INFO - __main__ - Step 120 Global step 120 Train loss 6.68 on epoch=59
06/19/2022 19:52:03 - INFO - __main__ - Step 130 Global step 130 Train loss 6.72 on epoch=64
06/19/2022 19:52:04 - INFO - __main__ - Step 140 Global step 140 Train loss 6.66 on epoch=69
06/19/2022 19:52:06 - INFO - __main__ - Step 150 Global step 150 Train loss 6.53 on epoch=74
06/19/2022 19:52:08 - INFO - __main__ - Global step 150 Train loss 6.68 ACC 0.0 on epoch=74
06/19/2022 19:52:10 - INFO - __main__ - Step 160 Global step 160 Train loss 6.60 on epoch=79
06/19/2022 19:52:11 - INFO - __main__ - Step 170 Global step 170 Train loss 6.43 on epoch=84
06/19/2022 19:52:12 - INFO - __main__ - Step 180 Global step 180 Train loss 6.47 on epoch=89
06/19/2022 19:52:13 - INFO - __main__ - Step 190 Global step 190 Train loss 6.30 on epoch=94
06/19/2022 19:52:15 - INFO - __main__ - Step 200 Global step 200 Train loss 6.20 on epoch=99
06/19/2022 19:52:22 - INFO - __main__ - Global step 200 Train loss 6.40 ACC 0.0 on epoch=99
06/19/2022 19:52:23 - INFO - __main__ - Step 210 Global step 210 Train loss 6.19 on epoch=104
06/19/2022 19:52:24 - INFO - __main__ - Step 220 Global step 220 Train loss 5.96 on epoch=109
06/19/2022 19:52:25 - INFO - __main__ - Step 230 Global step 230 Train loss 5.93 on epoch=114
06/19/2022 19:52:27 - INFO - __main__ - Step 240 Global step 240 Train loss 5.88 on epoch=119
06/19/2022 19:52:28 - INFO - __main__ - Step 250 Global step 250 Train loss 5.74 on epoch=124
06/19/2022 19:52:33 - INFO - __main__ - Global step 250 Train loss 5.94 ACC 0.0 on epoch=124
06/19/2022 19:52:34 - INFO - __main__ - Step 260 Global step 260 Train loss 5.84 on epoch=129
06/19/2022 19:52:35 - INFO - __main__ - Step 270 Global step 270 Train loss 5.68 on epoch=134
06/19/2022 19:52:36 - INFO - __main__ - Step 280 Global step 280 Train loss 5.64 on epoch=139
06/19/2022 19:52:38 - INFO - __main__ - Step 290 Global step 290 Train loss 5.65 on epoch=144
06/19/2022 19:52:39 - INFO - __main__ - Step 300 Global step 300 Train loss 5.53 on epoch=149
06/19/2022 19:52:49 - INFO - __main__ - Global step 300 Train loss 5.67 ACC 0.0 on epoch=149
06/19/2022 19:52:50 - INFO - __main__ - Step 310 Global step 310 Train loss 5.52 on epoch=154
06/19/2022 19:52:52 - INFO - __main__ - Step 320 Global step 320 Train loss 5.46 on epoch=159
06/19/2022 19:52:53 - INFO - __main__ - Step 330 Global step 330 Train loss 5.26 on epoch=164
06/19/2022 19:52:54 - INFO - __main__ - Step 340 Global step 340 Train loss 5.21 on epoch=169
06/19/2022 19:52:55 - INFO - __main__ - Step 350 Global step 350 Train loss 5.09 on epoch=174
06/19/2022 19:52:57 - INFO - __main__ - Global step 350 Train loss 5.31 ACC 0.0 on epoch=174
06/19/2022 19:52:58 - INFO - __main__ - Step 360 Global step 360 Train loss 5.15 on epoch=179
06/19/2022 19:53:00 - INFO - __main__ - Step 370 Global step 370 Train loss 5.13 on epoch=184
06/19/2022 19:53:01 - INFO - __main__ - Step 380 Global step 380 Train loss 5.00 on epoch=189
06/19/2022 19:53:02 - INFO - __main__ - Step 390 Global step 390 Train loss 4.94 on epoch=194
06/19/2022 19:53:03 - INFO - __main__ - Step 400 Global step 400 Train loss 4.84 on epoch=199
06/19/2022 19:53:05 - INFO - __main__ - Global step 400 Train loss 5.01 ACC 0.0 on epoch=199
06/19/2022 19:53:07 - INFO - __main__ - Step 410 Global step 410 Train loss 4.79 on epoch=204
06/19/2022 19:53:08 - INFO - __main__ - Step 420 Global step 420 Train loss 4.64 on epoch=209
06/19/2022 19:53:09 - INFO - __main__ - Step 430 Global step 430 Train loss 4.54 on epoch=214
06/19/2022 19:53:10 - INFO - __main__ - Step 440 Global step 440 Train loss 4.39 on epoch=219
06/19/2022 19:53:11 - INFO - __main__ - Step 450 Global step 450 Train loss 4.23 on epoch=224
06/19/2022 19:53:13 - INFO - __main__ - Global step 450 Train loss 4.52 ACC 0.0 on epoch=224
06/19/2022 19:53:14 - INFO - __main__ - Step 460 Global step 460 Train loss 4.17 on epoch=229
06/19/2022 19:53:15 - INFO - __main__ - Step 470 Global step 470 Train loss 4.14 on epoch=234
06/19/2022 19:53:16 - INFO - __main__ - Step 480 Global step 480 Train loss 4.05 on epoch=239
06/19/2022 19:53:17 - INFO - __main__ - Step 490 Global step 490 Train loss 3.94 on epoch=244
06/19/2022 19:53:19 - INFO - __main__ - Step 500 Global step 500 Train loss 3.80 on epoch=249
06/19/2022 19:53:22 - INFO - __main__ - Global step 500 Train loss 4.02 ACC 0.0 on epoch=249
06/19/2022 19:53:23 - INFO - __main__ - Step 510 Global step 510 Train loss 3.69 on epoch=254
06/19/2022 19:53:24 - INFO - __main__ - Step 520 Global step 520 Train loss 3.54 on epoch=259
06/19/2022 19:53:25 - INFO - __main__ - Step 530 Global step 530 Train loss 3.54 on epoch=264
06/19/2022 19:53:26 - INFO - __main__ - Step 540 Global step 540 Train loss 3.32 on epoch=269
06/19/2022 19:53:28 - INFO - __main__ - Step 550 Global step 550 Train loss 3.35 on epoch=274
06/19/2022 19:53:35 - INFO - __main__ - Global step 550 Train loss 3.49 ACC 0.0 on epoch=274
06/19/2022 19:53:36 - INFO - __main__ - Step 560 Global step 560 Train loss 3.26 on epoch=279
06/19/2022 19:53:37 - INFO - __main__ - Step 570 Global step 570 Train loss 3.06 on epoch=284
06/19/2022 19:53:38 - INFO - __main__ - Step 580 Global step 580 Train loss 3.02 on epoch=289
06/19/2022 19:53:39 - INFO - __main__ - Step 590 Global step 590 Train loss 2.79 on epoch=294
06/19/2022 19:53:41 - INFO - __main__ - Step 600 Global step 600 Train loss 2.84 on epoch=299
06/19/2022 19:53:43 - INFO - __main__ - Global step 600 Train loss 2.99 ACC 0.09375 on epoch=299
06/19/2022 19:53:43 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=299, global_step=600
06/19/2022 19:53:44 - INFO - __main__ - Step 610 Global step 610 Train loss 2.67 on epoch=304
06/19/2022 19:53:45 - INFO - __main__ - Step 620 Global step 620 Train loss 2.61 on epoch=309
06/19/2022 19:53:46 - INFO - __main__ - Step 630 Global step 630 Train loss 2.50 on epoch=314
06/19/2022 19:53:48 - INFO - __main__ - Step 640 Global step 640 Train loss 2.35 on epoch=319
06/19/2022 19:53:49 - INFO - __main__ - Step 650 Global step 650 Train loss 2.41 on epoch=324
06/19/2022 19:53:51 - INFO - __main__ - Global step 650 Train loss 2.51 ACC 0.5 on epoch=324
06/19/2022 19:53:51 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.5 on epoch=324, global_step=650
06/19/2022 19:53:52 - INFO - __main__ - Step 660 Global step 660 Train loss 2.28 on epoch=329
06/19/2022 19:53:54 - INFO - __main__ - Step 670 Global step 670 Train loss 2.17 on epoch=334
06/19/2022 19:53:55 - INFO - __main__ - Step 680 Global step 680 Train loss 2.20 on epoch=339
06/19/2022 19:53:56 - INFO - __main__ - Step 690 Global step 690 Train loss 2.10 on epoch=344
06/19/2022 19:53:57 - INFO - __main__ - Step 700 Global step 700 Train loss 1.85 on epoch=349
06/19/2022 19:54:04 - INFO - __main__ - Global step 700 Train loss 2.12 ACC 0.5 on epoch=349
06/19/2022 19:54:05 - INFO - __main__ - Step 710 Global step 710 Train loss 1.93 on epoch=354
06/19/2022 19:54:06 - INFO - __main__ - Step 720 Global step 720 Train loss 1.82 on epoch=359
06/19/2022 19:54:07 - INFO - __main__ - Step 730 Global step 730 Train loss 1.70 on epoch=364
06/19/2022 19:54:09 - INFO - __main__ - Step 740 Global step 740 Train loss 1.78 on epoch=369
06/19/2022 19:54:10 - INFO - __main__ - Step 750 Global step 750 Train loss 1.43 on epoch=374
06/19/2022 19:54:15 - INFO - __main__ - Global step 750 Train loss 1.73 ACC 0.5 on epoch=374
06/19/2022 19:54:16 - INFO - __main__ - Step 760 Global step 760 Train loss 1.50 on epoch=379
06/19/2022 19:54:17 - INFO - __main__ - Step 770 Global step 770 Train loss 1.42 on epoch=384
06/19/2022 19:54:19 - INFO - __main__ - Step 780 Global step 780 Train loss 1.40 on epoch=389
06/19/2022 19:54:20 - INFO - __main__ - Step 790 Global step 790 Train loss 1.32 on epoch=394
06/19/2022 19:54:21 - INFO - __main__ - Step 800 Global step 800 Train loss 1.27 on epoch=399
06/19/2022 19:54:23 - INFO - __main__ - Global step 800 Train loss 1.38 ACC 0.53125 on epoch=399
06/19/2022 19:54:23 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=399, global_step=800
06/19/2022 19:54:24 - INFO - __main__ - Step 810 Global step 810 Train loss 1.23 on epoch=404
06/19/2022 19:54:25 - INFO - __main__ - Step 820 Global step 820 Train loss 1.23 on epoch=409
06/19/2022 19:54:27 - INFO - __main__ - Step 830 Global step 830 Train loss 1.11 on epoch=414
06/19/2022 19:54:28 - INFO - __main__ - Step 840 Global step 840 Train loss 1.13 on epoch=419
06/19/2022 19:54:29 - INFO - __main__ - Step 850 Global step 850 Train loss 1.14 on epoch=424
06/19/2022 19:54:31 - INFO - __main__ - Global step 850 Train loss 1.17 ACC 0.5 on epoch=424
06/19/2022 19:54:32 - INFO - __main__ - Step 860 Global step 860 Train loss 1.10 on epoch=429
06/19/2022 19:54:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.99 on epoch=434
06/19/2022 19:54:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.97 on epoch=439
06/19/2022 19:54:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.94 on epoch=444
06/19/2022 19:54:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.90 on epoch=449
06/19/2022 19:54:43 - INFO - __main__ - Global step 900 Train loss 0.98 ACC 0.5625 on epoch=449
06/19/2022 19:54:43 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=449, global_step=900
06/19/2022 19:54:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.86 on epoch=454
06/19/2022 19:54:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.77 on epoch=459
06/19/2022 19:54:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.78 on epoch=464
06/19/2022 19:54:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.81 on epoch=469
06/19/2022 19:54:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=474
06/19/2022 19:54:50 - INFO - __main__ - Global step 950 Train loss 0.81 ACC 0.5 on epoch=474
06/19/2022 19:54:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=479
06/19/2022 19:54:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.70 on epoch=484
06/19/2022 19:54:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.76 on epoch=489
06/19/2022 19:54:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.66 on epoch=494
06/19/2022 19:54:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.64 on epoch=499
06/19/2022 19:54:57 - INFO - __main__ - Global step 1000 Train loss 0.70 ACC 0.5 on epoch=499
06/19/2022 19:54:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.70 on epoch=504
06/19/2022 19:54:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.62 on epoch=509
06/19/2022 19:55:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.54 on epoch=514
06/19/2022 19:55:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.66 on epoch=519
06/19/2022 19:55:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.54 on epoch=524
06/19/2022 19:55:04 - INFO - __main__ - Global step 1050 Train loss 0.61 ACC 0.5 on epoch=524
06/19/2022 19:55:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.58 on epoch=529
06/19/2022 19:55:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=534
06/19/2022 19:55:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=539
06/19/2022 19:55:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.54 on epoch=544
06/19/2022 19:55:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.54 on epoch=549
06/19/2022 19:55:11 - INFO - __main__ - Global step 1100 Train loss 0.56 ACC 0.65625 on epoch=549
06/19/2022 19:55:11 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=549, global_step=1100
06/19/2022 19:55:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.62 on epoch=554
06/19/2022 19:55:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.60 on epoch=559
06/19/2022 19:55:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
06/19/2022 19:55:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.56 on epoch=569
06/19/2022 19:55:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=574
06/19/2022 19:55:18 - INFO - __main__ - Global step 1150 Train loss 0.58 ACC 0.53125 on epoch=574
06/19/2022 19:55:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=579
06/19/2022 19:55:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=584
06/19/2022 19:55:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.51 on epoch=589
06/19/2022 19:55:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=594
06/19/2022 19:55:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.56 on epoch=599
06/19/2022 19:55:25 - INFO - __main__ - Global step 1200 Train loss 0.51 ACC 0.53125 on epoch=599
06/19/2022 19:55:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=604
06/19/2022 19:55:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=609
06/19/2022 19:55:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
06/19/2022 19:55:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.56 on epoch=619
06/19/2022 19:55:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.52 on epoch=624
06/19/2022 19:55:32 - INFO - __main__ - Global step 1250 Train loss 0.52 ACC 0.5 on epoch=624
06/19/2022 19:55:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=629
06/19/2022 19:55:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=634
06/19/2022 19:55:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=639
06/19/2022 19:55:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.52 on epoch=644
06/19/2022 19:55:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=649
06/19/2022 19:55:39 - INFO - __main__ - Global step 1300 Train loss 0.49 ACC 0.5 on epoch=649
06/19/2022 19:55:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
06/19/2022 19:55:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=659
06/19/2022 19:55:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=664
06/19/2022 19:55:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=669
06/19/2022 19:55:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=674
06/19/2022 19:55:45 - INFO - __main__ - Global step 1350 Train loss 0.47 ACC 0.5 on epoch=674
06/19/2022 19:55:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=679
06/19/2022 19:55:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=684
06/19/2022 19:55:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=689
06/19/2022 19:55:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=694
06/19/2022 19:55:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=699
06/19/2022 19:55:52 - INFO - __main__ - Global step 1400 Train loss 0.41 ACC 0.53125 on epoch=699
06/19/2022 19:55:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=704
06/19/2022 19:55:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=709
06/19/2022 19:55:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
06/19/2022 19:55:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
06/19/2022 19:55:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=724
06/19/2022 19:55:59 - INFO - __main__ - Global step 1450 Train loss 0.39 ACC 0.5 on epoch=724
06/19/2022 19:56:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
06/19/2022 19:56:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.48 on epoch=734
06/19/2022 19:56:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=739
06/19/2022 19:56:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=744
06/19/2022 19:56:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
06/19/2022 19:56:06 - INFO - __main__ - Global step 1500 Train loss 0.45 ACC 0.5 on epoch=749
06/19/2022 19:56:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
06/19/2022 19:56:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.52 on epoch=759
06/19/2022 19:56:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=764
06/19/2022 19:56:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=769
06/19/2022 19:56:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
06/19/2022 19:56:13 - INFO - __main__ - Global step 1550 Train loss 0.44 ACC 0.53125 on epoch=774
06/19/2022 19:56:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=779
06/19/2022 19:56:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
06/19/2022 19:56:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=789
06/19/2022 19:56:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=794
06/19/2022 19:56:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
06/19/2022 19:56:20 - INFO - __main__ - Global step 1600 Train loss 0.39 ACC 0.34375 on epoch=799
06/19/2022 19:56:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=804
06/19/2022 19:56:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=809
06/19/2022 19:56:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=814
06/19/2022 19:56:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=819
06/19/2022 19:56:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=824
06/19/2022 19:56:27 - INFO - __main__ - Global step 1650 Train loss 0.39 ACC 0.53125 on epoch=824
06/19/2022 19:56:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
06/19/2022 19:56:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=834
06/19/2022 19:56:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
06/19/2022 19:56:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=844
06/19/2022 19:56:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
06/19/2022 19:56:34 - INFO - __main__ - Global step 1700 Train loss 0.41 ACC 0.5 on epoch=849
06/19/2022 19:56:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=854
06/19/2022 19:56:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=859
06/19/2022 19:56:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=864
06/19/2022 19:56:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=869
06/19/2022 19:56:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=874
06/19/2022 19:56:41 - INFO - __main__ - Global step 1750 Train loss 0.42 ACC 0.5 on epoch=874
06/19/2022 19:56:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=879
06/19/2022 19:56:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
06/19/2022 19:56:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
06/19/2022 19:56:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
06/19/2022 19:56:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
06/19/2022 19:56:48 - INFO - __main__ - Global step 1800 Train loss 0.37 ACC 0.5 on epoch=899
06/19/2022 19:56:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
06/19/2022 19:56:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
06/19/2022 19:56:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=914
06/19/2022 19:56:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=919
06/19/2022 19:56:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
06/19/2022 19:56:55 - INFO - __main__ - Global step 1850 Train loss 0.38 ACC 0.4375 on epoch=924
06/19/2022 19:56:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=929
06/19/2022 19:56:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=934
06/19/2022 19:56:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=939
06/19/2022 19:57:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=944
06/19/2022 19:57:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
06/19/2022 19:57:02 - INFO - __main__ - Global step 1900 Train loss 0.37 ACC 0.4375 on epoch=949
06/19/2022 19:57:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
06/19/2022 19:57:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=959
06/19/2022 19:57:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
06/19/2022 19:57:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=969
06/19/2022 19:57:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
06/19/2022 19:57:09 - INFO - __main__ - Global step 1950 Train loss 0.37 ACC 0.4375 on epoch=974
06/19/2022 19:57:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=979
06/19/2022 19:57:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=984
06/19/2022 19:57:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=989
06/19/2022 19:57:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
06/19/2022 19:57:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
06/19/2022 19:57:16 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.59375 on epoch=999
06/19/2022 19:57:16 - INFO - __main__ - save last model!
06/19/2022 19:57:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 19:57:16 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 19:57:16 - INFO - __main__ - Printing 3 examples
06/19/2022 19:57:16 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 19:57:16 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:16 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 19:57:16 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:57:16 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 19:57:16 - INFO - __main__ - ['not_equivalent']
06/19/2022 19:57:16 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:57:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:57:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:57:16 - INFO - __main__ - Printing 3 examples
06/19/2022 19:57:16 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 19:57:16 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:16 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 19:57:16 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:16 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 19:57:16 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 19:57:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:57:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:57:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:57:16 - INFO - __main__ - Printing 3 examples
06/19/2022 19:57:16 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 19:57:16 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:16 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 19:57:16 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:16 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 19:57:16 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:16 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:57:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:57:16 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:57:16 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 19:57:22 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:57:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:57:23 - INFO - __main__ - Starting training!
06/19/2022 19:57:24 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.4_8_predictions.txt
06/19/2022 19:57:24 - INFO - __main__ - ACC on test data: 0.5735
06/19/2022 19:57:24 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.4, bsz=8, dev_performance=0.65625, test_performance=0.5735294117647058
06/19/2022 19:57:24 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.3, bsz=8 ...
06/19/2022 19:57:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:57:25 - INFO - __main__ - Printing 3 examples
06/19/2022 19:57:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 19:57:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 19:57:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:25 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 19:57:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 19:57:25 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:57:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 19:57:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 19:57:25 - INFO - __main__ - Printing 3 examples
06/19/2022 19:57:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 19:57:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:25 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 19:57:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:25 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 19:57:25 - INFO - __main__ - ['equivalent']
06/19/2022 19:57:25 - INFO - __main__ - Tokenizing Input ...
06/19/2022 19:57:25 - INFO - __main__ - Tokenizing Output ...
06/19/2022 19:57:26 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 19:57:31 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 19:57:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 19:57:32 - INFO - __main__ - Starting training!
06/19/2022 19:57:33 - INFO - __main__ - Step 10 Global step 10 Train loss 6.85 on epoch=4
06/19/2022 19:57:34 - INFO - __main__ - Step 20 Global step 20 Train loss 6.91 on epoch=9
06/19/2022 19:57:36 - INFO - __main__ - Step 30 Global step 30 Train loss 6.90 on epoch=14
06/19/2022 19:57:37 - INFO - __main__ - Step 40 Global step 40 Train loss 6.87 on epoch=19
06/19/2022 19:57:38 - INFO - __main__ - Step 50 Global step 50 Train loss 6.82 on epoch=24
06/19/2022 19:57:41 - INFO - __main__ - Global step 50 Train loss 6.87 ACC 0.0 on epoch=24
06/19/2022 19:57:41 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 19:57:42 - INFO - __main__ - Step 60 Global step 60 Train loss 6.79 on epoch=29
06/19/2022 19:57:44 - INFO - __main__ - Step 70 Global step 70 Train loss 6.87 on epoch=34
06/19/2022 19:57:45 - INFO - __main__ - Step 80 Global step 80 Train loss 6.72 on epoch=39
06/19/2022 19:57:46 - INFO - __main__ - Step 90 Global step 90 Train loss 6.79 on epoch=44
06/19/2022 19:57:48 - INFO - __main__ - Step 100 Global step 100 Train loss 6.74 on epoch=49
06/19/2022 19:57:48 - INFO - __main__ - Global step 100 Train loss 6.78 ACC 0.0 on epoch=49
06/19/2022 19:57:50 - INFO - __main__ - Step 110 Global step 110 Train loss 6.70 on epoch=54
06/19/2022 19:57:51 - INFO - __main__ - Step 120 Global step 120 Train loss 6.70 on epoch=59
06/19/2022 19:57:52 - INFO - __main__ - Step 130 Global step 130 Train loss 6.68 on epoch=64
06/19/2022 19:57:54 - INFO - __main__ - Step 140 Global step 140 Train loss 6.66 on epoch=69
06/19/2022 19:57:55 - INFO - __main__ - Step 150 Global step 150 Train loss 6.55 on epoch=74
06/19/2022 19:58:05 - INFO - __main__ - Global step 150 Train loss 6.66 ACC 0.0 on epoch=74
06/19/2022 19:58:06 - INFO - __main__ - Step 160 Global step 160 Train loss 6.59 on epoch=79
06/19/2022 19:58:08 - INFO - __main__ - Step 170 Global step 170 Train loss 6.54 on epoch=84
06/19/2022 19:58:09 - INFO - __main__ - Step 180 Global step 180 Train loss 6.48 on epoch=89
06/19/2022 19:58:10 - INFO - __main__ - Step 190 Global step 190 Train loss 6.50 on epoch=94
06/19/2022 19:58:12 - INFO - __main__ - Step 200 Global step 200 Train loss 6.35 on epoch=99
06/19/2022 19:58:19 - INFO - __main__ - Global step 200 Train loss 6.49 ACC 0.0 on epoch=99
06/19/2022 19:58:20 - INFO - __main__ - Step 210 Global step 210 Train loss 6.44 on epoch=104
06/19/2022 19:58:21 - INFO - __main__ - Step 220 Global step 220 Train loss 6.34 on epoch=109
06/19/2022 19:58:23 - INFO - __main__ - Step 230 Global step 230 Train loss 6.27 on epoch=114
06/19/2022 19:58:24 - INFO - __main__ - Step 240 Global step 240 Train loss 6.21 on epoch=119
06/19/2022 19:58:25 - INFO - __main__ - Step 250 Global step 250 Train loss 6.20 on epoch=124
06/19/2022 19:58:35 - INFO - __main__ - Global step 250 Train loss 6.29 ACC 0.0 on epoch=124
06/19/2022 19:58:37 - INFO - __main__ - Step 260 Global step 260 Train loss 6.20 on epoch=129
06/19/2022 19:58:38 - INFO - __main__ - Step 270 Global step 270 Train loss 6.17 on epoch=134
06/19/2022 19:58:39 - INFO - __main__ - Step 280 Global step 280 Train loss 6.28 on epoch=139
06/19/2022 19:58:40 - INFO - __main__ - Step 290 Global step 290 Train loss 6.13 on epoch=144
06/19/2022 19:58:42 - INFO - __main__ - Step 300 Global step 300 Train loss 6.17 on epoch=149
06/19/2022 19:58:48 - INFO - __main__ - Global step 300 Train loss 6.19 ACC 0.0 on epoch=149
06/19/2022 19:58:49 - INFO - __main__ - Step 310 Global step 310 Train loss 6.13 on epoch=154
06/19/2022 19:58:50 - INFO - __main__ - Step 320 Global step 320 Train loss 6.08 on epoch=159
06/19/2022 19:58:52 - INFO - __main__ - Step 330 Global step 330 Train loss 6.01 on epoch=164
06/19/2022 19:58:53 - INFO - __main__ - Step 340 Global step 340 Train loss 6.06 on epoch=169
06/19/2022 19:58:54 - INFO - __main__ - Step 350 Global step 350 Train loss 5.99 on epoch=174
06/19/2022 19:59:00 - INFO - __main__ - Global step 350 Train loss 6.05 ACC 0.0 on epoch=174
06/19/2022 19:59:01 - INFO - __main__ - Step 360 Global step 360 Train loss 5.99 on epoch=179
06/19/2022 19:59:02 - INFO - __main__ - Step 370 Global step 370 Train loss 5.99 on epoch=184
06/19/2022 19:59:04 - INFO - __main__ - Step 380 Global step 380 Train loss 5.97 on epoch=189
06/19/2022 19:59:05 - INFO - __main__ - Step 390 Global step 390 Train loss 5.81 on epoch=194
06/19/2022 19:59:06 - INFO - __main__ - Step 400 Global step 400 Train loss 5.89 on epoch=199
06/19/2022 19:59:15 - INFO - __main__ - Global step 400 Train loss 5.93 ACC 0.0 on epoch=199
06/19/2022 19:59:16 - INFO - __main__ - Step 410 Global step 410 Train loss 5.77 on epoch=204
06/19/2022 19:59:17 - INFO - __main__ - Step 420 Global step 420 Train loss 5.74 on epoch=209
06/19/2022 19:59:18 - INFO - __main__ - Step 430 Global step 430 Train loss 5.78 on epoch=214
06/19/2022 19:59:20 - INFO - __main__ - Step 440 Global step 440 Train loss 5.59 on epoch=219
06/19/2022 19:59:21 - INFO - __main__ - Step 450 Global step 450 Train loss 5.58 on epoch=224
06/19/2022 19:59:23 - INFO - __main__ - Global step 450 Train loss 5.69 ACC 0.0 on epoch=224
06/19/2022 19:59:24 - INFO - __main__ - Step 460 Global step 460 Train loss 5.59 on epoch=229
06/19/2022 19:59:25 - INFO - __main__ - Step 470 Global step 470 Train loss 5.53 on epoch=234
06/19/2022 19:59:27 - INFO - __main__ - Step 480 Global step 480 Train loss 5.63 on epoch=239
06/19/2022 19:59:28 - INFO - __main__ - Step 490 Global step 490 Train loss 5.62 on epoch=244
06/19/2022 19:59:29 - INFO - __main__ - Step 500 Global step 500 Train loss 5.62 on epoch=249
06/19/2022 19:59:35 - INFO - __main__ - Global step 500 Train loss 5.60 ACC 0.0 on epoch=249
06/19/2022 19:59:37 - INFO - __main__ - Step 510 Global step 510 Train loss 5.60 on epoch=254
06/19/2022 19:59:38 - INFO - __main__ - Step 520 Global step 520 Train loss 5.41 on epoch=259
06/19/2022 19:59:39 - INFO - __main__ - Step 530 Global step 530 Train loss 5.31 on epoch=264
06/19/2022 19:59:40 - INFO - __main__ - Step 540 Global step 540 Train loss 5.38 on epoch=269
06/19/2022 19:59:42 - INFO - __main__ - Step 550 Global step 550 Train loss 5.35 on epoch=274
06/19/2022 19:59:44 - INFO - __main__ - Global step 550 Train loss 5.41 ACC 0.0 on epoch=274
06/19/2022 19:59:45 - INFO - __main__ - Step 560 Global step 560 Train loss 5.31 on epoch=279
06/19/2022 19:59:47 - INFO - __main__ - Step 570 Global step 570 Train loss 5.41 on epoch=284
06/19/2022 19:59:48 - INFO - __main__ - Step 580 Global step 580 Train loss 5.23 on epoch=289
06/19/2022 19:59:49 - INFO - __main__ - Step 590 Global step 590 Train loss 5.20 on epoch=294
06/19/2022 19:59:50 - INFO - __main__ - Step 600 Global step 600 Train loss 5.11 on epoch=299
06/19/2022 19:59:53 - INFO - __main__ - Global step 600 Train loss 5.25 ACC 0.0 on epoch=299
06/19/2022 19:59:55 - INFO - __main__ - Step 610 Global step 610 Train loss 5.22 on epoch=304
06/19/2022 19:59:56 - INFO - __main__ - Step 620 Global step 620 Train loss 5.14 on epoch=309
06/19/2022 19:59:57 - INFO - __main__ - Step 630 Global step 630 Train loss 5.02 on epoch=314
06/19/2022 19:59:59 - INFO - __main__ - Step 640 Global step 640 Train loss 5.07 on epoch=319
06/19/2022 20:00:00 - INFO - __main__ - Step 650 Global step 650 Train loss 5.01 on epoch=324
06/19/2022 20:00:01 - INFO - __main__ - Global step 650 Train loss 5.09 ACC 0.0 on epoch=324
06/19/2022 20:00:02 - INFO - __main__ - Step 660 Global step 660 Train loss 5.04 on epoch=329
06/19/2022 20:00:03 - INFO - __main__ - Step 670 Global step 670 Train loss 5.04 on epoch=334
06/19/2022 20:00:05 - INFO - __main__ - Step 680 Global step 680 Train loss 5.15 on epoch=339
06/19/2022 20:00:06 - INFO - __main__ - Step 690 Global step 690 Train loss 5.06 on epoch=344
06/19/2022 20:00:07 - INFO - __main__ - Step 700 Global step 700 Train loss 4.95 on epoch=349
06/19/2022 20:00:13 - INFO - __main__ - Global step 700 Train loss 5.05 ACC 0.0 on epoch=349
06/19/2022 20:00:14 - INFO - __main__ - Step 710 Global step 710 Train loss 4.80 on epoch=354
06/19/2022 20:00:16 - INFO - __main__ - Step 720 Global step 720 Train loss 4.78 on epoch=359
06/19/2022 20:00:17 - INFO - __main__ - Step 730 Global step 730 Train loss 4.86 on epoch=364
06/19/2022 20:00:18 - INFO - __main__ - Step 740 Global step 740 Train loss 4.79 on epoch=369
06/19/2022 20:00:19 - INFO - __main__ - Step 750 Global step 750 Train loss 4.72 on epoch=374
06/19/2022 20:00:21 - INFO - __main__ - Global step 750 Train loss 4.79 ACC 0.0 on epoch=374
06/19/2022 20:00:22 - INFO - __main__ - Step 760 Global step 760 Train loss 4.61 on epoch=379
06/19/2022 20:00:23 - INFO - __main__ - Step 770 Global step 770 Train loss 4.76 on epoch=384
06/19/2022 20:00:25 - INFO - __main__ - Step 780 Global step 780 Train loss 4.61 on epoch=389
06/19/2022 20:00:26 - INFO - __main__ - Step 790 Global step 790 Train loss 4.58 on epoch=394
06/19/2022 20:00:27 - INFO - __main__ - Step 800 Global step 800 Train loss 4.46 on epoch=399
06/19/2022 20:00:28 - INFO - __main__ - Global step 800 Train loss 4.60 ACC 0.0 on epoch=399
06/19/2022 20:00:29 - INFO - __main__ - Step 810 Global step 810 Train loss 4.51 on epoch=404
06/19/2022 20:00:31 - INFO - __main__ - Step 820 Global step 820 Train loss 4.35 on epoch=409
06/19/2022 20:00:32 - INFO - __main__ - Step 830 Global step 830 Train loss 4.49 on epoch=414
06/19/2022 20:00:33 - INFO - __main__ - Step 840 Global step 840 Train loss 4.42 on epoch=419
06/19/2022 20:00:34 - INFO - __main__ - Step 850 Global step 850 Train loss 4.34 on epoch=424
06/19/2022 20:00:35 - INFO - __main__ - Global step 850 Train loss 4.42 ACC 0.0 on epoch=424
06/19/2022 20:00:37 - INFO - __main__ - Step 860 Global step 860 Train loss 4.22 on epoch=429
06/19/2022 20:00:38 - INFO - __main__ - Step 870 Global step 870 Train loss 4.26 on epoch=434
06/19/2022 20:00:39 - INFO - __main__ - Step 880 Global step 880 Train loss 4.12 on epoch=439
06/19/2022 20:00:40 - INFO - __main__ - Step 890 Global step 890 Train loss 4.08 on epoch=444
06/19/2022 20:00:42 - INFO - __main__ - Step 900 Global step 900 Train loss 4.08 on epoch=449
06/19/2022 20:00:42 - INFO - __main__ - Global step 900 Train loss 4.15 ACC 0.0 on epoch=449
06/19/2022 20:00:44 - INFO - __main__ - Step 910 Global step 910 Train loss 4.22 on epoch=454
06/19/2022 20:00:45 - INFO - __main__ - Step 920 Global step 920 Train loss 4.05 on epoch=459
06/19/2022 20:00:46 - INFO - __main__ - Step 930 Global step 930 Train loss 3.96 on epoch=464
06/19/2022 20:00:47 - INFO - __main__ - Step 940 Global step 940 Train loss 3.94 on epoch=469
06/19/2022 20:00:49 - INFO - __main__ - Step 950 Global step 950 Train loss 3.89 on epoch=474
06/19/2022 20:00:50 - INFO - __main__ - Global step 950 Train loss 4.01 ACC 0.0 on epoch=474
06/19/2022 20:00:51 - INFO - __main__ - Step 960 Global step 960 Train loss 3.86 on epoch=479
06/19/2022 20:00:52 - INFO - __main__ - Step 970 Global step 970 Train loss 3.69 on epoch=484
06/19/2022 20:00:54 - INFO - __main__ - Step 980 Global step 980 Train loss 3.74 on epoch=489
06/19/2022 20:00:55 - INFO - __main__ - Step 990 Global step 990 Train loss 3.84 on epoch=494
06/19/2022 20:00:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.74 on epoch=499
06/19/2022 20:00:57 - INFO - __main__ - Global step 1000 Train loss 3.77 ACC 0.0 on epoch=499
06/19/2022 20:00:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.71 on epoch=504
06/19/2022 20:01:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.71 on epoch=509
06/19/2022 20:01:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.65 on epoch=514
06/19/2022 20:01:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.48 on epoch=519
06/19/2022 20:01:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.58 on epoch=524
06/19/2022 20:01:05 - INFO - __main__ - Global step 1050 Train loss 3.63 ACC 0.0 on epoch=524
06/19/2022 20:01:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.49 on epoch=529
06/19/2022 20:01:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.42 on epoch=534
06/19/2022 20:01:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.30 on epoch=539
06/19/2022 20:01:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.40 on epoch=544
06/19/2022 20:01:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.35 on epoch=549
06/19/2022 20:01:15 - INFO - __main__ - Global step 1100 Train loss 3.39 ACC 0.0 on epoch=549
06/19/2022 20:01:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.27 on epoch=554
06/19/2022 20:01:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.25 on epoch=559
06/19/2022 20:01:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.17 on epoch=564
06/19/2022 20:01:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.17 on epoch=569
06/19/2022 20:01:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.25 on epoch=574
06/19/2022 20:01:24 - INFO - __main__ - Global step 1150 Train loss 3.22 ACC 0.0 on epoch=574
06/19/2022 20:01:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.91 on epoch=579
06/19/2022 20:01:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.97 on epoch=584
06/19/2022 20:01:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.05 on epoch=589
06/19/2022 20:01:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.05 on epoch=594
06/19/2022 20:01:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.96 on epoch=599
06/19/2022 20:01:34 - INFO - __main__ - Global step 1200 Train loss 2.99 ACC 0.0 on epoch=599
06/19/2022 20:01:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.86 on epoch=604
06/19/2022 20:01:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.75 on epoch=609
06/19/2022 20:01:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.75 on epoch=614
06/19/2022 20:01:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.64 on epoch=619
06/19/2022 20:01:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.63 on epoch=624
06/19/2022 20:01:42 - INFO - __main__ - Global step 1250 Train loss 2.73 ACC 0.0 on epoch=624
06/19/2022 20:01:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.64 on epoch=629
06/19/2022 20:01:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.53 on epoch=634
06/19/2022 20:01:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.44 on epoch=639
06/19/2022 20:01:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.53 on epoch=644
06/19/2022 20:01:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.42 on epoch=649
06/19/2022 20:01:51 - INFO - __main__ - Global step 1300 Train loss 2.51 ACC 0.0 on epoch=649
06/19/2022 20:01:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.45 on epoch=654
06/19/2022 20:01:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.45 on epoch=659
06/19/2022 20:01:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.36 on epoch=664
06/19/2022 20:01:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.44 on epoch=669
06/19/2022 20:01:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.37 on epoch=674
06/19/2022 20:01:59 - INFO - __main__ - Global step 1350 Train loss 2.41 ACC 0.0 on epoch=674
06/19/2022 20:02:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.43 on epoch=679
06/19/2022 20:02:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.35 on epoch=684
06/19/2022 20:02:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.29 on epoch=689
06/19/2022 20:02:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.19 on epoch=694
06/19/2022 20:02:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.19 on epoch=699
06/19/2022 20:02:08 - INFO - __main__ - Global step 1400 Train loss 2.29 ACC 0.0 on epoch=699
06/19/2022 20:02:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.19 on epoch=704
06/19/2022 20:02:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.11 on epoch=709
06/19/2022 20:02:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.11 on epoch=714
06/19/2022 20:02:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.10 on epoch=719
06/19/2022 20:02:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.14 on epoch=724
06/19/2022 20:02:17 - INFO - __main__ - Global step 1450 Train loss 2.13 ACC 0.0 on epoch=724
06/19/2022 20:02:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.11 on epoch=729
06/19/2022 20:02:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.05 on epoch=734
06/19/2022 20:02:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.00 on epoch=739
06/19/2022 20:02:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.04 on epoch=744
06/19/2022 20:02:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.95 on epoch=749
06/19/2022 20:02:25 - INFO - __main__ - Global step 1500 Train loss 2.03 ACC 0.21875 on epoch=749
06/19/2022 20:02:25 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.21875 on epoch=749, global_step=1500
06/19/2022 20:02:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.98 on epoch=754
06/19/2022 20:02:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.90 on epoch=759
06/19/2022 20:02:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.90 on epoch=764
06/19/2022 20:02:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.83 on epoch=769
06/19/2022 20:02:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.86 on epoch=774
06/19/2022 20:02:32 - INFO - __main__ - Global step 1550 Train loss 1.89 ACC 0.5 on epoch=774
06/19/2022 20:02:32 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.5 on epoch=774, global_step=1550
06/19/2022 20:02:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.95 on epoch=779
06/19/2022 20:02:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.88 on epoch=784
06/19/2022 20:02:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.79 on epoch=789
06/19/2022 20:02:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.82 on epoch=794
06/19/2022 20:02:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.84 on epoch=799
06/19/2022 20:02:39 - INFO - __main__ - Global step 1600 Train loss 1.86 ACC 0.5 on epoch=799
06/19/2022 20:02:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.73 on epoch=804
06/19/2022 20:02:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.78 on epoch=809
06/19/2022 20:02:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.60 on epoch=814
06/19/2022 20:02:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.66 on epoch=819
06/19/2022 20:02:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.68 on epoch=824
06/19/2022 20:02:46 - INFO - __main__ - Global step 1650 Train loss 1.69 ACC 0.5 on epoch=824
06/19/2022 20:02:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.61 on epoch=829
06/19/2022 20:02:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.60 on epoch=834
06/19/2022 20:02:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.52 on epoch=839
06/19/2022 20:02:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.70 on epoch=844
06/19/2022 20:02:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.51 on epoch=849
06/19/2022 20:02:55 - INFO - __main__ - Global step 1700 Train loss 1.59 ACC 0.5 on epoch=849
06/19/2022 20:02:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.68 on epoch=854
06/19/2022 20:02:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.55 on epoch=859
06/19/2022 20:02:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.53 on epoch=864
06/19/2022 20:03:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.42 on epoch=869
06/19/2022 20:03:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.43 on epoch=874
06/19/2022 20:03:03 - INFO - __main__ - Global step 1750 Train loss 1.52 ACC 0.5 on epoch=874
06/19/2022 20:03:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.38 on epoch=879
06/19/2022 20:03:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.50 on epoch=884
06/19/2022 20:03:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.40 on epoch=889
06/19/2022 20:03:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.29 on epoch=894
06/19/2022 20:03:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.34 on epoch=899
06/19/2022 20:03:11 - INFO - __main__ - Global step 1800 Train loss 1.38 ACC 0.5 on epoch=899
06/19/2022 20:03:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.32 on epoch=904
06/19/2022 20:03:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.22 on epoch=909
06/19/2022 20:03:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.24 on epoch=914
06/19/2022 20:03:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.21 on epoch=919
06/19/2022 20:03:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.25 on epoch=924
06/19/2022 20:03:19 - INFO - __main__ - Global step 1850 Train loss 1.25 ACC 0.5 on epoch=924
06/19/2022 20:03:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.20 on epoch=929
06/19/2022 20:03:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.23 on epoch=934
06/19/2022 20:03:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.20 on epoch=939
06/19/2022 20:03:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.13 on epoch=944
06/19/2022 20:03:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.10 on epoch=949
06/19/2022 20:03:27 - INFO - __main__ - Global step 1900 Train loss 1.17 ACC 0.5 on epoch=949
06/19/2022 20:03:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.06 on epoch=954
06/19/2022 20:03:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.10 on epoch=959
06/19/2022 20:03:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.07 on epoch=964
06/19/2022 20:03:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.03 on epoch=969
06/19/2022 20:03:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.94 on epoch=974
06/19/2022 20:03:40 - INFO - __main__ - Global step 1950 Train loss 1.04 ACC 0.46875 on epoch=974
06/19/2022 20:03:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.08 on epoch=979
06/19/2022 20:03:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.03 on epoch=984
06/19/2022 20:03:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.02 on epoch=989
06/19/2022 20:03:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.00 on epoch=994
06/19/2022 20:03:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.90 on epoch=999
06/19/2022 20:03:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:03:47 - INFO - __main__ - Printing 3 examples
06/19/2022 20:03:47 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 20:03:47 - INFO - __main__ - ['equivalent']
06/19/2022 20:03:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 20:03:47 - INFO - __main__ - ['equivalent']
06/19/2022 20:03:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 20:03:47 - INFO - __main__ - ['equivalent']
06/19/2022 20:03:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 20:03:47 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:03:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:03:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:03:47 - INFO - __main__ - Printing 3 examples
06/19/2022 20:03:47 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 20:03:47 - INFO - __main__ - ['equivalent']
06/19/2022 20:03:47 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 20:03:47 - INFO - __main__ - ['equivalent']
06/19/2022 20:03:47 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 20:03:47 - INFO - __main__ - ['equivalent']
06/19/2022 20:03:47 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:03:47 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:03:47 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:03:49 - INFO - __main__ - Global step 2000 Train loss 1.01 ACC 0.5625 on epoch=999
06/19/2022 20:03:49 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=999, global_step=2000
06/19/2022 20:03:49 - INFO - __main__ - save last model!
06/19/2022 20:03:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 20:03:49 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 20:03:49 - INFO - __main__ - Printing 3 examples
06/19/2022 20:03:49 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 20:03:49 - INFO - __main__ - ['equivalent']
06/19/2022 20:03:49 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 20:03:49 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:03:49 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 20:03:49 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:03:49 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:03:49 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:03:50 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 20:03:54 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:03:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:03:54 - INFO - __main__ - Starting training!
06/19/2022 20:04:40 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.3_8_predictions.txt
06/19/2022 20:04:40 - INFO - __main__ - ACC on test data: 0.3554
06/19/2022 20:04:40 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.3, bsz=8, dev_performance=0.5625, test_performance=0.3553921568627451
06/19/2022 20:04:40 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.2, bsz=8 ...
06/19/2022 20:04:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:04:41 - INFO - __main__ - Printing 3 examples
06/19/2022 20:04:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/19/2022 20:04:41 - INFO - __main__ - ['equivalent']
06/19/2022 20:04:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/19/2022 20:04:41 - INFO - __main__ - ['equivalent']
06/19/2022 20:04:41 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/19/2022 20:04:41 - INFO - __main__ - ['equivalent']
06/19/2022 20:04:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 20:04:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:04:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:04:41 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:04:41 - INFO - __main__ - Printing 3 examples
06/19/2022 20:04:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/19/2022 20:04:41 - INFO - __main__ - ['equivalent']
06/19/2022 20:04:41 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/19/2022 20:04:41 - INFO - __main__ - ['equivalent']
06/19/2022 20:04:41 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/19/2022 20:04:41 - INFO - __main__ - ['equivalent']
06/19/2022 20:04:41 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:04:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:04:41 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:04:47 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:04:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:04:47 - INFO - __main__ - Starting training!
06/19/2022 20:04:48 - INFO - __main__ - Step 10 Global step 10 Train loss 6.82 on epoch=4
06/19/2022 20:04:50 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=9
06/19/2022 20:04:51 - INFO - __main__ - Step 30 Global step 30 Train loss 6.89 on epoch=14
06/19/2022 20:04:53 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/19/2022 20:04:54 - INFO - __main__ - Step 50 Global step 50 Train loss 6.87 on epoch=24
06/19/2022 20:04:55 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/19/2022 20:04:55 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 20:04:56 - INFO - __main__ - Step 60 Global step 60 Train loss 6.85 on epoch=29
06/19/2022 20:04:58 - INFO - __main__ - Step 70 Global step 70 Train loss 6.71 on epoch=34
06/19/2022 20:04:59 - INFO - __main__ - Step 80 Global step 80 Train loss 6.84 on epoch=39
06/19/2022 20:05:00 - INFO - __main__ - Step 90 Global step 90 Train loss 6.80 on epoch=44
06/19/2022 20:05:02 - INFO - __main__ - Step 100 Global step 100 Train loss 6.79 on epoch=49
06/19/2022 20:05:03 - INFO - __main__ - Global step 100 Train loss 6.80 ACC 0.0 on epoch=49
06/19/2022 20:05:05 - INFO - __main__ - Step 110 Global step 110 Train loss 6.78 on epoch=54
06/19/2022 20:05:06 - INFO - __main__ - Step 120 Global step 120 Train loss 6.81 on epoch=59
06/19/2022 20:05:08 - INFO - __main__ - Step 130 Global step 130 Train loss 6.80 on epoch=64
06/19/2022 20:05:09 - INFO - __main__ - Step 140 Global step 140 Train loss 6.79 on epoch=69
06/19/2022 20:05:10 - INFO - __main__ - Step 150 Global step 150 Train loss 6.77 on epoch=74
06/19/2022 20:05:12 - INFO - __main__ - Global step 150 Train loss 6.79 ACC 0.0 on epoch=74
06/19/2022 20:05:13 - INFO - __main__ - Step 160 Global step 160 Train loss 6.73 on epoch=79
06/19/2022 20:05:14 - INFO - __main__ - Step 170 Global step 170 Train loss 6.76 on epoch=84
06/19/2022 20:05:16 - INFO - __main__ - Step 180 Global step 180 Train loss 6.73 on epoch=89
06/19/2022 20:05:17 - INFO - __main__ - Step 190 Global step 190 Train loss 6.76 on epoch=94
06/19/2022 20:05:18 - INFO - __main__ - Step 200 Global step 200 Train loss 6.65 on epoch=99
06/19/2022 20:05:22 - INFO - __main__ - Global step 200 Train loss 6.73 ACC 0.0 on epoch=99
06/19/2022 20:05:24 - INFO - __main__ - Step 210 Global step 210 Train loss 6.74 on epoch=104
06/19/2022 20:05:25 - INFO - __main__ - Step 220 Global step 220 Train loss 6.66 on epoch=109
06/19/2022 20:05:26 - INFO - __main__ - Step 230 Global step 230 Train loss 6.66 on epoch=114
06/19/2022 20:05:28 - INFO - __main__ - Step 240 Global step 240 Train loss 6.55 on epoch=119
06/19/2022 20:05:29 - INFO - __main__ - Step 250 Global step 250 Train loss 6.64 on epoch=124
06/19/2022 20:05:35 - INFO - __main__ - Global step 250 Train loss 6.65 ACC 0.0 on epoch=124
06/19/2022 20:05:36 - INFO - __main__ - Step 260 Global step 260 Train loss 6.51 on epoch=129
06/19/2022 20:05:38 - INFO - __main__ - Step 270 Global step 270 Train loss 6.54 on epoch=134
06/19/2022 20:05:39 - INFO - __main__ - Step 280 Global step 280 Train loss 6.58 on epoch=139
06/19/2022 20:05:40 - INFO - __main__ - Step 290 Global step 290 Train loss 6.53 on epoch=144
06/19/2022 20:05:42 - INFO - __main__ - Step 300 Global step 300 Train loss 6.49 on epoch=149
06/19/2022 20:05:48 - INFO - __main__ - Global step 300 Train loss 6.53 ACC 0.0 on epoch=149
06/19/2022 20:05:50 - INFO - __main__ - Step 310 Global step 310 Train loss 6.49 on epoch=154
06/19/2022 20:05:51 - INFO - __main__ - Step 320 Global step 320 Train loss 6.47 on epoch=159
06/19/2022 20:05:52 - INFO - __main__ - Step 330 Global step 330 Train loss 6.51 on epoch=164
06/19/2022 20:05:54 - INFO - __main__ - Step 340 Global step 340 Train loss 6.32 on epoch=169
06/19/2022 20:05:55 - INFO - __main__ - Step 350 Global step 350 Train loss 6.35 on epoch=174
06/19/2022 20:05:59 - INFO - __main__ - Global step 350 Train loss 6.43 ACC 0.0 on epoch=174
06/19/2022 20:06:00 - INFO - __main__ - Step 360 Global step 360 Train loss 6.37 on epoch=179
06/19/2022 20:06:02 - INFO - __main__ - Step 370 Global step 370 Train loss 6.30 on epoch=184
06/19/2022 20:06:03 - INFO - __main__ - Step 380 Global step 380 Train loss 6.30 on epoch=189
06/19/2022 20:06:05 - INFO - __main__ - Step 390 Global step 390 Train loss 6.33 on epoch=194
06/19/2022 20:06:06 - INFO - __main__ - Step 400 Global step 400 Train loss 6.19 on epoch=199
06/19/2022 20:06:13 - INFO - __main__ - Global step 400 Train loss 6.30 ACC 0.0 on epoch=199
06/19/2022 20:06:14 - INFO - __main__ - Step 410 Global step 410 Train loss 6.13 on epoch=204
06/19/2022 20:06:15 - INFO - __main__ - Step 420 Global step 420 Train loss 6.19 on epoch=209
06/19/2022 20:06:16 - INFO - __main__ - Step 430 Global step 430 Train loss 6.11 on epoch=214
06/19/2022 20:06:18 - INFO - __main__ - Step 440 Global step 440 Train loss 6.21 on epoch=219
06/19/2022 20:06:19 - INFO - __main__ - Step 450 Global step 450 Train loss 6.21 on epoch=224
06/19/2022 20:06:29 - INFO - __main__ - Global step 450 Train loss 6.17 ACC 0.0 on epoch=224
06/19/2022 20:06:31 - INFO - __main__ - Step 460 Global step 460 Train loss 6.21 on epoch=229
06/19/2022 20:06:32 - INFO - __main__ - Step 470 Global step 470 Train loss 6.14 on epoch=234
06/19/2022 20:06:33 - INFO - __main__ - Step 480 Global step 480 Train loss 6.07 on epoch=239
06/19/2022 20:06:34 - INFO - __main__ - Step 490 Global step 490 Train loss 6.18 on epoch=244
06/19/2022 20:06:36 - INFO - __main__ - Step 500 Global step 500 Train loss 5.99 on epoch=249
06/19/2022 20:06:46 - INFO - __main__ - Global step 500 Train loss 6.12 ACC 0.0 on epoch=249
06/19/2022 20:06:47 - INFO - __main__ - Step 510 Global step 510 Train loss 5.92 on epoch=254
06/19/2022 20:06:49 - INFO - __main__ - Step 520 Global step 520 Train loss 6.03 on epoch=259
06/19/2022 20:06:50 - INFO - __main__ - Step 530 Global step 530 Train loss 5.94 on epoch=264
06/19/2022 20:06:51 - INFO - __main__ - Step 540 Global step 540 Train loss 5.90 on epoch=269
06/19/2022 20:06:53 - INFO - __main__ - Step 550 Global step 550 Train loss 5.91 on epoch=274
06/19/2022 20:07:03 - INFO - __main__ - Global step 550 Train loss 5.94 ACC 0.0 on epoch=274
06/19/2022 20:07:04 - INFO - __main__ - Step 560 Global step 560 Train loss 5.92 on epoch=279
06/19/2022 20:07:06 - INFO - __main__ - Step 570 Global step 570 Train loss 5.84 on epoch=284
06/19/2022 20:07:07 - INFO - __main__ - Step 580 Global step 580 Train loss 5.88 on epoch=289
06/19/2022 20:07:08 - INFO - __main__ - Step 590 Global step 590 Train loss 5.84 on epoch=294
06/19/2022 20:07:10 - INFO - __main__ - Step 600 Global step 600 Train loss 5.79 on epoch=299
06/19/2022 20:07:17 - INFO - __main__ - Global step 600 Train loss 5.86 ACC 0.0 on epoch=299
06/19/2022 20:07:19 - INFO - __main__ - Step 610 Global step 610 Train loss 5.77 on epoch=304
06/19/2022 20:07:20 - INFO - __main__ - Step 620 Global step 620 Train loss 5.83 on epoch=309
06/19/2022 20:07:21 - INFO - __main__ - Step 630 Global step 630 Train loss 5.85 on epoch=314
06/19/2022 20:07:22 - INFO - __main__ - Step 640 Global step 640 Train loss 5.79 on epoch=319
06/19/2022 20:07:24 - INFO - __main__ - Step 650 Global step 650 Train loss 5.75 on epoch=324
06/19/2022 20:07:34 - INFO - __main__ - Global step 650 Train loss 5.80 ACC 0.0 on epoch=324
06/19/2022 20:07:36 - INFO - __main__ - Step 660 Global step 660 Train loss 5.79 on epoch=329
06/19/2022 20:07:37 - INFO - __main__ - Step 670 Global step 670 Train loss 5.69 on epoch=334
06/19/2022 20:07:38 - INFO - __main__ - Step 680 Global step 680 Train loss 5.71 on epoch=339
06/19/2022 20:07:39 - INFO - __main__ - Step 690 Global step 690 Train loss 5.66 on epoch=344
06/19/2022 20:07:41 - INFO - __main__ - Step 700 Global step 700 Train loss 5.65 on epoch=349
06/19/2022 20:07:48 - INFO - __main__ - Global step 700 Train loss 5.70 ACC 0.0 on epoch=349
06/19/2022 20:07:50 - INFO - __main__ - Step 710 Global step 710 Train loss 5.60 on epoch=354
06/19/2022 20:07:51 - INFO - __main__ - Step 720 Global step 720 Train loss 5.68 on epoch=359
06/19/2022 20:07:52 - INFO - __main__ - Step 730 Global step 730 Train loss 5.65 on epoch=364
06/19/2022 20:07:54 - INFO - __main__ - Step 740 Global step 740 Train loss 5.65 on epoch=369
06/19/2022 20:07:55 - INFO - __main__ - Step 750 Global step 750 Train loss 5.63 on epoch=374
06/19/2022 20:08:05 - INFO - __main__ - Global step 750 Train loss 5.64 ACC 0.0 on epoch=374
06/19/2022 20:08:06 - INFO - __main__ - Step 760 Global step 760 Train loss 5.50 on epoch=379
06/19/2022 20:08:08 - INFO - __main__ - Step 770 Global step 770 Train loss 5.52 on epoch=384
06/19/2022 20:08:09 - INFO - __main__ - Step 780 Global step 780 Train loss 5.52 on epoch=389
06/19/2022 20:08:10 - INFO - __main__ - Step 790 Global step 790 Train loss 5.41 on epoch=394
06/19/2022 20:08:12 - INFO - __main__ - Step 800 Global step 800 Train loss 5.47 on epoch=399
06/19/2022 20:08:18 - INFO - __main__ - Global step 800 Train loss 5.48 ACC 0.0 on epoch=399
06/19/2022 20:08:19 - INFO - __main__ - Step 810 Global step 810 Train loss 5.37 on epoch=404
06/19/2022 20:08:21 - INFO - __main__ - Step 820 Global step 820 Train loss 5.41 on epoch=409
06/19/2022 20:08:22 - INFO - __main__ - Step 830 Global step 830 Train loss 5.40 on epoch=414
06/19/2022 20:08:23 - INFO - __main__ - Step 840 Global step 840 Train loss 5.38 on epoch=419
06/19/2022 20:08:24 - INFO - __main__ - Step 850 Global step 850 Train loss 5.21 on epoch=424
06/19/2022 20:08:27 - INFO - __main__ - Global step 850 Train loss 5.35 ACC 0.0 on epoch=424
06/19/2022 20:08:28 - INFO - __main__ - Step 860 Global step 860 Train loss 5.21 on epoch=429
06/19/2022 20:08:29 - INFO - __main__ - Step 870 Global step 870 Train loss 5.34 on epoch=434
06/19/2022 20:08:31 - INFO - __main__ - Step 880 Global step 880 Train loss 5.14 on epoch=439
06/19/2022 20:08:32 - INFO - __main__ - Step 890 Global step 890 Train loss 5.12 on epoch=444
06/19/2022 20:08:33 - INFO - __main__ - Step 900 Global step 900 Train loss 5.20 on epoch=449
06/19/2022 20:08:38 - INFO - __main__ - Global step 900 Train loss 5.20 ACC 0.0 on epoch=449
06/19/2022 20:08:39 - INFO - __main__ - Step 910 Global step 910 Train loss 5.14 on epoch=454
06/19/2022 20:08:40 - INFO - __main__ - Step 920 Global step 920 Train loss 5.14 on epoch=459
06/19/2022 20:08:42 - INFO - __main__ - Step 930 Global step 930 Train loss 5.02 on epoch=464
06/19/2022 20:08:43 - INFO - __main__ - Step 940 Global step 940 Train loss 5.01 on epoch=469
06/19/2022 20:08:44 - INFO - __main__ - Step 950 Global step 950 Train loss 4.96 on epoch=474
06/19/2022 20:08:45 - INFO - __main__ - Global step 950 Train loss 5.05 ACC 0.0 on epoch=474
06/19/2022 20:08:47 - INFO - __main__ - Step 960 Global step 960 Train loss 4.93 on epoch=479
06/19/2022 20:08:48 - INFO - __main__ - Step 970 Global step 970 Train loss 4.87 on epoch=484
06/19/2022 20:08:49 - INFO - __main__ - Step 980 Global step 980 Train loss 4.82 on epoch=489
06/19/2022 20:08:50 - INFO - __main__ - Step 990 Global step 990 Train loss 4.69 on epoch=494
06/19/2022 20:08:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.68 on epoch=499
06/19/2022 20:08:54 - INFO - __main__ - Global step 1000 Train loss 4.80 ACC 0.0 on epoch=499
06/19/2022 20:08:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.60 on epoch=504
06/19/2022 20:08:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.56 on epoch=509
06/19/2022 20:08:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 4.54 on epoch=514
06/19/2022 20:08:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 4.59 on epoch=519
06/19/2022 20:09:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 4.51 on epoch=524
06/19/2022 20:09:03 - INFO - __main__ - Global step 1050 Train loss 4.56 ACC 0.0 on epoch=524
06/19/2022 20:09:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.44 on epoch=529
06/19/2022 20:09:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.24 on epoch=534
06/19/2022 20:09:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.19 on epoch=539
06/19/2022 20:09:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 4.20 on epoch=544
06/19/2022 20:09:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.23 on epoch=549
06/19/2022 20:09:13 - INFO - __main__ - Global step 1100 Train loss 4.26 ACC 0.0 on epoch=549
06/19/2022 20:09:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 4.18 on epoch=554
06/19/2022 20:09:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 4.02 on epoch=559
06/19/2022 20:09:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.98 on epoch=564
06/19/2022 20:09:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.81 on epoch=569
06/19/2022 20:09:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.89 on epoch=574
06/19/2022 20:09:22 - INFO - __main__ - Global step 1150 Train loss 3.98 ACC 0.0 on epoch=574
06/19/2022 20:09:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.71 on epoch=579
06/19/2022 20:09:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.84 on epoch=584
06/19/2022 20:09:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.77 on epoch=589
06/19/2022 20:09:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.54 on epoch=594
06/19/2022 20:09:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.53 on epoch=599
06/19/2022 20:09:32 - INFO - __main__ - Global step 1200 Train loss 3.68 ACC 0.0 on epoch=599
06/19/2022 20:09:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.61 on epoch=604
06/19/2022 20:09:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.60 on epoch=609
06/19/2022 20:09:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.58 on epoch=614
06/19/2022 20:09:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.60 on epoch=619
06/19/2022 20:09:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.42 on epoch=624
06/19/2022 20:09:45 - INFO - __main__ - Global step 1250 Train loss 3.56 ACC 0.0 on epoch=624
06/19/2022 20:09:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.46 on epoch=629
06/19/2022 20:09:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.41 on epoch=634
06/19/2022 20:09:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.28 on epoch=639
06/19/2022 20:09:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.35 on epoch=644
06/19/2022 20:09:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.27 on epoch=649
06/19/2022 20:09:54 - INFO - __main__ - Global step 1300 Train loss 3.35 ACC 0.0 on epoch=649
06/19/2022 20:09:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.23 on epoch=654
06/19/2022 20:09:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.23 on epoch=659
06/19/2022 20:09:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.15 on epoch=664
06/19/2022 20:09:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.09 on epoch=669
06/19/2022 20:10:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.04 on epoch=674
06/19/2022 20:10:07 - INFO - __main__ - Global step 1350 Train loss 3.15 ACC 0.09375 on epoch=674
06/19/2022 20:10:07 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=674, global_step=1350
06/19/2022 20:10:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 3.03 on epoch=679
06/19/2022 20:10:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.13 on epoch=684
06/19/2022 20:10:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.86 on epoch=689
06/19/2022 20:10:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.81 on epoch=694
06/19/2022 20:10:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.87 on epoch=699
06/19/2022 20:10:16 - INFO - __main__ - Global step 1400 Train loss 2.94 ACC 0.21875 on epoch=699
06/19/2022 20:10:16 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.21875 on epoch=699, global_step=1400
06/19/2022 20:10:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.72 on epoch=704
06/19/2022 20:10:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.72 on epoch=709
06/19/2022 20:10:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.75 on epoch=714
06/19/2022 20:10:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.58 on epoch=719
06/19/2022 20:10:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.71 on epoch=724
06/19/2022 20:10:29 - INFO - __main__ - Global step 1450 Train loss 2.70 ACC 0.375 on epoch=724
06/19/2022 20:10:29 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.375 on epoch=724, global_step=1450
06/19/2022 20:10:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.59 on epoch=729
06/19/2022 20:10:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.58 on epoch=734
06/19/2022 20:10:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.45 on epoch=739
06/19/2022 20:10:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.46 on epoch=744
06/19/2022 20:10:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.35 on epoch=749
06/19/2022 20:10:38 - INFO - __main__ - Global step 1500 Train loss 2.49 ACC 0.46875 on epoch=749
06/19/2022 20:10:38 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.46875 on epoch=749, global_step=1500
06/19/2022 20:10:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.40 on epoch=754
06/19/2022 20:10:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.34 on epoch=759
06/19/2022 20:10:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.21 on epoch=764
06/19/2022 20:10:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 2.25 on epoch=769
06/19/2022 20:10:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.11 on epoch=774
06/19/2022 20:10:48 - INFO - __main__ - Global step 1550 Train loss 2.26 ACC 0.5 on epoch=774
06/19/2022 20:10:48 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=774, global_step=1550
06/19/2022 20:10:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 2.07 on epoch=779
06/19/2022 20:10:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 2.02 on epoch=784
06/19/2022 20:10:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 2.13 on epoch=789
06/19/2022 20:10:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.03 on epoch=794
06/19/2022 20:10:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 2.15 on epoch=799
06/19/2022 20:10:59 - INFO - __main__ - Global step 1600 Train loss 2.08 ACC 0.5 on epoch=799
06/19/2022 20:11:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.90 on epoch=804
06/19/2022 20:11:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.05 on epoch=809
06/19/2022 20:11:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.88 on epoch=814
06/19/2022 20:11:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.91 on epoch=819
06/19/2022 20:11:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.99 on epoch=824
06/19/2022 20:11:09 - INFO - __main__ - Global step 1650 Train loss 1.95 ACC 0.46875 on epoch=824
06/19/2022 20:11:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.80 on epoch=829
06/19/2022 20:11:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.81 on epoch=834
06/19/2022 20:11:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.82 on epoch=839
06/19/2022 20:11:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.70 on epoch=844
06/19/2022 20:11:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.77 on epoch=849
06/19/2022 20:11:21 - INFO - __main__ - Global step 1700 Train loss 1.78 ACC 0.5 on epoch=849
06/19/2022 20:11:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.67 on epoch=854
06/19/2022 20:11:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.76 on epoch=859
06/19/2022 20:11:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.70 on epoch=864
06/19/2022 20:11:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.61 on epoch=869
06/19/2022 20:11:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.69 on epoch=874
06/19/2022 20:11:31 - INFO - __main__ - Global step 1750 Train loss 1.69 ACC 0.46875 on epoch=874
06/19/2022 20:11:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.68 on epoch=879
06/19/2022 20:11:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.67 on epoch=884
06/19/2022 20:11:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.51 on epoch=889
06/19/2022 20:11:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.61 on epoch=894
06/19/2022 20:11:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.55 on epoch=899
06/19/2022 20:11:39 - INFO - __main__ - Global step 1800 Train loss 1.60 ACC 0.53125 on epoch=899
06/19/2022 20:11:39 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=899, global_step=1800
06/19/2022 20:11:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.51 on epoch=904
06/19/2022 20:11:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.53 on epoch=909
06/19/2022 20:11:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.44 on epoch=914
06/19/2022 20:11:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.36 on epoch=919
06/19/2022 20:11:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.29 on epoch=924
06/19/2022 20:11:51 - INFO - __main__ - Global step 1850 Train loss 1.43 ACC 0.5 on epoch=924
06/19/2022 20:11:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.25 on epoch=929
06/19/2022 20:11:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.30 on epoch=934
06/19/2022 20:11:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.35 on epoch=939
06/19/2022 20:11:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.34 on epoch=944
06/19/2022 20:11:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.27 on epoch=949
06/19/2022 20:12:01 - INFO - __main__ - Global step 1900 Train loss 1.30 ACC 0.5 on epoch=949
06/19/2022 20:12:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.25 on epoch=954
06/19/2022 20:12:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.27 on epoch=959
06/19/2022 20:12:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.28 on epoch=964
06/19/2022 20:12:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.24 on epoch=969
06/19/2022 20:12:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.19 on epoch=974
06/19/2022 20:12:10 - INFO - __main__ - Global step 1950 Train loss 1.24 ACC 0.5 on epoch=974
06/19/2022 20:12:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.14 on epoch=979
06/19/2022 20:12:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.19 on epoch=984
06/19/2022 20:12:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.03 on epoch=989
06/19/2022 20:12:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.04 on epoch=994
06/19/2022 20:12:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.10 on epoch=999
06/19/2022 20:12:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:12:17 - INFO - __main__ - Printing 3 examples
06/19/2022 20:12:17 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 20:12:17 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:17 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 20:12:17 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:17 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 20:12:17 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 20:12:17 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:12:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:12:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:12:18 - INFO - __main__ - Printing 3 examples
06/19/2022 20:12:18 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 20:12:18 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:18 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 20:12:18 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 20:12:18 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:18 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:12:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:12:18 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:12:18 - INFO - __main__ - Global step 2000 Train loss 1.10 ACC 0.5 on epoch=999
06/19/2022 20:12:18 - INFO - __main__ - save last model!
06/19/2022 20:12:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 20:12:19 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 20:12:19 - INFO - __main__ - Printing 3 examples
06/19/2022 20:12:19 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 20:12:19 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:19 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 20:12:19 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:12:19 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 20:12:19 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:12:19 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:12:19 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:12:19 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 20:12:23 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:12:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:12:23 - INFO - __main__ - Starting training!
06/19/2022 20:12:41 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.2_8_predictions.txt
06/19/2022 20:12:41 - INFO - __main__ - ACC on test data: 0.6838
06/19/2022 20:12:42 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.2, bsz=8, dev_performance=0.53125, test_performance=0.6838235294117647
06/19/2022 20:12:42 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.5, bsz=8 ...
06/19/2022 20:12:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:12:43 - INFO - __main__ - Printing 3 examples
06/19/2022 20:12:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 20:12:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:43 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 20:12:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 20:12:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 20:12:43 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:12:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:12:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:12:43 - INFO - __main__ - Printing 3 examples
06/19/2022 20:12:43 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 20:12:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:43 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 20:12:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 20:12:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:12:43 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:12:43 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:12:43 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:12:48 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:12:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:12:48 - INFO - __main__ - Starting training!
06/19/2022 20:12:50 - INFO - __main__ - Step 10 Global step 10 Train loss 6.84 on epoch=4
06/19/2022 20:12:51 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=9
06/19/2022 20:12:52 - INFO - __main__ - Step 30 Global step 30 Train loss 6.79 on epoch=14
06/19/2022 20:12:53 - INFO - __main__ - Step 40 Global step 40 Train loss 6.81 on epoch=19
06/19/2022 20:12:55 - INFO - __main__ - Step 50 Global step 50 Train loss 6.73 on epoch=24
06/19/2022 20:12:56 - INFO - __main__ - Global step 50 Train loss 6.80 ACC 0.0 on epoch=24
06/19/2022 20:12:56 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 20:12:57 - INFO - __main__ - Step 60 Global step 60 Train loss 6.68 on epoch=29
06/19/2022 20:12:59 - INFO - __main__ - Step 70 Global step 70 Train loss 6.74 on epoch=34
06/19/2022 20:13:00 - INFO - __main__ - Step 80 Global step 80 Train loss 6.75 on epoch=39
06/19/2022 20:13:01 - INFO - __main__ - Step 90 Global step 90 Train loss 6.72 on epoch=44
06/19/2022 20:13:02 - INFO - __main__ - Step 100 Global step 100 Train loss 6.58 on epoch=49
06/19/2022 20:13:03 - INFO - __main__ - Global step 100 Train loss 6.69 ACC 0.0 on epoch=49
06/19/2022 20:13:05 - INFO - __main__ - Step 110 Global step 110 Train loss 6.55 on epoch=54
06/19/2022 20:13:06 - INFO - __main__ - Step 120 Global step 120 Train loss 6.44 on epoch=59
06/19/2022 20:13:07 - INFO - __main__ - Step 130 Global step 130 Train loss 6.42 on epoch=64
06/19/2022 20:13:08 - INFO - __main__ - Step 140 Global step 140 Train loss 6.36 on epoch=69
06/19/2022 20:13:10 - INFO - __main__ - Step 150 Global step 150 Train loss 6.32 on epoch=74
06/19/2022 20:13:14 - INFO - __main__ - Global step 150 Train loss 6.42 ACC 0.0 on epoch=74
06/19/2022 20:13:16 - INFO - __main__ - Step 160 Global step 160 Train loss 6.18 on epoch=79
06/19/2022 20:13:17 - INFO - __main__ - Step 170 Global step 170 Train loss 6.12 on epoch=84
06/19/2022 20:13:18 - INFO - __main__ - Step 180 Global step 180 Train loss 6.23 on epoch=89
06/19/2022 20:13:20 - INFO - __main__ - Step 190 Global step 190 Train loss 6.00 on epoch=94
06/19/2022 20:13:21 - INFO - __main__ - Step 200 Global step 200 Train loss 5.96 on epoch=99
06/19/2022 20:13:27 - INFO - __main__ - Global step 200 Train loss 6.10 ACC 0.0 on epoch=99
06/19/2022 20:13:29 - INFO - __main__ - Step 210 Global step 210 Train loss 5.98 on epoch=104
06/19/2022 20:13:30 - INFO - __main__ - Step 220 Global step 220 Train loss 5.95 on epoch=109
06/19/2022 20:13:31 - INFO - __main__ - Step 230 Global step 230 Train loss 5.80 on epoch=114
06/19/2022 20:13:33 - INFO - __main__ - Step 240 Global step 240 Train loss 5.61 on epoch=119
06/19/2022 20:13:34 - INFO - __main__ - Step 250 Global step 250 Train loss 5.59 on epoch=124
06/19/2022 20:13:44 - INFO - __main__ - Global step 250 Train loss 5.79 ACC 0.0 on epoch=124
06/19/2022 20:13:46 - INFO - __main__ - Step 260 Global step 260 Train loss 5.60 on epoch=129
06/19/2022 20:13:47 - INFO - __main__ - Step 270 Global step 270 Train loss 5.48 on epoch=134
06/19/2022 20:13:48 - INFO - __main__ - Step 280 Global step 280 Train loss 5.23 on epoch=139
06/19/2022 20:13:49 - INFO - __main__ - Step 290 Global step 290 Train loss 5.19 on epoch=144
06/19/2022 20:13:51 - INFO - __main__ - Step 300 Global step 300 Train loss 5.18 on epoch=149
06/19/2022 20:13:52 - INFO - __main__ - Global step 300 Train loss 5.34 ACC 0.0 on epoch=149
06/19/2022 20:13:53 - INFO - __main__ - Step 310 Global step 310 Train loss 5.04 on epoch=154
06/19/2022 20:13:55 - INFO - __main__ - Step 320 Global step 320 Train loss 4.81 on epoch=159
06/19/2022 20:13:56 - INFO - __main__ - Step 330 Global step 330 Train loss 4.92 on epoch=164
06/19/2022 20:13:57 - INFO - __main__ - Step 340 Global step 340 Train loss 4.78 on epoch=169
06/19/2022 20:13:58 - INFO - __main__ - Step 350 Global step 350 Train loss 4.77 on epoch=174
06/19/2022 20:14:00 - INFO - __main__ - Global step 350 Train loss 4.86 ACC 0.0 on epoch=174
06/19/2022 20:14:01 - INFO - __main__ - Step 360 Global step 360 Train loss 4.67 on epoch=179
06/19/2022 20:14:02 - INFO - __main__ - Step 370 Global step 370 Train loss 4.55 on epoch=184
06/19/2022 20:14:03 - INFO - __main__ - Step 380 Global step 380 Train loss 4.45 on epoch=189
06/19/2022 20:14:05 - INFO - __main__ - Step 390 Global step 390 Train loss 4.40 on epoch=194
06/19/2022 20:14:06 - INFO - __main__ - Step 400 Global step 400 Train loss 4.43 on epoch=199
06/19/2022 20:14:07 - INFO - __main__ - Global step 400 Train loss 4.50 ACC 0.0 on epoch=199
06/19/2022 20:14:09 - INFO - __main__ - Step 410 Global step 410 Train loss 4.27 on epoch=204
06/19/2022 20:14:10 - INFO - __main__ - Step 420 Global step 420 Train loss 4.16 on epoch=209
06/19/2022 20:14:11 - INFO - __main__ - Step 430 Global step 430 Train loss 4.01 on epoch=214
06/19/2022 20:14:13 - INFO - __main__ - Step 440 Global step 440 Train loss 4.03 on epoch=219
06/19/2022 20:14:14 - INFO - __main__ - Step 450 Global step 450 Train loss 3.96 on epoch=224
06/19/2022 20:14:15 - INFO - __main__ - Global step 450 Train loss 4.09 ACC 0.0 on epoch=224
06/19/2022 20:14:16 - INFO - __main__ - Step 460 Global step 460 Train loss 3.81 on epoch=229
06/19/2022 20:14:18 - INFO - __main__ - Step 470 Global step 470 Train loss 3.78 on epoch=234
06/19/2022 20:14:19 - INFO - __main__ - Step 480 Global step 480 Train loss 3.67 on epoch=239
06/19/2022 20:14:21 - INFO - __main__ - Step 490 Global step 490 Train loss 3.66 on epoch=244
06/19/2022 20:14:22 - INFO - __main__ - Step 500 Global step 500 Train loss 3.59 on epoch=249
06/19/2022 20:14:23 - INFO - __main__ - Global step 500 Train loss 3.70 ACC 0.0 on epoch=249
06/19/2022 20:14:25 - INFO - __main__ - Step 510 Global step 510 Train loss 3.57 on epoch=254
06/19/2022 20:14:26 - INFO - __main__ - Step 520 Global step 520 Train loss 3.60 on epoch=259
06/19/2022 20:14:27 - INFO - __main__ - Step 530 Global step 530 Train loss 3.57 on epoch=264
06/19/2022 20:14:29 - INFO - __main__ - Step 540 Global step 540 Train loss 3.36 on epoch=269
06/19/2022 20:14:30 - INFO - __main__ - Step 550 Global step 550 Train loss 3.37 on epoch=274
06/19/2022 20:14:32 - INFO - __main__ - Global step 550 Train loss 3.49 ACC 0.0 on epoch=274
06/19/2022 20:14:33 - INFO - __main__ - Step 560 Global step 560 Train loss 3.30 on epoch=279
06/19/2022 20:14:34 - INFO - __main__ - Step 570 Global step 570 Train loss 3.29 on epoch=284
06/19/2022 20:14:35 - INFO - __main__ - Step 580 Global step 580 Train loss 3.25 on epoch=289
06/19/2022 20:14:37 - INFO - __main__ - Step 590 Global step 590 Train loss 3.10 on epoch=294
06/19/2022 20:14:38 - INFO - __main__ - Step 600 Global step 600 Train loss 3.24 on epoch=299
06/19/2022 20:14:41 - INFO - __main__ - Global step 600 Train loss 3.24 ACC 0.0 on epoch=299
06/19/2022 20:14:42 - INFO - __main__ - Step 610 Global step 610 Train loss 3.19 on epoch=304
06/19/2022 20:14:43 - INFO - __main__ - Step 620 Global step 620 Train loss 3.16 on epoch=309
06/19/2022 20:14:44 - INFO - __main__ - Step 630 Global step 630 Train loss 3.15 on epoch=314
06/19/2022 20:14:46 - INFO - __main__ - Step 640 Global step 640 Train loss 2.99 on epoch=319
06/19/2022 20:14:47 - INFO - __main__ - Step 650 Global step 650 Train loss 3.08 on epoch=324
06/19/2022 20:14:54 - INFO - __main__ - Global step 650 Train loss 3.12 ACC 0.0 on epoch=324
06/19/2022 20:14:55 - INFO - __main__ - Step 660 Global step 660 Train loss 3.03 on epoch=329
06/19/2022 20:14:57 - INFO - __main__ - Step 670 Global step 670 Train loss 2.87 on epoch=334
06/19/2022 20:14:58 - INFO - __main__ - Step 680 Global step 680 Train loss 2.81 on epoch=339
06/19/2022 20:15:00 - INFO - __main__ - Step 690 Global step 690 Train loss 2.87 on epoch=344
06/19/2022 20:15:01 - INFO - __main__ - Step 700 Global step 700 Train loss 2.74 on epoch=349
06/19/2022 20:15:04 - INFO - __main__ - Global step 700 Train loss 2.86 ACC 0.0 on epoch=349
06/19/2022 20:15:05 - INFO - __main__ - Step 710 Global step 710 Train loss 2.78 on epoch=354
06/19/2022 20:15:07 - INFO - __main__ - Step 720 Global step 720 Train loss 2.58 on epoch=359
06/19/2022 20:15:08 - INFO - __main__ - Step 730 Global step 730 Train loss 2.59 on epoch=364
06/19/2022 20:15:09 - INFO - __main__ - Step 740 Global step 740 Train loss 2.57 on epoch=369
06/19/2022 20:15:11 - INFO - __main__ - Step 750 Global step 750 Train loss 2.47 on epoch=374
06/19/2022 20:15:18 - INFO - __main__ - Global step 750 Train loss 2.60 ACC 0.0 on epoch=374
06/19/2022 20:15:19 - INFO - __main__ - Step 760 Global step 760 Train loss 2.49 on epoch=379
06/19/2022 20:15:20 - INFO - __main__ - Step 770 Global step 770 Train loss 2.39 on epoch=384
06/19/2022 20:15:22 - INFO - __main__ - Step 780 Global step 780 Train loss 2.48 on epoch=389
06/19/2022 20:15:23 - INFO - __main__ - Step 790 Global step 790 Train loss 2.35 on epoch=394
06/19/2022 20:15:24 - INFO - __main__ - Step 800 Global step 800 Train loss 2.40 on epoch=399
06/19/2022 20:15:28 - INFO - __main__ - Global step 800 Train loss 2.42 ACC 0.0 on epoch=399
06/19/2022 20:15:29 - INFO - __main__ - Step 810 Global step 810 Train loss 2.22 on epoch=404
06/19/2022 20:15:31 - INFO - __main__ - Step 820 Global step 820 Train loss 2.37 on epoch=409
06/19/2022 20:15:32 - INFO - __main__ - Step 830 Global step 830 Train loss 2.37 on epoch=414
06/19/2022 20:15:34 - INFO - __main__ - Step 840 Global step 840 Train loss 2.16 on epoch=419
06/19/2022 20:15:35 - INFO - __main__ - Step 850 Global step 850 Train loss 2.12 on epoch=424
06/19/2022 20:15:38 - INFO - __main__ - Global step 850 Train loss 2.25 ACC 0.09375 on epoch=424
06/19/2022 20:15:38 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=424, global_step=850
06/19/2022 20:15:39 - INFO - __main__ - Step 860 Global step 860 Train loss 2.09 on epoch=429
06/19/2022 20:15:41 - INFO - __main__ - Step 870 Global step 870 Train loss 2.14 on epoch=434
06/19/2022 20:15:42 - INFO - __main__ - Step 880 Global step 880 Train loss 1.98 on epoch=439
06/19/2022 20:15:44 - INFO - __main__ - Step 890 Global step 890 Train loss 2.03 on epoch=444
06/19/2022 20:15:45 - INFO - __main__ - Step 900 Global step 900 Train loss 1.94 on epoch=449
06/19/2022 20:15:46 - INFO - __main__ - Global step 900 Train loss 2.04 ACC 0.40625 on epoch=449
06/19/2022 20:15:46 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.40625 on epoch=449, global_step=900
06/19/2022 20:15:47 - INFO - __main__ - Step 910 Global step 910 Train loss 1.95 on epoch=454
06/19/2022 20:15:48 - INFO - __main__ - Step 920 Global step 920 Train loss 1.84 on epoch=459
06/19/2022 20:15:50 - INFO - __main__ - Step 930 Global step 930 Train loss 1.84 on epoch=464
06/19/2022 20:15:51 - INFO - __main__ - Step 940 Global step 940 Train loss 1.76 on epoch=469
06/19/2022 20:15:53 - INFO - __main__ - Step 950 Global step 950 Train loss 1.76 on epoch=474
06/19/2022 20:15:54 - INFO - __main__ - Global step 950 Train loss 1.83 ACC 0.25 on epoch=474
06/19/2022 20:15:55 - INFO - __main__ - Step 960 Global step 960 Train loss 1.77 on epoch=479
06/19/2022 20:15:56 - INFO - __main__ - Step 970 Global step 970 Train loss 1.81 on epoch=484
06/19/2022 20:15:58 - INFO - __main__ - Step 980 Global step 980 Train loss 1.78 on epoch=489
06/19/2022 20:15:59 - INFO - __main__ - Step 990 Global step 990 Train loss 1.66 on epoch=494
06/19/2022 20:16:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.65 on epoch=499
06/19/2022 20:16:02 - INFO - __main__ - Global step 1000 Train loss 1.73 ACC 0.53125 on epoch=499
06/19/2022 20:16:02 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.53125 on epoch=499, global_step=1000
06/19/2022 20:16:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.63 on epoch=504
06/19/2022 20:16:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.52 on epoch=509
06/19/2022 20:16:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.59 on epoch=514
06/19/2022 20:16:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.47 on epoch=519
06/19/2022 20:16:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.42 on epoch=524
06/19/2022 20:16:09 - INFO - __main__ - Global step 1050 Train loss 1.53 ACC 0.5 on epoch=524
06/19/2022 20:16:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.34 on epoch=529
06/19/2022 20:16:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.25 on epoch=534
06/19/2022 20:16:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.23 on epoch=539
06/19/2022 20:16:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.22 on epoch=544
06/19/2022 20:16:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.19 on epoch=549
06/19/2022 20:16:18 - INFO - __main__ - Global step 1100 Train loss 1.24 ACC 0.5 on epoch=549
06/19/2022 20:16:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.02 on epoch=554
06/19/2022 20:16:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.11 on epoch=559
06/19/2022 20:16:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.19 on epoch=564
06/19/2022 20:16:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.03 on epoch=569
06/19/2022 20:16:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.00 on epoch=574
06/19/2022 20:16:28 - INFO - __main__ - Global step 1150 Train loss 1.07 ACC 0.5 on epoch=574
06/19/2022 20:16:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.00 on epoch=579
06/19/2022 20:16:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.01 on epoch=584
06/19/2022 20:16:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.89 on epoch=589
06/19/2022 20:16:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=594
06/19/2022 20:16:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.84 on epoch=599
06/19/2022 20:16:36 - INFO - __main__ - Global step 1200 Train loss 0.92 ACC 0.5 on epoch=599
06/19/2022 20:16:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=604
06/19/2022 20:16:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.99 on epoch=609
06/19/2022 20:16:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=614
06/19/2022 20:16:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=619
06/19/2022 20:16:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.76 on epoch=624
06/19/2022 20:16:45 - INFO - __main__ - Global step 1250 Train loss 0.84 ACC 0.5 on epoch=624
06/19/2022 20:16:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.79 on epoch=629
06/19/2022 20:16:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.77 on epoch=634
06/19/2022 20:16:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.61 on epoch=639
06/19/2022 20:16:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.72 on epoch=644
06/19/2022 20:16:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.74 on epoch=649
06/19/2022 20:16:53 - INFO - __main__ - Global step 1300 Train loss 0.73 ACC 0.5 on epoch=649
06/19/2022 20:16:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=654
06/19/2022 20:16:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.76 on epoch=659
06/19/2022 20:16:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.65 on epoch=664
06/19/2022 20:16:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.71 on epoch=669
06/19/2022 20:16:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.68 on epoch=674
06/19/2022 20:17:00 - INFO - __main__ - Global step 1350 Train loss 0.72 ACC 0.5 on epoch=674
06/19/2022 20:17:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.65 on epoch=679
06/19/2022 20:17:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.64 on epoch=684
06/19/2022 20:17:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.64 on epoch=689
06/19/2022 20:17:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.61 on epoch=694
06/19/2022 20:17:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.63 on epoch=699
06/19/2022 20:17:07 - INFO - __main__ - Global step 1400 Train loss 0.63 ACC 0.5 on epoch=699
06/19/2022 20:17:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.60 on epoch=704
06/19/2022 20:17:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.59 on epoch=709
06/19/2022 20:17:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.56 on epoch=714
06/19/2022 20:17:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.59 on epoch=719
06/19/2022 20:17:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.64 on epoch=724
06/19/2022 20:17:14 - INFO - __main__ - Global step 1450 Train loss 0.60 ACC 0.5 on epoch=724
06/19/2022 20:17:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.57 on epoch=729
06/19/2022 20:17:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.53 on epoch=734
06/19/2022 20:17:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.56 on epoch=739
06/19/2022 20:17:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.64 on epoch=744
06/19/2022 20:17:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.56 on epoch=749
06/19/2022 20:17:20 - INFO - __main__ - Global step 1500 Train loss 0.57 ACC 0.5 on epoch=749
06/19/2022 20:17:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.56 on epoch=754
06/19/2022 20:17:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=759
06/19/2022 20:17:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.57 on epoch=764
06/19/2022 20:17:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=769
06/19/2022 20:17:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=774
06/19/2022 20:17:27 - INFO - __main__ - Global step 1550 Train loss 0.53 ACC 0.5 on epoch=774
06/19/2022 20:17:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.59 on epoch=779
06/19/2022 20:17:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=784
06/19/2022 20:17:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
06/19/2022 20:17:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.48 on epoch=794
06/19/2022 20:17:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=799
06/19/2022 20:17:34 - INFO - __main__ - Global step 1600 Train loss 0.50 ACC 0.5 on epoch=799
06/19/2022 20:17:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.57 on epoch=804
06/19/2022 20:17:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=809
06/19/2022 20:17:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.57 on epoch=814
06/19/2022 20:17:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=819
06/19/2022 20:17:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=824
06/19/2022 20:17:41 - INFO - __main__ - Global step 1650 Train loss 0.50 ACC 0.5 on epoch=824
06/19/2022 20:17:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=829
06/19/2022 20:17:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.50 on epoch=834
06/19/2022 20:17:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.49 on epoch=839
06/19/2022 20:17:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=844
06/19/2022 20:17:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.48 on epoch=849
06/19/2022 20:17:50 - INFO - __main__ - Global step 1700 Train loss 0.48 ACC 0.5 on epoch=849
06/19/2022 20:17:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.51 on epoch=854
06/19/2022 20:17:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.53 on epoch=859
06/19/2022 20:17:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=864
06/19/2022 20:17:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=869
06/19/2022 20:17:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=874
06/19/2022 20:17:58 - INFO - __main__ - Global step 1750 Train loss 0.46 ACC 0.53125 on epoch=874
06/19/2022 20:17:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.50 on epoch=879
06/19/2022 20:18:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
06/19/2022 20:18:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
06/19/2022 20:18:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
06/19/2022 20:18:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
06/19/2022 20:18:05 - INFO - __main__ - Global step 1800 Train loss 0.44 ACC 0.5 on epoch=899
06/19/2022 20:18:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.50 on epoch=904
06/19/2022 20:18:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
06/19/2022 20:18:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=914
06/19/2022 20:18:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=919
06/19/2022 20:18:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=924
06/19/2022 20:18:12 - INFO - __main__ - Global step 1850 Train loss 0.45 ACC 0.5 on epoch=924
06/19/2022 20:18:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/19/2022 20:18:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=934
06/19/2022 20:18:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=939
06/19/2022 20:18:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=944
06/19/2022 20:18:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
06/19/2022 20:18:19 - INFO - __main__ - Global step 1900 Train loss 0.42 ACC 0.5 on epoch=949
06/19/2022 20:18:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.44 on epoch=954
06/19/2022 20:18:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/19/2022 20:18:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=964
06/19/2022 20:18:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=969
06/19/2022 20:18:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=974
06/19/2022 20:18:26 - INFO - __main__ - Global step 1950 Train loss 0.42 ACC 0.5 on epoch=974
06/19/2022 20:18:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
06/19/2022 20:18:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=984
06/19/2022 20:18:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=989
06/19/2022 20:18:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=994
06/19/2022 20:18:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=999
06/19/2022 20:18:33 - INFO - __main__ - Global step 2000 Train loss 0.42 ACC 0.5 on epoch=999
06/19/2022 20:18:33 - INFO - __main__ - save last model!
06/19/2022 20:18:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 20:18:33 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 20:18:33 - INFO - __main__ - Printing 3 examples
06/19/2022 20:18:33 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 20:18:33 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:33 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 20:18:33 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:18:33 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 20:18:33 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:18:33 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:18:33 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:18:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:18:34 - INFO - __main__ - Printing 3 examples
06/19/2022 20:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 20:18:34 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 20:18:34 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 20:18:34 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 20:18:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:18:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:18:34 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:18:34 - INFO - __main__ - Printing 3 examples
06/19/2022 20:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 20:18:34 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 20:18:34 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 20:18:34 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:34 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:18:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:18:34 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 20:18:34 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:18:39 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:18:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:18:39 - INFO - __main__ - Starting training!
06/19/2022 20:18:41 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.5_8_predictions.txt
06/19/2022 20:18:41 - INFO - __main__ - ACC on test data: 0.6716
06/19/2022 20:18:42 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.5, bsz=8, dev_performance=0.53125, test_performance=0.6715686274509803
06/19/2022 20:18:42 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.4, bsz=8 ...
06/19/2022 20:18:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:18:43 - INFO - __main__ - Printing 3 examples
06/19/2022 20:18:43 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 20:18:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:43 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 20:18:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 20:18:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 20:18:43 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:18:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:18:43 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:18:43 - INFO - __main__ - Printing 3 examples
06/19/2022 20:18:43 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 20:18:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:43 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 20:18:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:43 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 20:18:43 - INFO - __main__ - ['equivalent']
06/19/2022 20:18:43 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:18:43 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:18:43 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:18:48 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:18:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:18:48 - INFO - __main__ - Starting training!
06/19/2022 20:18:50 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/19/2022 20:18:51 - INFO - __main__ - Step 20 Global step 20 Train loss 6.88 on epoch=9
06/19/2022 20:18:53 - INFO - __main__ - Step 30 Global step 30 Train loss 6.85 on epoch=14
06/19/2022 20:18:54 - INFO - __main__ - Step 40 Global step 40 Train loss 6.77 on epoch=19
06/19/2022 20:18:55 - INFO - __main__ - Step 50 Global step 50 Train loss 6.86 on epoch=24
06/19/2022 20:18:56 - INFO - __main__ - Global step 50 Train loss 6.84 ACC 0.0 on epoch=24
06/19/2022 20:18:56 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 20:18:57 - INFO - __main__ - Step 60 Global step 60 Train loss 6.75 on epoch=29
06/19/2022 20:18:58 - INFO - __main__ - Step 70 Global step 70 Train loss 6.73 on epoch=34
06/19/2022 20:18:59 - INFO - __main__ - Step 80 Global step 80 Train loss 6.69 on epoch=39
06/19/2022 20:19:01 - INFO - __main__ - Step 90 Global step 90 Train loss 6.61 on epoch=44
06/19/2022 20:19:02 - INFO - __main__ - Step 100 Global step 100 Train loss 6.54 on epoch=49
06/19/2022 20:19:04 - INFO - __main__ - Global step 100 Train loss 6.66 ACC 0.0 on epoch=49
06/19/2022 20:19:06 - INFO - __main__ - Step 110 Global step 110 Train loss 6.62 on epoch=54
06/19/2022 20:19:07 - INFO - __main__ - Step 120 Global step 120 Train loss 6.59 on epoch=59
06/19/2022 20:19:08 - INFO - __main__ - Step 130 Global step 130 Train loss 6.44 on epoch=64
06/19/2022 20:19:10 - INFO - __main__ - Step 140 Global step 140 Train loss 6.46 on epoch=69
06/19/2022 20:19:11 - INFO - __main__ - Step 150 Global step 150 Train loss 6.39 on epoch=74
06/19/2022 20:19:13 - INFO - __main__ - Global step 150 Train loss 6.50 ACC 0.0 on epoch=74
06/19/2022 20:19:15 - INFO - __main__ - Step 160 Global step 160 Train loss 6.31 on epoch=79
06/19/2022 20:19:16 - INFO - __main__ - Step 170 Global step 170 Train loss 6.33 on epoch=84
06/19/2022 20:19:17 - INFO - __main__ - Step 180 Global step 180 Train loss 6.25 on epoch=89
06/19/2022 20:19:19 - INFO - __main__ - Step 190 Global step 190 Train loss 6.24 on epoch=94
06/19/2022 20:19:20 - INFO - __main__ - Step 200 Global step 200 Train loss 6.18 on epoch=99
06/19/2022 20:19:24 - INFO - __main__ - Global step 200 Train loss 6.26 ACC 0.0 on epoch=99
06/19/2022 20:19:25 - INFO - __main__ - Step 210 Global step 210 Train loss 6.10 on epoch=104
06/19/2022 20:19:26 - INFO - __main__ - Step 220 Global step 220 Train loss 5.99 on epoch=109
06/19/2022 20:19:27 - INFO - __main__ - Step 230 Global step 230 Train loss 5.96 on epoch=114
06/19/2022 20:19:29 - INFO - __main__ - Step 240 Global step 240 Train loss 5.83 on epoch=119
06/19/2022 20:19:30 - INFO - __main__ - Step 250 Global step 250 Train loss 5.92 on epoch=124
06/19/2022 20:19:36 - INFO - __main__ - Global step 250 Train loss 5.96 ACC 0.0 on epoch=124
06/19/2022 20:19:38 - INFO - __main__ - Step 260 Global step 260 Train loss 5.69 on epoch=129
06/19/2022 20:19:39 - INFO - __main__ - Step 270 Global step 270 Train loss 5.90 on epoch=134
06/19/2022 20:19:40 - INFO - __main__ - Step 280 Global step 280 Train loss 5.74 on epoch=139
06/19/2022 20:19:42 - INFO - __main__ - Step 290 Global step 290 Train loss 5.87 on epoch=144
06/19/2022 20:19:43 - INFO - __main__ - Step 300 Global step 300 Train loss 5.72 on epoch=149
06/19/2022 20:19:45 - INFO - __main__ - Global step 300 Train loss 5.78 ACC 0.0 on epoch=149
06/19/2022 20:19:46 - INFO - __main__ - Step 310 Global step 310 Train loss 5.58 on epoch=154
06/19/2022 20:19:48 - INFO - __main__ - Step 320 Global step 320 Train loss 5.54 on epoch=159
06/19/2022 20:19:49 - INFO - __main__ - Step 330 Global step 330 Train loss 5.52 on epoch=164
06/19/2022 20:19:50 - INFO - __main__ - Step 340 Global step 340 Train loss 5.35 on epoch=169
06/19/2022 20:19:52 - INFO - __main__ - Step 350 Global step 350 Train loss 5.36 on epoch=174
06/19/2022 20:19:54 - INFO - __main__ - Global step 350 Train loss 5.47 ACC 0.0 on epoch=174
06/19/2022 20:19:55 - INFO - __main__ - Step 360 Global step 360 Train loss 5.17 on epoch=179
06/19/2022 20:19:56 - INFO - __main__ - Step 370 Global step 370 Train loss 5.18 on epoch=184
06/19/2022 20:19:58 - INFO - __main__ - Step 380 Global step 380 Train loss 4.99 on epoch=189
06/19/2022 20:19:59 - INFO - __main__ - Step 390 Global step 390 Train loss 4.91 on epoch=194
06/19/2022 20:20:00 - INFO - __main__ - Step 400 Global step 400 Train loss 4.82 on epoch=199
06/19/2022 20:20:03 - INFO - __main__ - Global step 400 Train loss 5.01 ACC 0.0 on epoch=199
06/19/2022 20:20:04 - INFO - __main__ - Step 410 Global step 410 Train loss 4.81 on epoch=204
06/19/2022 20:20:06 - INFO - __main__ - Step 420 Global step 420 Train loss 4.71 on epoch=209
06/19/2022 20:20:07 - INFO - __main__ - Step 430 Global step 430 Train loss 4.68 on epoch=214
06/19/2022 20:20:08 - INFO - __main__ - Step 440 Global step 440 Train loss 4.47 on epoch=219
06/19/2022 20:20:09 - INFO - __main__ - Step 450 Global step 450 Train loss 4.42 on epoch=224
06/19/2022 20:20:11 - INFO - __main__ - Global step 450 Train loss 4.62 ACC 0.0 on epoch=224
06/19/2022 20:20:12 - INFO - __main__ - Step 460 Global step 460 Train loss 4.27 on epoch=229
06/19/2022 20:20:13 - INFO - __main__ - Step 470 Global step 470 Train loss 4.25 on epoch=234
06/19/2022 20:20:15 - INFO - __main__ - Step 480 Global step 480 Train loss 4.13 on epoch=239
06/19/2022 20:20:16 - INFO - __main__ - Step 490 Global step 490 Train loss 4.10 on epoch=244
06/19/2022 20:20:17 - INFO - __main__ - Step 500 Global step 500 Train loss 4.04 on epoch=249
06/19/2022 20:20:23 - INFO - __main__ - Global step 500 Train loss 4.16 ACC 0.0 on epoch=249
06/19/2022 20:20:24 - INFO - __main__ - Step 510 Global step 510 Train loss 3.93 on epoch=254
06/19/2022 20:20:26 - INFO - __main__ - Step 520 Global step 520 Train loss 3.81 on epoch=259
06/19/2022 20:20:27 - INFO - __main__ - Step 530 Global step 530 Train loss 3.73 on epoch=264
06/19/2022 20:20:28 - INFO - __main__ - Step 540 Global step 540 Train loss 3.64 on epoch=269
06/19/2022 20:20:29 - INFO - __main__ - Step 550 Global step 550 Train loss 3.60 on epoch=274
06/19/2022 20:20:36 - INFO - __main__ - Global step 550 Train loss 3.74 ACC 0.0 on epoch=274
06/19/2022 20:20:37 - INFO - __main__ - Step 560 Global step 560 Train loss 3.50 on epoch=279
06/19/2022 20:20:39 - INFO - __main__ - Step 570 Global step 570 Train loss 3.50 on epoch=284
06/19/2022 20:20:40 - INFO - __main__ - Step 580 Global step 580 Train loss 3.44 on epoch=289
06/19/2022 20:20:41 - INFO - __main__ - Step 590 Global step 590 Train loss 3.51 on epoch=294
06/19/2022 20:20:43 - INFO - __main__ - Step 600 Global step 600 Train loss 3.28 on epoch=299
06/19/2022 20:20:46 - INFO - __main__ - Global step 600 Train loss 3.44 ACC 0.0 on epoch=299
06/19/2022 20:20:48 - INFO - __main__ - Step 610 Global step 610 Train loss 3.27 on epoch=304
06/19/2022 20:20:49 - INFO - __main__ - Step 620 Global step 620 Train loss 3.19 on epoch=309
06/19/2022 20:20:50 - INFO - __main__ - Step 630 Global step 630 Train loss 3.24 on epoch=314
06/19/2022 20:20:51 - INFO - __main__ - Step 640 Global step 640 Train loss 3.06 on epoch=319
06/19/2022 20:20:53 - INFO - __main__ - Step 650 Global step 650 Train loss 2.95 on epoch=324
06/19/2022 20:21:01 - INFO - __main__ - Global step 650 Train loss 3.14 ACC 0.0 on epoch=324
06/19/2022 20:21:02 - INFO - __main__ - Step 660 Global step 660 Train loss 3.00 on epoch=329
06/19/2022 20:21:04 - INFO - __main__ - Step 670 Global step 670 Train loss 2.84 on epoch=334
06/19/2022 20:21:05 - INFO - __main__ - Step 680 Global step 680 Train loss 2.78 on epoch=339
06/19/2022 20:21:06 - INFO - __main__ - Step 690 Global step 690 Train loss 2.77 on epoch=344
06/19/2022 20:21:07 - INFO - __main__ - Step 700 Global step 700 Train loss 2.69 on epoch=349
06/19/2022 20:21:18 - INFO - __main__ - Global step 700 Train loss 2.81 ACC 0.0 on epoch=349
06/19/2022 20:21:19 - INFO - __main__ - Step 710 Global step 710 Train loss 2.69 on epoch=354
06/19/2022 20:21:20 - INFO - __main__ - Step 720 Global step 720 Train loss 2.54 on epoch=359
06/19/2022 20:21:21 - INFO - __main__ - Step 730 Global step 730 Train loss 2.54 on epoch=364
06/19/2022 20:21:23 - INFO - __main__ - Step 740 Global step 740 Train loss 2.41 on epoch=369
06/19/2022 20:21:24 - INFO - __main__ - Step 750 Global step 750 Train loss 2.39 on epoch=374
06/19/2022 20:21:27 - INFO - __main__ - Global step 750 Train loss 2.51 ACC 0.03125 on epoch=374
06/19/2022 20:21:27 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=374, global_step=750
06/19/2022 20:21:28 - INFO - __main__ - Step 760 Global step 760 Train loss 2.39 on epoch=379
06/19/2022 20:21:29 - INFO - __main__ - Step 770 Global step 770 Train loss 2.37 on epoch=384
06/19/2022 20:21:31 - INFO - __main__ - Step 780 Global step 780 Train loss 2.28 on epoch=389
06/19/2022 20:21:32 - INFO - __main__ - Step 790 Global step 790 Train loss 2.24 on epoch=394
06/19/2022 20:21:33 - INFO - __main__ - Step 800 Global step 800 Train loss 2.03 on epoch=399
06/19/2022 20:21:35 - INFO - __main__ - Global step 800 Train loss 2.26 ACC 0.4375 on epoch=399
06/19/2022 20:21:35 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.4375 on epoch=399, global_step=800
06/19/2022 20:21:36 - INFO - __main__ - Step 810 Global step 810 Train loss 2.09 on epoch=404
06/19/2022 20:21:38 - INFO - __main__ - Step 820 Global step 820 Train loss 1.95 on epoch=409
06/19/2022 20:21:39 - INFO - __main__ - Step 830 Global step 830 Train loss 1.97 on epoch=414
06/19/2022 20:21:40 - INFO - __main__ - Step 840 Global step 840 Train loss 1.88 on epoch=419
06/19/2022 20:21:42 - INFO - __main__ - Step 850 Global step 850 Train loss 1.87 on epoch=424
06/19/2022 20:21:43 - INFO - __main__ - Global step 850 Train loss 1.95 ACC 0.5 on epoch=424
06/19/2022 20:21:43 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=424, global_step=850
06/19/2022 20:21:44 - INFO - __main__ - Step 860 Global step 860 Train loss 1.80 on epoch=429
06/19/2022 20:21:45 - INFO - __main__ - Step 870 Global step 870 Train loss 1.84 on epoch=434
06/19/2022 20:21:47 - INFO - __main__ - Step 880 Global step 880 Train loss 1.68 on epoch=439
06/19/2022 20:21:48 - INFO - __main__ - Step 890 Global step 890 Train loss 1.75 on epoch=444
06/19/2022 20:21:49 - INFO - __main__ - Step 900 Global step 900 Train loss 1.62 on epoch=449
06/19/2022 20:21:50 - INFO - __main__ - Global step 900 Train loss 1.74 ACC 0.53125 on epoch=449
06/19/2022 20:21:50 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=449, global_step=900
06/19/2022 20:21:52 - INFO - __main__ - Step 910 Global step 910 Train loss 1.64 on epoch=454
06/19/2022 20:21:53 - INFO - __main__ - Step 920 Global step 920 Train loss 1.58 on epoch=459
06/19/2022 20:21:54 - INFO - __main__ - Step 930 Global step 930 Train loss 1.56 on epoch=464
06/19/2022 20:21:56 - INFO - __main__ - Step 940 Global step 940 Train loss 1.49 on epoch=469
06/19/2022 20:21:57 - INFO - __main__ - Step 950 Global step 950 Train loss 1.57 on epoch=474
06/19/2022 20:21:58 - INFO - __main__ - Global step 950 Train loss 1.57 ACC 0.40625 on epoch=474
06/19/2022 20:22:00 - INFO - __main__ - Step 960 Global step 960 Train loss 1.60 on epoch=479
06/19/2022 20:22:01 - INFO - __main__ - Step 970 Global step 970 Train loss 1.45 on epoch=484
06/19/2022 20:22:03 - INFO - __main__ - Step 980 Global step 980 Train loss 1.46 on epoch=489
06/19/2022 20:22:04 - INFO - __main__ - Step 990 Global step 990 Train loss 1.41 on epoch=494
06/19/2022 20:22:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.28 on epoch=499
06/19/2022 20:22:07 - INFO - __main__ - Global step 1000 Train loss 1.44 ACC 0.40625 on epoch=499
06/19/2022 20:22:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.43 on epoch=504
06/19/2022 20:22:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.31 on epoch=509
06/19/2022 20:22:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.38 on epoch=514
06/19/2022 20:22:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.22 on epoch=519
06/19/2022 20:22:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.26 on epoch=524
06/19/2022 20:22:15 - INFO - __main__ - Global step 1050 Train loss 1.32 ACC 0.53125 on epoch=524
06/19/2022 20:22:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.18 on epoch=529
06/19/2022 20:22:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.28 on epoch=534
06/19/2022 20:22:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.16 on epoch=539
06/19/2022 20:22:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.11 on epoch=544
06/19/2022 20:22:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.04 on epoch=549
06/19/2022 20:22:23 - INFO - __main__ - Global step 1100 Train loss 1.16 ACC 0.5 on epoch=549
06/19/2022 20:22:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.16 on epoch=554
06/19/2022 20:22:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.03 on epoch=559
06/19/2022 20:22:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.07 on epoch=564
06/19/2022 20:22:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.99 on epoch=569
06/19/2022 20:22:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.07 on epoch=574
06/19/2022 20:22:31 - INFO - __main__ - Global step 1150 Train loss 1.06 ACC 0.5 on epoch=574
06/19/2022 20:22:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.02 on epoch=579
06/19/2022 20:22:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.00 on epoch=584
06/19/2022 20:22:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.03 on epoch=589
06/19/2022 20:22:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.99 on epoch=594
06/19/2022 20:22:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.90 on epoch=599
06/19/2022 20:22:39 - INFO - __main__ - Global step 1200 Train loss 0.99 ACC 0.5 on epoch=599
06/19/2022 20:22:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.86 on epoch=604
06/19/2022 20:22:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.94 on epoch=609
06/19/2022 20:22:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.86 on epoch=614
06/19/2022 20:22:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.88 on epoch=619
06/19/2022 20:22:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.84 on epoch=624
06/19/2022 20:22:46 - INFO - __main__ - Global step 1250 Train loss 0.88 ACC 0.5 on epoch=624
06/19/2022 20:22:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.78 on epoch=629
06/19/2022 20:22:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.83 on epoch=634
06/19/2022 20:22:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.81 on epoch=639
06/19/2022 20:22:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.72 on epoch=644
06/19/2022 20:22:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.76 on epoch=649
06/19/2022 20:22:54 - INFO - __main__ - Global step 1300 Train loss 0.78 ACC 0.5 on epoch=649
06/19/2022 20:22:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.76 on epoch=654
06/19/2022 20:22:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.77 on epoch=659
06/19/2022 20:22:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.74 on epoch=664
06/19/2022 20:23:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.71 on epoch=669
06/19/2022 20:23:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.66 on epoch=674
06/19/2022 20:23:04 - INFO - __main__ - Global step 1350 Train loss 0.73 ACC 0.5 on epoch=674
06/19/2022 20:23:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.67 on epoch=679
06/19/2022 20:23:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.71 on epoch=684
06/19/2022 20:23:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.62 on epoch=689
06/19/2022 20:23:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.73 on epoch=694
06/19/2022 20:23:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.65 on epoch=699
06/19/2022 20:23:13 - INFO - __main__ - Global step 1400 Train loss 0.68 ACC 0.5 on epoch=699
06/19/2022 20:23:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.67 on epoch=704
06/19/2022 20:23:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.66 on epoch=709
06/19/2022 20:23:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.60 on epoch=714
06/19/2022 20:23:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
06/19/2022 20:23:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.67 on epoch=724
06/19/2022 20:23:21 - INFO - __main__ - Global step 1450 Train loss 0.64 ACC 0.5 on epoch=724
06/19/2022 20:23:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/19/2022 20:23:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.57 on epoch=734
06/19/2022 20:23:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.62 on epoch=739
06/19/2022 20:23:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.59 on epoch=744
06/19/2022 20:23:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.60 on epoch=749
06/19/2022 20:23:30 - INFO - __main__ - Global step 1500 Train loss 0.58 ACC 0.5 on epoch=749
06/19/2022 20:23:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=754
06/19/2022 20:23:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.58 on epoch=759
06/19/2022 20:23:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=764
06/19/2022 20:23:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.56 on epoch=769
06/19/2022 20:23:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.56 on epoch=774
06/19/2022 20:23:40 - INFO - __main__ - Global step 1550 Train loss 0.55 ACC 0.5 on epoch=774
06/19/2022 20:23:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
06/19/2022 20:23:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.54 on epoch=784
06/19/2022 20:23:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.53 on epoch=789
06/19/2022 20:23:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.55 on epoch=794
06/19/2022 20:23:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.51 on epoch=799
06/19/2022 20:23:49 - INFO - __main__ - Global step 1600 Train loss 0.54 ACC 0.5 on epoch=799
06/19/2022 20:23:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
06/19/2022 20:23:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=809
06/19/2022 20:23:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
06/19/2022 20:23:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=819
06/19/2022 20:23:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.55 on epoch=824
06/19/2022 20:23:59 - INFO - __main__ - Global step 1650 Train loss 0.51 ACC 0.5 on epoch=824
06/19/2022 20:24:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=829
06/19/2022 20:24:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=834
06/19/2022 20:24:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=839
06/19/2022 20:24:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=844
06/19/2022 20:24:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
06/19/2022 20:24:09 - INFO - __main__ - Global step 1700 Train loss 0.46 ACC 0.5 on epoch=849
06/19/2022 20:24:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=854
06/19/2022 20:24:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=859
06/19/2022 20:24:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
06/19/2022 20:24:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=869
06/19/2022 20:24:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.51 on epoch=874
06/19/2022 20:24:18 - INFO - __main__ - Global step 1750 Train loss 0.50 ACC 0.5 on epoch=874
06/19/2022 20:24:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=879
06/19/2022 20:24:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
06/19/2022 20:24:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
06/19/2022 20:24:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.47 on epoch=894
06/19/2022 20:24:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
06/19/2022 20:24:28 - INFO - __main__ - Global step 1800 Train loss 0.45 ACC 0.5 on epoch=899
06/19/2022 20:24:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.50 on epoch=904
06/19/2022 20:24:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=909
06/19/2022 20:24:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=914
06/19/2022 20:24:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=919
06/19/2022 20:24:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.45 on epoch=924
06/19/2022 20:24:35 - INFO - __main__ - Global step 1850 Train loss 0.45 ACC 0.5 on epoch=924
06/19/2022 20:24:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
06/19/2022 20:24:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.46 on epoch=934
06/19/2022 20:24:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
06/19/2022 20:24:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.50 on epoch=944
06/19/2022 20:24:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=949
06/19/2022 20:24:43 - INFO - __main__ - Global step 1900 Train loss 0.46 ACC 0.5 on epoch=949
06/19/2022 20:24:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=954
06/19/2022 20:24:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=959
06/19/2022 20:24:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
06/19/2022 20:24:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.48 on epoch=969
06/19/2022 20:24:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=974
06/19/2022 20:24:50 - INFO - __main__ - Global step 1950 Train loss 0.43 ACC 0.5 on epoch=974
06/19/2022 20:24:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=979
06/19/2022 20:24:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=984
06/19/2022 20:24:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=989
06/19/2022 20:24:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=994
06/19/2022 20:24:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=999
06/19/2022 20:24:57 - INFO - __main__ - Global step 2000 Train loss 0.44 ACC 0.5 on epoch=999
06/19/2022 20:24:57 - INFO - __main__ - save last model!
06/19/2022 20:24:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 20:24:57 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 20:24:57 - INFO - __main__ - Printing 3 examples
06/19/2022 20:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 20:24:57 - INFO - __main__ - ['equivalent']
06/19/2022 20:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 20:24:57 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 20:24:57 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:24:57 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:24:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:24:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:24:57 - INFO - __main__ - Printing 3 examples
06/19/2022 20:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 20:24:57 - INFO - __main__ - ['equivalent']
06/19/2022 20:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 20:24:57 - INFO - __main__ - ['equivalent']
06/19/2022 20:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 20:24:57 - INFO - __main__ - ['equivalent']
06/19/2022 20:24:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 20:24:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:24:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:24:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:24:57 - INFO - __main__ - Printing 3 examples
06/19/2022 20:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 20:24:57 - INFO - __main__ - ['equivalent']
06/19/2022 20:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 20:24:57 - INFO - __main__ - ['equivalent']
06/19/2022 20:24:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 20:24:57 - INFO - __main__ - ['equivalent']
06/19/2022 20:24:57 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:24:57 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 20:24:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:24:57 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:25:02 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:25:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:25:03 - INFO - __main__ - Starting training!
06/19/2022 20:25:05 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.4_8_predictions.txt
06/19/2022 20:25:05 - INFO - __main__ - ACC on test data: 0.6814
06/19/2022 20:25:05 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.4, bsz=8, dev_performance=0.53125, test_performance=0.6813725490196079
06/19/2022 20:25:05 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.3, bsz=8 ...
06/19/2022 20:25:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:25:06 - INFO - __main__ - Printing 3 examples
06/19/2022 20:25:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 20:25:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:25:06 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 20:25:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:25:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 20:25:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:25:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 20:25:06 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:25:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:25:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:25:06 - INFO - __main__ - Printing 3 examples
06/19/2022 20:25:06 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 20:25:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:25:06 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 20:25:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:25:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 20:25:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:25:06 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:25:06 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:25:06 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:25:13 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:25:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:25:13 - INFO - __main__ - Starting training!
06/19/2022 20:25:15 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/19/2022 20:25:16 - INFO - __main__ - Step 20 Global step 20 Train loss 6.95 on epoch=9
06/19/2022 20:25:17 - INFO - __main__ - Step 30 Global step 30 Train loss 6.87 on epoch=14
06/19/2022 20:25:18 - INFO - __main__ - Step 40 Global step 40 Train loss 6.77 on epoch=19
06/19/2022 20:25:20 - INFO - __main__ - Step 50 Global step 50 Train loss 6.81 on epoch=24
06/19/2022 20:25:27 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/19/2022 20:25:27 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 20:25:28 - INFO - __main__ - Step 60 Global step 60 Train loss 6.74 on epoch=29
06/19/2022 20:25:29 - INFO - __main__ - Step 70 Global step 70 Train loss 6.73 on epoch=34
06/19/2022 20:25:31 - INFO - __main__ - Step 80 Global step 80 Train loss 6.68 on epoch=39
06/19/2022 20:25:32 - INFO - __main__ - Step 90 Global step 90 Train loss 6.69 on epoch=44
06/19/2022 20:25:34 - INFO - __main__ - Step 100 Global step 100 Train loss 6.68 on epoch=49
06/19/2022 20:25:37 - INFO - __main__ - Global step 100 Train loss 6.70 ACC 0.0 on epoch=49
06/19/2022 20:25:38 - INFO - __main__ - Step 110 Global step 110 Train loss 6.61 on epoch=54
06/19/2022 20:25:40 - INFO - __main__ - Step 120 Global step 120 Train loss 6.71 on epoch=59
06/19/2022 20:25:41 - INFO - __main__ - Step 130 Global step 130 Train loss 6.57 on epoch=64
06/19/2022 20:25:42 - INFO - __main__ - Step 140 Global step 140 Train loss 6.65 on epoch=69
06/19/2022 20:25:43 - INFO - __main__ - Step 150 Global step 150 Train loss 6.66 on epoch=74
06/19/2022 20:25:50 - INFO - __main__ - Global step 150 Train loss 6.64 ACC 0.0 on epoch=74
06/19/2022 20:25:51 - INFO - __main__ - Step 160 Global step 160 Train loss 6.61 on epoch=79
06/19/2022 20:25:52 - INFO - __main__ - Step 170 Global step 170 Train loss 6.54 on epoch=84
06/19/2022 20:25:54 - INFO - __main__ - Step 180 Global step 180 Train loss 6.44 on epoch=89
06/19/2022 20:25:55 - INFO - __main__ - Step 190 Global step 190 Train loss 6.48 on epoch=94
06/19/2022 20:25:56 - INFO - __main__ - Step 200 Global step 200 Train loss 6.38 on epoch=99
06/19/2022 20:26:02 - INFO - __main__ - Global step 200 Train loss 6.49 ACC 0.0 on epoch=99
06/19/2022 20:26:04 - INFO - __main__ - Step 210 Global step 210 Train loss 6.42 on epoch=104
06/19/2022 20:26:05 - INFO - __main__ - Step 220 Global step 220 Train loss 6.38 on epoch=109
06/19/2022 20:26:06 - INFO - __main__ - Step 230 Global step 230 Train loss 6.35 on epoch=114
06/19/2022 20:26:08 - INFO - __main__ - Step 240 Global step 240 Train loss 6.40 on epoch=119
06/19/2022 20:26:09 - INFO - __main__ - Step 250 Global step 250 Train loss 6.27 on epoch=124
06/19/2022 20:26:19 - INFO - __main__ - Global step 250 Train loss 6.36 ACC 0.0 on epoch=124
06/19/2022 20:26:20 - INFO - __main__ - Step 260 Global step 260 Train loss 6.15 on epoch=129
06/19/2022 20:26:21 - INFO - __main__ - Step 270 Global step 270 Train loss 6.13 on epoch=134
06/19/2022 20:26:23 - INFO - __main__ - Step 280 Global step 280 Train loss 6.05 on epoch=139
06/19/2022 20:26:24 - INFO - __main__ - Step 290 Global step 290 Train loss 5.95 on epoch=144
06/19/2022 20:26:25 - INFO - __main__ - Step 300 Global step 300 Train loss 5.94 on epoch=149
06/19/2022 20:26:29 - INFO - __main__ - Global step 300 Train loss 6.04 ACC 0.0 on epoch=149
06/19/2022 20:26:30 - INFO - __main__ - Step 310 Global step 310 Train loss 5.73 on epoch=154
06/19/2022 20:26:32 - INFO - __main__ - Step 320 Global step 320 Train loss 5.71 on epoch=159
06/19/2022 20:26:33 - INFO - __main__ - Step 330 Global step 330 Train loss 5.63 on epoch=164
06/19/2022 20:26:34 - INFO - __main__ - Step 340 Global step 340 Train loss 5.60 on epoch=169
06/19/2022 20:26:36 - INFO - __main__ - Step 350 Global step 350 Train loss 5.55 on epoch=174
06/19/2022 20:26:37 - INFO - __main__ - Global step 350 Train loss 5.64 ACC 0.0 on epoch=174
06/19/2022 20:26:38 - INFO - __main__ - Step 360 Global step 360 Train loss 5.32 on epoch=179
06/19/2022 20:26:40 - INFO - __main__ - Step 370 Global step 370 Train loss 5.32 on epoch=184
06/19/2022 20:26:41 - INFO - __main__ - Step 380 Global step 380 Train loss 5.22 on epoch=189
06/19/2022 20:26:42 - INFO - __main__ - Step 390 Global step 390 Train loss 5.21 on epoch=194
06/19/2022 20:26:44 - INFO - __main__ - Step 400 Global step 400 Train loss 4.98 on epoch=199
06/19/2022 20:26:46 - INFO - __main__ - Global step 400 Train loss 5.21 ACC 0.0 on epoch=199
06/19/2022 20:26:47 - INFO - __main__ - Step 410 Global step 410 Train loss 5.01 on epoch=204
06/19/2022 20:26:48 - INFO - __main__ - Step 420 Global step 420 Train loss 4.94 on epoch=209
06/19/2022 20:26:49 - INFO - __main__ - Step 430 Global step 430 Train loss 4.94 on epoch=214
06/19/2022 20:26:51 - INFO - __main__ - Step 440 Global step 440 Train loss 4.88 on epoch=219
06/19/2022 20:26:52 - INFO - __main__ - Step 450 Global step 450 Train loss 4.67 on epoch=224
06/19/2022 20:26:53 - INFO - __main__ - Global step 450 Train loss 4.89 ACC 0.0 on epoch=224
06/19/2022 20:26:54 - INFO - __main__ - Step 460 Global step 460 Train loss 4.68 on epoch=229
06/19/2022 20:26:56 - INFO - __main__ - Step 470 Global step 470 Train loss 4.51 on epoch=234
06/19/2022 20:26:57 - INFO - __main__ - Step 480 Global step 480 Train loss 4.54 on epoch=239
06/19/2022 20:26:58 - INFO - __main__ - Step 490 Global step 490 Train loss 4.51 on epoch=244
06/19/2022 20:26:59 - INFO - __main__ - Step 500 Global step 500 Train loss 4.44 on epoch=249
06/19/2022 20:27:00 - INFO - __main__ - Global step 500 Train loss 4.54 ACC 0.0 on epoch=249
06/19/2022 20:27:02 - INFO - __main__ - Step 510 Global step 510 Train loss 4.58 on epoch=254
06/19/2022 20:27:03 - INFO - __main__ - Step 520 Global step 520 Train loss 4.48 on epoch=259
06/19/2022 20:27:04 - INFO - __main__ - Step 530 Global step 530 Train loss 4.37 on epoch=264
06/19/2022 20:27:05 - INFO - __main__ - Step 540 Global step 540 Train loss 4.37 on epoch=269
06/19/2022 20:27:07 - INFO - __main__ - Step 550 Global step 550 Train loss 4.25 on epoch=274
06/19/2022 20:27:13 - INFO - __main__ - Global step 550 Train loss 4.41 ACC 0.0 on epoch=274
06/19/2022 20:27:14 - INFO - __main__ - Step 560 Global step 560 Train loss 4.31 on epoch=279
06/19/2022 20:27:15 - INFO - __main__ - Step 570 Global step 570 Train loss 4.16 on epoch=284
06/19/2022 20:27:17 - INFO - __main__ - Step 580 Global step 580 Train loss 4.27 on epoch=289
06/19/2022 20:27:18 - INFO - __main__ - Step 590 Global step 590 Train loss 4.08 on epoch=294
06/19/2022 20:27:19 - INFO - __main__ - Step 600 Global step 600 Train loss 4.10 on epoch=299
06/19/2022 20:27:22 - INFO - __main__ - Global step 600 Train loss 4.18 ACC 0.0 on epoch=299
06/19/2022 20:27:24 - INFO - __main__ - Step 610 Global step 610 Train loss 4.18 on epoch=304
06/19/2022 20:27:25 - INFO - __main__ - Step 620 Global step 620 Train loss 3.96 on epoch=309
06/19/2022 20:27:26 - INFO - __main__ - Step 630 Global step 630 Train loss 3.93 on epoch=314
06/19/2022 20:27:27 - INFO - __main__ - Step 640 Global step 640 Train loss 3.90 on epoch=319
06/19/2022 20:27:29 - INFO - __main__ - Step 650 Global step 650 Train loss 3.88 on epoch=324
06/19/2022 20:27:35 - INFO - __main__ - Global step 650 Train loss 3.97 ACC 0.0 on epoch=324
06/19/2022 20:27:36 - INFO - __main__ - Step 660 Global step 660 Train loss 3.76 on epoch=329
06/19/2022 20:27:38 - INFO - __main__ - Step 670 Global step 670 Train loss 3.76 on epoch=334
06/19/2022 20:27:39 - INFO - __main__ - Step 680 Global step 680 Train loss 3.81 on epoch=339
06/19/2022 20:27:40 - INFO - __main__ - Step 690 Global step 690 Train loss 3.59 on epoch=344
06/19/2022 20:27:42 - INFO - __main__ - Step 700 Global step 700 Train loss 3.65 on epoch=349
06/19/2022 20:27:52 - INFO - __main__ - Global step 700 Train loss 3.71 ACC 0.0 on epoch=349
06/19/2022 20:27:53 - INFO - __main__ - Step 710 Global step 710 Train loss 3.58 on epoch=354
06/19/2022 20:27:55 - INFO - __main__ - Step 720 Global step 720 Train loss 3.52 on epoch=359
06/19/2022 20:27:56 - INFO - __main__ - Step 730 Global step 730 Train loss 3.41 on epoch=364
06/19/2022 20:27:57 - INFO - __main__ - Step 740 Global step 740 Train loss 3.42 on epoch=369
06/19/2022 20:27:59 - INFO - __main__ - Step 750 Global step 750 Train loss 3.38 on epoch=374
06/19/2022 20:28:09 - INFO - __main__ - Global step 750 Train loss 3.46 ACC 0.0 on epoch=374
06/19/2022 20:28:10 - INFO - __main__ - Step 760 Global step 760 Train loss 3.17 on epoch=379
06/19/2022 20:28:12 - INFO - __main__ - Step 770 Global step 770 Train loss 3.24 on epoch=384
06/19/2022 20:28:13 - INFO - __main__ - Step 780 Global step 780 Train loss 3.22 on epoch=389
06/19/2022 20:28:14 - INFO - __main__ - Step 790 Global step 790 Train loss 3.35 on epoch=394
06/19/2022 20:28:16 - INFO - __main__ - Step 800 Global step 800 Train loss 3.12 on epoch=399
06/19/2022 20:28:26 - INFO - __main__ - Global step 800 Train loss 3.22 ACC 0.0 on epoch=399
06/19/2022 20:28:27 - INFO - __main__ - Step 810 Global step 810 Train loss 3.05 on epoch=404
06/19/2022 20:28:29 - INFO - __main__ - Step 820 Global step 820 Train loss 2.99 on epoch=409
06/19/2022 20:28:30 - INFO - __main__ - Step 830 Global step 830 Train loss 3.18 on epoch=414
06/19/2022 20:28:31 - INFO - __main__ - Step 840 Global step 840 Train loss 3.03 on epoch=419
06/19/2022 20:28:33 - INFO - __main__ - Step 850 Global step 850 Train loss 2.96 on epoch=424
06/19/2022 20:28:43 - INFO - __main__ - Global step 850 Train loss 3.04 ACC 0.0 on epoch=424
06/19/2022 20:28:45 - INFO - __main__ - Step 860 Global step 860 Train loss 2.95 on epoch=429
06/19/2022 20:28:46 - INFO - __main__ - Step 870 Global step 870 Train loss 2.88 on epoch=434
06/19/2022 20:28:47 - INFO - __main__ - Step 880 Global step 880 Train loss 2.82 on epoch=439
06/19/2022 20:28:48 - INFO - __main__ - Step 890 Global step 890 Train loss 2.66 on epoch=444
06/19/2022 20:28:50 - INFO - __main__ - Step 900 Global step 900 Train loss 2.70 on epoch=449
06/19/2022 20:29:00 - INFO - __main__ - Global step 900 Train loss 2.80 ACC 0.0 on epoch=449
06/19/2022 20:29:01 - INFO - __main__ - Step 910 Global step 910 Train loss 2.67 on epoch=454
06/19/2022 20:29:03 - INFO - __main__ - Step 920 Global step 920 Train loss 2.75 on epoch=459
06/19/2022 20:29:04 - INFO - __main__ - Step 930 Global step 930 Train loss 2.67 on epoch=464
06/19/2022 20:29:05 - INFO - __main__ - Step 940 Global step 940 Train loss 2.55 on epoch=469
06/19/2022 20:29:06 - INFO - __main__ - Step 950 Global step 950 Train loss 2.44 on epoch=474
06/19/2022 20:29:14 - INFO - __main__ - Global step 950 Train loss 2.62 ACC 0.0 on epoch=474
06/19/2022 20:29:15 - INFO - __main__ - Step 960 Global step 960 Train loss 2.56 on epoch=479
06/19/2022 20:29:16 - INFO - __main__ - Step 970 Global step 970 Train loss 2.44 on epoch=484
06/19/2022 20:29:17 - INFO - __main__ - Step 980 Global step 980 Train loss 2.36 on epoch=489
06/19/2022 20:29:19 - INFO - __main__ - Step 990 Global step 990 Train loss 2.32 on epoch=494
06/19/2022 20:29:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.25 on epoch=499
06/19/2022 20:29:22 - INFO - __main__ - Global step 1000 Train loss 2.39 ACC 0.15625 on epoch=499
06/19/2022 20:29:22 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.15625 on epoch=499, global_step=1000
06/19/2022 20:29:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.22 on epoch=504
06/19/2022 20:29:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.24 on epoch=509
06/19/2022 20:29:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.14 on epoch=514
06/19/2022 20:29:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.17 on epoch=519
06/19/2022 20:29:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.10 on epoch=524
06/19/2022 20:29:29 - INFO - __main__ - Global step 1050 Train loss 2.17 ACC 0.53125 on epoch=524
06/19/2022 20:29:29 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.53125 on epoch=524, global_step=1050
06/19/2022 20:29:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.21 on epoch=529
06/19/2022 20:29:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.09 on epoch=534
06/19/2022 20:29:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.12 on epoch=539
06/19/2022 20:29:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.13 on epoch=544
06/19/2022 20:29:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.04 on epoch=549
06/19/2022 20:29:37 - INFO - __main__ - Global step 1100 Train loss 2.11 ACC 0.5 on epoch=549
06/19/2022 20:29:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.93 on epoch=554
06/19/2022 20:29:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.95 on epoch=559
06/19/2022 20:29:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.02 on epoch=564
06/19/2022 20:29:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.86 on epoch=569
06/19/2022 20:29:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.93 on epoch=574
06/19/2022 20:29:47 - INFO - __main__ - Global step 1150 Train loss 1.94 ACC 0.5625 on epoch=574
06/19/2022 20:29:47 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=574, global_step=1150
06/19/2022 20:29:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.86 on epoch=579
06/19/2022 20:29:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.77 on epoch=584
06/19/2022 20:29:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.75 on epoch=589
06/19/2022 20:29:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.68 on epoch=594
06/19/2022 20:29:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.59 on epoch=599
06/19/2022 20:30:00 - INFO - __main__ - Global step 1200 Train loss 1.73 ACC 0.46875 on epoch=599
06/19/2022 20:30:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.61 on epoch=604
06/19/2022 20:30:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.56 on epoch=609
06/19/2022 20:30:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.52 on epoch=614
06/19/2022 20:30:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.46 on epoch=619
06/19/2022 20:30:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.54 on epoch=624
06/19/2022 20:30:08 - INFO - __main__ - Global step 1250 Train loss 1.54 ACC 0.53125 on epoch=624
06/19/2022 20:30:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.40 on epoch=629
06/19/2022 20:30:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.32 on epoch=634
06/19/2022 20:30:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.28 on epoch=639
06/19/2022 20:30:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.42 on epoch=644
06/19/2022 20:30:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.29 on epoch=649
06/19/2022 20:30:17 - INFO - __main__ - Global step 1300 Train loss 1.34 ACC 0.46875 on epoch=649
06/19/2022 20:30:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.23 on epoch=654
06/19/2022 20:30:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.25 on epoch=659
06/19/2022 20:30:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.24 on epoch=664
06/19/2022 20:30:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.28 on epoch=669
06/19/2022 20:30:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.28 on epoch=674
06/19/2022 20:30:26 - INFO - __main__ - Global step 1350 Train loss 1.26 ACC 0.4375 on epoch=674
06/19/2022 20:30:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.25 on epoch=679
06/19/2022 20:30:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.12 on epoch=684
06/19/2022 20:30:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.13 on epoch=689
06/19/2022 20:30:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.14 on epoch=694
06/19/2022 20:30:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.05 on epoch=699
06/19/2022 20:30:40 - INFO - __main__ - Global step 1400 Train loss 1.14 ACC 0.5625 on epoch=699
06/19/2022 20:30:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.03 on epoch=704
06/19/2022 20:30:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.17 on epoch=709
06/19/2022 20:30:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.04 on epoch=714
06/19/2022 20:30:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.00 on epoch=719
06/19/2022 20:30:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.00 on epoch=724
06/19/2022 20:30:49 - INFO - __main__ - Global step 1450 Train loss 1.05 ACC 0.5 on epoch=724
06/19/2022 20:30:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.01 on epoch=729
06/19/2022 20:30:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.03 on epoch=734
06/19/2022 20:30:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.93 on epoch=739
06/19/2022 20:30:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.87 on epoch=744
06/19/2022 20:30:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.91 on epoch=749
06/19/2022 20:30:57 - INFO - __main__ - Global step 1500 Train loss 0.95 ACC 0.53125 on epoch=749
06/19/2022 20:30:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.92 on epoch=754
06/19/2022 20:31:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.89 on epoch=759
06/19/2022 20:31:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.84 on epoch=764
06/19/2022 20:31:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.85 on epoch=769
06/19/2022 20:31:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.84 on epoch=774
06/19/2022 20:31:05 - INFO - __main__ - Global step 1550 Train loss 0.87 ACC 0.5 on epoch=774
06/19/2022 20:31:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.85 on epoch=779
06/19/2022 20:31:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.79 on epoch=784
06/19/2022 20:31:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.71 on epoch=789
06/19/2022 20:31:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.67 on epoch=794
06/19/2022 20:31:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.75 on epoch=799
06/19/2022 20:31:13 - INFO - __main__ - Global step 1600 Train loss 0.76 ACC 0.5 on epoch=799
06/19/2022 20:31:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.71 on epoch=804
06/19/2022 20:31:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.75 on epoch=809
06/19/2022 20:31:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.70 on epoch=814
06/19/2022 20:31:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.66 on epoch=819
06/19/2022 20:31:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.58 on epoch=824
06/19/2022 20:31:20 - INFO - __main__ - Global step 1650 Train loss 0.68 ACC 0.5 on epoch=824
06/19/2022 20:31:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.57 on epoch=829
06/19/2022 20:31:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.58 on epoch=834
06/19/2022 20:31:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.58 on epoch=839
06/19/2022 20:31:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.56 on epoch=844
06/19/2022 20:31:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=849
06/19/2022 20:31:29 - INFO - __main__ - Global step 1700 Train loss 0.57 ACC 0.5 on epoch=849
06/19/2022 20:31:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.62 on epoch=854
06/19/2022 20:31:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.60 on epoch=859
06/19/2022 20:31:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.50 on epoch=864
06/19/2022 20:31:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.61 on epoch=869
06/19/2022 20:31:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.50 on epoch=874
06/19/2022 20:31:37 - INFO - __main__ - Global step 1750 Train loss 0.57 ACC 0.5 on epoch=874
06/19/2022 20:31:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.59 on epoch=879
06/19/2022 20:31:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.60 on epoch=884
06/19/2022 20:31:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.64 on epoch=889
06/19/2022 20:31:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=894
06/19/2022 20:31:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.56 on epoch=899
06/19/2022 20:31:44 - INFO - __main__ - Global step 1800 Train loss 0.58 ACC 0.5 on epoch=899
06/19/2022 20:31:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=904
06/19/2022 20:31:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.51 on epoch=909
06/19/2022 20:31:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=914
06/19/2022 20:31:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=919
06/19/2022 20:31:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.47 on epoch=924
06/19/2022 20:31:51 - INFO - __main__ - Global step 1850 Train loss 0.51 ACC 0.5 on epoch=924
06/19/2022 20:31:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.59 on epoch=929
06/19/2022 20:31:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.54 on epoch=934
06/19/2022 20:31:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=939
06/19/2022 20:31:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.55 on epoch=944
06/19/2022 20:31:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.49 on epoch=949
06/19/2022 20:31:58 - INFO - __main__ - Global step 1900 Train loss 0.51 ACC 0.53125 on epoch=949
06/19/2022 20:31:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.50 on epoch=954
06/19/2022 20:32:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/19/2022 20:32:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.51 on epoch=964
06/19/2022 20:32:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.53 on epoch=969
06/19/2022 20:32:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.46 on epoch=974
06/19/2022 20:32:04 - INFO - __main__ - Global step 1950 Train loss 0.49 ACC 0.59375 on epoch=974
06/19/2022 20:32:04 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=974, global_step=1950
06/19/2022 20:32:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=979
06/19/2022 20:32:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
06/19/2022 20:32:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=989
06/19/2022 20:32:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=994
06/19/2022 20:32:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=999
06/19/2022 20:32:11 - INFO - __main__ - Global step 2000 Train loss 0.44 ACC 0.46875 on epoch=999
06/19/2022 20:32:11 - INFO - __main__ - save last model!
06/19/2022 20:32:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 20:32:11 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 20:32:11 - INFO - __main__ - Printing 3 examples
06/19/2022 20:32:11 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 20:32:11 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 20:32:11 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:32:11 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 20:32:11 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:32:11 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:32:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:32:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:32:12 - INFO - __main__ - Printing 3 examples
06/19/2022 20:32:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 20:32:12 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:12 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 20:32:12 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 20:32:12 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 20:32:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:32:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:32:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:32:12 - INFO - __main__ - Printing 3 examples
06/19/2022 20:32:12 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 20:32:12 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:12 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 20:32:12 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 20:32:12 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:12 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:32:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:32:12 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 20:32:12 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:32:17 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:32:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:32:17 - INFO - __main__ - Starting training!
06/19/2022 20:32:20 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.3_8_predictions.txt
06/19/2022 20:32:20 - INFO - __main__ - ACC on test data: 0.6569
06/19/2022 20:32:20 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.3, bsz=8, dev_performance=0.59375, test_performance=0.6568627450980392
06/19/2022 20:32:20 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.2, bsz=8 ...
06/19/2022 20:32:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:32:21 - INFO - __main__ - Printing 3 examples
06/19/2022 20:32:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/19/2022 20:32:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:21 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher — a three-term congressman from Lexington — had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/19/2022 20:32:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/19/2022 20:32:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 20:32:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:32:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:32:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:32:21 - INFO - __main__ - Printing 3 examples
06/19/2022 20:32:21 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/19/2022 20:32:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:21 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/19/2022 20:32:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that – $ US51 billion – was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/19/2022 20:32:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:32:21 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:32:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:32:21 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:32:27 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:32:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:32:27 - INFO - __main__ - Starting training!
06/19/2022 20:32:29 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/19/2022 20:32:30 - INFO - __main__ - Step 20 Global step 20 Train loss 6.78 on epoch=9
06/19/2022 20:32:32 - INFO - __main__ - Step 30 Global step 30 Train loss 6.83 on epoch=14
06/19/2022 20:32:33 - INFO - __main__ - Step 40 Global step 40 Train loss 6.87 on epoch=19
06/19/2022 20:32:35 - INFO - __main__ - Step 50 Global step 50 Train loss 6.82 on epoch=24
06/19/2022 20:32:38 - INFO - __main__ - Global step 50 Train loss 6.83 ACC 0.0 on epoch=24
06/19/2022 20:32:38 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 20:32:39 - INFO - __main__ - Step 60 Global step 60 Train loss 6.76 on epoch=29
06/19/2022 20:32:41 - INFO - __main__ - Step 70 Global step 70 Train loss 6.83 on epoch=34
06/19/2022 20:32:42 - INFO - __main__ - Step 80 Global step 80 Train loss 6.91 on epoch=39
06/19/2022 20:32:43 - INFO - __main__ - Step 90 Global step 90 Train loss 6.86 on epoch=44
06/19/2022 20:32:45 - INFO - __main__ - Step 100 Global step 100 Train loss 6.88 on epoch=49
06/19/2022 20:32:47 - INFO - __main__ - Global step 100 Train loss 6.85 ACC 0.0 on epoch=49
06/19/2022 20:32:48 - INFO - __main__ - Step 110 Global step 110 Train loss 6.83 on epoch=54
06/19/2022 20:32:50 - INFO - __main__ - Step 120 Global step 120 Train loss 6.81 on epoch=59
06/19/2022 20:32:51 - INFO - __main__ - Step 130 Global step 130 Train loss 6.80 on epoch=64
06/19/2022 20:32:52 - INFO - __main__ - Step 140 Global step 140 Train loss 6.78 on epoch=69
06/19/2022 20:32:54 - INFO - __main__ - Step 150 Global step 150 Train loss 6.75 on epoch=74
06/19/2022 20:32:55 - INFO - __main__ - Global step 150 Train loss 6.79 ACC 0.0 on epoch=74
06/19/2022 20:32:56 - INFO - __main__ - Step 160 Global step 160 Train loss 6.71 on epoch=79
06/19/2022 20:32:57 - INFO - __main__ - Step 170 Global step 170 Train loss 6.69 on epoch=84
06/19/2022 20:32:58 - INFO - __main__ - Step 180 Global step 180 Train loss 6.67 on epoch=89
06/19/2022 20:33:00 - INFO - __main__ - Step 190 Global step 190 Train loss 6.77 on epoch=94
06/19/2022 20:33:01 - INFO - __main__ - Step 200 Global step 200 Train loss 6.66 on epoch=99
06/19/2022 20:33:04 - INFO - __main__ - Global step 200 Train loss 6.70 ACC 0.0 on epoch=99
06/19/2022 20:33:05 - INFO - __main__ - Step 210 Global step 210 Train loss 6.67 on epoch=104
06/19/2022 20:33:06 - INFO - __main__ - Step 220 Global step 220 Train loss 6.64 on epoch=109
06/19/2022 20:33:08 - INFO - __main__ - Step 230 Global step 230 Train loss 6.68 on epoch=114
06/19/2022 20:33:09 - INFO - __main__ - Step 240 Global step 240 Train loss 6.57 on epoch=119
06/19/2022 20:33:10 - INFO - __main__ - Step 250 Global step 250 Train loss 6.51 on epoch=124
06/19/2022 20:33:17 - INFO - __main__ - Global step 250 Train loss 6.61 ACC 0.0 on epoch=124
06/19/2022 20:33:19 - INFO - __main__ - Step 260 Global step 260 Train loss 6.55 on epoch=129
06/19/2022 20:33:20 - INFO - __main__ - Step 270 Global step 270 Train loss 6.54 on epoch=134
06/19/2022 20:33:21 - INFO - __main__ - Step 280 Global step 280 Train loss 6.42 on epoch=139
06/19/2022 20:33:23 - INFO - __main__ - Step 290 Global step 290 Train loss 6.47 on epoch=144
06/19/2022 20:33:24 - INFO - __main__ - Step 300 Global step 300 Train loss 6.38 on epoch=149
06/19/2022 20:33:31 - INFO - __main__ - Global step 300 Train loss 6.47 ACC 0.0 on epoch=149
06/19/2022 20:33:32 - INFO - __main__ - Step 310 Global step 310 Train loss 6.37 on epoch=154
06/19/2022 20:33:33 - INFO - __main__ - Step 320 Global step 320 Train loss 6.40 on epoch=159
06/19/2022 20:33:35 - INFO - __main__ - Step 330 Global step 330 Train loss 6.38 on epoch=164
06/19/2022 20:33:36 - INFO - __main__ - Step 340 Global step 340 Train loss 6.27 on epoch=169
06/19/2022 20:33:37 - INFO - __main__ - Step 350 Global step 350 Train loss 6.22 on epoch=174
06/19/2022 20:33:42 - INFO - __main__ - Global step 350 Train loss 6.33 ACC 0.0 on epoch=174
06/19/2022 20:33:43 - INFO - __main__ - Step 360 Global step 360 Train loss 6.18 on epoch=179
06/19/2022 20:33:44 - INFO - __main__ - Step 370 Global step 370 Train loss 6.24 on epoch=184
06/19/2022 20:33:45 - INFO - __main__ - Step 380 Global step 380 Train loss 6.11 on epoch=189
06/19/2022 20:33:46 - INFO - __main__ - Step 390 Global step 390 Train loss 6.10 on epoch=194
06/19/2022 20:33:48 - INFO - __main__ - Step 400 Global step 400 Train loss 6.16 on epoch=199
06/19/2022 20:33:50 - INFO - __main__ - Global step 400 Train loss 6.16 ACC 0.0 on epoch=199
06/19/2022 20:33:52 - INFO - __main__ - Step 410 Global step 410 Train loss 6.07 on epoch=204
06/19/2022 20:33:53 - INFO - __main__ - Step 420 Global step 420 Train loss 6.07 on epoch=209
06/19/2022 20:33:54 - INFO - __main__ - Step 430 Global step 430 Train loss 5.95 on epoch=214
06/19/2022 20:33:55 - INFO - __main__ - Step 440 Global step 440 Train loss 5.89 on epoch=219
06/19/2022 20:33:57 - INFO - __main__ - Step 450 Global step 450 Train loss 5.96 on epoch=224
06/19/2022 20:33:59 - INFO - __main__ - Global step 450 Train loss 5.99 ACC 0.0 on epoch=224
06/19/2022 20:34:00 - INFO - __main__ - Step 460 Global step 460 Train loss 5.75 on epoch=229
06/19/2022 20:34:01 - INFO - __main__ - Step 470 Global step 470 Train loss 5.71 on epoch=234
06/19/2022 20:34:03 - INFO - __main__ - Step 480 Global step 480 Train loss 5.71 on epoch=239
06/19/2022 20:34:04 - INFO - __main__ - Step 490 Global step 490 Train loss 5.78 on epoch=244
06/19/2022 20:34:05 - INFO - __main__ - Step 500 Global step 500 Train loss 5.56 on epoch=249
06/19/2022 20:34:07 - INFO - __main__ - Global step 500 Train loss 5.70 ACC 0.0 on epoch=249
06/19/2022 20:34:09 - INFO - __main__ - Step 510 Global step 510 Train loss 5.48 on epoch=254
06/19/2022 20:34:10 - INFO - __main__ - Step 520 Global step 520 Train loss 5.48 on epoch=259
06/19/2022 20:34:11 - INFO - __main__ - Step 530 Global step 530 Train loss 5.35 on epoch=264
06/19/2022 20:34:12 - INFO - __main__ - Step 540 Global step 540 Train loss 5.21 on epoch=269
06/19/2022 20:34:13 - INFO - __main__ - Step 550 Global step 550 Train loss 5.33 on epoch=274
06/19/2022 20:34:15 - INFO - __main__ - Global step 550 Train loss 5.37 ACC 0.0 on epoch=274
06/19/2022 20:34:17 - INFO - __main__ - Step 560 Global step 560 Train loss 5.33 on epoch=279
06/19/2022 20:34:18 - INFO - __main__ - Step 570 Global step 570 Train loss 4.98 on epoch=284
06/19/2022 20:34:19 - INFO - __main__ - Step 580 Global step 580 Train loss 5.21 on epoch=289
06/19/2022 20:34:20 - INFO - __main__ - Step 590 Global step 590 Train loss 5.41 on epoch=294
06/19/2022 20:34:22 - INFO - __main__ - Step 600 Global step 600 Train loss 5.43 on epoch=299
06/19/2022 20:34:27 - INFO - __main__ - Global step 600 Train loss 5.27 ACC 0.0 on epoch=299
06/19/2022 20:34:29 - INFO - __main__ - Step 610 Global step 610 Train loss 5.31 on epoch=304
06/19/2022 20:34:30 - INFO - __main__ - Step 620 Global step 620 Train loss 5.32 on epoch=309
06/19/2022 20:34:31 - INFO - __main__ - Step 630 Global step 630 Train loss 5.13 on epoch=314
06/19/2022 20:34:32 - INFO - __main__ - Step 640 Global step 640 Train loss 4.95 on epoch=319
06/19/2022 20:34:33 - INFO - __main__ - Step 650 Global step 650 Train loss 4.93 on epoch=324
06/19/2022 20:34:39 - INFO - __main__ - Global step 650 Train loss 5.13 ACC 0.0 on epoch=324
06/19/2022 20:34:40 - INFO - __main__ - Step 660 Global step 660 Train loss 4.94 on epoch=329
06/19/2022 20:34:42 - INFO - __main__ - Step 670 Global step 670 Train loss 4.91 on epoch=334
06/19/2022 20:34:43 - INFO - __main__ - Step 680 Global step 680 Train loss 4.84 on epoch=339
06/19/2022 20:34:44 - INFO - __main__ - Step 690 Global step 690 Train loss 4.91 on epoch=344
06/19/2022 20:34:45 - INFO - __main__ - Step 700 Global step 700 Train loss 4.83 on epoch=349
06/19/2022 20:34:48 - INFO - __main__ - Global step 700 Train loss 4.89 ACC 0.0 on epoch=349
06/19/2022 20:34:50 - INFO - __main__ - Step 710 Global step 710 Train loss 4.77 on epoch=354
06/19/2022 20:34:51 - INFO - __main__ - Step 720 Global step 720 Train loss 4.85 on epoch=359
06/19/2022 20:34:52 - INFO - __main__ - Step 730 Global step 730 Train loss 4.70 on epoch=364
06/19/2022 20:34:53 - INFO - __main__ - Step 740 Global step 740 Train loss 4.63 on epoch=369
06/19/2022 20:34:55 - INFO - __main__ - Step 750 Global step 750 Train loss 4.66 on epoch=374
06/19/2022 20:34:56 - INFO - __main__ - Global step 750 Train loss 4.72 ACC 0.0 on epoch=374
06/19/2022 20:34:57 - INFO - __main__ - Step 760 Global step 760 Train loss 4.63 on epoch=379
06/19/2022 20:34:59 - INFO - __main__ - Step 770 Global step 770 Train loss 4.67 on epoch=384
06/19/2022 20:35:00 - INFO - __main__ - Step 780 Global step 780 Train loss 4.79 on epoch=389
06/19/2022 20:35:01 - INFO - __main__ - Step 790 Global step 790 Train loss 4.63 on epoch=394
06/19/2022 20:35:03 - INFO - __main__ - Step 800 Global step 800 Train loss 4.68 on epoch=399
06/19/2022 20:35:10 - INFO - __main__ - Global step 800 Train loss 4.68 ACC 0.0 on epoch=399
06/19/2022 20:35:11 - INFO - __main__ - Step 810 Global step 810 Train loss 4.46 on epoch=404
06/19/2022 20:35:12 - INFO - __main__ - Step 820 Global step 820 Train loss 4.53 on epoch=409
06/19/2022 20:35:13 - INFO - __main__ - Step 830 Global step 830 Train loss 4.43 on epoch=414
06/19/2022 20:35:15 - INFO - __main__ - Step 840 Global step 840 Train loss 4.56 on epoch=419
06/19/2022 20:35:16 - INFO - __main__ - Step 850 Global step 850 Train loss 4.51 on epoch=424
06/19/2022 20:35:17 - INFO - __main__ - Global step 850 Train loss 4.50 ACC 0.0 on epoch=424
06/19/2022 20:35:19 - INFO - __main__ - Step 860 Global step 860 Train loss 4.43 on epoch=429
06/19/2022 20:35:20 - INFO - __main__ - Step 870 Global step 870 Train loss 4.43 on epoch=434
06/19/2022 20:35:21 - INFO - __main__ - Step 880 Global step 880 Train loss 4.53 on epoch=439
06/19/2022 20:35:22 - INFO - __main__ - Step 890 Global step 890 Train loss 4.47 on epoch=444
06/19/2022 20:35:24 - INFO - __main__ - Step 900 Global step 900 Train loss 4.33 on epoch=449
06/19/2022 20:35:24 - INFO - __main__ - Global step 900 Train loss 4.44 ACC 0.0 on epoch=449
06/19/2022 20:35:26 - INFO - __main__ - Step 910 Global step 910 Train loss 4.44 on epoch=454
06/19/2022 20:35:27 - INFO - __main__ - Step 920 Global step 920 Train loss 4.35 on epoch=459
06/19/2022 20:35:28 - INFO - __main__ - Step 930 Global step 930 Train loss 4.32 on epoch=464
06/19/2022 20:35:29 - INFO - __main__ - Step 940 Global step 940 Train loss 4.41 on epoch=469
06/19/2022 20:35:30 - INFO - __main__ - Step 950 Global step 950 Train loss 4.34 on epoch=474
06/19/2022 20:35:32 - INFO - __main__ - Global step 950 Train loss 4.37 ACC 0.0 on epoch=474
06/19/2022 20:35:33 - INFO - __main__ - Step 960 Global step 960 Train loss 4.48 on epoch=479
06/19/2022 20:35:34 - INFO - __main__ - Step 970 Global step 970 Train loss 4.20 on epoch=484
06/19/2022 20:35:35 - INFO - __main__ - Step 980 Global step 980 Train loss 4.35 on epoch=489
06/19/2022 20:35:36 - INFO - __main__ - Step 990 Global step 990 Train loss 4.26 on epoch=494
06/19/2022 20:35:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.27 on epoch=499
06/19/2022 20:35:39 - INFO - __main__ - Global step 1000 Train loss 4.31 ACC 0.0 on epoch=499
06/19/2022 20:35:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.31 on epoch=504
06/19/2022 20:35:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.15 on epoch=509
06/19/2022 20:35:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 4.13 on epoch=514
06/19/2022 20:35:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 4.18 on epoch=519
06/19/2022 20:35:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 4.18 on epoch=524
06/19/2022 20:35:47 - INFO - __main__ - Global step 1050 Train loss 4.19 ACC 0.0 on epoch=524
06/19/2022 20:35:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.12 on epoch=529
06/19/2022 20:35:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.04 on epoch=534
06/19/2022 20:35:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.09 on epoch=539
06/19/2022 20:35:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 4.06 on epoch=544
06/19/2022 20:35:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.00 on epoch=549
06/19/2022 20:35:56 - INFO - __main__ - Global step 1100 Train loss 4.06 ACC 0.0 on epoch=549
06/19/2022 20:35:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 4.03 on epoch=554
06/19/2022 20:35:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.95 on epoch=559
06/19/2022 20:36:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.94 on epoch=564
06/19/2022 20:36:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.92 on epoch=569
06/19/2022 20:36:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 4.08 on epoch=574
06/19/2022 20:36:06 - INFO - __main__ - Global step 1150 Train loss 3.98 ACC 0.0 on epoch=574
06/19/2022 20:36:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 4.05 on epoch=579
06/19/2022 20:36:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.96 on epoch=584
06/19/2022 20:36:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.98 on epoch=589
06/19/2022 20:36:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.88 on epoch=594
06/19/2022 20:36:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.93 on epoch=599
06/19/2022 20:36:13 - INFO - __main__ - Global step 1200 Train loss 3.96 ACC 0.0 on epoch=599
06/19/2022 20:36:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.90 on epoch=604
06/19/2022 20:36:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.65 on epoch=609
06/19/2022 20:36:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.88 on epoch=614
06/19/2022 20:36:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.81 on epoch=619
06/19/2022 20:36:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.80 on epoch=624
06/19/2022 20:36:21 - INFO - __main__ - Global step 1250 Train loss 3.81 ACC 0.0 on epoch=624
06/19/2022 20:36:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.80 on epoch=629
06/19/2022 20:36:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.88 on epoch=634
06/19/2022 20:36:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.75 on epoch=639
06/19/2022 20:36:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.66 on epoch=644
06/19/2022 20:36:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.66 on epoch=649
06/19/2022 20:36:34 - INFO - __main__ - Global step 1300 Train loss 3.75 ACC 0.0 on epoch=649
06/19/2022 20:36:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.48 on epoch=654
06/19/2022 20:36:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.65 on epoch=659
06/19/2022 20:36:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.63 on epoch=664
06/19/2022 20:36:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.65 on epoch=669
06/19/2022 20:36:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.52 on epoch=674
06/19/2022 20:36:47 - INFO - __main__ - Global step 1350 Train loss 3.59 ACC 0.0 on epoch=674
06/19/2022 20:36:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 3.60 on epoch=679
06/19/2022 20:36:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.46 on epoch=684
06/19/2022 20:36:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 3.61 on epoch=689
06/19/2022 20:36:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 3.48 on epoch=694
06/19/2022 20:36:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 3.40 on epoch=699
06/19/2022 20:36:57 - INFO - __main__ - Global step 1400 Train loss 3.51 ACC 0.0 on epoch=699
06/19/2022 20:36:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 3.37 on epoch=704
06/19/2022 20:37:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 3.36 on epoch=709
06/19/2022 20:37:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 3.32 on epoch=714
06/19/2022 20:37:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 3.31 on epoch=719
06/19/2022 20:37:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 3.23 on epoch=724
06/19/2022 20:37:08 - INFO - __main__ - Global step 1450 Train loss 3.31 ACC 0.0 on epoch=724
06/19/2022 20:37:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 3.37 on epoch=729
06/19/2022 20:37:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 3.40 on epoch=734
06/19/2022 20:37:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 3.30 on epoch=739
06/19/2022 20:37:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 3.15 on epoch=744
06/19/2022 20:37:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 3.29 on epoch=749
06/19/2022 20:37:16 - INFO - __main__ - Global step 1500 Train loss 3.30 ACC 0.0 on epoch=749
06/19/2022 20:37:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 3.32 on epoch=754
06/19/2022 20:37:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 3.17 on epoch=759
06/19/2022 20:37:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 3.25 on epoch=764
06/19/2022 20:37:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 3.11 on epoch=769
06/19/2022 20:37:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 3.20 on epoch=774
06/19/2022 20:37:31 - INFO - __main__ - Global step 1550 Train loss 3.21 ACC 0.0 on epoch=774
06/19/2022 20:37:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 3.19 on epoch=779
06/19/2022 20:37:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 3.07 on epoch=784
06/19/2022 20:37:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 3.19 on epoch=789
06/19/2022 20:37:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 3.05 on epoch=794
06/19/2022 20:37:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 3.03 on epoch=799
06/19/2022 20:37:40 - INFO - __main__ - Global step 1600 Train loss 3.11 ACC 0.0 on epoch=799
06/19/2022 20:37:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.89 on epoch=804
06/19/2022 20:37:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.95 on epoch=809
06/19/2022 20:37:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 2.90 on epoch=814
06/19/2022 20:37:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 2.95 on epoch=819
06/19/2022 20:37:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 2.93 on epoch=824
06/19/2022 20:37:48 - INFO - __main__ - Global step 1650 Train loss 2.92 ACC 0.0 on epoch=824
06/19/2022 20:37:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 2.78 on epoch=829
06/19/2022 20:37:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 2.89 on epoch=834
06/19/2022 20:37:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 2.82 on epoch=839
06/19/2022 20:37:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 2.90 on epoch=844
06/19/2022 20:37:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 2.72 on epoch=849
06/19/2022 20:37:59 - INFO - __main__ - Global step 1700 Train loss 2.82 ACC 0.0 on epoch=849
06/19/2022 20:38:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 2.78 on epoch=854
06/19/2022 20:38:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 2.74 on epoch=859
06/19/2022 20:38:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 2.71 on epoch=864
06/19/2022 20:38:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 2.71 on epoch=869
06/19/2022 20:38:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 2.69 on epoch=874
06/19/2022 20:38:10 - INFO - __main__ - Global step 1750 Train loss 2.73 ACC 0.0 on epoch=874
06/19/2022 20:38:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 2.71 on epoch=879
06/19/2022 20:38:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 2.66 on epoch=884
06/19/2022 20:38:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 2.61 on epoch=889
06/19/2022 20:38:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 2.56 on epoch=894
06/19/2022 20:38:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 2.54 on epoch=899
06/19/2022 20:38:18 - INFO - __main__ - Global step 1800 Train loss 2.62 ACC 0.0 on epoch=899
06/19/2022 20:38:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 2.45 on epoch=904
06/19/2022 20:38:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 2.52 on epoch=909
06/19/2022 20:38:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 2.43 on epoch=914
06/19/2022 20:38:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 2.38 on epoch=919
06/19/2022 20:38:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 2.41 on epoch=924
06/19/2022 20:38:28 - INFO - __main__ - Global step 1850 Train loss 2.44 ACC 0.0 on epoch=924
06/19/2022 20:38:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 2.33 on epoch=929
06/19/2022 20:38:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 2.32 on epoch=934
06/19/2022 20:38:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 2.28 on epoch=939
06/19/2022 20:38:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 2.32 on epoch=944
06/19/2022 20:38:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 2.27 on epoch=949
06/19/2022 20:38:37 - INFO - __main__ - Global step 1900 Train loss 2.30 ACC 0.0 on epoch=949
06/19/2022 20:38:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 2.25 on epoch=954
06/19/2022 20:38:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 2.16 on epoch=959
06/19/2022 20:38:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 2.17 on epoch=964
06/19/2022 20:38:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 2.17 on epoch=969
06/19/2022 20:38:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 2.25 on epoch=974
06/19/2022 20:38:45 - INFO - __main__ - Global step 1950 Train loss 2.20 ACC 0.0 on epoch=974
06/19/2022 20:38:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 2.18 on epoch=979
06/19/2022 20:38:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 2.11 on epoch=984
06/19/2022 20:38:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.93 on epoch=989
06/19/2022 20:38:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 2.06 on epoch=994
06/19/2022 20:38:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 2.03 on epoch=999
06/19/2022 20:38:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:38:53 - INFO - __main__ - Printing 3 examples
06/19/2022 20:38:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 20:38:53 - INFO - __main__ - ['equivalent']
06/19/2022 20:38:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 20:38:53 - INFO - __main__ - ['equivalent']
06/19/2022 20:38:53 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 20:38:53 - INFO - __main__ - ['equivalent']
06/19/2022 20:38:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 20:38:53 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:38:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:38:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:38:53 - INFO - __main__ - Printing 3 examples
06/19/2022 20:38:53 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 20:38:53 - INFO - __main__ - ['equivalent']
06/19/2022 20:38:53 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 20:38:53 - INFO - __main__ - ['equivalent']
06/19/2022 20:38:53 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 20:38:53 - INFO - __main__ - ['equivalent']
06/19/2022 20:38:53 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:38:53 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:38:53 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:38:53 - INFO - __main__ - Global step 2000 Train loss 2.06 ACC 0.09375 on epoch=999
06/19/2022 20:38:53 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=999, global_step=2000
06/19/2022 20:38:53 - INFO - __main__ - save last model!
06/19/2022 20:38:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 20:38:53 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 20:38:53 - INFO - __main__ - Printing 3 examples
06/19/2022 20:38:53 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 20:38:53 - INFO - __main__ - ['equivalent']
06/19/2022 20:38:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 20:38:53 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:38:53 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 20:38:53 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:38:53 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:38:54 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:38:54 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 20:38:59 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:39:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:39:00 - INFO - __main__ - Starting training!
06/19/2022 20:39:20 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.2_8_predictions.txt
06/19/2022 20:39:20 - INFO - __main__ - ACC on test data: 0.1103
06/19/2022 20:39:20 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.2, bsz=8, dev_performance=0.09375, test_performance=0.11029411764705882
06/19/2022 20:39:20 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.5, bsz=8 ...
06/19/2022 20:39:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:39:21 - INFO - __main__ - Printing 3 examples
06/19/2022 20:39:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 20:39:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:39:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 20:39:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:39:21 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 20:39:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:39:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 20:39:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:39:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:39:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:39:21 - INFO - __main__ - Printing 3 examples
06/19/2022 20:39:21 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 20:39:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:39:21 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 20:39:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:39:21 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 20:39:21 - INFO - __main__ - ['equivalent']
06/19/2022 20:39:21 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:39:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:39:21 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:39:27 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:39:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:39:27 - INFO - __main__ - Starting training!
06/19/2022 20:39:29 - INFO - __main__ - Step 10 Global step 10 Train loss 6.84 on epoch=4
06/19/2022 20:39:30 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=9
06/19/2022 20:39:31 - INFO - __main__ - Step 30 Global step 30 Train loss 6.82 on epoch=14
06/19/2022 20:39:33 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/19/2022 20:39:34 - INFO - __main__ - Step 50 Global step 50 Train loss 6.83 on epoch=24
06/19/2022 20:39:36 - INFO - __main__ - Global step 50 Train loss 6.83 ACC 0.0 on epoch=24
06/19/2022 20:39:36 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 20:39:37 - INFO - __main__ - Step 60 Global step 60 Train loss 6.75 on epoch=29
06/19/2022 20:39:39 - INFO - __main__ - Step 70 Global step 70 Train loss 6.79 on epoch=34
06/19/2022 20:39:40 - INFO - __main__ - Step 80 Global step 80 Train loss 6.72 on epoch=39
06/19/2022 20:39:41 - INFO - __main__ - Step 90 Global step 90 Train loss 6.73 on epoch=44
06/19/2022 20:39:42 - INFO - __main__ - Step 100 Global step 100 Train loss 6.58 on epoch=49
06/19/2022 20:39:53 - INFO - __main__ - Global step 100 Train loss 6.72 ACC 0.0 on epoch=49
06/19/2022 20:39:54 - INFO - __main__ - Step 110 Global step 110 Train loss 6.50 on epoch=54
06/19/2022 20:39:56 - INFO - __main__ - Step 120 Global step 120 Train loss 6.46 on epoch=59
06/19/2022 20:39:57 - INFO - __main__ - Step 130 Global step 130 Train loss 6.24 on epoch=64
06/19/2022 20:39:59 - INFO - __main__ - Step 140 Global step 140 Train loss 6.19 on epoch=69
06/19/2022 20:40:00 - INFO - __main__ - Step 150 Global step 150 Train loss 6.05 on epoch=74
06/19/2022 20:40:03 - INFO - __main__ - Global step 150 Train loss 6.29 ACC 0.0 on epoch=74
06/19/2022 20:40:04 - INFO - __main__ - Step 160 Global step 160 Train loss 6.13 on epoch=79
06/19/2022 20:40:05 - INFO - __main__ - Step 170 Global step 170 Train loss 6.00 on epoch=84
06/19/2022 20:40:06 - INFO - __main__ - Step 180 Global step 180 Train loss 5.83 on epoch=89
06/19/2022 20:40:08 - INFO - __main__ - Step 190 Global step 190 Train loss 5.75 on epoch=94
06/19/2022 20:40:09 - INFO - __main__ - Step 200 Global step 200 Train loss 5.60 on epoch=99
06/19/2022 20:40:15 - INFO - __main__ - Global step 200 Train loss 5.86 ACC 0.0 on epoch=99
06/19/2022 20:40:16 - INFO - __main__ - Step 210 Global step 210 Train loss 5.62 on epoch=104
06/19/2022 20:40:18 - INFO - __main__ - Step 220 Global step 220 Train loss 5.37 on epoch=109
06/19/2022 20:40:19 - INFO - __main__ - Step 230 Global step 230 Train loss 5.23 on epoch=114
06/19/2022 20:40:21 - INFO - __main__ - Step 240 Global step 240 Train loss 5.12 on epoch=119
06/19/2022 20:40:22 - INFO - __main__ - Step 250 Global step 250 Train loss 5.09 on epoch=124
06/19/2022 20:40:23 - INFO - __main__ - Global step 250 Train loss 5.29 ACC 0.0 on epoch=124
06/19/2022 20:40:24 - INFO - __main__ - Step 260 Global step 260 Train loss 4.89 on epoch=129
06/19/2022 20:40:26 - INFO - __main__ - Step 270 Global step 270 Train loss 4.76 on epoch=134
06/19/2022 20:40:27 - INFO - __main__ - Step 280 Global step 280 Train loss 4.73 on epoch=139
06/19/2022 20:40:28 - INFO - __main__ - Step 290 Global step 290 Train loss 4.48 on epoch=144
06/19/2022 20:40:29 - INFO - __main__ - Step 300 Global step 300 Train loss 4.32 on epoch=149
06/19/2022 20:40:31 - INFO - __main__ - Global step 300 Train loss 4.64 ACC 0.0 on epoch=149
06/19/2022 20:40:32 - INFO - __main__ - Step 310 Global step 310 Train loss 4.22 on epoch=154
06/19/2022 20:40:33 - INFO - __main__ - Step 320 Global step 320 Train loss 4.14 on epoch=159
06/19/2022 20:40:34 - INFO - __main__ - Step 330 Global step 330 Train loss 4.05 on epoch=164
06/19/2022 20:40:36 - INFO - __main__ - Step 340 Global step 340 Train loss 4.06 on epoch=169
06/19/2022 20:40:37 - INFO - __main__ - Step 350 Global step 350 Train loss 3.92 on epoch=174
06/19/2022 20:40:39 - INFO - __main__ - Global step 350 Train loss 4.08 ACC 0.0 on epoch=174
06/19/2022 20:40:40 - INFO - __main__ - Step 360 Global step 360 Train loss 3.86 on epoch=179
06/19/2022 20:40:41 - INFO - __main__ - Step 370 Global step 370 Train loss 3.59 on epoch=184
06/19/2022 20:40:43 - INFO - __main__ - Step 380 Global step 380 Train loss 3.56 on epoch=189
06/19/2022 20:40:44 - INFO - __main__ - Step 390 Global step 390 Train loss 3.63 on epoch=194
06/19/2022 20:40:45 - INFO - __main__ - Step 400 Global step 400 Train loss 3.39 on epoch=199
06/19/2022 20:40:48 - INFO - __main__ - Global step 400 Train loss 3.61 ACC 0.0 on epoch=199
06/19/2022 20:40:49 - INFO - __main__ - Step 410 Global step 410 Train loss 3.22 on epoch=204
06/19/2022 20:40:50 - INFO - __main__ - Step 420 Global step 420 Train loss 3.20 on epoch=209
06/19/2022 20:40:51 - INFO - __main__ - Step 430 Global step 430 Train loss 3.01 on epoch=214
06/19/2022 20:40:53 - INFO - __main__ - Step 440 Global step 440 Train loss 3.22 on epoch=219
06/19/2022 20:40:54 - INFO - __main__ - Step 450 Global step 450 Train loss 3.00 on epoch=224
06/19/2022 20:41:05 - INFO - __main__ - Global step 450 Train loss 3.13 ACC 0.0 on epoch=224
06/19/2022 20:41:06 - INFO - __main__ - Step 460 Global step 460 Train loss 2.93 on epoch=229
06/19/2022 20:41:07 - INFO - __main__ - Step 470 Global step 470 Train loss 2.76 on epoch=234
06/19/2022 20:41:09 - INFO - __main__ - Step 480 Global step 480 Train loss 2.81 on epoch=239
06/19/2022 20:41:10 - INFO - __main__ - Step 490 Global step 490 Train loss 2.59 on epoch=244
06/19/2022 20:41:11 - INFO - __main__ - Step 500 Global step 500 Train loss 2.54 on epoch=249
06/19/2022 20:41:21 - INFO - __main__ - Global step 500 Train loss 2.73 ACC 0.0 on epoch=249
06/19/2022 20:41:22 - INFO - __main__ - Step 510 Global step 510 Train loss 2.41 on epoch=254
06/19/2022 20:41:23 - INFO - __main__ - Step 520 Global step 520 Train loss 2.52 on epoch=259
06/19/2022 20:41:25 - INFO - __main__ - Step 530 Global step 530 Train loss 2.39 on epoch=264
06/19/2022 20:41:26 - INFO - __main__ - Step 540 Global step 540 Train loss 2.44 on epoch=269
06/19/2022 20:41:27 - INFO - __main__ - Step 550 Global step 550 Train loss 2.39 on epoch=274
06/19/2022 20:41:31 - INFO - __main__ - Global step 550 Train loss 2.43 ACC 0.0 on epoch=274
06/19/2022 20:41:32 - INFO - __main__ - Step 560 Global step 560 Train loss 2.16 on epoch=279
06/19/2022 20:41:33 - INFO - __main__ - Step 570 Global step 570 Train loss 2.18 on epoch=284
06/19/2022 20:41:34 - INFO - __main__ - Step 580 Global step 580 Train loss 2.02 on epoch=289
06/19/2022 20:41:36 - INFO - __main__ - Step 590 Global step 590 Train loss 1.90 on epoch=294
06/19/2022 20:41:37 - INFO - __main__ - Step 600 Global step 600 Train loss 1.87 on epoch=299
06/19/2022 20:41:38 - INFO - __main__ - Global step 600 Train loss 2.02 ACC 0.4375 on epoch=299
06/19/2022 20:41:38 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.4375 on epoch=299, global_step=600
06/19/2022 20:41:40 - INFO - __main__ - Step 610 Global step 610 Train loss 1.73 on epoch=304
06/19/2022 20:41:41 - INFO - __main__ - Step 620 Global step 620 Train loss 1.89 on epoch=309
06/19/2022 20:41:42 - INFO - __main__ - Step 630 Global step 630 Train loss 1.68 on epoch=314
06/19/2022 20:41:43 - INFO - __main__ - Step 640 Global step 640 Train loss 1.71 on epoch=319
06/19/2022 20:41:44 - INFO - __main__ - Step 650 Global step 650 Train loss 1.63 on epoch=324
06/19/2022 20:41:46 - INFO - __main__ - Global step 650 Train loss 1.73 ACC 0.5 on epoch=324
06/19/2022 20:41:46 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=324, global_step=650
06/19/2022 20:41:47 - INFO - __main__ - Step 660 Global step 660 Train loss 1.64 on epoch=329
06/19/2022 20:41:48 - INFO - __main__ - Step 670 Global step 670 Train loss 1.51 on epoch=334
06/19/2022 20:41:49 - INFO - __main__ - Step 680 Global step 680 Train loss 1.45 on epoch=339
06/19/2022 20:41:51 - INFO - __main__ - Step 690 Global step 690 Train loss 1.28 on epoch=344
06/19/2022 20:41:52 - INFO - __main__ - Step 700 Global step 700 Train loss 1.27 on epoch=349
06/19/2022 20:41:54 - INFO - __main__ - Global step 700 Train loss 1.43 ACC 0.46875 on epoch=349
06/19/2022 20:41:55 - INFO - __main__ - Step 710 Global step 710 Train loss 1.18 on epoch=354
06/19/2022 20:41:56 - INFO - __main__ - Step 720 Global step 720 Train loss 1.11 on epoch=359
06/19/2022 20:41:58 - INFO - __main__ - Step 730 Global step 730 Train loss 1.23 on epoch=364
06/19/2022 20:41:59 - INFO - __main__ - Step 740 Global step 740 Train loss 1.18 on epoch=369
06/19/2022 20:42:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.99 on epoch=374
06/19/2022 20:42:03 - INFO - __main__ - Global step 750 Train loss 1.14 ACC 0.46875 on epoch=374
06/19/2022 20:42:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.89 on epoch=379
06/19/2022 20:42:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.80 on epoch=384
06/19/2022 20:42:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=389
06/19/2022 20:42:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.80 on epoch=394
06/19/2022 20:42:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.78 on epoch=399
06/19/2022 20:42:11 - INFO - __main__ - Global step 800 Train loss 0.82 ACC 0.53125 on epoch=399
06/19/2022 20:42:11 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=399, global_step=800
06/19/2022 20:42:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.72 on epoch=404
06/19/2022 20:42:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.66 on epoch=409
06/19/2022 20:42:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.65 on epoch=414
06/19/2022 20:42:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.64 on epoch=419
06/19/2022 20:42:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=424
06/19/2022 20:42:18 - INFO - __main__ - Global step 850 Train loss 0.65 ACC 0.53125 on epoch=424
06/19/2022 20:42:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.54 on epoch=429
06/19/2022 20:42:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.67 on epoch=434
06/19/2022 20:42:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.68 on epoch=439
06/19/2022 20:42:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=444
06/19/2022 20:42:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.64 on epoch=449
06/19/2022 20:42:25 - INFO - __main__ - Global step 900 Train loss 0.61 ACC 0.40625 on epoch=449
06/19/2022 20:42:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.55 on epoch=454
06/19/2022 20:42:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.59 on epoch=459
06/19/2022 20:42:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.53 on epoch=464
06/19/2022 20:42:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.59 on epoch=469
06/19/2022 20:42:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=474
06/19/2022 20:42:32 - INFO - __main__ - Global step 950 Train loss 0.55 ACC 0.5 on epoch=474
06/19/2022 20:42:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.56 on epoch=479
06/19/2022 20:42:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=484
06/19/2022 20:42:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=489
06/19/2022 20:42:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=494
06/19/2022 20:42:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=499
06/19/2022 20:42:39 - INFO - __main__ - Global step 1000 Train loss 0.46 ACC 0.5 on epoch=499
06/19/2022 20:42:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
06/19/2022 20:42:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=509
06/19/2022 20:42:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.51 on epoch=514
06/19/2022 20:42:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.52 on epoch=519
06/19/2022 20:42:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=524
06/19/2022 20:42:45 - INFO - __main__ - Global step 1050 Train loss 0.48 ACC 0.59375 on epoch=524
06/19/2022 20:42:46 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=524, global_step=1050
06/19/2022 20:42:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=529
06/19/2022 20:42:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=534
06/19/2022 20:42:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=539
06/19/2022 20:42:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
06/19/2022 20:42:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=549
06/19/2022 20:42:52 - INFO - __main__ - Global step 1100 Train loss 0.43 ACC 0.53125 on epoch=549
06/19/2022 20:42:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=554
06/19/2022 20:42:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=559
06/19/2022 20:42:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=564
06/19/2022 20:42:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=569
06/19/2022 20:42:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.49 on epoch=574
06/19/2022 20:42:59 - INFO - __main__ - Global step 1150 Train loss 0.45 ACC 0.5 on epoch=574
06/19/2022 20:43:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=579
06/19/2022 20:43:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=584
06/19/2022 20:43:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=589
06/19/2022 20:43:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=594
06/19/2022 20:43:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=599
06/19/2022 20:43:06 - INFO - __main__ - Global step 1200 Train loss 0.39 ACC 0.46875 on epoch=599
06/19/2022 20:43:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=604
06/19/2022 20:43:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=609
06/19/2022 20:43:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=614
06/19/2022 20:43:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
06/19/2022 20:43:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=624
06/19/2022 20:43:13 - INFO - __main__ - Global step 1250 Train loss 0.39 ACC 0.5 on epoch=624
06/19/2022 20:43:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=629
06/19/2022 20:43:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=634
06/19/2022 20:43:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=639
06/19/2022 20:43:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=644
06/19/2022 20:43:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=649
06/19/2022 20:43:20 - INFO - __main__ - Global step 1300 Train loss 0.39 ACC 0.5 on epoch=649
06/19/2022 20:43:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=654
06/19/2022 20:43:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=659
06/19/2022 20:43:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=664
06/19/2022 20:43:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=669
06/19/2022 20:43:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=674
06/19/2022 20:43:26 - INFO - __main__ - Global step 1350 Train loss 0.39 ACC 0.625 on epoch=674
06/19/2022 20:43:26 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=674, global_step=1350
06/19/2022 20:43:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=679
06/19/2022 20:43:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=684
06/19/2022 20:43:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=689
06/19/2022 20:43:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=694
06/19/2022 20:43:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=699
06/19/2022 20:43:33 - INFO - __main__ - Global step 1400 Train loss 0.37 ACC 0.65625 on epoch=699
06/19/2022 20:43:33 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=699, global_step=1400
06/19/2022 20:43:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=704
06/19/2022 20:43:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=709
06/19/2022 20:43:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=714
06/19/2022 20:43:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
06/19/2022 20:43:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
06/19/2022 20:43:40 - INFO - __main__ - Global step 1450 Train loss 0.36 ACC 0.5 on epoch=724
06/19/2022 20:43:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=729
06/19/2022 20:43:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=734
06/19/2022 20:43:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
06/19/2022 20:43:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=744
06/19/2022 20:43:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=749
06/19/2022 20:43:47 - INFO - __main__ - Global step 1500 Train loss 0.35 ACC 0.46875 on epoch=749
06/19/2022 20:43:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=754
06/19/2022 20:43:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=759
06/19/2022 20:43:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=764
06/19/2022 20:43:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=769
06/19/2022 20:43:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=774
06/19/2022 20:43:54 - INFO - __main__ - Global step 1550 Train loss 0.35 ACC 0.5625 on epoch=774
06/19/2022 20:43:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=779
06/19/2022 20:43:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=784
06/19/2022 20:43:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
06/19/2022 20:43:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=794
06/19/2022 20:44:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=799
06/19/2022 20:44:01 - INFO - __main__ - Global step 1600 Train loss 0.35 ACC 0.5 on epoch=799
06/19/2022 20:44:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=804
06/19/2022 20:44:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
06/19/2022 20:44:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=814
06/19/2022 20:44:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=819
06/19/2022 20:44:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=824
06/19/2022 20:44:07 - INFO - __main__ - Global step 1650 Train loss 0.37 ACC 0.5 on epoch=824
06/19/2022 20:44:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=829
06/19/2022 20:44:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=834
06/19/2022 20:44:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.34 on epoch=839
06/19/2022 20:44:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=844
06/19/2022 20:44:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=849
06/19/2022 20:44:14 - INFO - __main__ - Global step 1700 Train loss 0.34 ACC 0.5 on epoch=849
06/19/2022 20:44:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=854
06/19/2022 20:44:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=859
06/19/2022 20:44:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.32 on epoch=864
06/19/2022 20:44:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=869
06/19/2022 20:44:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=874
06/19/2022 20:44:21 - INFO - __main__ - Global step 1750 Train loss 0.35 ACC 0.5 on epoch=874
06/19/2022 20:44:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=879
06/19/2022 20:44:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=884
06/19/2022 20:44:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=889
06/19/2022 20:44:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=894
06/19/2022 20:44:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=899
06/19/2022 20:44:28 - INFO - __main__ - Global step 1800 Train loss 0.35 ACC 0.5 on epoch=899
06/19/2022 20:44:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
06/19/2022 20:44:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=909
06/19/2022 20:44:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=914
06/19/2022 20:44:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=919
06/19/2022 20:44:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.32 on epoch=924
06/19/2022 20:44:35 - INFO - __main__ - Global step 1850 Train loss 0.33 ACC 0.53125 on epoch=924
06/19/2022 20:44:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=929
06/19/2022 20:44:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
06/19/2022 20:44:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=939
06/19/2022 20:44:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=944
06/19/2022 20:44:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=949
06/19/2022 20:44:42 - INFO - __main__ - Global step 1900 Train loss 0.33 ACC 0.46875 on epoch=949
06/19/2022 20:44:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.36 on epoch=954
06/19/2022 20:44:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
06/19/2022 20:44:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=964
06/19/2022 20:44:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
06/19/2022 20:44:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=974
06/19/2022 20:44:49 - INFO - __main__ - Global step 1950 Train loss 0.33 ACC 0.5 on epoch=974
06/19/2022 20:44:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.31 on epoch=979
06/19/2022 20:44:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
06/19/2022 20:44:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
06/19/2022 20:44:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.29 on epoch=994
06/19/2022 20:44:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
06/19/2022 20:44:56 - INFO - __main__ - Global step 2000 Train loss 0.32 ACC 0.5 on epoch=999
06/19/2022 20:44:56 - INFO - __main__ - save last model!
06/19/2022 20:44:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 20:44:56 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 20:44:56 - INFO - __main__ - Printing 3 examples
06/19/2022 20:44:56 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 20:44:56 - INFO - __main__ - ['equivalent']
06/19/2022 20:44:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 20:44:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:44:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 20:44:56 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:44:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:44:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:44:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:44:56 - INFO - __main__ - Printing 3 examples
06/19/2022 20:44:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 20:44:56 - INFO - __main__ - ['equivalent']
06/19/2022 20:44:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 20:44:56 - INFO - __main__ - ['equivalent']
06/19/2022 20:44:56 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 20:44:56 - INFO - __main__ - ['equivalent']
06/19/2022 20:44:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 20:44:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:44:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:44:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:44:56 - INFO - __main__ - Printing 3 examples
06/19/2022 20:44:56 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 20:44:56 - INFO - __main__ - ['equivalent']
06/19/2022 20:44:56 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 20:44:56 - INFO - __main__ - ['equivalent']
06/19/2022 20:44:56 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 20:44:56 - INFO - __main__ - ['equivalent']
06/19/2022 20:44:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:44:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:44:56 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:44:56 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 20:45:02 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:45:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:45:02 - INFO - __main__ - Starting training!
06/19/2022 20:45:05 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_42_0.5_8_predictions.txt
06/19/2022 20:45:05 - INFO - __main__ - ACC on test data: 0.3260
06/19/2022 20:45:05 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.32598039215686275
06/19/2022 20:45:05 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.4, bsz=8 ...
06/19/2022 20:45:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:45:06 - INFO - __main__ - Printing 3 examples
06/19/2022 20:45:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 20:45:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:45:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 20:45:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:45:06 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 20:45:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:45:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 20:45:06 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:45:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:45:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:45:06 - INFO - __main__ - Printing 3 examples
06/19/2022 20:45:06 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 20:45:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:45:06 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 20:45:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:45:06 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 20:45:06 - INFO - __main__ - ['equivalent']
06/19/2022 20:45:06 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:45:06 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:45:06 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:45:12 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:45:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:45:12 - INFO - __main__ - Starting training!
06/19/2022 20:45:14 - INFO - __main__ - Step 10 Global step 10 Train loss 6.93 on epoch=4
06/19/2022 20:45:15 - INFO - __main__ - Step 20 Global step 20 Train loss 6.88 on epoch=9
06/19/2022 20:45:17 - INFO - __main__ - Step 30 Global step 30 Train loss 6.83 on epoch=14
06/19/2022 20:45:18 - INFO - __main__ - Step 40 Global step 40 Train loss 6.90 on epoch=19
06/19/2022 20:45:19 - INFO - __main__ - Step 50 Global step 50 Train loss 6.78 on epoch=24
06/19/2022 20:45:20 - INFO - __main__ - Global step 50 Train loss 6.86 ACC 0.0 on epoch=24
06/19/2022 20:45:20 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 20:45:21 - INFO - __main__ - Step 60 Global step 60 Train loss 6.76 on epoch=29
06/19/2022 20:45:23 - INFO - __main__ - Step 70 Global step 70 Train loss 6.72 on epoch=34
06/19/2022 20:45:24 - INFO - __main__ - Step 80 Global step 80 Train loss 6.79 on epoch=39
06/19/2022 20:45:25 - INFO - __main__ - Step 90 Global step 90 Train loss 6.73 on epoch=44
06/19/2022 20:45:26 - INFO - __main__ - Step 100 Global step 100 Train loss 6.73 on epoch=49
06/19/2022 20:45:28 - INFO - __main__ - Global step 100 Train loss 6.75 ACC 0.0 on epoch=49
06/19/2022 20:45:29 - INFO - __main__ - Step 110 Global step 110 Train loss 6.67 on epoch=54
06/19/2022 20:45:31 - INFO - __main__ - Step 120 Global step 120 Train loss 6.64 on epoch=59
06/19/2022 20:45:32 - INFO - __main__ - Step 130 Global step 130 Train loss 6.56 on epoch=64
06/19/2022 20:45:34 - INFO - __main__ - Step 140 Global step 140 Train loss 6.51 on epoch=69
06/19/2022 20:45:36 - INFO - __main__ - Step 150 Global step 150 Train loss 6.41 on epoch=74
06/19/2022 20:45:37 - INFO - __main__ - Global step 150 Train loss 6.56 ACC 0.0 on epoch=74
06/19/2022 20:45:39 - INFO - __main__ - Step 160 Global step 160 Train loss 6.33 on epoch=79
06/19/2022 20:45:40 - INFO - __main__ - Step 170 Global step 170 Train loss 6.25 on epoch=84
06/19/2022 20:45:41 - INFO - __main__ - Step 180 Global step 180 Train loss 6.27 on epoch=89
06/19/2022 20:45:42 - INFO - __main__ - Step 190 Global step 190 Train loss 6.20 on epoch=94
06/19/2022 20:45:44 - INFO - __main__ - Step 200 Global step 200 Train loss 6.18 on epoch=99
06/19/2022 20:45:48 - INFO - __main__ - Global step 200 Train loss 6.25 ACC 0.0 on epoch=99
06/19/2022 20:45:50 - INFO - __main__ - Step 210 Global step 210 Train loss 6.09 on epoch=104
06/19/2022 20:45:51 - INFO - __main__ - Step 220 Global step 220 Train loss 6.10 on epoch=109
06/19/2022 20:45:52 - INFO - __main__ - Step 230 Global step 230 Train loss 6.01 on epoch=114
06/19/2022 20:45:53 - INFO - __main__ - Step 240 Global step 240 Train loss 5.96 on epoch=119
06/19/2022 20:45:55 - INFO - __main__ - Step 250 Global step 250 Train loss 5.97 on epoch=124
06/19/2022 20:46:05 - INFO - __main__ - Global step 250 Train loss 6.03 ACC 0.0 on epoch=124
06/19/2022 20:46:07 - INFO - __main__ - Step 260 Global step 260 Train loss 5.91 on epoch=129
06/19/2022 20:46:08 - INFO - __main__ - Step 270 Global step 270 Train loss 5.91 on epoch=134
06/19/2022 20:46:10 - INFO - __main__ - Step 280 Global step 280 Train loss 5.84 on epoch=139
06/19/2022 20:46:11 - INFO - __main__ - Step 290 Global step 290 Train loss 5.82 on epoch=144
06/19/2022 20:46:12 - INFO - __main__ - Step 300 Global step 300 Train loss 5.74 on epoch=149
06/19/2022 20:46:22 - INFO - __main__ - Global step 300 Train loss 5.84 ACC 0.0 on epoch=149
06/19/2022 20:46:24 - INFO - __main__ - Step 310 Global step 310 Train loss 5.80 on epoch=154
06/19/2022 20:46:25 - INFO - __main__ - Step 320 Global step 320 Train loss 5.71 on epoch=159
06/19/2022 20:46:26 - INFO - __main__ - Step 330 Global step 330 Train loss 5.62 on epoch=164
06/19/2022 20:46:28 - INFO - __main__ - Step 340 Global step 340 Train loss 5.64 on epoch=169
06/19/2022 20:46:29 - INFO - __main__ - Step 350 Global step 350 Train loss 5.47 on epoch=174
06/19/2022 20:46:36 - INFO - __main__ - Global step 350 Train loss 5.65 ACC 0.0 on epoch=174
06/19/2022 20:46:37 - INFO - __main__ - Step 360 Global step 360 Train loss 5.54 on epoch=179
06/19/2022 20:46:38 - INFO - __main__ - Step 370 Global step 370 Train loss 5.40 on epoch=184
06/19/2022 20:46:39 - INFO - __main__ - Step 380 Global step 380 Train loss 5.31 on epoch=189
06/19/2022 20:46:41 - INFO - __main__ - Step 390 Global step 390 Train loss 5.40 on epoch=194
06/19/2022 20:46:42 - INFO - __main__ - Step 400 Global step 400 Train loss 5.46 on epoch=199
06/19/2022 20:46:52 - INFO - __main__ - Global step 400 Train loss 5.42 ACC 0.0 on epoch=199
06/19/2022 20:46:54 - INFO - __main__ - Step 410 Global step 410 Train loss 5.63 on epoch=204
06/19/2022 20:46:55 - INFO - __main__ - Step 420 Global step 420 Train loss 5.58 on epoch=209
06/19/2022 20:46:56 - INFO - __main__ - Step 430 Global step 430 Train loss 5.43 on epoch=214
06/19/2022 20:46:58 - INFO - __main__ - Step 440 Global step 440 Train loss 5.38 on epoch=219
06/19/2022 20:46:59 - INFO - __main__ - Step 450 Global step 450 Train loss 5.27 on epoch=224
06/19/2022 20:47:02 - INFO - __main__ - Global step 450 Train loss 5.46 ACC 0.0 on epoch=224
06/19/2022 20:47:03 - INFO - __main__ - Step 460 Global step 460 Train loss 5.16 on epoch=229
06/19/2022 20:47:04 - INFO - __main__ - Step 470 Global step 470 Train loss 5.02 on epoch=234
06/19/2022 20:47:06 - INFO - __main__ - Step 480 Global step 480 Train loss 4.95 on epoch=239
06/19/2022 20:47:07 - INFO - __main__ - Step 490 Global step 490 Train loss 4.95 on epoch=244
06/19/2022 20:47:08 - INFO - __main__ - Step 500 Global step 500 Train loss 4.84 on epoch=249
06/19/2022 20:47:09 - INFO - __main__ - Global step 500 Train loss 4.98 ACC 0.0 on epoch=249
06/19/2022 20:47:11 - INFO - __main__ - Step 510 Global step 510 Train loss 4.78 on epoch=254
06/19/2022 20:47:12 - INFO - __main__ - Step 520 Global step 520 Train loss 4.70 on epoch=259
06/19/2022 20:47:13 - INFO - __main__ - Step 530 Global step 530 Train loss 4.61 on epoch=264
06/19/2022 20:47:15 - INFO - __main__ - Step 540 Global step 540 Train loss 4.46 on epoch=269
06/19/2022 20:47:16 - INFO - __main__ - Step 550 Global step 550 Train loss 4.51 on epoch=274
06/19/2022 20:47:18 - INFO - __main__ - Global step 550 Train loss 4.61 ACC 0.0 on epoch=274
06/19/2022 20:47:19 - INFO - __main__ - Step 560 Global step 560 Train loss 4.59 on epoch=279
06/19/2022 20:47:20 - INFO - __main__ - Step 570 Global step 570 Train loss 4.45 on epoch=284
06/19/2022 20:47:22 - INFO - __main__ - Step 580 Global step 580 Train loss 4.46 on epoch=289
06/19/2022 20:47:23 - INFO - __main__ - Step 590 Global step 590 Train loss 4.40 on epoch=294
06/19/2022 20:47:24 - INFO - __main__ - Step 600 Global step 600 Train loss 4.40 on epoch=299
06/19/2022 20:47:26 - INFO - __main__ - Global step 600 Train loss 4.46 ACC 0.0 on epoch=299
06/19/2022 20:47:27 - INFO - __main__ - Step 610 Global step 610 Train loss 4.39 on epoch=304
06/19/2022 20:47:28 - INFO - __main__ - Step 620 Global step 620 Train loss 4.28 on epoch=309
06/19/2022 20:47:30 - INFO - __main__ - Step 630 Global step 630 Train loss 4.36 on epoch=314
06/19/2022 20:47:31 - INFO - __main__ - Step 640 Global step 640 Train loss 4.15 on epoch=319
06/19/2022 20:47:32 - INFO - __main__ - Step 650 Global step 650 Train loss 4.16 on epoch=324
06/19/2022 20:47:34 - INFO - __main__ - Global step 650 Train loss 4.27 ACC 0.0 on epoch=324
06/19/2022 20:47:35 - INFO - __main__ - Step 660 Global step 660 Train loss 4.36 on epoch=329
06/19/2022 20:47:36 - INFO - __main__ - Step 670 Global step 670 Train loss 3.94 on epoch=334
06/19/2022 20:47:38 - INFO - __main__ - Step 680 Global step 680 Train loss 3.99 on epoch=339
06/19/2022 20:47:39 - INFO - __main__ - Step 690 Global step 690 Train loss 4.06 on epoch=344
06/19/2022 20:47:40 - INFO - __main__ - Step 700 Global step 700 Train loss 4.00 on epoch=349
06/19/2022 20:47:43 - INFO - __main__ - Global step 700 Train loss 4.07 ACC 0.0 on epoch=349
06/19/2022 20:47:44 - INFO - __main__ - Step 710 Global step 710 Train loss 4.16 on epoch=354
06/19/2022 20:47:45 - INFO - __main__ - Step 720 Global step 720 Train loss 3.88 on epoch=359
06/19/2022 20:47:47 - INFO - __main__ - Step 730 Global step 730 Train loss 3.84 on epoch=364
06/19/2022 20:47:48 - INFO - __main__ - Step 740 Global step 740 Train loss 3.81 on epoch=369
06/19/2022 20:47:49 - INFO - __main__ - Step 750 Global step 750 Train loss 3.71 on epoch=374
06/19/2022 20:47:51 - INFO - __main__ - Global step 750 Train loss 3.88 ACC 0.0 on epoch=374
06/19/2022 20:47:52 - INFO - __main__ - Step 760 Global step 760 Train loss 3.68 on epoch=379
06/19/2022 20:47:53 - INFO - __main__ - Step 770 Global step 770 Train loss 3.58 on epoch=384
06/19/2022 20:47:55 - INFO - __main__ - Step 780 Global step 780 Train loss 3.63 on epoch=389
06/19/2022 20:47:56 - INFO - __main__ - Step 790 Global step 790 Train loss 3.49 on epoch=394
06/19/2022 20:47:57 - INFO - __main__ - Step 800 Global step 800 Train loss 3.40 on epoch=399
06/19/2022 20:47:59 - INFO - __main__ - Global step 800 Train loss 3.55 ACC 0.0 on epoch=399
06/19/2022 20:48:01 - INFO - __main__ - Step 810 Global step 810 Train loss 3.37 on epoch=404
06/19/2022 20:48:02 - INFO - __main__ - Step 820 Global step 820 Train loss 3.36 on epoch=409
06/19/2022 20:48:03 - INFO - __main__ - Step 830 Global step 830 Train loss 3.28 on epoch=414
06/19/2022 20:48:05 - INFO - __main__ - Step 840 Global step 840 Train loss 3.21 on epoch=419
06/19/2022 20:48:06 - INFO - __main__ - Step 850 Global step 850 Train loss 3.14 on epoch=424
06/19/2022 20:48:08 - INFO - __main__ - Global step 850 Train loss 3.27 ACC 0.0 on epoch=424
06/19/2022 20:48:09 - INFO - __main__ - Step 860 Global step 860 Train loss 3.04 on epoch=429
06/19/2022 20:48:11 - INFO - __main__ - Step 870 Global step 870 Train loss 3.14 on epoch=434
06/19/2022 20:48:12 - INFO - __main__ - Step 880 Global step 880 Train loss 2.93 on epoch=439
06/19/2022 20:48:13 - INFO - __main__ - Step 890 Global step 890 Train loss 2.96 on epoch=444
06/19/2022 20:48:15 - INFO - __main__ - Step 900 Global step 900 Train loss 2.89 on epoch=449
06/19/2022 20:48:16 - INFO - __main__ - Global step 900 Train loss 2.99 ACC 0.0 on epoch=449
06/19/2022 20:48:18 - INFO - __main__ - Step 910 Global step 910 Train loss 2.87 on epoch=454
06/19/2022 20:48:19 - INFO - __main__ - Step 920 Global step 920 Train loss 2.83 on epoch=459
06/19/2022 20:48:20 - INFO - __main__ - Step 930 Global step 930 Train loss 2.83 on epoch=464
06/19/2022 20:48:22 - INFO - __main__ - Step 940 Global step 940 Train loss 2.78 on epoch=469
06/19/2022 20:48:23 - INFO - __main__ - Step 950 Global step 950 Train loss 2.61 on epoch=474
06/19/2022 20:48:34 - INFO - __main__ - Global step 950 Train loss 2.78 ACC 0.0 on epoch=474
06/19/2022 20:48:35 - INFO - __main__ - Step 960 Global step 960 Train loss 2.59 on epoch=479
06/19/2022 20:48:36 - INFO - __main__ - Step 970 Global step 970 Train loss 2.66 on epoch=484
06/19/2022 20:48:37 - INFO - __main__ - Step 980 Global step 980 Train loss 2.61 on epoch=489
06/19/2022 20:48:39 - INFO - __main__ - Step 990 Global step 990 Train loss 2.61 on epoch=494
06/19/2022 20:48:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.55 on epoch=499
06/19/2022 20:48:51 - INFO - __main__ - Global step 1000 Train loss 2.60 ACC 0.0 on epoch=499
06/19/2022 20:48:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.47 on epoch=504
06/19/2022 20:48:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.58 on epoch=509
06/19/2022 20:48:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.53 on epoch=514
06/19/2022 20:48:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.37 on epoch=519
06/19/2022 20:48:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.29 on epoch=524
06/19/2022 20:49:09 - INFO - __main__ - Global step 1050 Train loss 2.45 ACC 0.0625 on epoch=524
06/19/2022 20:49:09 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=524, global_step=1050
06/19/2022 20:49:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.22 on epoch=529
06/19/2022 20:49:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.35 on epoch=534
06/19/2022 20:49:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.37 on epoch=539
06/19/2022 20:49:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.34 on epoch=544
06/19/2022 20:49:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.23 on epoch=549
06/19/2022 20:49:21 - INFO - __main__ - Global step 1100 Train loss 2.30 ACC 0.28125 on epoch=549
06/19/2022 20:49:21 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.28125 on epoch=549, global_step=1100
06/19/2022 20:49:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.16 on epoch=554
06/19/2022 20:49:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.16 on epoch=559
06/19/2022 20:49:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.18 on epoch=564
06/19/2022 20:49:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.10 on epoch=569
06/19/2022 20:49:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.98 on epoch=574
06/19/2022 20:49:33 - INFO - __main__ - Global step 1150 Train loss 2.12 ACC 0.3125 on epoch=574
06/19/2022 20:49:33 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=574, global_step=1150
06/19/2022 20:49:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.98 on epoch=579
06/19/2022 20:49:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.90 on epoch=584
06/19/2022 20:49:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.89 on epoch=589
06/19/2022 20:49:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.90 on epoch=594
06/19/2022 20:49:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.84 on epoch=599
06/19/2022 20:49:46 - INFO - __main__ - Global step 1200 Train loss 1.90 ACC 0.4375 on epoch=599
06/19/2022 20:49:46 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.4375 on epoch=599, global_step=1200
06/19/2022 20:49:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.86 on epoch=604
06/19/2022 20:49:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.77 on epoch=609
06/19/2022 20:49:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.80 on epoch=614
06/19/2022 20:49:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.70 on epoch=619
06/19/2022 20:49:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.73 on epoch=624
06/19/2022 20:50:03 - INFO - __main__ - Global step 1250 Train loss 1.77 ACC 0.46875 on epoch=624
06/19/2022 20:50:03 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.46875 on epoch=624, global_step=1250
06/19/2022 20:50:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.67 on epoch=629
06/19/2022 20:50:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.62 on epoch=634
06/19/2022 20:50:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.62 on epoch=639
06/19/2022 20:50:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.60 on epoch=644
06/19/2022 20:50:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.58 on epoch=649
06/19/2022 20:50:10 - INFO - __main__ - Global step 1300 Train loss 1.62 ACC 0.5 on epoch=649
06/19/2022 20:50:10 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=649, global_step=1300
06/19/2022 20:50:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.47 on epoch=654
06/19/2022 20:50:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.50 on epoch=659
06/19/2022 20:50:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.53 on epoch=664
06/19/2022 20:50:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.52 on epoch=669
06/19/2022 20:50:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.45 on epoch=674
06/19/2022 20:50:17 - INFO - __main__ - Global step 1350 Train loss 1.50 ACC 0.5 on epoch=674
06/19/2022 20:50:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.40 on epoch=679
06/19/2022 20:50:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.46 on epoch=684
06/19/2022 20:50:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.33 on epoch=689
06/19/2022 20:50:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.28 on epoch=694
06/19/2022 20:50:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.35 on epoch=699
06/19/2022 20:50:24 - INFO - __main__ - Global step 1400 Train loss 1.37 ACC 0.5 on epoch=699
06/19/2022 20:50:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.32 on epoch=704
06/19/2022 20:50:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.33 on epoch=709
06/19/2022 20:50:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.30 on epoch=714
06/19/2022 20:50:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.30 on epoch=719
06/19/2022 20:50:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.23 on epoch=724
06/19/2022 20:50:31 - INFO - __main__ - Global step 1450 Train loss 1.30 ACC 0.5 on epoch=724
06/19/2022 20:50:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.23 on epoch=729
06/19/2022 20:50:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.27 on epoch=734
06/19/2022 20:50:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.15 on epoch=739
06/19/2022 20:50:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.19 on epoch=744
06/19/2022 20:50:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.08 on epoch=749
06/19/2022 20:50:39 - INFO - __main__ - Global step 1500 Train loss 1.19 ACC 0.5 on epoch=749
06/19/2022 20:50:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.15 on epoch=754
06/19/2022 20:50:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.14 on epoch=759
06/19/2022 20:50:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.14 on epoch=764
06/19/2022 20:50:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.18 on epoch=769
06/19/2022 20:50:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.09 on epoch=774
06/19/2022 20:50:46 - INFO - __main__ - Global step 1550 Train loss 1.14 ACC 0.5 on epoch=774
06/19/2022 20:50:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.04 on epoch=779
06/19/2022 20:50:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.99 on epoch=784
06/19/2022 20:50:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.12 on epoch=789
06/19/2022 20:50:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.08 on epoch=794
06/19/2022 20:50:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.00 on epoch=799
06/19/2022 20:50:53 - INFO - __main__ - Global step 1600 Train loss 1.05 ACC 0.5 on epoch=799
06/19/2022 20:50:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.94 on epoch=804
06/19/2022 20:50:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.87 on epoch=809
06/19/2022 20:50:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.01 on epoch=814
06/19/2022 20:50:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.97 on epoch=819
06/19/2022 20:51:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.96 on epoch=824
06/19/2022 20:51:01 - INFO - __main__ - Global step 1650 Train loss 0.95 ACC 0.5 on epoch=824
06/19/2022 20:51:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.86 on epoch=829
06/19/2022 20:51:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.94 on epoch=834
06/19/2022 20:51:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.90 on epoch=839
06/19/2022 20:51:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.85 on epoch=844
06/19/2022 20:51:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.88 on epoch=849
06/19/2022 20:51:09 - INFO - __main__ - Global step 1700 Train loss 0.89 ACC 0.5 on epoch=849
06/19/2022 20:51:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.82 on epoch=854
06/19/2022 20:51:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.83 on epoch=859
06/19/2022 20:51:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.83 on epoch=864
06/19/2022 20:51:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.84 on epoch=869
06/19/2022 20:51:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.83 on epoch=874
06/19/2022 20:51:18 - INFO - __main__ - Global step 1750 Train loss 0.83 ACC 0.5 on epoch=874
06/19/2022 20:51:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.83 on epoch=879
06/19/2022 20:51:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.79 on epoch=884
06/19/2022 20:51:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.82 on epoch=889
06/19/2022 20:51:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.82 on epoch=894
06/19/2022 20:51:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.75 on epoch=899
06/19/2022 20:51:29 - INFO - __main__ - Global step 1800 Train loss 0.80 ACC 0.59375 on epoch=899
06/19/2022 20:51:29 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=899, global_step=1800
06/19/2022 20:51:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.72 on epoch=904
06/19/2022 20:51:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.82 on epoch=909
06/19/2022 20:51:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.65 on epoch=914
06/19/2022 20:51:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.69 on epoch=919
06/19/2022 20:51:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.77 on epoch=924
06/19/2022 20:51:41 - INFO - __main__ - Global step 1850 Train loss 0.73 ACC 0.625 on epoch=924
06/19/2022 20:51:41 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=924, global_step=1850
06/19/2022 20:51:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.70 on epoch=929
06/19/2022 20:51:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.68 on epoch=934
06/19/2022 20:51:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.66 on epoch=939
06/19/2022 20:51:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.78 on epoch=944
06/19/2022 20:51:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.69 on epoch=949
06/19/2022 20:51:53 - INFO - __main__ - Global step 1900 Train loss 0.70 ACC 0.5 on epoch=949
06/19/2022 20:51:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.72 on epoch=954
06/19/2022 20:51:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.72 on epoch=959
06/19/2022 20:51:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.72 on epoch=964
06/19/2022 20:51:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.69 on epoch=969
06/19/2022 20:51:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.71 on epoch=974
06/19/2022 20:52:01 - INFO - __main__ - Global step 1950 Train loss 0.71 ACC 0.75 on epoch=974
06/19/2022 20:52:01 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.75 on epoch=974, global_step=1950
06/19/2022 20:52:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.69 on epoch=979
06/19/2022 20:52:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.67 on epoch=984
06/19/2022 20:52:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.61 on epoch=989
06/19/2022 20:52:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.65 on epoch=994
06/19/2022 20:52:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.59 on epoch=999
06/19/2022 20:52:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:52:09 - INFO - __main__ - Printing 3 examples
06/19/2022 20:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 20:52:09 - INFO - __main__ - ['equivalent']
06/19/2022 20:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 20:52:09 - INFO - __main__ - ['equivalent']
06/19/2022 20:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 20:52:09 - INFO - __main__ - ['equivalent']
06/19/2022 20:52:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 20:52:09 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:52:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:52:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:52:09 - INFO - __main__ - Printing 3 examples
06/19/2022 20:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 20:52:09 - INFO - __main__ - ['equivalent']
06/19/2022 20:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 20:52:09 - INFO - __main__ - ['equivalent']
06/19/2022 20:52:09 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 20:52:09 - INFO - __main__ - ['equivalent']
06/19/2022 20:52:09 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:52:09 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:52:09 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:52:10 - INFO - __main__ - Global step 2000 Train loss 0.64 ACC 0.625 on epoch=999
06/19/2022 20:52:10 - INFO - __main__ - save last model!
06/19/2022 20:52:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 20:52:10 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 20:52:10 - INFO - __main__ - Printing 3 examples
06/19/2022 20:52:10 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 20:52:10 - INFO - __main__ - ['equivalent']
06/19/2022 20:52:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 20:52:10 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:52:10 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 20:52:10 - INFO - __main__ - ['not_equivalent']
06/19/2022 20:52:10 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:52:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:52:10 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 20:52:14 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:52:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:52:14 - INFO - __main__ - Starting training!
06/19/2022 20:53:10 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_42_0.4_8_predictions.txt
06/19/2022 20:53:10 - INFO - __main__ - ACC on test data: 0.5147
06/19/2022 20:53:10 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.4, bsz=8, dev_performance=0.75, test_performance=0.5147058823529411
06/19/2022 20:53:10 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.3, bsz=8 ...
06/19/2022 20:53:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:53:11 - INFO - __main__ - Printing 3 examples
06/19/2022 20:53:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 20:53:11 - INFO - __main__ - ['equivalent']
06/19/2022 20:53:11 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 20:53:11 - INFO - __main__ - ['equivalent']
06/19/2022 20:53:11 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 20:53:11 - INFO - __main__ - ['equivalent']
06/19/2022 20:53:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 20:53:11 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:53:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 20:53:11 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 20:53:11 - INFO - __main__ - Printing 3 examples
06/19/2022 20:53:11 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 20:53:11 - INFO - __main__ - ['equivalent']
06/19/2022 20:53:11 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 20:53:11 - INFO - __main__ - ['equivalent']
06/19/2022 20:53:11 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 20:53:11 - INFO - __main__ - ['equivalent']
06/19/2022 20:53:11 - INFO - __main__ - Tokenizing Input ...
06/19/2022 20:53:11 - INFO - __main__ - Tokenizing Output ...
06/19/2022 20:53:11 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 20:53:17 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 20:53:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 20:53:17 - INFO - __main__ - Starting training!
06/19/2022 20:53:18 - INFO - __main__ - Step 10 Global step 10 Train loss 6.86 on epoch=4
06/19/2022 20:53:20 - INFO - __main__ - Step 20 Global step 20 Train loss 6.85 on epoch=9
06/19/2022 20:53:21 - INFO - __main__ - Step 30 Global step 30 Train loss 6.87 on epoch=14
06/19/2022 20:53:22 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/19/2022 20:53:24 - INFO - __main__ - Step 50 Global step 50 Train loss 6.68 on epoch=24
06/19/2022 20:53:27 - INFO - __main__ - Global step 50 Train loss 6.82 ACC 0.0 on epoch=24
06/19/2022 20:53:27 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 20:53:29 - INFO - __main__ - Step 60 Global step 60 Train loss 6.82 on epoch=29
06/19/2022 20:53:30 - INFO - __main__ - Step 70 Global step 70 Train loss 6.82 on epoch=34
06/19/2022 20:53:31 - INFO - __main__ - Step 80 Global step 80 Train loss 6.79 on epoch=39
06/19/2022 20:53:33 - INFO - __main__ - Step 90 Global step 90 Train loss 6.74 on epoch=44
06/19/2022 20:53:34 - INFO - __main__ - Step 100 Global step 100 Train loss 6.79 on epoch=49
06/19/2022 20:53:35 - INFO - __main__ - Global step 100 Train loss 6.79 ACC 0.0 on epoch=49
06/19/2022 20:53:37 - INFO - __main__ - Step 110 Global step 110 Train loss 6.75 on epoch=54
06/19/2022 20:53:38 - INFO - __main__ - Step 120 Global step 120 Train loss 6.71 on epoch=59
06/19/2022 20:53:39 - INFO - __main__ - Step 130 Global step 130 Train loss 6.72 on epoch=64
06/19/2022 20:53:40 - INFO - __main__ - Step 140 Global step 140 Train loss 6.59 on epoch=69
06/19/2022 20:53:42 - INFO - __main__ - Step 150 Global step 150 Train loss 6.74 on epoch=74
06/19/2022 20:53:45 - INFO - __main__ - Global step 150 Train loss 6.70 ACC 0.0 on epoch=74
06/19/2022 20:53:47 - INFO - __main__ - Step 160 Global step 160 Train loss 6.70 on epoch=79
06/19/2022 20:53:48 - INFO - __main__ - Step 170 Global step 170 Train loss 6.70 on epoch=84
06/19/2022 20:53:49 - INFO - __main__ - Step 180 Global step 180 Train loss 6.65 on epoch=89
06/19/2022 20:53:50 - INFO - __main__ - Step 190 Global step 190 Train loss 6.66 on epoch=94
06/19/2022 20:53:52 - INFO - __main__ - Step 200 Global step 200 Train loss 6.61 on epoch=99
06/19/2022 20:53:54 - INFO - __main__ - Global step 200 Train loss 6.66 ACC 0.0 on epoch=99
06/19/2022 20:53:55 - INFO - __main__ - Step 210 Global step 210 Train loss 6.64 on epoch=104
06/19/2022 20:53:57 - INFO - __main__ - Step 220 Global step 220 Train loss 6.58 on epoch=109
06/19/2022 20:53:58 - INFO - __main__ - Step 230 Global step 230 Train loss 6.57 on epoch=114
06/19/2022 20:54:00 - INFO - __main__ - Step 240 Global step 240 Train loss 6.58 on epoch=119
06/19/2022 20:54:01 - INFO - __main__ - Step 250 Global step 250 Train loss 6.45 on epoch=124
06/19/2022 20:54:07 - INFO - __main__ - Global step 250 Train loss 6.56 ACC 0.0 on epoch=124
06/19/2022 20:54:08 - INFO - __main__ - Step 260 Global step 260 Train loss 6.45 on epoch=129
06/19/2022 20:54:09 - INFO - __main__ - Step 270 Global step 270 Train loss 6.43 on epoch=134
06/19/2022 20:54:11 - INFO - __main__ - Step 280 Global step 280 Train loss 6.36 on epoch=139
06/19/2022 20:54:12 - INFO - __main__ - Step 290 Global step 290 Train loss 6.35 on epoch=144
06/19/2022 20:54:13 - INFO - __main__ - Step 300 Global step 300 Train loss 6.37 on epoch=149
06/19/2022 20:54:23 - INFO - __main__ - Global step 300 Train loss 6.39 ACC 0.0 on epoch=149
06/19/2022 20:54:25 - INFO - __main__ - Step 310 Global step 310 Train loss 6.26 on epoch=154
06/19/2022 20:54:26 - INFO - __main__ - Step 320 Global step 320 Train loss 6.20 on epoch=159
06/19/2022 20:54:27 - INFO - __main__ - Step 330 Global step 330 Train loss 6.25 on epoch=164
06/19/2022 20:54:29 - INFO - __main__ - Step 340 Global step 340 Train loss 6.05 on epoch=169
06/19/2022 20:54:30 - INFO - __main__ - Step 350 Global step 350 Train loss 5.98 on epoch=174
06/19/2022 20:54:34 - INFO - __main__ - Global step 350 Train loss 6.15 ACC 0.0 on epoch=174
06/19/2022 20:54:35 - INFO - __main__ - Step 360 Global step 360 Train loss 6.02 on epoch=179
06/19/2022 20:54:37 - INFO - __main__ - Step 370 Global step 370 Train loss 5.97 on epoch=184
06/19/2022 20:54:38 - INFO - __main__ - Step 380 Global step 380 Train loss 5.88 on epoch=189
06/19/2022 20:54:39 - INFO - __main__ - Step 390 Global step 390 Train loss 5.75 on epoch=194
06/19/2022 20:54:41 - INFO - __main__ - Step 400 Global step 400 Train loss 5.79 on epoch=199
06/19/2022 20:54:48 - INFO - __main__ - Global step 400 Train loss 5.88 ACC 0.0 on epoch=199
06/19/2022 20:54:49 - INFO - __main__ - Step 410 Global step 410 Train loss 5.61 on epoch=204
06/19/2022 20:54:50 - INFO - __main__ - Step 420 Global step 420 Train loss 5.44 on epoch=209
06/19/2022 20:54:51 - INFO - __main__ - Step 430 Global step 430 Train loss 5.57 on epoch=214
06/19/2022 20:54:53 - INFO - __main__ - Step 440 Global step 440 Train loss 5.34 on epoch=219
06/19/2022 20:54:54 - INFO - __main__ - Step 450 Global step 450 Train loss 5.30 on epoch=224
06/19/2022 20:54:55 - INFO - __main__ - Global step 450 Train loss 5.45 ACC 0.0 on epoch=224
06/19/2022 20:54:57 - INFO - __main__ - Step 460 Global step 460 Train loss 5.28 on epoch=229
06/19/2022 20:54:58 - INFO - __main__ - Step 470 Global step 470 Train loss 5.20 on epoch=234
06/19/2022 20:54:59 - INFO - __main__ - Step 480 Global step 480 Train loss 5.24 on epoch=239
06/19/2022 20:55:01 - INFO - __main__ - Step 490 Global step 490 Train loss 5.04 on epoch=244
06/19/2022 20:55:02 - INFO - __main__ - Step 500 Global step 500 Train loss 4.99 on epoch=249
06/19/2022 20:55:03 - INFO - __main__ - Global step 500 Train loss 5.15 ACC 0.0 on epoch=249
06/19/2022 20:55:05 - INFO - __main__ - Step 510 Global step 510 Train loss 4.94 on epoch=254
06/19/2022 20:55:06 - INFO - __main__ - Step 520 Global step 520 Train loss 5.02 on epoch=259
06/19/2022 20:55:07 - INFO - __main__ - Step 530 Global step 530 Train loss 4.79 on epoch=264
06/19/2022 20:55:09 - INFO - __main__ - Step 540 Global step 540 Train loss 4.72 on epoch=269
06/19/2022 20:55:10 - INFO - __main__ - Step 550 Global step 550 Train loss 4.72 on epoch=274
06/19/2022 20:55:11 - INFO - __main__ - Global step 550 Train loss 4.84 ACC 0.0 on epoch=274
06/19/2022 20:55:13 - INFO - __main__ - Step 560 Global step 560 Train loss 4.47 on epoch=279
06/19/2022 20:55:14 - INFO - __main__ - Step 570 Global step 570 Train loss 4.54 on epoch=284
06/19/2022 20:55:15 - INFO - __main__ - Step 580 Global step 580 Train loss 4.49 on epoch=289
06/19/2022 20:55:16 - INFO - __main__ - Step 590 Global step 590 Train loss 4.42 on epoch=294
06/19/2022 20:55:18 - INFO - __main__ - Step 600 Global step 600 Train loss 4.40 on epoch=299
06/19/2022 20:55:19 - INFO - __main__ - Global step 600 Train loss 4.46 ACC 0.0 on epoch=299
06/19/2022 20:55:21 - INFO - __main__ - Step 610 Global step 610 Train loss 4.31 on epoch=304
06/19/2022 20:55:22 - INFO - __main__ - Step 620 Global step 620 Train loss 4.33 on epoch=309
06/19/2022 20:55:24 - INFO - __main__ - Step 630 Global step 630 Train loss 4.27 on epoch=314
06/19/2022 20:55:26 - INFO - __main__ - Step 640 Global step 640 Train loss 4.17 on epoch=319
06/19/2022 20:55:27 - INFO - __main__ - Step 650 Global step 650 Train loss 4.08 on epoch=324
06/19/2022 20:55:29 - INFO - __main__ - Global step 650 Train loss 4.23 ACC 0.0 on epoch=324
06/19/2022 20:55:30 - INFO - __main__ - Step 660 Global step 660 Train loss 4.03 on epoch=329
06/19/2022 20:55:31 - INFO - __main__ - Step 670 Global step 670 Train loss 4.14 on epoch=334
06/19/2022 20:55:33 - INFO - __main__ - Step 680 Global step 680 Train loss 3.99 on epoch=339
06/19/2022 20:55:34 - INFO - __main__ - Step 690 Global step 690 Train loss 3.90 on epoch=344
06/19/2022 20:55:35 - INFO - __main__ - Step 700 Global step 700 Train loss 3.70 on epoch=349
06/19/2022 20:55:37 - INFO - __main__ - Global step 700 Train loss 3.95 ACC 0.0 on epoch=349
06/19/2022 20:55:39 - INFO - __main__ - Step 710 Global step 710 Train loss 3.79 on epoch=354
06/19/2022 20:55:40 - INFO - __main__ - Step 720 Global step 720 Train loss 3.70 on epoch=359
06/19/2022 20:55:41 - INFO - __main__ - Step 730 Global step 730 Train loss 3.59 on epoch=364
06/19/2022 20:55:43 - INFO - __main__ - Step 740 Global step 740 Train loss 3.55 on epoch=369
06/19/2022 20:55:44 - INFO - __main__ - Step 750 Global step 750 Train loss 3.38 on epoch=374
06/19/2022 20:55:46 - INFO - __main__ - Global step 750 Train loss 3.60 ACC 0.0 on epoch=374
06/19/2022 20:55:47 - INFO - __main__ - Step 760 Global step 760 Train loss 3.41 on epoch=379
06/19/2022 20:55:48 - INFO - __main__ - Step 770 Global step 770 Train loss 3.24 on epoch=384
06/19/2022 20:55:50 - INFO - __main__ - Step 780 Global step 780 Train loss 3.33 on epoch=389
06/19/2022 20:55:51 - INFO - __main__ - Step 790 Global step 790 Train loss 3.25 on epoch=394
06/19/2022 20:55:52 - INFO - __main__ - Step 800 Global step 800 Train loss 3.13 on epoch=399
06/19/2022 20:55:54 - INFO - __main__ - Global step 800 Train loss 3.27 ACC 0.0 on epoch=399
06/19/2022 20:55:56 - INFO - __main__ - Step 810 Global step 810 Train loss 3.17 on epoch=404
06/19/2022 20:55:57 - INFO - __main__ - Step 820 Global step 820 Train loss 3.15 on epoch=409
06/19/2022 20:55:58 - INFO - __main__ - Step 830 Global step 830 Train loss 3.04 on epoch=414
06/19/2022 20:56:00 - INFO - __main__ - Step 840 Global step 840 Train loss 3.01 on epoch=419
06/19/2022 20:56:01 - INFO - __main__ - Step 850 Global step 850 Train loss 3.00 on epoch=424
06/19/2022 20:56:10 - INFO - __main__ - Global step 850 Train loss 3.07 ACC 0.0 on epoch=424
06/19/2022 20:56:11 - INFO - __main__ - Step 860 Global step 860 Train loss 3.03 on epoch=429
06/19/2022 20:56:12 - INFO - __main__ - Step 870 Global step 870 Train loss 2.92 on epoch=434
06/19/2022 20:56:13 - INFO - __main__ - Step 880 Global step 880 Train loss 2.90 on epoch=439
06/19/2022 20:56:15 - INFO - __main__ - Step 890 Global step 890 Train loss 2.78 on epoch=444
06/19/2022 20:56:16 - INFO - __main__ - Step 900 Global step 900 Train loss 2.76 on epoch=449
06/19/2022 20:56:20 - INFO - __main__ - Global step 900 Train loss 2.88 ACC 0.0 on epoch=449
06/19/2022 20:56:21 - INFO - __main__ - Step 910 Global step 910 Train loss 2.73 on epoch=454
06/19/2022 20:56:23 - INFO - __main__ - Step 920 Global step 920 Train loss 2.78 on epoch=459
06/19/2022 20:56:24 - INFO - __main__ - Step 930 Global step 930 Train loss 2.65 on epoch=464
06/19/2022 20:56:25 - INFO - __main__ - Step 940 Global step 940 Train loss 2.58 on epoch=469
06/19/2022 20:56:26 - INFO - __main__ - Step 950 Global step 950 Train loss 2.63 on epoch=474
06/19/2022 20:56:30 - INFO - __main__ - Global step 950 Train loss 2.67 ACC 0.0 on epoch=474
06/19/2022 20:56:32 - INFO - __main__ - Step 960 Global step 960 Train loss 2.60 on epoch=479
06/19/2022 20:56:33 - INFO - __main__ - Step 970 Global step 970 Train loss 2.52 on epoch=484
06/19/2022 20:56:34 - INFO - __main__ - Step 980 Global step 980 Train loss 2.31 on epoch=489
06/19/2022 20:56:36 - INFO - __main__ - Step 990 Global step 990 Train loss 2.36 on epoch=494
06/19/2022 20:56:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.31 on epoch=499
06/19/2022 20:56:43 - INFO - __main__ - Global step 1000 Train loss 2.42 ACC 0.0 on epoch=499
06/19/2022 20:56:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.35 on epoch=504
06/19/2022 20:56:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.31 on epoch=509
06/19/2022 20:56:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.34 on epoch=514
06/19/2022 20:56:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.28 on epoch=519
06/19/2022 20:56:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.22 on epoch=524
06/19/2022 20:57:00 - INFO - __main__ - Global step 1050 Train loss 2.30 ACC 0.0 on epoch=524
06/19/2022 20:57:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.19 on epoch=529
06/19/2022 20:57:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.20 on epoch=534
06/19/2022 20:57:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.15 on epoch=539
06/19/2022 20:57:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.06 on epoch=544
06/19/2022 20:57:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.00 on epoch=549
06/19/2022 20:57:17 - INFO - __main__ - Global step 1100 Train loss 2.12 ACC 0.25 on epoch=549
06/19/2022 20:57:17 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.25 on epoch=549, global_step=1100
06/19/2022 20:57:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.01 on epoch=554
06/19/2022 20:57:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.07 on epoch=559
06/19/2022 20:57:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.99 on epoch=564
06/19/2022 20:57:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.94 on epoch=569
06/19/2022 20:57:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.02 on epoch=574
06/19/2022 20:57:25 - INFO - __main__ - Global step 1150 Train loss 2.01 ACC 0.5625 on epoch=574
06/19/2022 20:57:25 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.5625 on epoch=574, global_step=1150
06/19/2022 20:57:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.88 on epoch=579
06/19/2022 20:57:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.85 on epoch=584
06/19/2022 20:57:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.73 on epoch=589
06/19/2022 20:57:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.71 on epoch=594
06/19/2022 20:57:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.73 on epoch=599
06/19/2022 20:57:37 - INFO - __main__ - Global step 1200 Train loss 1.78 ACC 0.5 on epoch=599
06/19/2022 20:57:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.73 on epoch=604
06/19/2022 20:57:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.66 on epoch=609
06/19/2022 20:57:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.65 on epoch=614
06/19/2022 20:57:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.72 on epoch=619
06/19/2022 20:57:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.70 on epoch=624
06/19/2022 20:57:50 - INFO - __main__ - Global step 1250 Train loss 1.69 ACC 0.5 on epoch=624
06/19/2022 20:57:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.60 on epoch=629
06/19/2022 20:57:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.57 on epoch=634
06/19/2022 20:57:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.45 on epoch=639
06/19/2022 20:57:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.52 on epoch=644
06/19/2022 20:57:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.37 on epoch=649
06/19/2022 20:58:00 - INFO - __main__ - Global step 1300 Train loss 1.50 ACC 0.5625 on epoch=649
06/19/2022 20:58:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.38 on epoch=654
06/19/2022 20:58:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.37 on epoch=659
06/19/2022 20:58:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.27 on epoch=664
06/19/2022 20:58:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.23 on epoch=669
06/19/2022 20:58:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.23 on epoch=674
06/19/2022 20:58:09 - INFO - __main__ - Global step 1350 Train loss 1.29 ACC 0.5 on epoch=674
06/19/2022 20:58:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.27 on epoch=679
06/19/2022 20:58:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.23 on epoch=684
06/19/2022 20:58:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.16 on epoch=689
06/19/2022 20:58:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.16 on epoch=694
06/19/2022 20:58:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.06 on epoch=699
06/19/2022 20:58:23 - INFO - __main__ - Global step 1400 Train loss 1.18 ACC 0.5 on epoch=699
06/19/2022 20:58:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.15 on epoch=704
06/19/2022 20:58:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.03 on epoch=709
06/19/2022 20:58:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.05 on epoch=714
06/19/2022 20:58:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.11 on epoch=719
06/19/2022 20:58:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.02 on epoch=724
06/19/2022 20:58:35 - INFO - __main__ - Global step 1450 Train loss 1.07 ACC 0.5 on epoch=724
06/19/2022 20:58:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.86 on epoch=729
06/19/2022 20:58:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.96 on epoch=734
06/19/2022 20:58:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.86 on epoch=739
06/19/2022 20:58:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.84 on epoch=744
06/19/2022 20:58:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.92 on epoch=749
06/19/2022 20:58:45 - INFO - __main__ - Global step 1500 Train loss 0.89 ACC 0.5 on epoch=749
06/19/2022 20:58:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.75 on epoch=754
06/19/2022 20:58:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.82 on epoch=759
06/19/2022 20:58:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.75 on epoch=764
06/19/2022 20:58:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.77 on epoch=769
06/19/2022 20:58:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.74 on epoch=774
06/19/2022 20:58:53 - INFO - __main__ - Global step 1550 Train loss 0.76 ACC 0.5 on epoch=774
06/19/2022 20:58:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.70 on epoch=779
06/19/2022 20:58:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.74 on epoch=784
06/19/2022 20:58:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.71 on epoch=789
06/19/2022 20:58:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.67 on epoch=794
06/19/2022 20:58:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.71 on epoch=799
06/19/2022 20:59:00 - INFO - __main__ - Global step 1600 Train loss 0.71 ACC 0.53125 on epoch=799
06/19/2022 20:59:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.66 on epoch=804
06/19/2022 20:59:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.70 on epoch=809
06/19/2022 20:59:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.66 on epoch=814
06/19/2022 20:59:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.66 on epoch=819
06/19/2022 20:59:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.61 on epoch=824
06/19/2022 20:59:07 - INFO - __main__ - Global step 1650 Train loss 0.66 ACC 0.46875 on epoch=824
06/19/2022 20:59:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.69 on epoch=829
06/19/2022 20:59:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.70 on epoch=834
06/19/2022 20:59:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.62 on epoch=839
06/19/2022 20:59:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.70 on epoch=844
06/19/2022 20:59:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.66 on epoch=849
06/19/2022 20:59:15 - INFO - __main__ - Global step 1700 Train loss 0.67 ACC 0.5625 on epoch=849
06/19/2022 20:59:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.63 on epoch=854
06/19/2022 20:59:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.51 on epoch=859
06/19/2022 20:59:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.56 on epoch=864
06/19/2022 20:59:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.65 on epoch=869
06/19/2022 20:59:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.60 on epoch=874
06/19/2022 20:59:23 - INFO - __main__ - Global step 1750 Train loss 0.59 ACC 0.5 on epoch=874
06/19/2022 20:59:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.75 on epoch=879
06/19/2022 20:59:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.55 on epoch=884
06/19/2022 20:59:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.63 on epoch=889
06/19/2022 20:59:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.57 on epoch=894
06/19/2022 20:59:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.53 on epoch=899
06/19/2022 20:59:31 - INFO - __main__ - Global step 1800 Train loss 0.61 ACC 0.59375 on epoch=899
06/19/2022 20:59:31 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=899, global_step=1800
06/19/2022 20:59:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.63 on epoch=904
06/19/2022 20:59:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.60 on epoch=909
06/19/2022 20:59:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=914
06/19/2022 20:59:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.58 on epoch=919
06/19/2022 20:59:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.53 on epoch=924
06/19/2022 20:59:39 - INFO - __main__ - Global step 1850 Train loss 0.58 ACC 0.5 on epoch=924
06/19/2022 20:59:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.60 on epoch=929
06/19/2022 20:59:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.57 on epoch=934
06/19/2022 20:59:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=939
06/19/2022 20:59:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.54 on epoch=944
06/19/2022 20:59:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.61 on epoch=949
06/19/2022 20:59:46 - INFO - __main__ - Global step 1900 Train loss 0.57 ACC 0.5 on epoch=949
06/19/2022 20:59:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.53 on epoch=954
06/19/2022 20:59:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.53 on epoch=959
06/19/2022 20:59:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=964
06/19/2022 20:59:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.47 on epoch=969
06/19/2022 20:59:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.52 on epoch=974
06/19/2022 20:59:53 - INFO - __main__ - Global step 1950 Train loss 0.50 ACC 0.5 on epoch=974
06/19/2022 20:59:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.54 on epoch=979
06/19/2022 20:59:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.52 on epoch=984
06/19/2022 20:59:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.54 on epoch=989
06/19/2022 20:59:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.49 on epoch=994
06/19/2022 21:00:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.56 on epoch=999
06/19/2022 21:00:01 - INFO - __main__ - Global step 2000 Train loss 0.53 ACC 0.5 on epoch=999
06/19/2022 21:00:01 - INFO - __main__ - save last model!
06/19/2022 21:00:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:00:01 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 21:00:01 - INFO - __main__ - Printing 3 examples
06/19/2022 21:00:01 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 21:00:01 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:01 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 21:00:01 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:00:01 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 21:00:01 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:00:01 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:00:01 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:00:01 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:00:01 - INFO - __main__ - Printing 3 examples
06/19/2022 21:00:01 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 21:00:01 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:01 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 21:00:01 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:01 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 21:00:01 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:00:01 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:00:01 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 21:00:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:00:01 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:00:01 - INFO - __main__ - Printing 3 examples
06/19/2022 21:00:01 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 21:00:01 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:01 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 21:00:01 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:01 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 21:00:01 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:01 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:00:01 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:00:01 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:00:07 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:00:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:00:08 - INFO - __main__ - Starting training!
06/19/2022 21:00:09 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_42_0.3_8_predictions.txt
06/19/2022 21:00:09 - INFO - __main__ - ACC on test data: 0.6642
06/19/2022 21:00:09 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.3, bsz=8, dev_performance=0.59375, test_performance=0.6642156862745098
06/19/2022 21:00:09 - INFO - __main__ - Running ... prefix=glue-mrpc_16_42, lr=0.2, bsz=8 ...
06/19/2022 21:00:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:00:10 - INFO - __main__ - Printing 3 examples
06/19/2022 21:00:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company . [SEP] sentence 2: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .
06/19/2022 21:00:10 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Yesterday , Taiwan reported 35 new infections , bringing the total number of cases to 418 . [SEP] sentence 2: The island reported another 35 probable cases yesterday , taking its total to 418 .
06/19/2022 21:00:10 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:10 - INFO - __main__ -  [glue-mrpc] sentence 1: A month ago , the Commerce Department estimated that GDP had grown at a 7.2 percent rate in the third quarter . [SEP] sentence 2: A month ago , the Commerce Department said GDP grew at a 7.2 percent rate .
06/19/2022 21:00:10 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:00:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:00:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:00:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:00:10 - INFO - __main__ - Printing 3 examples
06/19/2022 21:00:10 - INFO - __main__ -  [glue-mrpc] sentence 1: A draft statement , obtained by Reuters on Friday , stopped short of endorsing the U.S. charge but said some aspects of Iran 's programme raised " serious concern " . [SEP] sentence 2: The EU statement stopped short of endorsing the U.S. charge that Tehran is seeking nuclear weapons but said some aspects of Iran 's programme raised " serious concern " .
06/19/2022 21:00:10 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:10 - INFO - __main__ -  [glue-mrpc] sentence 1: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of the aircraft financing capability in this country , " Mr. Tellier said . [SEP] sentence 2: " Prospects for the whole Canadian aerospace industry are improving with recent developments contributing to the enhancement of aircraft financing in this country , " said Paul Tellier , Bombardier chief executive .
06/19/2022 21:00:10 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:10 - INFO - __main__ -  [glue-mrpc] sentence 1: All of those governments have said their support will not waver , though public sentiment is rising against it . [SEP] sentence 2: The governments of those countries have said that despite rising public opposition , their support will not waver .
06/19/2022 21:00:10 - INFO - __main__ - ['equivalent']
06/19/2022 21:00:10 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:00:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:00:10 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:00:16 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:00:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:00:16 - INFO - __main__ - Starting training!
06/19/2022 21:00:18 - INFO - __main__ - Step 10 Global step 10 Train loss 6.91 on epoch=4
06/19/2022 21:00:19 - INFO - __main__ - Step 20 Global step 20 Train loss 6.83 on epoch=9
06/19/2022 21:00:21 - INFO - __main__ - Step 30 Global step 30 Train loss 6.90 on epoch=14
06/19/2022 21:00:22 - INFO - __main__ - Step 40 Global step 40 Train loss 6.90 on epoch=19
06/19/2022 21:00:23 - INFO - __main__ - Step 50 Global step 50 Train loss 6.70 on epoch=24
06/19/2022 21:00:25 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/19/2022 21:00:25 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:00:26 - INFO - __main__ - Step 60 Global step 60 Train loss 6.88 on epoch=29
06/19/2022 21:00:27 - INFO - __main__ - Step 70 Global step 70 Train loss 6.80 on epoch=34
06/19/2022 21:00:29 - INFO - __main__ - Step 80 Global step 80 Train loss 6.79 on epoch=39
06/19/2022 21:00:30 - INFO - __main__ - Step 90 Global step 90 Train loss 6.78 on epoch=44
06/19/2022 21:00:31 - INFO - __main__ - Step 100 Global step 100 Train loss 6.79 on epoch=49
06/19/2022 21:00:34 - INFO - __main__ - Global step 100 Train loss 6.81 ACC 0.0 on epoch=49
06/19/2022 21:00:35 - INFO - __main__ - Step 110 Global step 110 Train loss 6.78 on epoch=54
06/19/2022 21:00:37 - INFO - __main__ - Step 120 Global step 120 Train loss 6.77 on epoch=59
06/19/2022 21:00:38 - INFO - __main__ - Step 130 Global step 130 Train loss 6.79 on epoch=64
06/19/2022 21:00:39 - INFO - __main__ - Step 140 Global step 140 Train loss 6.79 on epoch=69
06/19/2022 21:00:40 - INFO - __main__ - Step 150 Global step 150 Train loss 6.82 on epoch=74
06/19/2022 21:00:45 - INFO - __main__ - Global step 150 Train loss 6.79 ACC 0.0 on epoch=74
06/19/2022 21:00:46 - INFO - __main__ - Step 160 Global step 160 Train loss 6.77 on epoch=79
06/19/2022 21:00:47 - INFO - __main__ - Step 170 Global step 170 Train loss 6.68 on epoch=84
06/19/2022 21:00:49 - INFO - __main__ - Step 180 Global step 180 Train loss 6.73 on epoch=89
06/19/2022 21:00:50 - INFO - __main__ - Step 190 Global step 190 Train loss 6.72 on epoch=94
06/19/2022 21:00:51 - INFO - __main__ - Step 200 Global step 200 Train loss 6.69 on epoch=99
06/19/2022 21:00:53 - INFO - __main__ - Global step 200 Train loss 6.72 ACC 0.0 on epoch=99
06/19/2022 21:00:54 - INFO - __main__ - Step 210 Global step 210 Train loss 6.74 on epoch=104
06/19/2022 21:00:55 - INFO - __main__ - Step 220 Global step 220 Train loss 6.63 on epoch=109
06/19/2022 21:00:56 - INFO - __main__ - Step 230 Global step 230 Train loss 6.70 on epoch=114
06/19/2022 21:00:58 - INFO - __main__ - Step 240 Global step 240 Train loss 6.66 on epoch=119
06/19/2022 21:00:59 - INFO - __main__ - Step 250 Global step 250 Train loss 6.66 on epoch=124
06/19/2022 21:01:10 - INFO - __main__ - Global step 250 Train loss 6.67 ACC 0.0 on epoch=124
06/19/2022 21:01:11 - INFO - __main__ - Step 260 Global step 260 Train loss 6.65 on epoch=129
06/19/2022 21:01:12 - INFO - __main__ - Step 270 Global step 270 Train loss 6.60 on epoch=134
06/19/2022 21:01:13 - INFO - __main__ - Step 280 Global step 280 Train loss 6.62 on epoch=139
06/19/2022 21:01:15 - INFO - __main__ - Step 290 Global step 290 Train loss 6.57 on epoch=144
06/19/2022 21:01:16 - INFO - __main__ - Step 300 Global step 300 Train loss 6.51 on epoch=149
06/19/2022 21:01:23 - INFO - __main__ - Global step 300 Train loss 6.59 ACC 0.0 on epoch=149
06/19/2022 21:01:24 - INFO - __main__ - Step 310 Global step 310 Train loss 6.44 on epoch=154
06/19/2022 21:01:26 - INFO - __main__ - Step 320 Global step 320 Train loss 6.46 on epoch=159
06/19/2022 21:01:27 - INFO - __main__ - Step 330 Global step 330 Train loss 6.47 on epoch=164
06/19/2022 21:01:28 - INFO - __main__ - Step 340 Global step 340 Train loss 6.36 on epoch=169
06/19/2022 21:01:29 - INFO - __main__ - Step 350 Global step 350 Train loss 6.43 on epoch=174
06/19/2022 21:01:33 - INFO - __main__ - Global step 350 Train loss 6.43 ACC 0.0 on epoch=174
06/19/2022 21:01:34 - INFO - __main__ - Step 360 Global step 360 Train loss 6.42 on epoch=179
06/19/2022 21:01:35 - INFO - __main__ - Step 370 Global step 370 Train loss 6.35 on epoch=184
06/19/2022 21:01:36 - INFO - __main__ - Step 380 Global step 380 Train loss 6.28 on epoch=189
06/19/2022 21:01:38 - INFO - __main__ - Step 390 Global step 390 Train loss 6.33 on epoch=194
06/19/2022 21:01:39 - INFO - __main__ - Step 400 Global step 400 Train loss 6.32 on epoch=199
06/19/2022 21:01:50 - INFO - __main__ - Global step 400 Train loss 6.34 ACC 0.0 on epoch=199
06/19/2022 21:01:51 - INFO - __main__ - Step 410 Global step 410 Train loss 6.20 on epoch=204
06/19/2022 21:01:52 - INFO - __main__ - Step 420 Global step 420 Train loss 6.22 on epoch=209
06/19/2022 21:01:53 - INFO - __main__ - Step 430 Global step 430 Train loss 6.25 on epoch=214
06/19/2022 21:01:55 - INFO - __main__ - Step 440 Global step 440 Train loss 6.21 on epoch=219
06/19/2022 21:01:56 - INFO - __main__ - Step 450 Global step 450 Train loss 6.20 on epoch=224
06/19/2022 21:02:07 - INFO - __main__ - Global step 450 Train loss 6.21 ACC 0.0 on epoch=224
06/19/2022 21:02:08 - INFO - __main__ - Step 460 Global step 460 Train loss 6.18 on epoch=229
06/19/2022 21:02:09 - INFO - __main__ - Step 470 Global step 470 Train loss 6.11 on epoch=234
06/19/2022 21:02:11 - INFO - __main__ - Step 480 Global step 480 Train loss 6.06 on epoch=239
06/19/2022 21:02:12 - INFO - __main__ - Step 490 Global step 490 Train loss 6.05 on epoch=244
06/19/2022 21:02:13 - INFO - __main__ - Step 500 Global step 500 Train loss 6.12 on epoch=249
06/19/2022 21:02:24 - INFO - __main__ - Global step 500 Train loss 6.10 ACC 0.0 on epoch=249
06/19/2022 21:02:25 - INFO - __main__ - Step 510 Global step 510 Train loss 6.10 on epoch=254
06/19/2022 21:02:27 - INFO - __main__ - Step 520 Global step 520 Train loss 5.97 on epoch=259
06/19/2022 21:02:28 - INFO - __main__ - Step 530 Global step 530 Train loss 6.03 on epoch=264
06/19/2022 21:02:29 - INFO - __main__ - Step 540 Global step 540 Train loss 5.96 on epoch=269
06/19/2022 21:02:30 - INFO - __main__ - Step 550 Global step 550 Train loss 5.95 on epoch=274
06/19/2022 21:02:41 - INFO - __main__ - Global step 550 Train loss 6.00 ACC 0.0 on epoch=274
06/19/2022 21:02:43 - INFO - __main__ - Step 560 Global step 560 Train loss 5.98 on epoch=279
06/19/2022 21:02:44 - INFO - __main__ - Step 570 Global step 570 Train loss 6.02 on epoch=284
06/19/2022 21:02:45 - INFO - __main__ - Step 580 Global step 580 Train loss 5.88 on epoch=289
06/19/2022 21:02:46 - INFO - __main__ - Step 590 Global step 590 Train loss 5.90 on epoch=294
06/19/2022 21:02:48 - INFO - __main__ - Step 600 Global step 600 Train loss 5.81 on epoch=299
06/19/2022 21:02:54 - INFO - __main__ - Global step 600 Train loss 5.92 ACC 0.0 on epoch=299
06/19/2022 21:02:55 - INFO - __main__ - Step 610 Global step 610 Train loss 5.82 on epoch=304
06/19/2022 21:02:57 - INFO - __main__ - Step 620 Global step 620 Train loss 5.81 on epoch=309
06/19/2022 21:02:58 - INFO - __main__ - Step 630 Global step 630 Train loss 5.68 on epoch=314
06/19/2022 21:02:59 - INFO - __main__ - Step 640 Global step 640 Train loss 5.71 on epoch=319
06/19/2022 21:03:01 - INFO - __main__ - Step 650 Global step 650 Train loss 5.65 on epoch=324
06/19/2022 21:03:03 - INFO - __main__ - Global step 650 Train loss 5.74 ACC 0.0 on epoch=324
06/19/2022 21:03:04 - INFO - __main__ - Step 660 Global step 660 Train loss 5.73 on epoch=329
06/19/2022 21:03:05 - INFO - __main__ - Step 670 Global step 670 Train loss 5.63 on epoch=334
06/19/2022 21:03:07 - INFO - __main__ - Step 680 Global step 680 Train loss 5.46 on epoch=339
06/19/2022 21:03:08 - INFO - __main__ - Step 690 Global step 690 Train loss 5.55 on epoch=344
06/19/2022 21:03:09 - INFO - __main__ - Step 700 Global step 700 Train loss 5.40 on epoch=349
06/19/2022 21:03:16 - INFO - __main__ - Global step 700 Train loss 5.56 ACC 0.0 on epoch=349
06/19/2022 21:03:18 - INFO - __main__ - Step 710 Global step 710 Train loss 5.42 on epoch=354
06/19/2022 21:03:19 - INFO - __main__ - Step 720 Global step 720 Train loss 5.40 on epoch=359
06/19/2022 21:03:20 - INFO - __main__ - Step 730 Global step 730 Train loss 5.24 on epoch=364
06/19/2022 21:03:22 - INFO - __main__ - Step 740 Global step 740 Train loss 5.26 on epoch=369
06/19/2022 21:03:23 - INFO - __main__ - Step 750 Global step 750 Train loss 5.22 on epoch=374
06/19/2022 21:03:31 - INFO - __main__ - Global step 750 Train loss 5.31 ACC 0.0 on epoch=374
06/19/2022 21:03:33 - INFO - __main__ - Step 760 Global step 760 Train loss 5.22 on epoch=379
06/19/2022 21:03:34 - INFO - __main__ - Step 770 Global step 770 Train loss 5.16 on epoch=384
06/19/2022 21:03:35 - INFO - __main__ - Step 780 Global step 780 Train loss 5.31 on epoch=389
06/19/2022 21:03:36 - INFO - __main__ - Step 790 Global step 790 Train loss 5.05 on epoch=394
06/19/2022 21:03:37 - INFO - __main__ - Step 800 Global step 800 Train loss 5.03 on epoch=399
06/19/2022 21:03:48 - INFO - __main__ - Global step 800 Train loss 5.15 ACC 0.0 on epoch=399
06/19/2022 21:03:49 - INFO - __main__ - Step 810 Global step 810 Train loss 4.94 on epoch=404
06/19/2022 21:03:50 - INFO - __main__ - Step 820 Global step 820 Train loss 4.88 on epoch=409
06/19/2022 21:03:52 - INFO - __main__ - Step 830 Global step 830 Train loss 4.96 on epoch=414
06/19/2022 21:03:53 - INFO - __main__ - Step 840 Global step 840 Train loss 4.97 on epoch=419
06/19/2022 21:03:54 - INFO - __main__ - Step 850 Global step 850 Train loss 4.76 on epoch=424
06/19/2022 21:04:01 - INFO - __main__ - Global step 850 Train loss 4.90 ACC 0.0 on epoch=424
06/19/2022 21:04:02 - INFO - __main__ - Step 860 Global step 860 Train loss 4.75 on epoch=429
06/19/2022 21:04:03 - INFO - __main__ - Step 870 Global step 870 Train loss 4.69 on epoch=434
06/19/2022 21:04:04 - INFO - __main__ - Step 880 Global step 880 Train loss 4.70 on epoch=439
06/19/2022 21:04:06 - INFO - __main__ - Step 890 Global step 890 Train loss 4.61 on epoch=444
06/19/2022 21:04:07 - INFO - __main__ - Step 900 Global step 900 Train loss 4.57 on epoch=449
06/19/2022 21:04:10 - INFO - __main__ - Global step 900 Train loss 4.67 ACC 0.0 on epoch=449
06/19/2022 21:04:11 - INFO - __main__ - Step 910 Global step 910 Train loss 4.49 on epoch=454
06/19/2022 21:04:12 - INFO - __main__ - Step 920 Global step 920 Train loss 4.54 on epoch=459
06/19/2022 21:04:13 - INFO - __main__ - Step 930 Global step 930 Train loss 4.38 on epoch=464
06/19/2022 21:04:15 - INFO - __main__ - Step 940 Global step 940 Train loss 4.34 on epoch=469
06/19/2022 21:04:16 - INFO - __main__ - Step 950 Global step 950 Train loss 4.18 on epoch=474
06/19/2022 21:04:22 - INFO - __main__ - Global step 950 Train loss 4.39 ACC 0.0 on epoch=474
06/19/2022 21:04:23 - INFO - __main__ - Step 960 Global step 960 Train loss 4.23 on epoch=479
06/19/2022 21:04:25 - INFO - __main__ - Step 970 Global step 970 Train loss 4.17 on epoch=484
06/19/2022 21:04:26 - INFO - __main__ - Step 980 Global step 980 Train loss 4.12 on epoch=489
06/19/2022 21:04:27 - INFO - __main__ - Step 990 Global step 990 Train loss 4.01 on epoch=494
06/19/2022 21:04:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.95 on epoch=499
06/19/2022 21:04:31 - INFO - __main__ - Global step 1000 Train loss 4.09 ACC 0.0 on epoch=499
06/19/2022 21:04:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.90 on epoch=504
06/19/2022 21:04:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.80 on epoch=509
06/19/2022 21:04:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.80 on epoch=514
06/19/2022 21:04:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.93 on epoch=519
06/19/2022 21:04:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.83 on epoch=524
06/19/2022 21:04:40 - INFO - __main__ - Global step 1050 Train loss 3.85 ACC 0.0 on epoch=524
06/19/2022 21:04:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.64 on epoch=529
06/19/2022 21:04:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.62 on epoch=534
06/19/2022 21:04:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.55 on epoch=539
06/19/2022 21:04:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.43 on epoch=544
06/19/2022 21:04:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.45 on epoch=549
06/19/2022 21:04:48 - INFO - __main__ - Global step 1100 Train loss 3.54 ACC 0.0 on epoch=549
06/19/2022 21:04:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.46 on epoch=554
06/19/2022 21:04:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.26 on epoch=559
06/19/2022 21:04:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.21 on epoch=564
06/19/2022 21:04:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.18 on epoch=569
06/19/2022 21:04:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.09 on epoch=574
06/19/2022 21:04:58 - INFO - __main__ - Global step 1150 Train loss 3.24 ACC 0.0 on epoch=574
06/19/2022 21:04:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.02 on epoch=579
06/19/2022 21:05:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.13 on epoch=584
06/19/2022 21:05:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.03 on epoch=589
06/19/2022 21:05:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.99 on epoch=594
06/19/2022 21:05:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.01 on epoch=599
06/19/2022 21:05:07 - INFO - __main__ - Global step 1200 Train loss 3.03 ACC 0.0 on epoch=599
06/19/2022 21:05:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.07 on epoch=604
06/19/2022 21:05:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.82 on epoch=609
06/19/2022 21:05:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.77 on epoch=614
06/19/2022 21:05:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.79 on epoch=619
06/19/2022 21:05:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.70 on epoch=624
06/19/2022 21:05:16 - INFO - __main__ - Global step 1250 Train loss 2.83 ACC 0.0625 on epoch=624
06/19/2022 21:05:16 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=624, global_step=1250
06/19/2022 21:05:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.72 on epoch=629
06/19/2022 21:05:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.64 on epoch=634
06/19/2022 21:05:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.58 on epoch=639
06/19/2022 21:05:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.55 on epoch=644
06/19/2022 21:05:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.38 on epoch=649
06/19/2022 21:05:25 - INFO - __main__ - Global step 1300 Train loss 2.57 ACC 0.3125 on epoch=649
06/19/2022 21:05:25 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.3125 on epoch=649, global_step=1300
06/19/2022 21:05:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.50 on epoch=654
06/19/2022 21:05:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.35 on epoch=659
06/19/2022 21:05:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.37 on epoch=664
06/19/2022 21:05:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.35 on epoch=669
06/19/2022 21:05:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.22 on epoch=674
06/19/2022 21:05:33 - INFO - __main__ - Global step 1350 Train loss 2.36 ACC 0.5 on epoch=674
06/19/2022 21:05:33 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.5 on epoch=674, global_step=1350
06/19/2022 21:05:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.33 on epoch=679
06/19/2022 21:05:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.18 on epoch=684
06/19/2022 21:05:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.15 on epoch=689
06/19/2022 21:05:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.06 on epoch=694
06/19/2022 21:05:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.07 on epoch=699
06/19/2022 21:05:41 - INFO - __main__ - Global step 1400 Train loss 2.16 ACC 0.5 on epoch=699
06/19/2022 21:05:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.96 on epoch=704
06/19/2022 21:05:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.08 on epoch=709
06/19/2022 21:05:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.96 on epoch=714
06/19/2022 21:05:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.87 on epoch=719
06/19/2022 21:05:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.81 on epoch=724
06/19/2022 21:05:52 - INFO - __main__ - Global step 1450 Train loss 1.94 ACC 0.46875 on epoch=724
06/19/2022 21:05:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.79 on epoch=729
06/19/2022 21:05:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.66 on epoch=734
06/19/2022 21:05:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.82 on epoch=739
06/19/2022 21:05:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.77 on epoch=744
06/19/2022 21:05:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.70 on epoch=749
06/19/2022 21:06:00 - INFO - __main__ - Global step 1500 Train loss 1.75 ACC 0.5 on epoch=749
06/19/2022 21:06:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.68 on epoch=754
06/19/2022 21:06:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.61 on epoch=759
06/19/2022 21:06:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.56 on epoch=764
06/19/2022 21:06:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.51 on epoch=769
06/19/2022 21:06:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.55 on epoch=774
06/19/2022 21:06:09 - INFO - __main__ - Global step 1550 Train loss 1.58 ACC 0.375 on epoch=774
06/19/2022 21:06:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.45 on epoch=779
06/19/2022 21:06:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.55 on epoch=784
06/19/2022 21:06:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.49 on epoch=789
06/19/2022 21:06:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.52 on epoch=794
06/19/2022 21:06:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.48 on epoch=799
06/19/2022 21:06:17 - INFO - __main__ - Global step 1600 Train loss 1.50 ACC 0.5 on epoch=799
06/19/2022 21:06:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.39 on epoch=804
06/19/2022 21:06:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.47 on epoch=809
06/19/2022 21:06:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.36 on epoch=814
06/19/2022 21:06:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.19 on epoch=819
06/19/2022 21:06:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.19 on epoch=824
06/19/2022 21:06:25 - INFO - __main__ - Global step 1650 Train loss 1.32 ACC 0.5 on epoch=824
06/19/2022 21:06:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.31 on epoch=829
06/19/2022 21:06:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.33 on epoch=834
06/19/2022 21:06:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.19 on epoch=839
06/19/2022 21:06:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.20 on epoch=844
06/19/2022 21:06:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.11 on epoch=849
06/19/2022 21:06:34 - INFO - __main__ - Global step 1700 Train loss 1.23 ACC 0.5 on epoch=849
06/19/2022 21:06:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.14 on epoch=854
06/19/2022 21:06:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.16 on epoch=859
06/19/2022 21:06:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.04 on epoch=864
06/19/2022 21:06:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.18 on epoch=869
06/19/2022 21:06:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.09 on epoch=874
06/19/2022 21:06:42 - INFO - __main__ - Global step 1750 Train loss 1.12 ACC 0.5 on epoch=874
06/19/2022 21:06:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.08 on epoch=879
06/19/2022 21:06:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.13 on epoch=884
06/19/2022 21:06:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.02 on epoch=889
06/19/2022 21:06:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.02 on epoch=894
06/19/2022 21:06:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.98 on epoch=899
06/19/2022 21:06:50 - INFO - __main__ - Global step 1800 Train loss 1.05 ACC 0.5 on epoch=899
06/19/2022 21:06:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.95 on epoch=904
06/19/2022 21:06:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.95 on epoch=909
06/19/2022 21:06:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.98 on epoch=914
06/19/2022 21:06:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.98 on epoch=919
06/19/2022 21:06:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.97 on epoch=924
06/19/2022 21:06:58 - INFO - __main__ - Global step 1850 Train loss 0.97 ACC 0.5 on epoch=924
06/19/2022 21:06:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.87 on epoch=929
06/19/2022 21:07:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.96 on epoch=934
06/19/2022 21:07:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.88 on epoch=939
06/19/2022 21:07:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.82 on epoch=944
06/19/2022 21:07:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.85 on epoch=949
06/19/2022 21:07:05 - INFO - __main__ - Global step 1900 Train loss 0.87 ACC 0.5 on epoch=949
06/19/2022 21:07:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.88 on epoch=954
06/19/2022 21:07:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.93 on epoch=959
06/19/2022 21:07:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.85 on epoch=964
06/19/2022 21:07:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.96 on epoch=969
06/19/2022 21:07:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.85 on epoch=974
06/19/2022 21:07:14 - INFO - __main__ - Global step 1950 Train loss 0.89 ACC 0.5 on epoch=974
06/19/2022 21:07:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.84 on epoch=979
06/19/2022 21:07:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.84 on epoch=984
06/19/2022 21:07:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.83 on epoch=989
06/19/2022 21:07:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.87 on epoch=994
06/19/2022 21:07:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.78 on epoch=999
06/19/2022 21:07:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:07:21 - INFO - __main__ - Printing 3 examples
06/19/2022 21:07:21 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 21:07:21 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:21 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 21:07:21 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:21 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 21:07:21 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:07:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:07:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:07:21 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:07:21 - INFO - __main__ - Printing 3 examples
06/19/2022 21:07:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 21:07:21 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:21 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 21:07:21 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:21 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 21:07:21 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:21 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:07:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:07:22 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:07:22 - INFO - __main__ - Global step 2000 Train loss 0.83 ACC 0.5 on epoch=999
06/19/2022 21:07:22 - INFO - __main__ - save last model!
06/19/2022 21:07:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:07:22 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 21:07:22 - INFO - __main__ - Printing 3 examples
06/19/2022 21:07:22 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 21:07:22 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:22 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 21:07:22 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:07:22 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 21:07:22 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:07:22 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:07:22 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:07:23 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 21:07:27 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:07:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:07:27 - INFO - __main__ - Starting training!
06/19/2022 21:07:48 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_42_0.2_8_predictions.txt
06/19/2022 21:07:48 - INFO - __main__ - ACC on test data: 0.6838
06/19/2022 21:07:49 - INFO - __main__ - prefix=glue-mrpc_16_42, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.6838235294117647
06/19/2022 21:07:49 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.5, bsz=8 ...
06/19/2022 21:07:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:07:50 - INFO - __main__ - Printing 3 examples
06/19/2022 21:07:50 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 21:07:50 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:50 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 21:07:50 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:50 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 21:07:50 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:07:50 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:07:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:07:50 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:07:50 - INFO - __main__ - Printing 3 examples
06/19/2022 21:07:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 21:07:50 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:50 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 21:07:50 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:50 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 21:07:50 - INFO - __main__ - ['equivalent']
06/19/2022 21:07:50 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:07:50 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:07:50 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:07:56 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:07:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:07:56 - INFO - __main__ - Starting training!
06/19/2022 21:07:57 - INFO - __main__ - Step 10 Global step 10 Train loss 6.95 on epoch=4
06/19/2022 21:07:59 - INFO - __main__ - Step 20 Global step 20 Train loss 6.91 on epoch=9
06/19/2022 21:08:00 - INFO - __main__ - Step 30 Global step 30 Train loss 6.83 on epoch=14
06/19/2022 21:08:01 - INFO - __main__ - Step 40 Global step 40 Train loss 6.75 on epoch=19
06/19/2022 21:08:02 - INFO - __main__ - Step 50 Global step 50 Train loss 6.78 on epoch=24
06/19/2022 21:08:04 - INFO - __main__ - Global step 50 Train loss 6.84 ACC 0.0 on epoch=24
06/19/2022 21:08:04 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:08:06 - INFO - __main__ - Step 60 Global step 60 Train loss 6.73 on epoch=29
06/19/2022 21:08:07 - INFO - __main__ - Step 70 Global step 70 Train loss 6.68 on epoch=34
06/19/2022 21:08:08 - INFO - __main__ - Step 80 Global step 80 Train loss 6.61 on epoch=39
06/19/2022 21:08:10 - INFO - __main__ - Step 90 Global step 90 Train loss 6.61 on epoch=44
06/19/2022 21:08:11 - INFO - __main__ - Step 100 Global step 100 Train loss 6.57 on epoch=49
06/19/2022 21:08:18 - INFO - __main__ - Global step 100 Train loss 6.64 ACC 0.0 on epoch=49
06/19/2022 21:08:19 - INFO - __main__ - Step 110 Global step 110 Train loss 6.41 on epoch=54
06/19/2022 21:08:20 - INFO - __main__ - Step 120 Global step 120 Train loss 6.45 on epoch=59
06/19/2022 21:08:21 - INFO - __main__ - Step 130 Global step 130 Train loss 6.33 on epoch=64
06/19/2022 21:08:22 - INFO - __main__ - Step 140 Global step 140 Train loss 6.41 on epoch=69
06/19/2022 21:08:24 - INFO - __main__ - Step 150 Global step 150 Train loss 6.25 on epoch=74
06/19/2022 21:08:28 - INFO - __main__ - Global step 150 Train loss 6.37 ACC 0.0 on epoch=74
06/19/2022 21:08:29 - INFO - __main__ - Step 160 Global step 160 Train loss 6.17 on epoch=79
06/19/2022 21:08:30 - INFO - __main__ - Step 170 Global step 170 Train loss 6.08 on epoch=84
06/19/2022 21:08:31 - INFO - __main__ - Step 180 Global step 180 Train loss 6.08 on epoch=89
06/19/2022 21:08:33 - INFO - __main__ - Step 190 Global step 190 Train loss 5.97 on epoch=94
06/19/2022 21:08:34 - INFO - __main__ - Step 200 Global step 200 Train loss 6.00 on epoch=99
06/19/2022 21:08:42 - INFO - __main__ - Global step 200 Train loss 6.06 ACC 0.0 on epoch=99
06/19/2022 21:08:43 - INFO - __main__ - Step 210 Global step 210 Train loss 5.87 on epoch=104
06/19/2022 21:08:44 - INFO - __main__ - Step 220 Global step 220 Train loss 5.85 on epoch=109
06/19/2022 21:08:45 - INFO - __main__ - Step 230 Global step 230 Train loss 5.75 on epoch=114
06/19/2022 21:08:47 - INFO - __main__ - Step 240 Global step 240 Train loss 5.89 on epoch=119
06/19/2022 21:08:48 - INFO - __main__ - Step 250 Global step 250 Train loss 5.76 on epoch=124
06/19/2022 21:08:51 - INFO - __main__ - Global step 250 Train loss 5.82 ACC 0.0 on epoch=124
06/19/2022 21:08:52 - INFO - __main__ - Step 260 Global step 260 Train loss 5.69 on epoch=129
06/19/2022 21:08:53 - INFO - __main__ - Step 270 Global step 270 Train loss 5.73 on epoch=134
06/19/2022 21:08:54 - INFO - __main__ - Step 280 Global step 280 Train loss 5.77 on epoch=139
06/19/2022 21:08:56 - INFO - __main__ - Step 290 Global step 290 Train loss 5.63 on epoch=144
06/19/2022 21:08:57 - INFO - __main__ - Step 300 Global step 300 Train loss 5.53 on epoch=149
06/19/2022 21:09:07 - INFO - __main__ - Global step 300 Train loss 5.67 ACC 0.0 on epoch=149
06/19/2022 21:09:09 - INFO - __main__ - Step 310 Global step 310 Train loss 5.52 on epoch=154
06/19/2022 21:09:10 - INFO - __main__ - Step 320 Global step 320 Train loss 5.36 on epoch=159
06/19/2022 21:09:11 - INFO - __main__ - Step 330 Global step 330 Train loss 5.33 on epoch=164
06/19/2022 21:09:12 - INFO - __main__ - Step 340 Global step 340 Train loss 5.36 on epoch=169
06/19/2022 21:09:13 - INFO - __main__ - Step 350 Global step 350 Train loss 5.20 on epoch=174
06/19/2022 21:09:20 - INFO - __main__ - Global step 350 Train loss 5.35 ACC 0.0 on epoch=174
06/19/2022 21:09:21 - INFO - __main__ - Step 360 Global step 360 Train loss 5.13 on epoch=179
06/19/2022 21:09:23 - INFO - __main__ - Step 370 Global step 370 Train loss 4.93 on epoch=184
06/19/2022 21:09:24 - INFO - __main__ - Step 380 Global step 380 Train loss 4.96 on epoch=189
06/19/2022 21:09:25 - INFO - __main__ - Step 390 Global step 390 Train loss 5.02 on epoch=194
06/19/2022 21:09:26 - INFO - __main__ - Step 400 Global step 400 Train loss 4.77 on epoch=199
06/19/2022 21:09:36 - INFO - __main__ - Global step 400 Train loss 4.96 ACC 0.0 on epoch=199
06/19/2022 21:09:37 - INFO - __main__ - Step 410 Global step 410 Train loss 4.68 on epoch=204
06/19/2022 21:09:38 - INFO - __main__ - Step 420 Global step 420 Train loss 4.71 on epoch=209
06/19/2022 21:09:40 - INFO - __main__ - Step 430 Global step 430 Train loss 4.63 on epoch=214
06/19/2022 21:09:41 - INFO - __main__ - Step 440 Global step 440 Train loss 4.54 on epoch=219
06/19/2022 21:09:42 - INFO - __main__ - Step 450 Global step 450 Train loss 4.50 on epoch=224
06/19/2022 21:09:52 - INFO - __main__ - Global step 450 Train loss 4.61 ACC 0.0 on epoch=224
06/19/2022 21:09:53 - INFO - __main__ - Step 460 Global step 460 Train loss 4.42 on epoch=229
06/19/2022 21:09:54 - INFO - __main__ - Step 470 Global step 470 Train loss 4.34 on epoch=234
06/19/2022 21:09:56 - INFO - __main__ - Step 480 Global step 480 Train loss 4.31 on epoch=239
06/19/2022 21:09:57 - INFO - __main__ - Step 490 Global step 490 Train loss 4.20 on epoch=244
06/19/2022 21:09:58 - INFO - __main__ - Step 500 Global step 500 Train loss 4.15 on epoch=249
06/19/2022 21:10:09 - INFO - __main__ - Global step 500 Train loss 4.28 ACC 0.0 on epoch=249
06/19/2022 21:10:10 - INFO - __main__ - Step 510 Global step 510 Train loss 4.15 on epoch=254
06/19/2022 21:10:11 - INFO - __main__ - Step 520 Global step 520 Train loss 4.07 on epoch=259
06/19/2022 21:10:12 - INFO - __main__ - Step 530 Global step 530 Train loss 3.95 on epoch=264
06/19/2022 21:10:13 - INFO - __main__ - Step 540 Global step 540 Train loss 3.85 on epoch=269
06/19/2022 21:10:15 - INFO - __main__ - Step 550 Global step 550 Train loss 3.90 on epoch=274
06/19/2022 21:10:25 - INFO - __main__ - Global step 550 Train loss 3.99 ACC 0.0 on epoch=274
06/19/2022 21:10:27 - INFO - __main__ - Step 560 Global step 560 Train loss 3.72 on epoch=279
06/19/2022 21:10:28 - INFO - __main__ - Step 570 Global step 570 Train loss 3.60 on epoch=284
06/19/2022 21:10:29 - INFO - __main__ - Step 580 Global step 580 Train loss 3.58 on epoch=289
06/19/2022 21:10:30 - INFO - __main__ - Step 590 Global step 590 Train loss 3.51 on epoch=294
06/19/2022 21:10:31 - INFO - __main__ - Step 600 Global step 600 Train loss 3.50 on epoch=299
06/19/2022 21:10:42 - INFO - __main__ - Global step 600 Train loss 3.58 ACC 0.0 on epoch=299
06/19/2022 21:10:43 - INFO - __main__ - Step 610 Global step 610 Train loss 3.52 on epoch=304
06/19/2022 21:10:45 - INFO - __main__ - Step 620 Global step 620 Train loss 3.53 on epoch=309
06/19/2022 21:10:46 - INFO - __main__ - Step 630 Global step 630 Train loss 3.47 on epoch=314
06/19/2022 21:10:48 - INFO - __main__ - Step 640 Global step 640 Train loss 3.34 on epoch=319
06/19/2022 21:10:49 - INFO - __main__ - Step 650 Global step 650 Train loss 3.40 on epoch=324
06/19/2022 21:10:55 - INFO - __main__ - Global step 650 Train loss 3.45 ACC 0.0 on epoch=324
06/19/2022 21:10:57 - INFO - __main__ - Step 660 Global step 660 Train loss 3.13 on epoch=329
06/19/2022 21:10:58 - INFO - __main__ - Step 670 Global step 670 Train loss 3.15 on epoch=334
06/19/2022 21:10:59 - INFO - __main__ - Step 680 Global step 680 Train loss 3.12 on epoch=339
06/19/2022 21:11:00 - INFO - __main__ - Step 690 Global step 690 Train loss 2.95 on epoch=344
06/19/2022 21:11:01 - INFO - __main__ - Step 700 Global step 700 Train loss 2.90 on epoch=349
06/19/2022 21:11:11 - INFO - __main__ - Global step 700 Train loss 3.05 ACC 0.0 on epoch=349
06/19/2022 21:11:12 - INFO - __main__ - Step 710 Global step 710 Train loss 2.89 on epoch=354
06/19/2022 21:11:13 - INFO - __main__ - Step 720 Global step 720 Train loss 2.85 on epoch=359
06/19/2022 21:11:14 - INFO - __main__ - Step 730 Global step 730 Train loss 2.70 on epoch=364
06/19/2022 21:11:15 - INFO - __main__ - Step 740 Global step 740 Train loss 2.66 on epoch=369
06/19/2022 21:11:17 - INFO - __main__ - Step 750 Global step 750 Train loss 2.50 on epoch=374
06/19/2022 21:11:26 - INFO - __main__ - Global step 750 Train loss 2.72 ACC 0.125 on epoch=374
06/19/2022 21:11:26 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.125 on epoch=374, global_step=750
06/19/2022 21:11:27 - INFO - __main__ - Step 760 Global step 760 Train loss 2.60 on epoch=379
06/19/2022 21:11:28 - INFO - __main__ - Step 770 Global step 770 Train loss 2.59 on epoch=384
06/19/2022 21:11:29 - INFO - __main__ - Step 780 Global step 780 Train loss 2.42 on epoch=389
06/19/2022 21:11:31 - INFO - __main__ - Step 790 Global step 790 Train loss 2.29 on epoch=394
06/19/2022 21:11:32 - INFO - __main__ - Step 800 Global step 800 Train loss 2.27 on epoch=399
06/19/2022 21:11:42 - INFO - __main__ - Global step 800 Train loss 2.43 ACC 0.21875 on epoch=399
06/19/2022 21:11:42 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.21875 on epoch=399, global_step=800
06/19/2022 21:11:43 - INFO - __main__ - Step 810 Global step 810 Train loss 2.24 on epoch=404
06/19/2022 21:11:45 - INFO - __main__ - Step 820 Global step 820 Train loss 2.22 on epoch=409
06/19/2022 21:11:46 - INFO - __main__ - Step 830 Global step 830 Train loss 2.11 on epoch=414
06/19/2022 21:11:47 - INFO - __main__ - Step 840 Global step 840 Train loss 2.08 on epoch=419
06/19/2022 21:11:48 - INFO - __main__ - Step 850 Global step 850 Train loss 2.11 on epoch=424
06/19/2022 21:11:50 - INFO - __main__ - Global step 850 Train loss 2.15 ACC 0.5625 on epoch=424
06/19/2022 21:11:50 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.5625 on epoch=424, global_step=850
06/19/2022 21:11:51 - INFO - __main__ - Step 860 Global step 860 Train loss 1.97 on epoch=429
06/19/2022 21:11:52 - INFO - __main__ - Step 870 Global step 870 Train loss 1.89 on epoch=434
06/19/2022 21:11:53 - INFO - __main__ - Step 880 Global step 880 Train loss 1.80 on epoch=439
06/19/2022 21:11:54 - INFO - __main__ - Step 890 Global step 890 Train loss 1.77 on epoch=444
06/19/2022 21:11:56 - INFO - __main__ - Step 900 Global step 900 Train loss 1.78 on epoch=449
06/19/2022 21:11:57 - INFO - __main__ - Global step 900 Train loss 1.84 ACC 0.5 on epoch=449
06/19/2022 21:11:58 - INFO - __main__ - Step 910 Global step 910 Train loss 1.59 on epoch=454
06/19/2022 21:11:59 - INFO - __main__ - Step 920 Global step 920 Train loss 1.72 on epoch=459
06/19/2022 21:12:01 - INFO - __main__ - Step 930 Global step 930 Train loss 1.68 on epoch=464
06/19/2022 21:12:02 - INFO - __main__ - Step 940 Global step 940 Train loss 1.64 on epoch=469
06/19/2022 21:12:03 - INFO - __main__ - Step 950 Global step 950 Train loss 1.58 on epoch=474
06/19/2022 21:12:05 - INFO - __main__ - Global step 950 Train loss 1.64 ACC 0.5 on epoch=474
06/19/2022 21:12:06 - INFO - __main__ - Step 960 Global step 960 Train loss 1.44 on epoch=479
06/19/2022 21:12:07 - INFO - __main__ - Step 970 Global step 970 Train loss 1.46 on epoch=484
06/19/2022 21:12:08 - INFO - __main__ - Step 980 Global step 980 Train loss 1.39 on epoch=489
06/19/2022 21:12:09 - INFO - __main__ - Step 990 Global step 990 Train loss 1.26 on epoch=494
06/19/2022 21:12:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.32 on epoch=499
06/19/2022 21:12:13 - INFO - __main__ - Global step 1000 Train loss 1.37 ACC 0.5 on epoch=499
06/19/2022 21:12:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.39 on epoch=504
06/19/2022 21:12:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.32 on epoch=509
06/19/2022 21:12:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.26 on epoch=514
06/19/2022 21:12:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.22 on epoch=519
06/19/2022 21:12:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.11 on epoch=524
06/19/2022 21:12:20 - INFO - __main__ - Global step 1050 Train loss 1.26 ACC 0.5 on epoch=524
06/19/2022 21:12:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.23 on epoch=529
06/19/2022 21:12:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.11 on epoch=534
06/19/2022 21:12:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.07 on epoch=539
06/19/2022 21:12:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.12 on epoch=544
06/19/2022 21:12:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.05 on epoch=549
06/19/2022 21:12:28 - INFO - __main__ - Global step 1100 Train loss 1.12 ACC 0.46875 on epoch=549
06/19/2022 21:12:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.97 on epoch=554
06/19/2022 21:12:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.05 on epoch=559
06/19/2022 21:12:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.96 on epoch=564
06/19/2022 21:12:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.91 on epoch=569
06/19/2022 21:12:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.92 on epoch=574
06/19/2022 21:12:38 - INFO - __main__ - Global step 1150 Train loss 0.96 ACC 0.46875 on epoch=574
06/19/2022 21:12:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.82 on epoch=579
06/19/2022 21:12:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=584
06/19/2022 21:12:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.81 on epoch=589
06/19/2022 21:12:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.75 on epoch=594
06/19/2022 21:12:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.77 on epoch=599
06/19/2022 21:12:46 - INFO - __main__ - Global step 1200 Train loss 0.80 ACC 0.5 on epoch=599
06/19/2022 21:12:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.78 on epoch=604
06/19/2022 21:12:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=609
06/19/2022 21:12:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.84 on epoch=614
06/19/2022 21:12:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=619
06/19/2022 21:12:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.73 on epoch=624
06/19/2022 21:12:53 - INFO - __main__ - Global step 1250 Train loss 0.79 ACC 0.5 on epoch=624
06/19/2022 21:12:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.76 on epoch=629
06/19/2022 21:12:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.77 on epoch=634
06/19/2022 21:12:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.72 on epoch=639
06/19/2022 21:12:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.70 on epoch=644
06/19/2022 21:12:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.56 on epoch=649
06/19/2022 21:12:59 - INFO - __main__ - Global step 1300 Train loss 0.70 ACC 0.5 on epoch=649
06/19/2022 21:13:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.64 on epoch=654
06/19/2022 21:13:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.66 on epoch=659
06/19/2022 21:13:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.56 on epoch=664
06/19/2022 21:13:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.62 on epoch=669
06/19/2022 21:13:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.65 on epoch=674
06/19/2022 21:13:06 - INFO - __main__ - Global step 1350 Train loss 0.63 ACC 0.5 on epoch=674
06/19/2022 21:13:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.61 on epoch=679
06/19/2022 21:13:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.58 on epoch=684
06/19/2022 21:13:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.52 on epoch=689
06/19/2022 21:13:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.59 on epoch=694
06/19/2022 21:13:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.61 on epoch=699
06/19/2022 21:13:13 - INFO - __main__ - Global step 1400 Train loss 0.58 ACC 0.5 on epoch=699
06/19/2022 21:13:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.57 on epoch=704
06/19/2022 21:13:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.55 on epoch=709
06/19/2022 21:13:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.50 on epoch=714
06/19/2022 21:13:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.55 on epoch=719
06/19/2022 21:13:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.60 on epoch=724
06/19/2022 21:13:20 - INFO - __main__ - Global step 1450 Train loss 0.55 ACC 0.46875 on epoch=724
06/19/2022 21:13:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.55 on epoch=729
06/19/2022 21:13:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=734
06/19/2022 21:13:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
06/19/2022 21:13:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.52 on epoch=744
06/19/2022 21:13:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.53 on epoch=749
06/19/2022 21:13:26 - INFO - __main__ - Global step 1500 Train loss 0.53 ACC 0.5 on epoch=749
06/19/2022 21:13:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.48 on epoch=754
06/19/2022 21:13:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=759
06/19/2022 21:13:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
06/19/2022 21:13:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.48 on epoch=769
06/19/2022 21:13:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.51 on epoch=774
06/19/2022 21:13:33 - INFO - __main__ - Global step 1550 Train loss 0.47 ACC 0.46875 on epoch=774
06/19/2022 21:13:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.50 on epoch=779
06/19/2022 21:13:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=784
06/19/2022 21:13:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
06/19/2022 21:13:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=794
06/19/2022 21:13:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=799
06/19/2022 21:13:40 - INFO - __main__ - Global step 1600 Train loss 0.49 ACC 0.46875 on epoch=799
06/19/2022 21:13:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.47 on epoch=804
06/19/2022 21:13:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.54 on epoch=809
06/19/2022 21:13:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
06/19/2022 21:13:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=819
06/19/2022 21:13:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=824
06/19/2022 21:13:47 - INFO - __main__ - Global step 1650 Train loss 0.49 ACC 0.40625 on epoch=824
06/19/2022 21:13:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=829
06/19/2022 21:13:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=834
06/19/2022 21:13:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
06/19/2022 21:13:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=844
06/19/2022 21:13:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
06/19/2022 21:13:54 - INFO - __main__ - Global step 1700 Train loss 0.40 ACC 0.5 on epoch=849
06/19/2022 21:13:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=854
06/19/2022 21:13:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=859
06/19/2022 21:13:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.48 on epoch=864
06/19/2022 21:13:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=869
06/19/2022 21:14:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
06/19/2022 21:14:00 - INFO - __main__ - Global step 1750 Train loss 0.43 ACC 0.5 on epoch=874
06/19/2022 21:14:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=879
06/19/2022 21:14:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=884
06/19/2022 21:14:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=889
06/19/2022 21:14:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=894
06/19/2022 21:14:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=899
06/19/2022 21:14:07 - INFO - __main__ - Global step 1800 Train loss 0.41 ACC 0.53125 on epoch=899
06/19/2022 21:14:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
06/19/2022 21:14:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
06/19/2022 21:14:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=914
06/19/2022 21:14:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=919
06/19/2022 21:14:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=924
06/19/2022 21:14:14 - INFO - __main__ - Global step 1850 Train loss 0.39 ACC 0.5 on epoch=924
06/19/2022 21:14:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
06/19/2022 21:14:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=934
06/19/2022 21:14:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=939
06/19/2022 21:14:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=944
06/19/2022 21:14:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=949
06/19/2022 21:14:21 - INFO - __main__ - Global step 1900 Train loss 0.39 ACC 0.5 on epoch=949
06/19/2022 21:14:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
06/19/2022 21:14:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
06/19/2022 21:14:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
06/19/2022 21:14:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=969
06/19/2022 21:14:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=974
06/19/2022 21:14:27 - INFO - __main__ - Global step 1950 Train loss 0.38 ACC 0.5 on epoch=974
06/19/2022 21:14:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=979
06/19/2022 21:14:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=984
06/19/2022 21:14:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
06/19/2022 21:14:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=994
06/19/2022 21:14:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=999
06/19/2022 21:14:34 - INFO - __main__ - Global step 2000 Train loss 0.40 ACC 0.5 on epoch=999
06/19/2022 21:14:34 - INFO - __main__ - save last model!
06/19/2022 21:14:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:14:34 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 21:14:34 - INFO - __main__ - Printing 3 examples
06/19/2022 21:14:34 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 21:14:34 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:34 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 21:14:34 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:14:34 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 21:14:34 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:14:34 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:14:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:14:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:14:35 - INFO - __main__ - Printing 3 examples
06/19/2022 21:14:35 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 21:14:35 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:35 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 21:14:35 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:35 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 21:14:35 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:14:35 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:14:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:14:35 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:14:35 - INFO - __main__ - Printing 3 examples
06/19/2022 21:14:35 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 21:14:35 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:35 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 21:14:35 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:35 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 21:14:35 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:35 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:14:35 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:14:35 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:14:35 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 21:14:41 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:14:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:14:41 - INFO - __main__ - Starting training!
06/19/2022 21:14:43 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_87_0.5_8_predictions.txt
06/19/2022 21:14:43 - INFO - __main__ - ACC on test data: 0.6789
06/19/2022 21:14:43 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.5, bsz=8, dev_performance=0.5625, test_performance=0.678921568627451
06/19/2022 21:14:43 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.4, bsz=8 ...
06/19/2022 21:14:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:14:44 - INFO - __main__ - Printing 3 examples
06/19/2022 21:14:44 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 21:14:44 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:44 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 21:14:44 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:44 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 21:14:44 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:14:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:14:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:14:44 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:14:44 - INFO - __main__ - Printing 3 examples
06/19/2022 21:14:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 21:14:44 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:44 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 21:14:44 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:44 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 21:14:44 - INFO - __main__ - ['equivalent']
06/19/2022 21:14:44 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:14:44 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:14:44 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:14:50 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:14:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:14:50 - INFO - __main__ - Starting training!
06/19/2022 21:14:52 - INFO - __main__ - Step 10 Global step 10 Train loss 6.80 on epoch=4
06/19/2022 21:14:53 - INFO - __main__ - Step 20 Global step 20 Train loss 6.81 on epoch=9
06/19/2022 21:14:54 - INFO - __main__ - Step 30 Global step 30 Train loss 6.77 on epoch=14
06/19/2022 21:14:55 - INFO - __main__ - Step 40 Global step 40 Train loss 6.75 on epoch=19
06/19/2022 21:14:56 - INFO - __main__ - Step 50 Global step 50 Train loss 6.78 on epoch=24
06/19/2022 21:15:01 - INFO - __main__ - Global step 50 Train loss 6.78 ACC 0.0 on epoch=24
06/19/2022 21:15:01 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:15:02 - INFO - __main__ - Step 60 Global step 60 Train loss 6.76 on epoch=29
06/19/2022 21:15:03 - INFO - __main__ - Step 70 Global step 70 Train loss 6.76 on epoch=34
06/19/2022 21:15:05 - INFO - __main__ - Step 80 Global step 80 Train loss 6.74 on epoch=39
06/19/2022 21:15:06 - INFO - __main__ - Step 90 Global step 90 Train loss 6.73 on epoch=44
06/19/2022 21:15:07 - INFO - __main__ - Step 100 Global step 100 Train loss 6.79 on epoch=49
06/19/2022 21:15:11 - INFO - __main__ - Global step 100 Train loss 6.76 ACC 0.0 on epoch=49
06/19/2022 21:15:12 - INFO - __main__ - Step 110 Global step 110 Train loss 6.65 on epoch=54
06/19/2022 21:15:14 - INFO - __main__ - Step 120 Global step 120 Train loss 6.69 on epoch=59
06/19/2022 21:15:15 - INFO - __main__ - Step 130 Global step 130 Train loss 6.63 on epoch=64
06/19/2022 21:15:16 - INFO - __main__ - Step 140 Global step 140 Train loss 6.66 on epoch=69
06/19/2022 21:15:17 - INFO - __main__ - Step 150 Global step 150 Train loss 6.62 on epoch=74
06/19/2022 21:15:21 - INFO - __main__ - Global step 150 Train loss 6.65 ACC 0.0 on epoch=74
06/19/2022 21:15:22 - INFO - __main__ - Step 160 Global step 160 Train loss 6.59 on epoch=79
06/19/2022 21:15:24 - INFO - __main__ - Step 170 Global step 170 Train loss 6.52 on epoch=84
06/19/2022 21:15:25 - INFO - __main__ - Step 180 Global step 180 Train loss 6.51 on epoch=89
06/19/2022 21:15:26 - INFO - __main__ - Step 190 Global step 190 Train loss 6.42 on epoch=94
06/19/2022 21:15:27 - INFO - __main__ - Step 200 Global step 200 Train loss 6.45 on epoch=99
06/19/2022 21:15:34 - INFO - __main__ - Global step 200 Train loss 6.50 ACC 0.0 on epoch=99
06/19/2022 21:15:35 - INFO - __main__ - Step 210 Global step 210 Train loss 6.38 on epoch=104
06/19/2022 21:15:36 - INFO - __main__ - Step 220 Global step 220 Train loss 6.25 on epoch=109
06/19/2022 21:15:38 - INFO - __main__ - Step 230 Global step 230 Train loss 6.22 on epoch=114
06/19/2022 21:15:39 - INFO - __main__ - Step 240 Global step 240 Train loss 6.07 on epoch=119
06/19/2022 21:15:40 - INFO - __main__ - Step 250 Global step 250 Train loss 5.97 on epoch=124
06/19/2022 21:15:42 - INFO - __main__ - Global step 250 Train loss 6.18 ACC 0.0 on epoch=124
06/19/2022 21:15:44 - INFO - __main__ - Step 260 Global step 260 Train loss 6.00 on epoch=129
06/19/2022 21:15:45 - INFO - __main__ - Step 270 Global step 270 Train loss 5.84 on epoch=134
06/19/2022 21:15:46 - INFO - __main__ - Step 280 Global step 280 Train loss 5.77 on epoch=139
06/19/2022 21:15:47 - INFO - __main__ - Step 290 Global step 290 Train loss 5.63 on epoch=144
06/19/2022 21:15:49 - INFO - __main__ - Step 300 Global step 300 Train loss 5.63 on epoch=149
06/19/2022 21:15:55 - INFO - __main__ - Global step 300 Train loss 5.77 ACC 0.0 on epoch=149
06/19/2022 21:15:56 - INFO - __main__ - Step 310 Global step 310 Train loss 5.52 on epoch=154
06/19/2022 21:15:57 - INFO - __main__ - Step 320 Global step 320 Train loss 5.37 on epoch=159
06/19/2022 21:15:58 - INFO - __main__ - Step 330 Global step 330 Train loss 5.20 on epoch=164
06/19/2022 21:16:00 - INFO - __main__ - Step 340 Global step 340 Train loss 5.19 on epoch=169
06/19/2022 21:16:01 - INFO - __main__ - Step 350 Global step 350 Train loss 5.33 on epoch=174
06/19/2022 21:16:03 - INFO - __main__ - Global step 350 Train loss 5.32 ACC 0.0 on epoch=174
06/19/2022 21:16:05 - INFO - __main__ - Step 360 Global step 360 Train loss 5.08 on epoch=179
06/19/2022 21:16:06 - INFO - __main__ - Step 370 Global step 370 Train loss 5.02 on epoch=184
06/19/2022 21:16:07 - INFO - __main__ - Step 380 Global step 380 Train loss 4.92 on epoch=189
06/19/2022 21:16:08 - INFO - __main__ - Step 390 Global step 390 Train loss 4.92 on epoch=194
06/19/2022 21:16:10 - INFO - __main__ - Step 400 Global step 400 Train loss 4.86 on epoch=199
06/19/2022 21:16:11 - INFO - __main__ - Global step 400 Train loss 4.96 ACC 0.0 on epoch=199
06/19/2022 21:16:12 - INFO - __main__ - Step 410 Global step 410 Train loss 4.84 on epoch=204
06/19/2022 21:16:13 - INFO - __main__ - Step 420 Global step 420 Train loss 4.74 on epoch=209
06/19/2022 21:16:15 - INFO - __main__ - Step 430 Global step 430 Train loss 4.75 on epoch=214
06/19/2022 21:16:16 - INFO - __main__ - Step 440 Global step 440 Train loss 4.64 on epoch=219
06/19/2022 21:16:17 - INFO - __main__ - Step 450 Global step 450 Train loss 4.66 on epoch=224
06/19/2022 21:16:19 - INFO - __main__ - Global step 450 Train loss 4.72 ACC 0.0 on epoch=224
06/19/2022 21:16:20 - INFO - __main__ - Step 460 Global step 460 Train loss 4.79 on epoch=229
06/19/2022 21:16:21 - INFO - __main__ - Step 470 Global step 470 Train loss 4.66 on epoch=234
06/19/2022 21:16:23 - INFO - __main__ - Step 480 Global step 480 Train loss 4.62 on epoch=239
06/19/2022 21:16:24 - INFO - __main__ - Step 490 Global step 490 Train loss 4.56 on epoch=244
06/19/2022 21:16:25 - INFO - __main__ - Step 500 Global step 500 Train loss 4.51 on epoch=249
06/19/2022 21:16:29 - INFO - __main__ - Global step 500 Train loss 4.63 ACC 0.0 on epoch=249
06/19/2022 21:16:30 - INFO - __main__ - Step 510 Global step 510 Train loss 4.49 on epoch=254
06/19/2022 21:16:31 - INFO - __main__ - Step 520 Global step 520 Train loss 4.46 on epoch=259
06/19/2022 21:16:33 - INFO - __main__ - Step 530 Global step 530 Train loss 4.26 on epoch=264
06/19/2022 21:16:34 - INFO - __main__ - Step 540 Global step 540 Train loss 4.30 on epoch=269
06/19/2022 21:16:35 - INFO - __main__ - Step 550 Global step 550 Train loss 4.40 on epoch=274
06/19/2022 21:16:38 - INFO - __main__ - Global step 550 Train loss 4.38 ACC 0.0 on epoch=274
06/19/2022 21:16:39 - INFO - __main__ - Step 560 Global step 560 Train loss 4.36 on epoch=279
06/19/2022 21:16:40 - INFO - __main__ - Step 570 Global step 570 Train loss 4.11 on epoch=284
06/19/2022 21:16:41 - INFO - __main__ - Step 580 Global step 580 Train loss 4.23 on epoch=289
06/19/2022 21:16:43 - INFO - __main__ - Step 590 Global step 590 Train loss 4.17 on epoch=294
06/19/2022 21:16:44 - INFO - __main__ - Step 600 Global step 600 Train loss 4.02 on epoch=299
06/19/2022 21:16:51 - INFO - __main__ - Global step 600 Train loss 4.18 ACC 0.0 on epoch=299
06/19/2022 21:16:52 - INFO - __main__ - Step 610 Global step 610 Train loss 4.01 on epoch=304
06/19/2022 21:16:53 - INFO - __main__ - Step 620 Global step 620 Train loss 4.11 on epoch=309
06/19/2022 21:16:55 - INFO - __main__ - Step 630 Global step 630 Train loss 4.09 on epoch=314
06/19/2022 21:16:56 - INFO - __main__ - Step 640 Global step 640 Train loss 3.92 on epoch=319
06/19/2022 21:16:57 - INFO - __main__ - Step 650 Global step 650 Train loss 3.96 on epoch=324
06/19/2022 21:17:00 - INFO - __main__ - Global step 650 Train loss 4.02 ACC 0.0 on epoch=324
06/19/2022 21:17:02 - INFO - __main__ - Step 660 Global step 660 Train loss 3.94 on epoch=329
06/19/2022 21:17:03 - INFO - __main__ - Step 670 Global step 670 Train loss 3.83 on epoch=334
06/19/2022 21:17:04 - INFO - __main__ - Step 680 Global step 680 Train loss 3.81 on epoch=339
06/19/2022 21:17:06 - INFO - __main__ - Step 690 Global step 690 Train loss 3.88 on epoch=344
06/19/2022 21:17:07 - INFO - __main__ - Step 700 Global step 700 Train loss 3.67 on epoch=349
06/19/2022 21:17:10 - INFO - __main__ - Global step 700 Train loss 3.83 ACC 0.0 on epoch=349
06/19/2022 21:17:11 - INFO - __main__ - Step 710 Global step 710 Train loss 3.62 on epoch=354
06/19/2022 21:17:13 - INFO - __main__ - Step 720 Global step 720 Train loss 3.51 on epoch=359
06/19/2022 21:17:14 - INFO - __main__ - Step 730 Global step 730 Train loss 3.56 on epoch=364
06/19/2022 21:17:15 - INFO - __main__ - Step 740 Global step 740 Train loss 3.37 on epoch=369
06/19/2022 21:17:17 - INFO - __main__ - Step 750 Global step 750 Train loss 3.33 on epoch=374
06/19/2022 21:17:27 - INFO - __main__ - Global step 750 Train loss 3.48 ACC 0.0625 on epoch=374
06/19/2022 21:17:27 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=374, global_step=750
06/19/2022 21:17:29 - INFO - __main__ - Step 760 Global step 760 Train loss 3.30 on epoch=379
06/19/2022 21:17:30 - INFO - __main__ - Step 770 Global step 770 Train loss 3.20 on epoch=384
06/19/2022 21:17:31 - INFO - __main__ - Step 780 Global step 780 Train loss 3.12 on epoch=389
06/19/2022 21:17:33 - INFO - __main__ - Step 790 Global step 790 Train loss 3.13 on epoch=394
06/19/2022 21:17:34 - INFO - __main__ - Step 800 Global step 800 Train loss 3.10 on epoch=399
06/19/2022 21:17:40 - INFO - __main__ - Global step 800 Train loss 3.17 ACC 0.1875 on epoch=399
06/19/2022 21:17:40 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.1875 on epoch=399, global_step=800
06/19/2022 21:17:42 - INFO - __main__ - Step 810 Global step 810 Train loss 3.03 on epoch=404
06/19/2022 21:17:43 - INFO - __main__ - Step 820 Global step 820 Train loss 2.92 on epoch=409
06/19/2022 21:17:44 - INFO - __main__ - Step 830 Global step 830 Train loss 2.81 on epoch=414
06/19/2022 21:17:46 - INFO - __main__ - Step 840 Global step 840 Train loss 2.83 on epoch=419
06/19/2022 21:17:47 - INFO - __main__ - Step 850 Global step 850 Train loss 2.73 on epoch=424
06/19/2022 21:17:56 - INFO - __main__ - Global step 850 Train loss 2.86 ACC 0.40625 on epoch=424
06/19/2022 21:17:56 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.40625 on epoch=424, global_step=850
06/19/2022 21:17:57 - INFO - __main__ - Step 860 Global step 860 Train loss 2.79 on epoch=429
06/19/2022 21:17:58 - INFO - __main__ - Step 870 Global step 870 Train loss 2.63 on epoch=434
06/19/2022 21:18:00 - INFO - __main__ - Step 880 Global step 880 Train loss 2.65 on epoch=439
06/19/2022 21:18:01 - INFO - __main__ - Step 890 Global step 890 Train loss 2.60 on epoch=444
06/19/2022 21:18:02 - INFO - __main__ - Step 900 Global step 900 Train loss 2.68 on epoch=449
06/19/2022 21:18:12 - INFO - __main__ - Global step 900 Train loss 2.67 ACC 0.34375 on epoch=449
06/19/2022 21:18:14 - INFO - __main__ - Step 910 Global step 910 Train loss 2.69 on epoch=454
06/19/2022 21:18:15 - INFO - __main__ - Step 920 Global step 920 Train loss 2.57 on epoch=459
06/19/2022 21:18:16 - INFO - __main__ - Step 930 Global step 930 Train loss 2.60 on epoch=464
06/19/2022 21:18:18 - INFO - __main__ - Step 940 Global step 940 Train loss 2.57 on epoch=469
06/19/2022 21:18:19 - INFO - __main__ - Step 950 Global step 950 Train loss 2.47 on epoch=474
06/19/2022 21:18:25 - INFO - __main__ - Global step 950 Train loss 2.58 ACC 0.46875 on epoch=474
06/19/2022 21:18:25 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=474, global_step=950
06/19/2022 21:18:26 - INFO - __main__ - Step 960 Global step 960 Train loss 2.46 on epoch=479
06/19/2022 21:18:27 - INFO - __main__ - Step 970 Global step 970 Train loss 2.43 on epoch=484
06/19/2022 21:18:29 - INFO - __main__ - Step 980 Global step 980 Train loss 2.47 on epoch=489
06/19/2022 21:18:30 - INFO - __main__ - Step 990 Global step 990 Train loss 2.49 on epoch=494
06/19/2022 21:18:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.38 on epoch=499
06/19/2022 21:18:32 - INFO - __main__ - Global step 1000 Train loss 2.44 ACC 0.5 on epoch=499
06/19/2022 21:18:32 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=499, global_step=1000
06/19/2022 21:18:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.26 on epoch=504
06/19/2022 21:18:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.36 on epoch=509
06/19/2022 21:18:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.16 on epoch=514
06/19/2022 21:18:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.28 on epoch=519
06/19/2022 21:18:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.26 on epoch=524
06/19/2022 21:18:40 - INFO - __main__ - Global step 1050 Train loss 2.27 ACC 0.53125 on epoch=524
06/19/2022 21:18:40 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=524, global_step=1050
06/19/2022 21:18:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.17 on epoch=529
06/19/2022 21:18:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.13 on epoch=534
06/19/2022 21:18:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.17 on epoch=539
06/19/2022 21:18:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.10 on epoch=544
06/19/2022 21:18:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.99 on epoch=549
06/19/2022 21:18:47 - INFO - __main__ - Global step 1100 Train loss 2.11 ACC 0.5 on epoch=549
06/19/2022 21:18:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.93 on epoch=554
06/19/2022 21:18:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.80 on epoch=559
06/19/2022 21:18:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.87 on epoch=564
06/19/2022 21:18:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.95 on epoch=569
06/19/2022 21:18:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.95 on epoch=574
06/19/2022 21:18:54 - INFO - __main__ - Global step 1150 Train loss 1.90 ACC 0.46875 on epoch=574
06/19/2022 21:18:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.76 on epoch=579
06/19/2022 21:18:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.72 on epoch=584
06/19/2022 21:18:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.79 on epoch=589
06/19/2022 21:18:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.76 on epoch=594
06/19/2022 21:19:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.69 on epoch=599
06/19/2022 21:19:01 - INFO - __main__ - Global step 1200 Train loss 1.75 ACC 0.5 on epoch=599
06/19/2022 21:19:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.68 on epoch=604
06/19/2022 21:19:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.56 on epoch=609
06/19/2022 21:19:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.63 on epoch=614
06/19/2022 21:19:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.63 on epoch=619
06/19/2022 21:19:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.55 on epoch=624
06/19/2022 21:19:07 - INFO - __main__ - Global step 1250 Train loss 1.61 ACC 0.5 on epoch=624
06/19/2022 21:19:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.58 on epoch=629
06/19/2022 21:19:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.64 on epoch=634
06/19/2022 21:19:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.61 on epoch=639
06/19/2022 21:19:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.59 on epoch=644
06/19/2022 21:19:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.42 on epoch=649
06/19/2022 21:19:14 - INFO - __main__ - Global step 1300 Train loss 1.57 ACC 0.5 on epoch=649
06/19/2022 21:19:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.41 on epoch=654
06/19/2022 21:19:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.46 on epoch=659
06/19/2022 21:19:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.53 on epoch=664
06/19/2022 21:19:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.38 on epoch=669
06/19/2022 21:19:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.45 on epoch=674
06/19/2022 21:19:21 - INFO - __main__ - Global step 1350 Train loss 1.44 ACC 0.5 on epoch=674
06/19/2022 21:19:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.34 on epoch=679
06/19/2022 21:19:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.25 on epoch=684
06/19/2022 21:19:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.48 on epoch=689
06/19/2022 21:19:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.31 on epoch=694
06/19/2022 21:19:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.21 on epoch=699
06/19/2022 21:19:31 - INFO - __main__ - Global step 1400 Train loss 1.32 ACC 0.5 on epoch=699
06/19/2022 21:19:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.26 on epoch=704
06/19/2022 21:19:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.28 on epoch=709
06/19/2022 21:19:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.25 on epoch=714
06/19/2022 21:19:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.35 on epoch=719
06/19/2022 21:19:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.25 on epoch=724
06/19/2022 21:19:43 - INFO - __main__ - Global step 1450 Train loss 1.28 ACC 0.5 on epoch=724
06/19/2022 21:19:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.32 on epoch=729
06/19/2022 21:19:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.30 on epoch=734
06/19/2022 21:19:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.08 on epoch=739
06/19/2022 21:19:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.14 on epoch=744
06/19/2022 21:19:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.01 on epoch=749
06/19/2022 21:19:50 - INFO - __main__ - Global step 1500 Train loss 1.17 ACC 0.5 on epoch=749
06/19/2022 21:19:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.16 on epoch=754
06/19/2022 21:19:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.10 on epoch=759
06/19/2022 21:19:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.03 on epoch=764
06/19/2022 21:19:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.98 on epoch=769
06/19/2022 21:19:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.92 on epoch=774
06/19/2022 21:19:57 - INFO - __main__ - Global step 1550 Train loss 1.04 ACC 0.5 on epoch=774
06/19/2022 21:19:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.94 on epoch=779
06/19/2022 21:19:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.08 on epoch=784
06/19/2022 21:20:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.90 on epoch=789
06/19/2022 21:20:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.87 on epoch=794
06/19/2022 21:20:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.98 on epoch=799
06/19/2022 21:20:03 - INFO - __main__ - Global step 1600 Train loss 0.96 ACC 0.5 on epoch=799
06/19/2022 21:20:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.98 on epoch=804
06/19/2022 21:20:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.84 on epoch=809
06/19/2022 21:20:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.83 on epoch=814
06/19/2022 21:20:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.85 on epoch=819
06/19/2022 21:20:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.77 on epoch=824
06/19/2022 21:20:10 - INFO - __main__ - Global step 1650 Train loss 0.85 ACC 0.5 on epoch=824
06/19/2022 21:20:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.93 on epoch=829
06/19/2022 21:20:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.83 on epoch=834
06/19/2022 21:20:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.83 on epoch=839
06/19/2022 21:20:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.77 on epoch=844
06/19/2022 21:20:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.80 on epoch=849
06/19/2022 21:20:21 - INFO - __main__ - Global step 1700 Train loss 0.83 ACC 0.5 on epoch=849
06/19/2022 21:20:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.84 on epoch=854
06/19/2022 21:20:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.83 on epoch=859
06/19/2022 21:20:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.75 on epoch=864
06/19/2022 21:20:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.72 on epoch=869
06/19/2022 21:20:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.79 on epoch=874
06/19/2022 21:20:28 - INFO - __main__ - Global step 1750 Train loss 0.79 ACC 0.40625 on epoch=874
06/19/2022 21:20:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.73 on epoch=879
06/19/2022 21:20:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.76 on epoch=884
06/19/2022 21:20:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.74 on epoch=889
06/19/2022 21:20:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.75 on epoch=894
06/19/2022 21:20:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.72 on epoch=899
06/19/2022 21:20:35 - INFO - __main__ - Global step 1800 Train loss 0.74 ACC 0.5 on epoch=899
06/19/2022 21:20:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.72 on epoch=904
06/19/2022 21:20:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.69 on epoch=909
06/19/2022 21:20:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.66 on epoch=914
06/19/2022 21:20:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.75 on epoch=919
06/19/2022 21:20:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.70 on epoch=924
06/19/2022 21:20:42 - INFO - __main__ - Global step 1850 Train loss 0.71 ACC 0.5 on epoch=924
06/19/2022 21:20:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.68 on epoch=929
06/19/2022 21:20:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.76 on epoch=934
06/19/2022 21:20:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.64 on epoch=939
06/19/2022 21:20:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.58 on epoch=944
06/19/2022 21:20:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.61 on epoch=949
06/19/2022 21:20:49 - INFO - __main__ - Global step 1900 Train loss 0.66 ACC 0.5 on epoch=949
06/19/2022 21:20:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.65 on epoch=954
06/19/2022 21:20:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.68 on epoch=959
06/19/2022 21:20:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.65 on epoch=964
06/19/2022 21:20:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.64 on epoch=969
06/19/2022 21:20:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.64 on epoch=974
06/19/2022 21:20:56 - INFO - __main__ - Global step 1950 Train loss 0.65 ACC 0.5 on epoch=974
06/19/2022 21:20:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.60 on epoch=979
06/19/2022 21:20:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.64 on epoch=984
06/19/2022 21:21:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.65 on epoch=989
06/19/2022 21:21:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.57 on epoch=994
06/19/2022 21:21:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.58 on epoch=999
06/19/2022 21:21:03 - INFO - __main__ - Global step 2000 Train loss 0.61 ACC 0.5 on epoch=999
06/19/2022 21:21:03 - INFO - __main__ - save last model!
06/19/2022 21:21:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:21:03 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 21:21:03 - INFO - __main__ - Printing 3 examples
06/19/2022 21:21:03 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 21:21:03 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 21:21:03 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:21:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 21:21:03 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:21:03 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:21:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:21:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:21:03 - INFO - __main__ - Printing 3 examples
06/19/2022 21:21:03 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 21:21:03 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:03 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 21:21:03 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:03 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 21:21:03 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:21:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:21:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:21:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:21:03 - INFO - __main__ - Printing 3 examples
06/19/2022 21:21:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 21:21:03 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:03 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 21:21:03 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:03 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 21:21:03 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:03 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:21:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:21:03 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:21:03 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 21:21:09 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:21:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:21:09 - INFO - __main__ - Starting training!
06/19/2022 21:21:11 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_87_0.4_8_predictions.txt
06/19/2022 21:21:11 - INFO - __main__ - ACC on test data: 0.6838
06/19/2022 21:21:11 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.4, bsz=8, dev_performance=0.53125, test_performance=0.6838235294117647
06/19/2022 21:21:11 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.3, bsz=8 ...
06/19/2022 21:21:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:21:12 - INFO - __main__ - Printing 3 examples
06/19/2022 21:21:12 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 21:21:12 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:12 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 21:21:12 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:12 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 21:21:12 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:21:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:21:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:21:12 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:21:12 - INFO - __main__ - Printing 3 examples
06/19/2022 21:21:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 21:21:12 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:12 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 21:21:12 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:12 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 21:21:12 - INFO - __main__ - ['equivalent']
06/19/2022 21:21:12 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:21:12 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:21:12 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:21:18 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:21:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:21:19 - INFO - __main__ - Starting training!
06/19/2022 21:21:20 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/19/2022 21:21:21 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=9
06/19/2022 21:21:23 - INFO - __main__ - Step 30 Global step 30 Train loss 6.86 on epoch=14
06/19/2022 21:21:24 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/19/2022 21:21:25 - INFO - __main__ - Step 50 Global step 50 Train loss 6.84 on epoch=24
06/19/2022 21:21:27 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/19/2022 21:21:27 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:21:28 - INFO - __main__ - Step 60 Global step 60 Train loss 6.87 on epoch=29
06/19/2022 21:21:29 - INFO - __main__ - Step 70 Global step 70 Train loss 6.79 on epoch=34
06/19/2022 21:21:30 - INFO - __main__ - Step 80 Global step 80 Train loss 6.78 on epoch=39
06/19/2022 21:21:32 - INFO - __main__ - Step 90 Global step 90 Train loss 6.72 on epoch=44
06/19/2022 21:21:33 - INFO - __main__ - Step 100 Global step 100 Train loss 6.69 on epoch=49
06/19/2022 21:21:35 - INFO - __main__ - Global step 100 Train loss 6.77 ACC 0.0 on epoch=49
06/19/2022 21:21:36 - INFO - __main__ - Step 110 Global step 110 Train loss 6.74 on epoch=54
06/19/2022 21:21:38 - INFO - __main__ - Step 120 Global step 120 Train loss 6.70 on epoch=59
06/19/2022 21:21:39 - INFO - __main__ - Step 130 Global step 130 Train loss 6.72 on epoch=64
06/19/2022 21:21:40 - INFO - __main__ - Step 140 Global step 140 Train loss 6.75 on epoch=69
06/19/2022 21:21:41 - INFO - __main__ - Step 150 Global step 150 Train loss 6.78 on epoch=74
06/19/2022 21:21:43 - INFO - __main__ - Global step 150 Train loss 6.74 ACC 0.0 on epoch=74
06/19/2022 21:21:45 - INFO - __main__ - Step 160 Global step 160 Train loss 6.62 on epoch=79
06/19/2022 21:21:46 - INFO - __main__ - Step 170 Global step 170 Train loss 6.73 on epoch=84
06/19/2022 21:21:47 - INFO - __main__ - Step 180 Global step 180 Train loss 6.62 on epoch=89
06/19/2022 21:21:49 - INFO - __main__ - Step 190 Global step 190 Train loss 6.59 on epoch=94
06/19/2022 21:21:50 - INFO - __main__ - Step 200 Global step 200 Train loss 6.57 on epoch=99
06/19/2022 21:22:00 - INFO - __main__ - Global step 200 Train loss 6.63 ACC 0.0 on epoch=99
06/19/2022 21:22:01 - INFO - __main__ - Step 210 Global step 210 Train loss 6.54 on epoch=104
06/19/2022 21:22:02 - INFO - __main__ - Step 220 Global step 220 Train loss 6.57 on epoch=109
06/19/2022 21:22:04 - INFO - __main__ - Step 230 Global step 230 Train loss 6.48 on epoch=114
06/19/2022 21:22:05 - INFO - __main__ - Step 240 Global step 240 Train loss 6.51 on epoch=119
06/19/2022 21:22:06 - INFO - __main__ - Step 250 Global step 250 Train loss 6.36 on epoch=124
06/19/2022 21:22:15 - INFO - __main__ - Global step 250 Train loss 6.49 ACC 0.0 on epoch=124
06/19/2022 21:22:16 - INFO - __main__ - Step 260 Global step 260 Train loss 6.32 on epoch=129
06/19/2022 21:22:18 - INFO - __main__ - Step 270 Global step 270 Train loss 6.31 on epoch=134
06/19/2022 21:22:19 - INFO - __main__ - Step 280 Global step 280 Train loss 6.31 on epoch=139
06/19/2022 21:22:20 - INFO - __main__ - Step 290 Global step 290 Train loss 6.28 on epoch=144
06/19/2022 21:22:21 - INFO - __main__ - Step 300 Global step 300 Train loss 6.14 on epoch=149
06/19/2022 21:22:25 - INFO - __main__ - Global step 300 Train loss 6.27 ACC 0.0 on epoch=149
06/19/2022 21:22:26 - INFO - __main__ - Step 310 Global step 310 Train loss 6.32 on epoch=154
06/19/2022 21:22:27 - INFO - __main__ - Step 320 Global step 320 Train loss 6.18 on epoch=159
06/19/2022 21:22:29 - INFO - __main__ - Step 330 Global step 330 Train loss 6.05 on epoch=164
06/19/2022 21:22:30 - INFO - __main__ - Step 340 Global step 340 Train loss 6.05 on epoch=169
06/19/2022 21:22:31 - INFO - __main__ - Step 350 Global step 350 Train loss 6.09 on epoch=174
06/19/2022 21:22:39 - INFO - __main__ - Global step 350 Train loss 6.14 ACC 0.0 on epoch=174
06/19/2022 21:22:40 - INFO - __main__ - Step 360 Global step 360 Train loss 6.07 on epoch=179
06/19/2022 21:22:41 - INFO - __main__ - Step 370 Global step 370 Train loss 6.03 on epoch=184
06/19/2022 21:22:42 - INFO - __main__ - Step 380 Global step 380 Train loss 5.98 on epoch=189
06/19/2022 21:22:44 - INFO - __main__ - Step 390 Global step 390 Train loss 5.87 on epoch=194
06/19/2022 21:22:45 - INFO - __main__ - Step 400 Global step 400 Train loss 5.92 on epoch=199
06/19/2022 21:22:52 - INFO - __main__ - Global step 400 Train loss 5.98 ACC 0.0 on epoch=199
06/19/2022 21:22:54 - INFO - __main__ - Step 410 Global step 410 Train loss 5.84 on epoch=204
06/19/2022 21:22:55 - INFO - __main__ - Step 420 Global step 420 Train loss 5.76 on epoch=209
06/19/2022 21:22:56 - INFO - __main__ - Step 430 Global step 430 Train loss 5.90 on epoch=214
06/19/2022 21:22:57 - INFO - __main__ - Step 440 Global step 440 Train loss 5.71 on epoch=219
06/19/2022 21:22:58 - INFO - __main__ - Step 450 Global step 450 Train loss 5.71 on epoch=224
06/19/2022 21:23:05 - INFO - __main__ - Global step 450 Train loss 5.78 ACC 0.0 on epoch=224
06/19/2022 21:23:06 - INFO - __main__ - Step 460 Global step 460 Train loss 5.57 on epoch=229
06/19/2022 21:23:07 - INFO - __main__ - Step 470 Global step 470 Train loss 5.48 on epoch=234
06/19/2022 21:23:09 - INFO - __main__ - Step 480 Global step 480 Train loss 5.50 on epoch=239
06/19/2022 21:23:10 - INFO - __main__ - Step 490 Global step 490 Train loss 5.49 on epoch=244
06/19/2022 21:23:11 - INFO - __main__ - Step 500 Global step 500 Train loss 5.37 on epoch=249
06/19/2022 21:23:12 - INFO - __main__ - Global step 500 Train loss 5.48 ACC 0.0 on epoch=249
06/19/2022 21:23:14 - INFO - __main__ - Step 510 Global step 510 Train loss 5.29 on epoch=254
06/19/2022 21:23:15 - INFO - __main__ - Step 520 Global step 520 Train loss 5.34 on epoch=259
06/19/2022 21:23:16 - INFO - __main__ - Step 530 Global step 530 Train loss 5.17 on epoch=264
06/19/2022 21:23:17 - INFO - __main__ - Step 540 Global step 540 Train loss 5.14 on epoch=269
06/19/2022 21:23:19 - INFO - __main__ - Step 550 Global step 550 Train loss 5.21 on epoch=274
06/19/2022 21:23:21 - INFO - __main__ - Global step 550 Train loss 5.23 ACC 0.0 on epoch=274
06/19/2022 21:23:22 - INFO - __main__ - Step 560 Global step 560 Train loss 5.14 on epoch=279
06/19/2022 21:23:23 - INFO - __main__ - Step 570 Global step 570 Train loss 5.02 on epoch=284
06/19/2022 21:23:24 - INFO - __main__ - Step 580 Global step 580 Train loss 4.96 on epoch=289
06/19/2022 21:23:26 - INFO - __main__ - Step 590 Global step 590 Train loss 4.98 on epoch=294
06/19/2022 21:23:27 - INFO - __main__ - Step 600 Global step 600 Train loss 5.07 on epoch=299
06/19/2022 21:23:29 - INFO - __main__ - Global step 600 Train loss 5.03 ACC 0.0 on epoch=299
06/19/2022 21:23:30 - INFO - __main__ - Step 610 Global step 610 Train loss 4.87 on epoch=304
06/19/2022 21:23:31 - INFO - __main__ - Step 620 Global step 620 Train loss 4.96 on epoch=309
06/19/2022 21:23:32 - INFO - __main__ - Step 630 Global step 630 Train loss 4.83 on epoch=314
06/19/2022 21:23:34 - INFO - __main__ - Step 640 Global step 640 Train loss 4.88 on epoch=319
06/19/2022 21:23:35 - INFO - __main__ - Step 650 Global step 650 Train loss 4.77 on epoch=324
06/19/2022 21:23:38 - INFO - __main__ - Global step 650 Train loss 4.86 ACC 0.0 on epoch=324
06/19/2022 21:23:39 - INFO - __main__ - Step 660 Global step 660 Train loss 4.93 on epoch=329
06/19/2022 21:23:40 - INFO - __main__ - Step 670 Global step 670 Train loss 4.88 on epoch=334
06/19/2022 21:23:41 - INFO - __main__ - Step 680 Global step 680 Train loss 4.70 on epoch=339
06/19/2022 21:23:43 - INFO - __main__ - Step 690 Global step 690 Train loss 4.76 on epoch=344
06/19/2022 21:23:44 - INFO - __main__ - Step 700 Global step 700 Train loss 4.73 on epoch=349
06/19/2022 21:23:46 - INFO - __main__ - Global step 700 Train loss 4.80 ACC 0.0 on epoch=349
06/19/2022 21:23:47 - INFO - __main__ - Step 710 Global step 710 Train loss 4.70 on epoch=354
06/19/2022 21:23:48 - INFO - __main__ - Step 720 Global step 720 Train loss 4.49 on epoch=359
06/19/2022 21:23:50 - INFO - __main__ - Step 730 Global step 730 Train loss 4.55 on epoch=364
06/19/2022 21:23:51 - INFO - __main__ - Step 740 Global step 740 Train loss 4.47 on epoch=369
06/19/2022 21:23:52 - INFO - __main__ - Step 750 Global step 750 Train loss 4.44 on epoch=374
06/19/2022 21:23:54 - INFO - __main__ - Global step 750 Train loss 4.53 ACC 0.0 on epoch=374
06/19/2022 21:23:56 - INFO - __main__ - Step 760 Global step 760 Train loss 4.43 on epoch=379
06/19/2022 21:23:57 - INFO - __main__ - Step 770 Global step 770 Train loss 4.48 on epoch=384
06/19/2022 21:23:58 - INFO - __main__ - Step 780 Global step 780 Train loss 4.36 on epoch=389
06/19/2022 21:23:59 - INFO - __main__ - Step 790 Global step 790 Train loss 4.30 on epoch=394
06/19/2022 21:24:00 - INFO - __main__ - Step 800 Global step 800 Train loss 4.33 on epoch=399
06/19/2022 21:24:04 - INFO - __main__ - Global step 800 Train loss 4.38 ACC 0.0 on epoch=399
06/19/2022 21:24:05 - INFO - __main__ - Step 810 Global step 810 Train loss 4.38 on epoch=404
06/19/2022 21:24:06 - INFO - __main__ - Step 820 Global step 820 Train loss 4.26 on epoch=409
06/19/2022 21:24:08 - INFO - __main__ - Step 830 Global step 830 Train loss 4.23 on epoch=414
06/19/2022 21:24:09 - INFO - __main__ - Step 840 Global step 840 Train loss 4.19 on epoch=419
06/19/2022 21:24:10 - INFO - __main__ - Step 850 Global step 850 Train loss 4.15 on epoch=424
06/19/2022 21:24:14 - INFO - __main__ - Global step 850 Train loss 4.24 ACC 0.0 on epoch=424
06/19/2022 21:24:15 - INFO - __main__ - Step 860 Global step 860 Train loss 4.00 on epoch=429
06/19/2022 21:24:16 - INFO - __main__ - Step 870 Global step 870 Train loss 3.99 on epoch=434
06/19/2022 21:24:17 - INFO - __main__ - Step 880 Global step 880 Train loss 4.06 on epoch=439
06/19/2022 21:24:19 - INFO - __main__ - Step 890 Global step 890 Train loss 3.95 on epoch=444
06/19/2022 21:24:20 - INFO - __main__ - Step 900 Global step 900 Train loss 3.89 on epoch=449
06/19/2022 21:24:30 - INFO - __main__ - Global step 900 Train loss 3.98 ACC 0.0 on epoch=449
06/19/2022 21:24:31 - INFO - __main__ - Step 910 Global step 910 Train loss 3.86 on epoch=454
06/19/2022 21:24:32 - INFO - __main__ - Step 920 Global step 920 Train loss 3.78 on epoch=459
06/19/2022 21:24:33 - INFO - __main__ - Step 930 Global step 930 Train loss 3.78 on epoch=464
06/19/2022 21:24:35 - INFO - __main__ - Step 940 Global step 940 Train loss 3.81 on epoch=469
06/19/2022 21:24:36 - INFO - __main__ - Step 950 Global step 950 Train loss 3.81 on epoch=474
06/19/2022 21:24:38 - INFO - __main__ - Global step 950 Train loss 3.81 ACC 0.0 on epoch=474
06/19/2022 21:24:39 - INFO - __main__ - Step 960 Global step 960 Train loss 3.59 on epoch=479
06/19/2022 21:24:41 - INFO - __main__ - Step 970 Global step 970 Train loss 3.70 on epoch=484
06/19/2022 21:24:42 - INFO - __main__ - Step 980 Global step 980 Train loss 3.60 on epoch=489
06/19/2022 21:24:43 - INFO - __main__ - Step 990 Global step 990 Train loss 3.58 on epoch=494
06/19/2022 21:24:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.51 on epoch=499
06/19/2022 21:24:47 - INFO - __main__ - Global step 1000 Train loss 3.60 ACC 0.03125 on epoch=499
06/19/2022 21:24:47 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=499, global_step=1000
06/19/2022 21:24:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.40 on epoch=504
06/19/2022 21:24:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.43 on epoch=509
06/19/2022 21:24:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.39 on epoch=514
06/19/2022 21:24:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.27 on epoch=519
06/19/2022 21:24:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.28 on epoch=524
06/19/2022 21:25:00 - INFO - __main__ - Global step 1050 Train loss 3.35 ACC 0.25 on epoch=524
06/19/2022 21:25:00 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.25 on epoch=524, global_step=1050
06/19/2022 21:25:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.28 on epoch=529
06/19/2022 21:25:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.18 on epoch=534
06/19/2022 21:25:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.04 on epoch=539
06/19/2022 21:25:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.16 on epoch=544
06/19/2022 21:25:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.05 on epoch=549
06/19/2022 21:25:12 - INFO - __main__ - Global step 1100 Train loss 3.14 ACC 0.375 on epoch=549
06/19/2022 21:25:12 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.375 on epoch=549, global_step=1100
06/19/2022 21:25:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.09 on epoch=554
06/19/2022 21:25:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.07 on epoch=559
06/19/2022 21:25:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.91 on epoch=564
06/19/2022 21:25:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.92 on epoch=569
06/19/2022 21:25:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.82 on epoch=574
06/19/2022 21:25:29 - INFO - __main__ - Global step 1150 Train loss 2.96 ACC 0.40625 on epoch=574
06/19/2022 21:25:29 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.40625 on epoch=574, global_step=1150
06/19/2022 21:25:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.74 on epoch=579
06/19/2022 21:25:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.81 on epoch=584
06/19/2022 21:25:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 2.68 on epoch=589
06/19/2022 21:25:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.72 on epoch=594
06/19/2022 21:25:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.62 on epoch=599
06/19/2022 21:25:37 - INFO - __main__ - Global step 1200 Train loss 2.72 ACC 0.46875 on epoch=599
06/19/2022 21:25:37 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=599, global_step=1200
06/19/2022 21:25:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.57 on epoch=604
06/19/2022 21:25:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.53 on epoch=609
06/19/2022 21:25:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.58 on epoch=614
06/19/2022 21:25:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.45 on epoch=619
06/19/2022 21:25:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.43 on epoch=624
06/19/2022 21:25:54 - INFO - __main__ - Global step 1250 Train loss 2.51 ACC 0.40625 on epoch=624
06/19/2022 21:25:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.50 on epoch=629
06/19/2022 21:25:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.40 on epoch=634
06/19/2022 21:25:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.31 on epoch=639
06/19/2022 21:25:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.09 on epoch=644
06/19/2022 21:26:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.18 on epoch=649
06/19/2022 21:26:02 - INFO - __main__ - Global step 1300 Train loss 2.30 ACC 0.4375 on epoch=649
06/19/2022 21:26:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.08 on epoch=654
06/19/2022 21:26:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.07 on epoch=659
06/19/2022 21:26:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.03 on epoch=664
06/19/2022 21:26:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.08 on epoch=669
06/19/2022 21:26:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.01 on epoch=674
06/19/2022 21:26:10 - INFO - __main__ - Global step 1350 Train loss 2.05 ACC 0.4375 on epoch=674
06/19/2022 21:26:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.88 on epoch=679
06/19/2022 21:26:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.90 on epoch=684
06/19/2022 21:26:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.86 on epoch=689
06/19/2022 21:26:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.86 on epoch=694
06/19/2022 21:26:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.64 on epoch=699
06/19/2022 21:26:18 - INFO - __main__ - Global step 1400 Train loss 1.83 ACC 0.53125 on epoch=699
06/19/2022 21:26:18 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.53125 on epoch=699, global_step=1400
06/19/2022 21:26:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.72 on epoch=704
06/19/2022 21:26:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.74 on epoch=709
06/19/2022 21:26:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.76 on epoch=714
06/19/2022 21:26:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.86 on epoch=719
06/19/2022 21:26:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.74 on epoch=724
06/19/2022 21:26:25 - INFO - __main__ - Global step 1450 Train loss 1.76 ACC 0.53125 on epoch=724
06/19/2022 21:26:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.67 on epoch=729
06/19/2022 21:26:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.54 on epoch=734
06/19/2022 21:26:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.58 on epoch=739
06/19/2022 21:26:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.61 on epoch=744
06/19/2022 21:26:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.66 on epoch=749
06/19/2022 21:26:33 - INFO - __main__ - Global step 1500 Train loss 1.61 ACC 0.5 on epoch=749
06/19/2022 21:26:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.51 on epoch=754
06/19/2022 21:26:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.47 on epoch=759
06/19/2022 21:26:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.46 on epoch=764
06/19/2022 21:26:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.37 on epoch=769
06/19/2022 21:26:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.39 on epoch=774
06/19/2022 21:26:41 - INFO - __main__ - Global step 1550 Train loss 1.44 ACC 0.5 on epoch=774
06/19/2022 21:26:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.41 on epoch=779
06/19/2022 21:26:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.37 on epoch=784
06/19/2022 21:26:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.44 on epoch=789
06/19/2022 21:26:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.41 on epoch=794
06/19/2022 21:26:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.36 on epoch=799
06/19/2022 21:26:48 - INFO - __main__ - Global step 1600 Train loss 1.40 ACC 0.5 on epoch=799
06/19/2022 21:26:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.30 on epoch=804
06/19/2022 21:26:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.31 on epoch=809
06/19/2022 21:26:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.28 on epoch=814
06/19/2022 21:26:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.32 on epoch=819
06/19/2022 21:26:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.30 on epoch=824
06/19/2022 21:26:56 - INFO - __main__ - Global step 1650 Train loss 1.30 ACC 0.5 on epoch=824
06/19/2022 21:26:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.23 on epoch=829
06/19/2022 21:26:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.23 on epoch=834
06/19/2022 21:27:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.34 on epoch=839
06/19/2022 21:27:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.10 on epoch=844
06/19/2022 21:27:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.16 on epoch=849
06/19/2022 21:27:04 - INFO - __main__ - Global step 1700 Train loss 1.21 ACC 0.5 on epoch=849
06/19/2022 21:27:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.14 on epoch=854
06/19/2022 21:27:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.21 on epoch=859
06/19/2022 21:27:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.06 on epoch=864
06/19/2022 21:27:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.97 on epoch=869
06/19/2022 21:27:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.04 on epoch=874
06/19/2022 21:27:12 - INFO - __main__ - Global step 1750 Train loss 1.08 ACC 0.5 on epoch=874
06/19/2022 21:27:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.04 on epoch=879
06/19/2022 21:27:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.02 on epoch=884
06/19/2022 21:27:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.04 on epoch=889
06/19/2022 21:27:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.03 on epoch=894
06/19/2022 21:27:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.96 on epoch=899
06/19/2022 21:27:21 - INFO - __main__ - Global step 1800 Train loss 1.02 ACC 0.5 on epoch=899
06/19/2022 21:27:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.96 on epoch=904
06/19/2022 21:27:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.95 on epoch=909
06/19/2022 21:27:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.83 on epoch=914
06/19/2022 21:27:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.91 on epoch=919
06/19/2022 21:27:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.90 on epoch=924
06/19/2022 21:27:35 - INFO - __main__ - Global step 1850 Train loss 0.91 ACC 0.5 on epoch=924
06/19/2022 21:27:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.89 on epoch=929
06/19/2022 21:27:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.88 on epoch=934
06/19/2022 21:27:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.96 on epoch=939
06/19/2022 21:27:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.90 on epoch=944
06/19/2022 21:27:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.90 on epoch=949
06/19/2022 21:27:43 - INFO - __main__ - Global step 1900 Train loss 0.90 ACC 0.5 on epoch=949
06/19/2022 21:27:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.95 on epoch=954
06/19/2022 21:27:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.89 on epoch=959
06/19/2022 21:27:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.79 on epoch=964
06/19/2022 21:27:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.88 on epoch=969
06/19/2022 21:27:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.74 on epoch=974
06/19/2022 21:27:52 - INFO - __main__ - Global step 1950 Train loss 0.85 ACC 0.5 on epoch=974
06/19/2022 21:27:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.87 on epoch=979
06/19/2022 21:27:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.83 on epoch=984
06/19/2022 21:27:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.80 on epoch=989
06/19/2022 21:27:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.78 on epoch=994
06/19/2022 21:27:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.81 on epoch=999
06/19/2022 21:27:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:27:59 - INFO - __main__ - Printing 3 examples
06/19/2022 21:27:59 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 21:27:59 - INFO - __main__ - ['equivalent']
06/19/2022 21:27:59 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 21:27:59 - INFO - __main__ - ['equivalent']
06/19/2022 21:27:59 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 21:27:59 - INFO - __main__ - ['equivalent']
06/19/2022 21:27:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:27:59 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:27:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:27:59 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:27:59 - INFO - __main__ - Printing 3 examples
06/19/2022 21:27:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 21:27:59 - INFO - __main__ - ['equivalent']
06/19/2022 21:27:59 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 21:27:59 - INFO - __main__ - ['equivalent']
06/19/2022 21:27:59 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 21:27:59 - INFO - __main__ - ['equivalent']
06/19/2022 21:27:59 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:27:59 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:27:59 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:28:00 - INFO - __main__ - Global step 2000 Train loss 0.82 ACC 0.5 on epoch=999
06/19/2022 21:28:00 - INFO - __main__ - save last model!
06/19/2022 21:28:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:28:00 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 21:28:00 - INFO - __main__ - Printing 3 examples
06/19/2022 21:28:00 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 21:28:00 - INFO - __main__ - ['equivalent']
06/19/2022 21:28:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 21:28:00 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:28:00 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 21:28:00 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:28:00 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:28:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:28:00 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 21:28:05 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:28:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:28:05 - INFO - __main__ - Starting training!
06/19/2022 21:28:22 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_87_0.3_8_predictions.txt
06/19/2022 21:28:22 - INFO - __main__ - ACC on test data: 0.6814
06/19/2022 21:28:22 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.3, bsz=8, dev_performance=0.53125, test_performance=0.6813725490196079
06/19/2022 21:28:22 - INFO - __main__ - Running ... prefix=glue-mrpc_16_87, lr=0.2, bsz=8 ...
06/19/2022 21:28:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:28:23 - INFO - __main__ - Printing 3 examples
06/19/2022 21:28:23 - INFO - __main__ -  [glue-mrpc] sentence 1: His dissent was joined by Chief Justice William H. Rehnquist and Justice Clarence Thomas . [SEP] sentence 2: Chief Justice William H. Rehnquist and Justices Antonin Scalia and Clarence Thomas dissented .
06/19/2022 21:28:23 - INFO - __main__ - ['equivalent']
06/19/2022 21:28:23 - INFO - __main__ -  [glue-mrpc] sentence 1: The 20 master computers are located in the United States , Canada and Korea , Mr. Kuo said . [SEP] sentence 2: The computers were located in the United States , Canada and South Korea , he said .
06/19/2022 21:28:23 - INFO - __main__ - ['equivalent']
06/19/2022 21:28:23 - INFO - __main__ -  [glue-mrpc] sentence 1: He said there were complex reasons for the increased numbers of cases and scientists were only just beginning to understand the risk factors . [SEP] sentence 2: However , he said , The reasons behind the increase in incidence are more complex and were just beginning to understand the risk factors .
06/19/2022 21:28:23 - INFO - __main__ - ['equivalent']
06/19/2022 21:28:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:28:23 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:28:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:28:23 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:28:23 - INFO - __main__ - Printing 3 examples
06/19/2022 21:28:23 - INFO - __main__ -  [glue-mrpc] sentence 1: Treasury prices rose slightly , sending the 10-year note yield down to 4.38 percent from 4.39 percent late Friday . [SEP] sentence 2: Treasury prices gained for the third day in a row , pushing the 10-year note yield down to 4.35 percent from 4.36 percent late Monday .
06/19/2022 21:28:23 - INFO - __main__ - ['equivalent']
06/19/2022 21:28:23 - INFO - __main__ -  [glue-mrpc] sentence 1: That means that if the planet is in a season , it will continue to brighten for the next 20 years . [SEP] sentence 2: If what scientists are observing is truly seasonal change , the planet will continue to brighten for another 20 years .
06/19/2022 21:28:23 - INFO - __main__ - ['equivalent']
06/19/2022 21:28:23 - INFO - __main__ -  [glue-mrpc] sentence 1: Meanwhile , rival contender , General Electric 's NBC , submitted a letter of interest , a source familiar with the matter said . [SEP] sentence 2: Other contenders included General Electric 's GE.N NBC , which submitted a letter of interest , a source familiar with the matter said .
06/19/2022 21:28:23 - INFO - __main__ - ['equivalent']
06/19/2022 21:28:23 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:28:23 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:28:23 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:28:28 - INFO - __main__ - load prompt embedding from ckpt
06/19/2022 21:28:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/19/2022 21:28:29 - INFO - __main__ - Starting training!
06/19/2022 21:28:30 - INFO - __main__ - Step 10 Global step 10 Train loss 6.92 on epoch=4
06/19/2022 21:28:31 - INFO - __main__ - Step 20 Global step 20 Train loss 6.87 on epoch=9
06/19/2022 21:28:33 - INFO - __main__ - Step 30 Global step 30 Train loss 6.78 on epoch=14
06/19/2022 21:28:34 - INFO - __main__ - Step 40 Global step 40 Train loss 6.80 on epoch=19
06/19/2022 21:28:35 - INFO - __main__ - Step 50 Global step 50 Train loss 6.80 on epoch=24
06/19/2022 21:28:36 - INFO - __main__ - Global step 50 Train loss 6.83 ACC 0.0 on epoch=24
06/19/2022 21:28:36 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/19/2022 21:28:38 - INFO - __main__ - Step 60 Global step 60 Train loss 6.77 on epoch=29
06/19/2022 21:28:39 - INFO - __main__ - Step 70 Global step 70 Train loss 6.76 on epoch=34
06/19/2022 21:28:40 - INFO - __main__ - Step 80 Global step 80 Train loss 6.78 on epoch=39
06/19/2022 21:28:41 - INFO - __main__ - Step 90 Global step 90 Train loss 6.74 on epoch=44
06/19/2022 21:28:42 - INFO - __main__ - Step 100 Global step 100 Train loss 6.76 on epoch=49
06/19/2022 21:28:45 - INFO - __main__ - Global step 100 Train loss 6.76 ACC 0.0 on epoch=49
06/19/2022 21:28:46 - INFO - __main__ - Step 110 Global step 110 Train loss 6.75 on epoch=54
06/19/2022 21:28:47 - INFO - __main__ - Step 120 Global step 120 Train loss 6.76 on epoch=59
06/19/2022 21:28:49 - INFO - __main__ - Step 130 Global step 130 Train loss 6.66 on epoch=64
06/19/2022 21:28:50 - INFO - __main__ - Step 140 Global step 140 Train loss 6.65 on epoch=69
06/19/2022 21:28:51 - INFO - __main__ - Step 150 Global step 150 Train loss 6.69 on epoch=74
06/19/2022 21:28:52 - INFO - __main__ - Global step 150 Train loss 6.70 ACC 0.0 on epoch=74
06/19/2022 21:28:53 - INFO - __main__ - Step 160 Global step 160 Train loss 6.69 on epoch=79
06/19/2022 21:28:54 - INFO - __main__ - Step 170 Global step 170 Train loss 6.65 on epoch=84
06/19/2022 21:28:56 - INFO - __main__ - Step 180 Global step 180 Train loss 6.57 on epoch=89
06/19/2022 21:28:57 - INFO - __main__ - Step 190 Global step 190 Train loss 6.62 on epoch=94
06/19/2022 21:28:58 - INFO - __main__ - Step 200 Global step 200 Train loss 6.67 on epoch=99
06/19/2022 21:29:01 - INFO - __main__ - Global step 200 Train loss 6.64 ACC 0.0 on epoch=99
06/19/2022 21:29:02 - INFO - __main__ - Step 210 Global step 210 Train loss 6.65 on epoch=104
06/19/2022 21:29:04 - INFO - __main__ - Step 220 Global step 220 Train loss 6.56 on epoch=109
06/19/2022 21:29:05 - INFO - __main__ - Step 230 Global step 230 Train loss 6.56 on epoch=114
06/19/2022 21:29:06 - INFO - __main__ - Step 240 Global step 240 Train loss 6.53 on epoch=119
06/19/2022 21:29:07 - INFO - __main__ - Step 250 Global step 250 Train loss 6.63 on epoch=124
06/19/2022 21:29:10 - INFO - __main__ - Global step 250 Train loss 6.59 ACC 0.0 on epoch=124
06/19/2022 21:29:11 - INFO - __main__ - Step 260 Global step 260 Train loss 6.46 on epoch=129
06/19/2022 21:29:12 - INFO - __main__ - Step 270 Global step 270 Train loss 6.47 on epoch=134
06/19/2022 21:29:13 - INFO - __main__ - Step 280 Global step 280 Train loss 6.51 on epoch=139
06/19/2022 21:29:14 - INFO - __main__ - Step 290 Global step 290 Train loss 6.45 on epoch=144
06/19/2022 21:29:16 - INFO - __main__ - Step 300 Global step 300 Train loss 6.40 on epoch=149
06/19/2022 21:29:20 - INFO - __main__ - Global step 300 Train loss 6.46 ACC 0.0 on epoch=149
06/19/2022 21:29:21 - INFO - __main__ - Step 310 Global step 310 Train loss 6.41 on epoch=154
06/19/2022 21:29:22 - INFO - __main__ - Step 320 Global step 320 Train loss 6.30 on epoch=159
06/19/2022 21:29:24 - INFO - __main__ - Step 330 Global step 330 Train loss 6.25 on epoch=164
06/19/2022 21:29:25 - INFO - __main__ - Step 340 Global step 340 Train loss 6.24 on epoch=169
06/19/2022 21:29:26 - INFO - __main__ - Step 350 Global step 350 Train loss 6.22 on epoch=174
06/19/2022 21:29:30 - INFO - __main__ - Global step 350 Train loss 6.29 ACC 0.0 on epoch=174
06/19/2022 21:29:31 - INFO - __main__ - Step 360 Global step 360 Train loss 6.11 on epoch=179
06/19/2022 21:29:32 - INFO - __main__ - Step 370 Global step 370 Train loss 6.22 on epoch=184
06/19/2022 21:29:34 - INFO - __main__ - Step 380 Global step 380 Train loss 6.19 on epoch=189
06/19/2022 21:29:35 - INFO - __main__ - Step 390 Global step 390 Train loss 6.16 on epoch=194
06/19/2022 21:29:36 - INFO - __main__ - Step 400 Global step 400 Train loss 6.09 on epoch=199
06/19/2022 21:29:44 - INFO - __main__ - Global step 400 Train loss 6.15 ACC 0.0 on epoch=199
06/19/2022 21:29:45 - INFO - __main__ - Step 410 Global step 410 Train loss 6.05 on epoch=204
06/19/2022 21:29:46 - INFO - __main__ - Step 420 Global step 420 Train loss 6.03 on epoch=209
06/19/2022 21:29:48 - INFO - __main__ - Step 430 Global step 430 Train loss 6.00 on epoch=214
06/19/2022 21:29:49 - INFO - __main__ - Step 440 Global step 440 Train loss 5.87 on epoch=219
06/19/2022 21:29:50 - INFO - __main__ - Step 450 Global step 450 Train loss 5.92 on epoch=224
06/19/2022 21:29:54 - INFO - __main__ - Global step 450 Train loss 5.98 ACC 0.0 on epoch=224
06/19/2022 21:29:55 - INFO - __main__ - Step 460 Global step 460 Train loss 5.87 on epoch=229
06/19/2022 21:29:56 - INFO - __main__ - Step 470 Global step 470 Train loss 5.74 on epoch=234
06/19/2022 21:29:57 - INFO - __main__ - Step 480 Global step 480 Train loss 5.65 on epoch=239
06/19/2022 21:29:59 - INFO - __main__ - Step 490 Global step 490 Train loss 5.74 on epoch=244
06/19/2022 21:30:00 - INFO - __main__ - Step 500 Global step 500 Train loss 5.51 on epoch=249
06/19/2022 21:30:01 - INFO - __main__ - Global step 500 Train loss 5.70 ACC 0.0 on epoch=249
06/19/2022 21:30:03 - INFO - __main__ - Step 510 Global step 510 Train loss 5.64 on epoch=254
06/19/2022 21:30:04 - INFO - __main__ - Step 520 Global step 520 Train loss 5.38 on epoch=259
06/19/2022 21:30:05 - INFO - __main__ - Step 530 Global step 530 Train loss 5.40 on epoch=264
06/19/2022 21:30:06 - INFO - __main__ - Step 540 Global step 540 Train loss 5.26 on epoch=269
06/19/2022 21:30:08 - INFO - __main__ - Step 550 Global step 550 Train loss 5.33 on epoch=274
06/19/2022 21:30:09 - INFO - __main__ - Global step 550 Train loss 5.40 ACC 0.0 on epoch=274
06/19/2022 21:30:10 - INFO - __main__ - Step 560 Global step 560 Train loss 5.18 on epoch=279
06/19/2022 21:30:11 - INFO - __main__ - Step 570 Global step 570 Train loss 5.26 on epoch=284
06/19/2022 21:30:13 - INFO - __main__ - Step 580 Global step 580 Train loss 5.15 on epoch=289
06/19/2022 21:30:14 - INFO - __main__ - Step 590 Global step 590 Train loss 5.09 on epoch=294
06/19/2022 21:30:15 - INFO - __main__ - Step 600 Global step 600 Train loss 5.00 on epoch=299
06/19/2022 21:30:16 - INFO - __main__ - Global step 600 Train loss 5.14 ACC 0.0 on epoch=299
06/19/2022 21:30:18 - INFO - __main__ - Step 610 Global step 610 Train loss 4.88 on epoch=304
06/19/2022 21:30:19 - INFO - __main__ - Step 620 Global step 620 Train loss 4.94 on epoch=309
06/19/2022 21:30:20 - INFO - __main__ - Step 630 Global step 630 Train loss 4.87 on epoch=314
06/19/2022 21:30:21 - INFO - __main__ - Step 640 Global step 640 Train loss 4.80 on epoch=319
06/19/2022 21:30:22 - INFO - __main__ - Step 650 Global step 650 Train loss 4.80 on epoch=324
06/19/2022 21:30:25 - INFO - __main__ - Global step 650 Train loss 4.86 ACC 0.0 on epoch=324
06/19/2022 21:30:26 - INFO - __main__ - Step 660 Global step 660 Train loss 4.74 on epoch=329
06/19/2022 21:30:28 - INFO - __main__ - Step 670 Global step 670 Train loss 4.68 on epoch=334
06/19/2022 21:30:29 - INFO - __main__ - Step 680 Global step 680 Train loss 4.67 on epoch=339
06/19/2022 21:30:30 - INFO - __main__ - Step 690 Global step 690 Train loss 4.57 on epoch=344
06/19/2022 21:30:31 - INFO - __main__ - Step 700 Global step 700 Train loss 4.58 on epoch=349
06/19/2022 21:30:33 - INFO - __main__ - Global step 700 Train loss 4.65 ACC 0.0 on epoch=349
06/19/2022 21:30:34 - INFO - __main__ - Step 710 Global step 710 Train loss 4.64 on epoch=354
06/19/2022 21:30:35 - INFO - __main__ - Step 720 Global step 720 Train loss 4.50 on epoch=359
06/19/2022 21:30:37 - INFO - __main__ - Step 730 Global step 730 Train loss 4.47 on epoch=364
06/19/2022 21:30:38 - INFO - __main__ - Step 740 Global step 740 Train loss 4.38 on epoch=369
06/19/2022 21:30:39 - INFO - __main__ - Step 750 Global step 750 Train loss 4.29 on epoch=374
06/19/2022 21:30:40 - INFO - __main__ - Global step 750 Train loss 4.46 ACC 0.0 on epoch=374
06/19/2022 21:30:41 - INFO - __main__ - Step 760 Global step 760 Train loss 4.21 on epoch=379
06/19/2022 21:30:43 - INFO - __main__ - Step 770 Global step 770 Train loss 4.38 on epoch=384
06/19/2022 21:30:44 - INFO - __main__ - Step 780 Global step 780 Train loss 4.23 on epoch=389
06/19/2022 21:30:45 - INFO - __main__ - Step 790 Global step 790 Train loss 4.45 on epoch=394
06/19/2022 21:30:46 - INFO - __main__ - Step 800 Global step 800 Train loss 4.27 on epoch=399
06/19/2022 21:30:49 - INFO - __main__ - Global step 800 Train loss 4.31 ACC 0.0 on epoch=399
06/19/2022 21:30:50 - INFO - __main__ - Step 810 Global step 810 Train loss 4.21 on epoch=404
06/19/2022 21:30:51 - INFO - __main__ - Step 820 Global step 820 Train loss 4.16 on epoch=409
06/19/2022 21:30:52 - INFO - __main__ - Step 830 Global step 830 Train loss 4.20 on epoch=414
06/19/2022 21:30:54 - INFO - __main__ - Step 840 Global step 840 Train loss 4.21 on epoch=419
06/19/2022 21:30:55 - INFO - __main__ - Step 850 Global step 850 Train loss 4.04 on epoch=424
06/19/2022 21:30:56 - INFO - __main__ - Global step 850 Train loss 4.16 ACC 0.0 on epoch=424
06/19/2022 21:30:57 - INFO - __main__ - Step 860 Global step 860 Train loss 4.02 on epoch=429
06/19/2022 21:30:58 - INFO - __main__ - Step 870 Global step 870 Train loss 4.05 on epoch=434
06/19/2022 21:31:00 - INFO - __main__ - Step 880 Global step 880 Train loss 3.91 on epoch=439
06/19/2022 21:31:01 - INFO - __main__ - Step 890 Global step 890 Train loss 4.01 on epoch=444
06/19/2022 21:31:02 - INFO - __main__ - Step 900 Global step 900 Train loss 3.99 on epoch=449
06/19/2022 21:31:03 - INFO - __main__ - Global step 900 Train loss 3.99 ACC 0.0 on epoch=449
06/19/2022 21:31:04 - INFO - __main__ - Step 910 Global step 910 Train loss 3.88 on epoch=454
06/19/2022 21:31:05 - INFO - __main__ - Step 920 Global step 920 Train loss 3.90 on epoch=459
06/19/2022 21:31:07 - INFO - __main__ - Step 930 Global step 930 Train loss 4.01 on epoch=464
06/19/2022 21:31:08 - INFO - __main__ - Step 940 Global step 940 Train loss 3.75 on epoch=469
06/19/2022 21:31:09 - INFO - __main__ - Step 950 Global step 950 Train loss 3.82 on epoch=474
06/19/2022 21:31:10 - INFO - __main__ - Global step 950 Train loss 3.87 ACC 0.0 on epoch=474
06/19/2022 21:31:11 - INFO - __main__ - Step 960 Global step 960 Train loss 3.63 on epoch=479
06/19/2022 21:31:13 - INFO - __main__ - Step 970 Global step 970 Train loss 3.82 on epoch=484
06/19/2022 21:31:14 - INFO - __main__ - Step 980 Global step 980 Train loss 3.69 on epoch=489
06/19/2022 21:31:15 - INFO - __main__ - Step 990 Global step 990 Train loss 3.73 on epoch=494
06/19/2022 21:31:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.62 on epoch=499
06/19/2022 21:31:17 - INFO - __main__ - Global step 1000 Train loss 3.70 ACC 0.0 on epoch=499
06/19/2022 21:31:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.62 on epoch=504
06/19/2022 21:31:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.54 on epoch=509
06/19/2022 21:31:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.46 on epoch=514
06/19/2022 21:31:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.44 on epoch=519
06/19/2022 21:31:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.36 on epoch=524
06/19/2022 21:31:27 - INFO - __main__ - Global step 1050 Train loss 3.48 ACC 0.0 on epoch=524
06/19/2022 21:31:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.35 on epoch=529
06/19/2022 21:31:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.45 on epoch=534
06/19/2022 21:31:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.35 on epoch=539
06/19/2022 21:31:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.29 on epoch=544
06/19/2022 21:31:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.37 on epoch=549
06/19/2022 21:31:35 - INFO - __main__ - Global step 1100 Train loss 3.36 ACC 0.0 on epoch=549
06/19/2022 21:31:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.23 on epoch=554
06/19/2022 21:31:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.16 on epoch=559
06/19/2022 21:31:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.01 on epoch=564
06/19/2022 21:31:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.16 on epoch=569
06/19/2022 21:31:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.12 on epoch=574
06/19/2022 21:31:44 - INFO - __main__ - Global step 1150 Train loss 3.14 ACC 0.0 on epoch=574
06/19/2022 21:31:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.97 on epoch=579
06/19/2022 21:31:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.94 on epoch=584
06/19/2022 21:31:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 2.99 on epoch=589
06/19/2022 21:31:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.93 on epoch=594
06/19/2022 21:31:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.93 on epoch=599
06/19/2022 21:31:51 - INFO - __main__ - Global step 1200 Train loss 2.95 ACC 0.0 on epoch=599
06/19/2022 21:31:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.86 on epoch=604
06/19/2022 21:31:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.78 on epoch=609
06/19/2022 21:31:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.82 on epoch=614
06/19/2022 21:31:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.68 on epoch=619
06/19/2022 21:31:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.66 on epoch=624
06/19/2022 21:31:58 - INFO - __main__ - Global step 1250 Train loss 2.76 ACC 0.0 on epoch=624
06/19/2022 21:31:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.64 on epoch=629
06/19/2022 21:32:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.60 on epoch=634
06/19/2022 21:32:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.68 on epoch=639
06/19/2022 21:32:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.50 on epoch=644
06/19/2022 21:32:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.52 on epoch=649
06/19/2022 21:32:05 - INFO - __main__ - Global step 1300 Train loss 2.59 ACC 0.1875 on epoch=649
06/19/2022 21:32:05 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.1875 on epoch=649, global_step=1300
06/19/2022 21:32:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.43 on epoch=654
06/19/2022 21:32:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.41 on epoch=659
06/19/2022 21:32:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.37 on epoch=664
06/19/2022 21:32:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.34 on epoch=669
06/19/2022 21:32:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.37 on epoch=674
06/19/2022 21:32:13 - INFO - __main__ - Global step 1350 Train loss 2.39 ACC 0.25 on epoch=674
06/19/2022 21:32:13 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.25 on epoch=674, global_step=1350
06/19/2022 21:32:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.29 on epoch=679
06/19/2022 21:32:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.27 on epoch=684
06/19/2022 21:32:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.20 on epoch=689
06/19/2022 21:32:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.19 on epoch=694
06/19/2022 21:32:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.12 on epoch=699
06/19/2022 21:32:20 - INFO - __main__ - Global step 1400 Train loss 2.21 ACC 0.46875 on epoch=699
06/19/2022 21:32:20 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.46875 on epoch=699, global_step=1400
06/19/2022 21:32:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.01 on epoch=704
06/19/2022 21:32:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.14 on epoch=709
06/19/2022 21:32:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.10 on epoch=714
06/19/2022 21:32:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.00 on epoch=719
06/19/2022 21:32:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.00 on epoch=724
06/19/2022 21:32:28 - INFO - __main__ - Global step 1450 Train loss 2.05 ACC 0.5 on epoch=724
06/19/2022 21:32:28 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=724, global_step=1450
06/19/2022 21:32:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.98 on epoch=729
06/19/2022 21:32:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.89 on epoch=734
06/19/2022 21:32:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.78 on epoch=739
06/19/2022 21:32:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.72 on epoch=744
06/19/2022 21:32:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.70 on epoch=749
06/19/2022 21:32:40 - INFO - __main__ - Global step 1500 Train loss 1.82 ACC 0.5 on epoch=749
06/19/2022 21:32:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.76 on epoch=754
06/19/2022 21:32:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.72 on epoch=759
06/19/2022 21:32:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.66 on epoch=764
06/19/2022 21:32:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.61 on epoch=769
06/19/2022 21:32:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.66 on epoch=774
06/19/2022 21:32:49 - INFO - __main__ - Global step 1550 Train loss 1.68 ACC 0.5 on epoch=774
06/19/2022 21:32:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.64 on epoch=779
06/19/2022 21:32:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.58 on epoch=784
06/19/2022 21:32:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.46 on epoch=789
06/19/2022 21:32:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.54 on epoch=794
06/19/2022 21:32:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.49 on epoch=799
06/19/2022 21:32:57 - INFO - __main__ - Global step 1600 Train loss 1.54 ACC 0.375 on epoch=799
06/19/2022 21:32:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.48 on epoch=804
06/19/2022 21:33:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.54 on epoch=809
06/19/2022 21:33:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.53 on epoch=814
06/19/2022 21:33:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.46 on epoch=819
06/19/2022 21:33:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.44 on epoch=824
06/19/2022 21:33:05 - INFO - __main__ - Global step 1650 Train loss 1.49 ACC 0.46875 on epoch=824
06/19/2022 21:33:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.34 on epoch=829
06/19/2022 21:33:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.37 on epoch=834
06/19/2022 21:33:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.37 on epoch=839
06/19/2022 21:33:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.36 on epoch=844
06/19/2022 21:33:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.40 on epoch=849
06/19/2022 21:33:14 - INFO - __main__ - Global step 1700 Train loss 1.37 ACC 0.46875 on epoch=849
06/19/2022 21:33:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.25 on epoch=854
06/19/2022 21:33:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.34 on epoch=859
06/19/2022 21:33:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.23 on epoch=864
06/19/2022 21:33:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.26 on epoch=869
06/19/2022 21:33:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.20 on epoch=874
06/19/2022 21:33:22 - INFO - __main__ - Global step 1750 Train loss 1.26 ACC 0.5 on epoch=874
06/19/2022 21:33:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.11 on epoch=879
06/19/2022 21:33:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.09 on epoch=884
06/19/2022 21:33:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.14 on epoch=889
06/19/2022 21:33:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.18 on epoch=894
06/19/2022 21:33:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.04 on epoch=899
06/19/2022 21:33:36 - INFO - __main__ - Global step 1800 Train loss 1.11 ACC 0.53125 on epoch=899
06/19/2022 21:33:36 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=899, global_step=1800
06/19/2022 21:33:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.10 on epoch=904
06/19/2022 21:33:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.21 on epoch=909
06/19/2022 21:33:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.04 on epoch=914
06/19/2022 21:33:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.08 on epoch=919
06/19/2022 21:33:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.98 on epoch=924
06/19/2022 21:33:49 - INFO - __main__ - Global step 1850 Train loss 1.08 ACC 0.46875 on epoch=924
06/19/2022 21:33:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.12 on epoch=929
06/19/2022 21:33:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.99 on epoch=934
06/19/2022 21:33:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.05 on epoch=939
06/19/2022 21:33:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.95 on epoch=944
06/19/2022 21:33:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.94 on epoch=949
06/19/2022 21:33:58 - INFO - __main__ - Global step 1900 Train loss 1.01 ACC 0.5 on epoch=949
06/19/2022 21:33:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.96 on epoch=954
06/19/2022 21:34:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.94 on epoch=959
06/19/2022 21:34:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.91 on epoch=964
06/19/2022 21:34:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.85 on epoch=969
06/19/2022 21:34:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.87 on epoch=974
06/19/2022 21:34:06 - INFO - __main__ - Global step 1950 Train loss 0.90 ACC 0.5 on epoch=974
06/19/2022 21:34:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.88 on epoch=979
06/19/2022 21:34:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.78 on epoch=984
06/19/2022 21:34:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.80 on epoch=989
06/19/2022 21:34:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.83 on epoch=994
06/19/2022 21:34:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.82 on epoch=999
06/19/2022 21:34:15 - INFO - __main__ - Global step 2000 Train loss 0.82 ACC 0.5 on epoch=999
06/19/2022 21:34:15 - INFO - __main__ - save last model!
06/19/2022 21:34:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/19/2022 21:34:15 - INFO - __main__ - Start tokenizing ... 408 instances
06/19/2022 21:34:15 - INFO - __main__ - Printing 3 examples
06/19/2022 21:34:15 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/19/2022 21:34:15 - INFO - __main__ - ['equivalent']
06/19/2022 21:34:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/19/2022 21:34:15 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:34:15 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/19/2022 21:34:15 - INFO - __main__ - ['not_equivalent']
06/19/2022 21:34:15 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:34:15 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:34:15 - INFO - __main__ - Loaded 408 examples from test data
06/19/2022 21:34:58 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_87_0.2_8_predictions.txt
06/19/2022 21:34:58 - INFO - __main__ - ACC on test data: 0.6765
06/19/2022 21:34:59 - INFO - __main__ - prefix=glue-mrpc_16_87, lr=0.2, bsz=8, dev_performance=0.53125, test_performance=0.6764705882352942
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (88058): No such process
Task: glue-qqp, Checkpoint: models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt, Identifier: T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_reptile/singletask_from_reptile_nopara2para.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_reptile/singletask_from_reptile_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_reptile/models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp/log.txt'
06/19/2022 21:35:04 - INFO - __main__ - Namespace(task_dir='data/glue-qqp/', task_name='glue-qqp', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/19/2022 21:35:04 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_reptile_nopara2para.py', '--local_rank=1', '--task_dir', 'data/glue-qqp/', '--task_name', 'glue-qqp', '--identifier', 'T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', '--checkpoint', 'models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-qqp', '--cuda', '6,7', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 88063
Killing subprocess 88064
++++++++++++++++++++++++++++++
kill: (88070): No such process
Task: medical_questions_pairs, Checkpoint: models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt, Identifier: T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_reptile/singletask_from_reptile_nopara2para.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_reptile/singletask_from_reptile_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_reptile/models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs/log.txt'
06/19/2022 21:35:08 - INFO - __main__ - Namespace(task_dir='data/medical_questions_pairs/', task_name='medical_questions_pairs', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/19/2022 21:35:08 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_reptile_nopara2para.py', '--local_rank=1', '--task_dir', 'data/medical_questions_pairs/', '--task_name', 'medical_questions_pairs', '--identifier', 'T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', '--checkpoint', 'models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-medical_questions_pairs', '--cuda', '6,7', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 88075
Killing subprocess 88076
++++++++++++++++++++++++++++++
kill: (88082): No such process
Task: paws, Checkpoint: models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt, Identifier: T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_reptile/singletask_from_reptile_nopara2para.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_reptile/singletask_from_reptile_nopara2para.py", line 142, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_reptile/models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws/log.txt'
06/19/2022 21:35:12 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/19/2022 21:35:12 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_reptile_nopara2para.py', '--local_rank=1', '--task_dir', 'data/paws/', '--task_name', 'paws', '--identifier', 'T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', '--checkpoint', 'models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-paws', '--cuda', '6,7', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', '--model', 'google/t5-v1_1-base', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 88087
Killing subprocess 88088
++++++++++++++++++++++++++++++
kill: (88094): No such process
t5base para ft downstream
Task: glue-mrpc, Checkpoint: None, Identifier: T5-base-ft-nopara2para
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFitFT/tune_singletask_nopara2para.py", line 225, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFitFT/tune_singletask_nopara2para.py", line 137, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFitFT/models/T5-base-ft-nopara2para/singletask-glue-mrpc/log.txt'
06/19/2022 21:35:16 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/19/2022 21:35:16 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-glue-mrpc
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'tune_singletask_nopara2para.py', '--local_rank=1', '--task_dir', 'data/glue-mrpc/', '--task_name', 'glue-mrpc', '--identifier', 'T5-base-ft-nopara2para', '--checkpoint', 'None', '--do_train', '--do_predict', '--learning_rate_list', '5e-4', '3e-4', '2e-4', '1e-4', '--bsz_list', '8', '--predict_batch_size', '32', '--total_steps', '1000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '300.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-ft-nopara2para/singletask-glue-mrpc', '--cuda', '6,7', '--model', 'google/t5-v1_1-base']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 88109
Killing subprocess 88110
++++++++++++++++++++++++++++++
kill: (88116): No such process
Task: glue-qqp, Checkpoint: None, Identifier: T5-base-ft-nopara2para
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFitFT/tune_singletask_nopara2para.py", line 225, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFitFT/tune_singletask_nopara2para.py", line 137, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFitFT/models/T5-base-ft-nopara2para/singletask-glue-qqp/log.txt'
06/19/2022 21:35:20 - INFO - __main__ - Namespace(task_dir='data/glue-qqp/', task_name='glue-qqp', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-glue-qqp', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/19/2022 21:35:20 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-glue-qqp
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'tune_singletask_nopara2para.py', '--local_rank=1', '--task_dir', 'data/glue-qqp/', '--task_name', 'glue-qqp', '--identifier', 'T5-base-ft-nopara2para', '--checkpoint', 'None', '--do_train', '--do_predict', '--learning_rate_list', '5e-4', '3e-4', '2e-4', '1e-4', '--bsz_list', '8', '--predict_batch_size', '32', '--total_steps', '1000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '300.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-ft-nopara2para/singletask-glue-qqp', '--cuda', '6,7', '--model', 'google/t5-v1_1-base']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 88121
Killing subprocess 88122
++++++++++++++++++++++++++++++
kill: (88128): No such process
Task: medical_questions_pairs, Checkpoint: None, Identifier: T5-base-ft-nopara2para
06/19/2022 21:35:25 - INFO - __main__ - Namespace(task_dir='data/medical_questions_pairs/', task_name='medical_questions_pairs', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-medical_questions_pairs', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/19/2022 21:35:25 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-medical_questions_pairs
06/19/2022 21:35:25 - INFO - __main__ - Namespace(task_dir='data/medical_questions_pairs/', task_name='medical_questions_pairs', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-medical_questions_pairs', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/19/2022 21:35:25 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-medical_questions_pairs
06/19/2022 21:35:25 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/19/2022 21:35:25 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/19/2022 21:35:25 - INFO - __main__ - args.device: cuda:0
06/19/2022 21:35:25 - INFO - __main__ - Using 2 gpus
06/19/2022 21:35:25 - INFO - __main__ - args.device: cuda:1
06/19/2022 21:35:25 - INFO - __main__ - Using 2 gpus
06/19/2022 21:35:25 - INFO - __main__ - Fine-tuning the following samples: ['medical_questions_pairs_16_100', 'medical_questions_pairs_16_13', 'medical_questions_pairs_16_21', 'medical_questions_pairs_16_42', 'medical_questions_pairs_16_87']
06/19/2022 21:35:25 - INFO - __main__ - Fine-tuning the following samples: ['medical_questions_pairs_16_100', 'medical_questions_pairs_16_13', 'medical_questions_pairs_16_21', 'medical_questions_pairs_16_42', 'medical_questions_pairs_16_87']
06/19/2022 21:35:30 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.0005, bsz=8 ...
06/19/2022 21:35:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:35:31 - INFO - __main__ - Printing 3 examples
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:35:31 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:35:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:35:31 - INFO - __main__ - Printing 3 examples
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:35:31 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:35:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:35:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:35:31 - INFO - __main__ - Printing 3 examples
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:35:31 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:35:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:35:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:35:31 - INFO - __main__ - Printing 3 examples
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/19/2022 21:35:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:35:31 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:35:31 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:35:31 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:35:31 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:35:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:35:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:35:35 - INFO - __main__ - Starting training!
06/19/2022 21:35:35 - INFO - __main__ - Starting training!
06/19/2022 21:35:38 - INFO - __main__ - Step 10 Global step 10 Train loss 18.292217 on epoch=4
06/19/2022 21:35:40 - INFO - __main__ - Step 20 Global step 20 Train loss 11.763894 on epoch=9
06/19/2022 21:35:42 - INFO - __main__ - Step 30 Global step 30 Train loss 7.198427 on epoch=14
06/19/2022 21:35:45 - INFO - __main__ - Step 40 Global step 40 Train loss 4.383221 on epoch=19
06/19/2022 21:35:47 - INFO - __main__ - Step 50 Global step 50 Train loss 3.636153 on epoch=24
06/19/2022 21:35:47 - INFO - __main__ - Global step 50 Train loss 9.054782 ACC 0.5 on epoch=24
06/19/2022 21:35:50 - INFO - __main__ - Step 60 Global step 60 Train loss 2.264397 on epoch=29
06/19/2022 21:35:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.799293 on epoch=34
06/19/2022 21:35:55 - INFO - __main__ - Step 80 Global step 80 Train loss 1.042050 on epoch=39
06/19/2022 21:35:58 - INFO - __main__ - Step 90 Global step 90 Train loss 1.208293 on epoch=44
06/19/2022 21:36:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.856700 on epoch=49
06/19/2022 21:36:00 - INFO - __main__ - Global step 100 Train loss 1.434146 ACC 0.375 on epoch=49
06/19/2022 21:36:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.689891 on epoch=54
06/19/2022 21:36:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.635929 on epoch=59
06/19/2022 21:36:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.683454 on epoch=64
06/19/2022 21:36:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.554282 on epoch=69
06/19/2022 21:36:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.593168 on epoch=74
06/19/2022 21:36:13 - INFO - __main__ - Global step 150 Train loss 0.631345 ACC 0.46875 on epoch=74
06/19/2022 21:36:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.436952 on epoch=79
06/19/2022 21:36:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.497202 on epoch=84
06/19/2022 21:36:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.627608 on epoch=89
06/19/2022 21:36:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.599124 on epoch=94
06/19/2022 21:36:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.510811 on epoch=99
06/19/2022 21:36:25 - INFO - __main__ - Global step 200 Train loss 0.534340 ACC 0.46875 on epoch=99
06/19/2022 21:36:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.445314 on epoch=104
06/19/2022 21:36:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.367169 on epoch=109
06/19/2022 21:36:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.296885 on epoch=114
06/19/2022 21:36:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.319538 on epoch=119
06/19/2022 21:36:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.272624 on epoch=124
06/19/2022 21:36:38 - INFO - __main__ - Global step 250 Train loss 0.340306 ACC 0.5 on epoch=124
06/19/2022 21:36:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.372177 on epoch=129
06/19/2022 21:36:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.292544 on epoch=134
06/19/2022 21:36:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.207753 on epoch=139
06/19/2022 21:36:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.304217 on epoch=144
06/19/2022 21:36:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.213044 on epoch=149
06/19/2022 21:36:51 - INFO - __main__ - Global step 300 Train loss 0.277947 ACC 0.5 on epoch=149
06/19/2022 21:36:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.195807 on epoch=154
06/19/2022 21:36:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.132290 on epoch=159
06/19/2022 21:36:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.062740 on epoch=164
06/19/2022 21:37:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.070258 on epoch=169
06/19/2022 21:37:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.036428 on epoch=174
06/19/2022 21:37:03 - INFO - __main__ - Global step 350 Train loss 0.099504 ACC 0.53125 on epoch=174
06/19/2022 21:37:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.083139 on epoch=179
06/19/2022 21:37:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.011822 on epoch=184
06/19/2022 21:37:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.013819 on epoch=189
06/19/2022 21:37:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.054732 on epoch=194
06/19/2022 21:37:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.036381 on epoch=199
06/19/2022 21:37:17 - INFO - __main__ - Global step 400 Train loss 0.039979 ACC 0.5 on epoch=199
06/19/2022 21:37:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.015582 on epoch=204
06/19/2022 21:37:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.006432 on epoch=209
06/19/2022 21:37:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.016510 on epoch=214
06/19/2022 21:37:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.064939 on epoch=219
06/19/2022 21:37:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.013026 on epoch=224
06/19/2022 21:37:29 - INFO - __main__ - Global step 450 Train loss 0.023298 ACC 0.5 on epoch=224
06/19/2022 21:37:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.022012 on epoch=229
06/19/2022 21:37:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.011207 on epoch=234
06/19/2022 21:37:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.005794 on epoch=239
06/19/2022 21:37:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002037 on epoch=244
06/19/2022 21:37:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.117404 on epoch=249
06/19/2022 21:37:41 - INFO - __main__ - Global step 500 Train loss 0.031691 ACC 0.5625 on epoch=249
06/19/2022 21:37:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.221278 on epoch=254
06/19/2022 21:37:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.089254 on epoch=259
06/19/2022 21:37:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.267143 on epoch=264
06/19/2022 21:37:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.550492 on epoch=269
06/19/2022 21:37:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.245829 on epoch=274
06/19/2022 21:37:54 - INFO - __main__ - Global step 550 Train loss 0.274799 ACC 0.46875 on epoch=274
06/19/2022 21:37:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.122098 on epoch=279
06/19/2022 21:37:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.200334 on epoch=284
06/19/2022 21:38:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.304807 on epoch=289
06/19/2022 21:38:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.480651 on epoch=294
06/19/2022 21:38:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.253897 on epoch=299
06/19/2022 21:38:07 - INFO - __main__ - Global step 600 Train loss 0.272357 ACC 0.5 on epoch=299
06/19/2022 21:38:07 - INFO - __main__ - save last model!
06/19/2022 21:38:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:38:07 - INFO - __main__ - Printing 3 examples
06/19/2022 21:38:07 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/19/2022 21:38:07 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:07 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/19/2022 21:38:07 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:07 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/19/2022 21:38:07 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:38:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:38:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:38:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:38:07 - INFO - __main__ - Printing 3 examples
06/19/2022 21:38:07 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/19/2022 21:38:07 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:07 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/19/2022 21:38:07 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:07 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/19/2022 21:38:07 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:07 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:38:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:38:07 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:38:09 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 21:38:10 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 21:38:10 - INFO - __main__ - Printing 3 examples
06/19/2022 21:38:10 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 21:38:10 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:10 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 21:38:10 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:10 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 21:38:10 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:10 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:38:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:38:10 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 21:38:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:38:11 - INFO - __main__ - Starting training!
06/19/2022 21:38:15 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.0005_8_predictions.txt
06/19/2022 21:38:15 - INFO - __main__ - ACC on test data: 0.5475
06/19/2022 21:38:15 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.0005, bsz=8, dev_performance=0.5625, test_performance=0.5475409836065573
06/19/2022 21:38:15 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.0003, bsz=8 ...
06/19/2022 21:38:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:38:16 - INFO - __main__ - Printing 3 examples
06/19/2022 21:38:16 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/19/2022 21:38:16 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:16 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/19/2022 21:38:16 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:16 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/19/2022 21:38:16 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:38:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:38:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:38:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:38:16 - INFO - __main__ - Printing 3 examples
06/19/2022 21:38:16 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/19/2022 21:38:16 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:16 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/19/2022 21:38:16 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:16 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/19/2022 21:38:16 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:38:16 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:38:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:38:16 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:38:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:38:20 - INFO - __main__ - Starting training!
06/19/2022 21:38:22 - INFO - __main__ - Step 10 Global step 10 Train loss 18.512966 on epoch=4
06/19/2022 21:38:25 - INFO - __main__ - Step 20 Global step 20 Train loss 13.394961 on epoch=9
06/19/2022 21:38:27 - INFO - __main__ - Step 30 Global step 30 Train loss 8.946405 on epoch=14
06/19/2022 21:38:30 - INFO - __main__ - Step 40 Global step 40 Train loss 6.171593 on epoch=19
06/19/2022 21:38:32 - INFO - __main__ - Step 50 Global step 50 Train loss 4.848401 on epoch=24
06/19/2022 21:38:33 - INFO - __main__ - Global step 50 Train loss 10.374866 ACC 0.5 on epoch=24
06/19/2022 21:38:35 - INFO - __main__ - Step 60 Global step 60 Train loss 3.417814 on epoch=29
06/19/2022 21:38:38 - INFO - __main__ - Step 70 Global step 70 Train loss 2.316872 on epoch=34
06/19/2022 21:38:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.597111 on epoch=39
06/19/2022 21:38:43 - INFO - __main__ - Step 90 Global step 90 Train loss 1.315001 on epoch=44
06/19/2022 21:38:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.912171 on epoch=49
06/19/2022 21:38:45 - INFO - __main__ - Global step 100 Train loss 1.911794 ACC 0.53125 on epoch=49
06/19/2022 21:38:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.900126 on epoch=54
06/19/2022 21:38:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.863416 on epoch=59
06/19/2022 21:38:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.659087 on epoch=64
06/19/2022 21:38:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.622054 on epoch=69
06/19/2022 21:38:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.660814 on epoch=74
06/19/2022 21:38:59 - INFO - __main__ - Global step 150 Train loss 0.741099 ACC 0.5 on epoch=74
06/19/2022 21:39:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.702142 on epoch=79
06/19/2022 21:39:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.509159 on epoch=84
06/19/2022 21:39:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.529595 on epoch=89
06/19/2022 21:39:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.474778 on epoch=94
06/19/2022 21:39:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.418135 on epoch=99
06/19/2022 21:39:11 - INFO - __main__ - Global step 200 Train loss 0.526762 ACC 0.46875 on epoch=99
06/19/2022 21:39:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.516649 on epoch=104
06/19/2022 21:39:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.478963 on epoch=109
06/19/2022 21:39:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.330470 on epoch=114
06/19/2022 21:39:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.548578 on epoch=119
06/19/2022 21:39:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.464475 on epoch=124
06/19/2022 21:39:24 - INFO - __main__ - Global step 250 Train loss 0.467827 ACC 0.5 on epoch=124
06/19/2022 21:39:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.478038 on epoch=129
06/19/2022 21:39:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.419422 on epoch=134
06/19/2022 21:39:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.451495 on epoch=139
06/19/2022 21:39:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.491247 on epoch=144
06/19/2022 21:39:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.351802 on epoch=149
06/19/2022 21:39:37 - INFO - __main__ - Global step 300 Train loss 0.438401 ACC 0.5 on epoch=149
06/19/2022 21:39:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.366699 on epoch=154
06/19/2022 21:39:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.356871 on epoch=159
06/19/2022 21:39:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.510481 on epoch=164
06/19/2022 21:39:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.279363 on epoch=169
06/19/2022 21:39:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.421021 on epoch=174
06/19/2022 21:39:49 - INFO - __main__ - Global step 350 Train loss 0.386887 ACC 0.59375 on epoch=174
06/19/2022 21:39:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.371612 on epoch=179
06/19/2022 21:39:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.219842 on epoch=184
06/19/2022 21:39:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.277974 on epoch=189
06/19/2022 21:40:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.299365 on epoch=194
06/19/2022 21:40:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.301898 on epoch=199
06/19/2022 21:40:02 - INFO - __main__ - Global step 400 Train loss 0.294138 ACC 0.5 on epoch=199
06/19/2022 21:40:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.214362 on epoch=204
06/19/2022 21:40:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.212177 on epoch=209
06/19/2022 21:40:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.175158 on epoch=214
06/19/2022 21:40:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.216231 on epoch=219
06/19/2022 21:40:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.189886 on epoch=224
06/19/2022 21:40:15 - INFO - __main__ - Global step 450 Train loss 0.201563 ACC 0.5 on epoch=224
06/19/2022 21:40:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.220386 on epoch=229
06/19/2022 21:40:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.150696 on epoch=234
06/19/2022 21:40:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.115919 on epoch=239
06/19/2022 21:40:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.121634 on epoch=244
06/19/2022 21:40:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.268814 on epoch=249
06/19/2022 21:40:28 - INFO - __main__ - Global step 500 Train loss 0.175490 ACC 0.4375 on epoch=249
06/19/2022 21:40:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.223831 on epoch=254
06/19/2022 21:40:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.277812 on epoch=259
06/19/2022 21:40:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.277348 on epoch=264
06/19/2022 21:40:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.275595 on epoch=269
06/19/2022 21:40:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.243429 on epoch=274
06/19/2022 21:40:41 - INFO - __main__ - Global step 550 Train loss 0.259603 ACC 0.4375 on epoch=274
06/19/2022 21:40:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.287457 on epoch=279
06/19/2022 21:40:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.163306 on epoch=284
06/19/2022 21:40:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.211434 on epoch=289
06/19/2022 21:40:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.256914 on epoch=294
06/19/2022 21:40:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.139433 on epoch=299
06/19/2022 21:40:53 - INFO - __main__ - Global step 600 Train loss 0.211709 ACC 0.375 on epoch=299
06/19/2022 21:40:53 - INFO - __main__ - save last model!
06/19/2022 21:40:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:40:54 - INFO - __main__ - Printing 3 examples
06/19/2022 21:40:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/19/2022 21:40:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:40:54 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/19/2022 21:40:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:40:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/19/2022 21:40:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:40:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:40:54 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:40:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:40:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:40:54 - INFO - __main__ - Printing 3 examples
06/19/2022 21:40:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/19/2022 21:40:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:40:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/19/2022 21:40:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:40:54 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/19/2022 21:40:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:40:54 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:40:54 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:40:54 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:40:56 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 21:40:56 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 21:40:56 - INFO - __main__ - Printing 3 examples
06/19/2022 21:40:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 21:40:56 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:40:56 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 21:40:56 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:40:56 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 21:40:56 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:40:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:40:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:40:57 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 21:40:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:40:58 - INFO - __main__ - Starting training!
06/19/2022 21:41:02 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.0003_8_predictions.txt
06/19/2022 21:41:02 - INFO - __main__ - ACC on test data: 0.5295
06/19/2022 21:41:02 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.5295081967213114
06/19/2022 21:41:02 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.0002, bsz=8 ...
06/19/2022 21:41:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:41:03 - INFO - __main__ - Printing 3 examples
06/19/2022 21:41:03 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/19/2022 21:41:03 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:41:03 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/19/2022 21:41:03 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:41:03 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/19/2022 21:41:03 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:41:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:41:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:41:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:41:03 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:41:03 - INFO - __main__ - Printing 3 examples
06/19/2022 21:41:03 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/19/2022 21:41:03 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:41:03 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/19/2022 21:41:03 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:41:03 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/19/2022 21:41:03 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:41:03 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:41:03 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:41:03 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:41:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:41:07 - INFO - __main__ - Starting training!
06/19/2022 21:41:09 - INFO - __main__ - Step 10 Global step 10 Train loss 18.237942 on epoch=4
06/19/2022 21:41:11 - INFO - __main__ - Step 20 Global step 20 Train loss 14.513977 on epoch=9
06/19/2022 21:41:14 - INFO - __main__ - Step 30 Global step 30 Train loss 9.943667 on epoch=14
06/19/2022 21:41:16 - INFO - __main__ - Step 40 Global step 40 Train loss 8.041578 on epoch=19
06/19/2022 21:41:18 - INFO - __main__ - Step 50 Global step 50 Train loss 6.561862 on epoch=24
06/19/2022 21:41:20 - INFO - __main__ - Global step 50 Train loss 11.459805 ACC 0.0 on epoch=24
06/19/2022 21:41:23 - INFO - __main__ - Step 60 Global step 60 Train loss 5.236145 on epoch=29
06/19/2022 21:41:25 - INFO - __main__ - Step 70 Global step 70 Train loss 4.424365 on epoch=34
06/19/2022 21:41:28 - INFO - __main__ - Step 80 Global step 80 Train loss 3.707605 on epoch=39
06/19/2022 21:41:30 - INFO - __main__ - Step 90 Global step 90 Train loss 3.142318 on epoch=44
06/19/2022 21:41:33 - INFO - __main__ - Step 100 Global step 100 Train loss 2.134815 on epoch=49
06/19/2022 21:41:33 - INFO - __main__ - Global step 100 Train loss 3.729049 ACC 0.53125 on epoch=49
06/19/2022 21:41:36 - INFO - __main__ - Step 110 Global step 110 Train loss 2.039449 on epoch=54
06/19/2022 21:41:38 - INFO - __main__ - Step 120 Global step 120 Train loss 1.799205 on epoch=59
06/19/2022 21:41:41 - INFO - __main__ - Step 130 Global step 130 Train loss 1.343674 on epoch=64
06/19/2022 21:41:43 - INFO - __main__ - Step 140 Global step 140 Train loss 1.187591 on epoch=69
06/19/2022 21:41:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.938548 on epoch=74
06/19/2022 21:41:46 - INFO - __main__ - Global step 150 Train loss 1.461693 ACC 0.46875 on epoch=74
06/19/2022 21:41:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.844516 on epoch=79
06/19/2022 21:41:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.684753 on epoch=84
06/19/2022 21:41:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.563166 on epoch=89
06/19/2022 21:41:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.769018 on epoch=94
06/19/2022 21:41:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.548014 on epoch=99
06/19/2022 21:41:58 - INFO - __main__ - Global step 200 Train loss 0.681893 ACC 0.5 on epoch=99
06/19/2022 21:42:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.600550 on epoch=104
06/19/2022 21:42:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.624826 on epoch=109
06/19/2022 21:42:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.628997 on epoch=114
06/19/2022 21:42:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.495099 on epoch=119
06/19/2022 21:42:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.496335 on epoch=124
06/19/2022 21:42:11 - INFO - __main__ - Global step 250 Train loss 0.569161 ACC 0.5 on epoch=124
06/19/2022 21:42:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.327226 on epoch=129
06/19/2022 21:42:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.439164 on epoch=134
06/19/2022 21:42:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.423400 on epoch=139
06/19/2022 21:42:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.445518 on epoch=144
06/19/2022 21:42:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.373967 on epoch=149
06/19/2022 21:42:23 - INFO - __main__ - Global step 300 Train loss 0.401855 ACC 0.59375 on epoch=149
06/19/2022 21:42:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.417346 on epoch=154
06/19/2022 21:42:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.375232 on epoch=159
06/19/2022 21:42:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.395395 on epoch=164
06/19/2022 21:42:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.394499 on epoch=169
06/19/2022 21:42:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.286188 on epoch=174
06/19/2022 21:42:36 - INFO - __main__ - Global step 350 Train loss 0.373732 ACC 0.59375 on epoch=174
06/19/2022 21:42:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.357652 on epoch=179
06/19/2022 21:42:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.214704 on epoch=184
06/19/2022 21:42:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.244549 on epoch=189
06/19/2022 21:42:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.322835 on epoch=194
06/19/2022 21:42:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.224391 on epoch=199
06/19/2022 21:42:49 - INFO - __main__ - Global step 400 Train loss 0.272826 ACC 0.5 on epoch=199
06/19/2022 21:42:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.226598 on epoch=204
06/19/2022 21:42:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.362122 on epoch=209
06/19/2022 21:42:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.154417 on epoch=214
06/19/2022 21:42:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.284636 on epoch=219
06/19/2022 21:43:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.207119 on epoch=224
06/19/2022 21:43:01 - INFO - __main__ - Global step 450 Train loss 0.246979 ACC 0.5625 on epoch=224
06/19/2022 21:43:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.156022 on epoch=229
06/19/2022 21:43:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.186044 on epoch=234
06/19/2022 21:43:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.155692 on epoch=239
06/19/2022 21:43:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.189334 on epoch=244
06/19/2022 21:43:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.153519 on epoch=249
06/19/2022 21:43:14 - INFO - __main__ - Global step 500 Train loss 0.168122 ACC 0.53125 on epoch=249
06/19/2022 21:43:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.161081 on epoch=254
06/19/2022 21:43:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.173157 on epoch=259
06/19/2022 21:43:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.168884 on epoch=264
06/19/2022 21:43:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.107929 on epoch=269
06/19/2022 21:43:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.135921 on epoch=274
06/19/2022 21:43:26 - INFO - __main__ - Global step 550 Train loss 0.149394 ACC 0.5625 on epoch=274
06/19/2022 21:43:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.118829 on epoch=279
06/19/2022 21:43:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.049706 on epoch=284
06/19/2022 21:43:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.066961 on epoch=289
06/19/2022 21:43:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.073278 on epoch=294
06/19/2022 21:43:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.111684 on epoch=299
06/19/2022 21:43:39 - INFO - __main__ - Global step 600 Train loss 0.084092 ACC 0.59375 on epoch=299
06/19/2022 21:43:39 - INFO - __main__ - save last model!
06/19/2022 21:43:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:43:40 - INFO - __main__ - Printing 3 examples
06/19/2022 21:43:40 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/19/2022 21:43:40 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:40 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/19/2022 21:43:40 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:40 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/19/2022 21:43:40 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:43:40 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:43:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:43:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:43:40 - INFO - __main__ - Printing 3 examples
06/19/2022 21:43:40 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/19/2022 21:43:40 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:40 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/19/2022 21:43:40 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:40 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/19/2022 21:43:40 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:40 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:43:40 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:43:40 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:43:42 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 21:43:42 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 21:43:42 - INFO - __main__ - Printing 3 examples
06/19/2022 21:43:42 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 21:43:42 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:42 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 21:43:42 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:42 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 21:43:42 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:42 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:43:42 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:43:43 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 21:43:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:43:44 - INFO - __main__ - Starting training!
06/19/2022 21:43:48 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.0002_8_predictions.txt
06/19/2022 21:43:48 - INFO - __main__ - ACC on test data: 0.5557
06/19/2022 21:43:48 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.0002, bsz=8, dev_performance=0.59375, test_performance=0.5557377049180328
06/19/2022 21:43:48 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_100, lr=0.0001, bsz=8 ...
06/19/2022 21:43:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:43:49 - INFO - __main__ - Printing 3 examples
06/19/2022 21:43:49 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a bruise circling a insect bite,is this common? [SEP] question 2: I got a bug bite, now I notice a bruise around it, is that normal?
06/19/2022 21:43:49 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:49 - INFO - __main__ -  [medical_questions_pairs] question 1: My first sonogram said I was 7 weeks pregnant due date 3/22. Next appointment (5 weeks later) sono measured bby at 13weeks. Duedate changed to 3/15. Y? [SEP] question 2: In my first sonogram at 7 weeks my due date is 3/22 and now at 13 weeks my due date is 3/15. Which is correct?
06/19/2022 21:43:49 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:49 - INFO - __main__ -  [medical_questions_pairs] question 1: I have arthritis in my knees and lower back. Normally when I get out of bed they are stiff. My hips achey and stiff are if I lay on my back at night? [SEP] question 2: How can I manage stiffness in my knees, lower back etc. I do have arthritis
06/19/2022 21:43:49 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:43:49 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:43:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:43:49 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:43:49 - INFO - __main__ - Printing 3 examples
06/19/2022 21:43:49 - INFO - __main__ -  [medical_questions_pairs] question 1: I came on my period two time last month this month my boyfriend came inside me and I'm bleeding 4 days after we had sex? [SEP] question 2: We had unprotected sex 4 days back while I experienced two period last month. Now I am bleeding ever since we had an intercourse. 
06/19/2022 21:43:49 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:49 - INFO - __main__ -  [medical_questions_pairs] question 1: Rash all over body starts on head itchy but bearable, then spreads all over body rapidly first looks like nettle sting but then turns extremely red ? [SEP] question 2: I was picking weeds in my backyard and noticed an itchy rash all over my body which starts on head and spreads rapidly to the rest of the body, turns very red and looks like stinging nettle or poison ivy. How do I know what it could be? Should I go to the ER?
06/19/2022 21:43:49 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:49 - INFO - __main__ -  [medical_questions_pairs] question 1: After I had intercourse with hubby I noticed slight fresh blood and burning sensation and urge to urinate. What might be the cause? [SEP] question 2: Okay, so 1st, this has never happened before. But after doing it with my husband today, I've had some bleeding, ithcing and an urgency to pee! Is it a UTI?
06/19/2022 21:43:49 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:43:49 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:43:49 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:43:49 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:43:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:43:53 - INFO - __main__ - Starting training!
06/19/2022 21:43:55 - INFO - __main__ - Step 10 Global step 10 Train loss 18.328205 on epoch=4
06/19/2022 21:43:57 - INFO - __main__ - Step 20 Global step 20 Train loss 16.588856 on epoch=9
06/19/2022 21:43:59 - INFO - __main__ - Step 30 Global step 30 Train loss 10.834525 on epoch=14
06/19/2022 21:44:02 - INFO - __main__ - Step 40 Global step 40 Train loss 9.646507 on epoch=19
06/19/2022 21:44:04 - INFO - __main__ - Step 50 Global step 50 Train loss 8.626382 on epoch=24
06/19/2022 21:44:07 - INFO - __main__ - Global step 50 Train loss 12.804895 ACC 0.0 on epoch=24
06/19/2022 21:44:10 - INFO - __main__ - Step 60 Global step 60 Train loss 8.102094 on epoch=29
06/19/2022 21:44:12 - INFO - __main__ - Step 70 Global step 70 Train loss 6.743029 on epoch=34
06/19/2022 21:44:15 - INFO - __main__ - Step 80 Global step 80 Train loss 6.342576 on epoch=39
06/19/2022 21:44:17 - INFO - __main__ - Step 90 Global step 90 Train loss 6.298541 on epoch=44
06/19/2022 21:44:20 - INFO - __main__ - Step 100 Global step 100 Train loss 4.967235 on epoch=49
06/19/2022 21:44:22 - INFO - __main__ - Global step 100 Train loss 6.490695 ACC 0.0 on epoch=49
06/19/2022 21:44:24 - INFO - __main__ - Step 110 Global step 110 Train loss 5.106722 on epoch=54
06/19/2022 21:44:27 - INFO - __main__ - Step 120 Global step 120 Train loss 4.678239 on epoch=59
06/19/2022 21:44:29 - INFO - __main__ - Step 130 Global step 130 Train loss 3.414955 on epoch=64
06/19/2022 21:44:32 - INFO - __main__ - Step 140 Global step 140 Train loss 3.441778 on epoch=69
06/19/2022 21:44:34 - INFO - __main__ - Step 150 Global step 150 Train loss 2.879587 on epoch=74
06/19/2022 21:44:34 - INFO - __main__ - Global step 150 Train loss 3.904256 ACC 0.5 on epoch=74
06/19/2022 21:44:37 - INFO - __main__ - Step 160 Global step 160 Train loss 2.753782 on epoch=79
06/19/2022 21:44:40 - INFO - __main__ - Step 170 Global step 170 Train loss 1.447110 on epoch=84
06/19/2022 21:44:42 - INFO - __main__ - Step 180 Global step 180 Train loss 1.525288 on epoch=89
06/19/2022 21:44:45 - INFO - __main__ - Step 190 Global step 190 Train loss 1.329550 on epoch=94
06/19/2022 21:44:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.967410 on epoch=99
06/19/2022 21:44:47 - INFO - __main__ - Global step 200 Train loss 1.604628 ACC 0.53125 on epoch=99
06/19/2022 21:44:50 - INFO - __main__ - Step 210 Global step 210 Train loss 1.066308 on epoch=104
06/19/2022 21:44:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.929689 on epoch=109
06/19/2022 21:44:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.873893 on epoch=114
06/19/2022 21:44:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.767223 on epoch=119
06/19/2022 21:45:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.774645 on epoch=124
06/19/2022 21:45:01 - INFO - __main__ - Global step 250 Train loss 0.882352 ACC 0.46875 on epoch=124
06/19/2022 21:45:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.746282 on epoch=129
06/19/2022 21:45:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.567542 on epoch=134
06/19/2022 21:45:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.674801 on epoch=139
06/19/2022 21:45:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.763925 on epoch=144
06/19/2022 21:45:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.649881 on epoch=149
06/19/2022 21:45:13 - INFO - __main__ - Global step 300 Train loss 0.680487 ACC 0.5 on epoch=149
06/19/2022 21:45:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.376681 on epoch=154
06/19/2022 21:45:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.344922 on epoch=159
06/19/2022 21:45:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.421222 on epoch=164
06/19/2022 21:45:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.541286 on epoch=169
06/19/2022 21:45:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.601493 on epoch=174
06/19/2022 21:45:26 - INFO - __main__ - Global step 350 Train loss 0.457121 ACC 0.5 on epoch=174
06/19/2022 21:45:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.543256 on epoch=179
06/19/2022 21:45:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.503356 on epoch=184
06/19/2022 21:45:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.577731 on epoch=189
06/19/2022 21:45:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.434373 on epoch=194
06/19/2022 21:45:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.458723 on epoch=199
06/19/2022 21:45:39 - INFO - __main__ - Global step 400 Train loss 0.503488 ACC 0.5625 on epoch=199
06/19/2022 21:45:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.713529 on epoch=204
06/19/2022 21:45:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.526357 on epoch=209
06/19/2022 21:45:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.387896 on epoch=214
06/19/2022 21:45:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.456520 on epoch=219
06/19/2022 21:45:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.457191 on epoch=224
06/19/2022 21:45:52 - INFO - __main__ - Global step 450 Train loss 0.508299 ACC 0.46875 on epoch=224
06/19/2022 21:45:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.403555 on epoch=229
06/19/2022 21:45:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.494326 on epoch=234
06/19/2022 21:46:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.332156 on epoch=239
06/19/2022 21:46:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.392230 on epoch=244
06/19/2022 21:46:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.581065 on epoch=249
06/19/2022 21:46:05 - INFO - __main__ - Global step 500 Train loss 0.440666 ACC 0.5 on epoch=249
06/19/2022 21:46:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.372220 on epoch=254
06/19/2022 21:46:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.369672 on epoch=259
06/19/2022 21:46:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.473292 on epoch=264
06/19/2022 21:46:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.512437 on epoch=269
06/19/2022 21:46:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.395049 on epoch=274
06/19/2022 21:46:18 - INFO - __main__ - Global step 550 Train loss 0.424534 ACC 0.46875 on epoch=274
06/19/2022 21:46:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.343129 on epoch=279
06/19/2022 21:46:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.376333 on epoch=284
06/19/2022 21:46:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.369782 on epoch=289
06/19/2022 21:46:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.293529 on epoch=294
06/19/2022 21:46:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.306716 on epoch=299
06/19/2022 21:46:30 - INFO - __main__ - Global step 600 Train loss 0.337898 ACC 0.59375 on epoch=299
06/19/2022 21:46:31 - INFO - __main__ - save last model!
06/19/2022 21:46:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:46:31 - INFO - __main__ - Printing 3 examples
06/19/2022 21:46:31 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/19/2022 21:46:31 - INFO - __main__ - ['Similar']
06/19/2022 21:46:31 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/19/2022 21:46:31 - INFO - __main__ - ['Similar']
06/19/2022 21:46:31 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/19/2022 21:46:31 - INFO - __main__ - ['Similar']
06/19/2022 21:46:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:46:31 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:46:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:46:31 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:46:31 - INFO - __main__ - Printing 3 examples
06/19/2022 21:46:31 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/19/2022 21:46:31 - INFO - __main__ - ['Similar']
06/19/2022 21:46:31 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/19/2022 21:46:31 - INFO - __main__ - ['Similar']
06/19/2022 21:46:31 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/19/2022 21:46:31 - INFO - __main__ - ['Similar']
06/19/2022 21:46:31 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:46:31 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:46:31 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:46:33 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 21:46:34 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 21:46:34 - INFO - __main__ - Printing 3 examples
06/19/2022 21:46:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 21:46:34 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:46:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 21:46:34 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:46:34 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 21:46:34 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:46:34 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:46:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:46:35 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 21:46:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:46:35 - INFO - __main__ - Starting training!
06/19/2022 21:46:39 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_100_0.0001_8_predictions.txt
06/19/2022 21:46:39 - INFO - __main__ - ACC on test data: 0.5443
06/19/2022 21:46:39 - INFO - __main__ - prefix=medical_questions_pairs_16_100, lr=0.0001, bsz=8, dev_performance=0.59375, test_performance=0.5442622950819672
06/19/2022 21:46:39 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.0005, bsz=8 ...
06/19/2022 21:46:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:46:40 - INFO - __main__ - Printing 3 examples
06/19/2022 21:46:40 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/19/2022 21:46:40 - INFO - __main__ - ['Similar']
06/19/2022 21:46:40 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/19/2022 21:46:40 - INFO - __main__ - ['Similar']
06/19/2022 21:46:40 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/19/2022 21:46:40 - INFO - __main__ - ['Similar']
06/19/2022 21:46:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:46:40 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:46:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:46:40 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:46:40 - INFO - __main__ - Printing 3 examples
06/19/2022 21:46:40 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/19/2022 21:46:40 - INFO - __main__ - ['Similar']
06/19/2022 21:46:40 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/19/2022 21:46:40 - INFO - __main__ - ['Similar']
06/19/2022 21:46:40 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/19/2022 21:46:40 - INFO - __main__ - ['Similar']
06/19/2022 21:46:40 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:46:40 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:46:41 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:46:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:46:45 - INFO - __main__ - Starting training!
06/19/2022 21:46:47 - INFO - __main__ - Step 10 Global step 10 Train loss 18.027409 on epoch=4
06/19/2022 21:46:49 - INFO - __main__ - Step 20 Global step 20 Train loss 11.006708 on epoch=9
06/19/2022 21:46:51 - INFO - __main__ - Step 30 Global step 30 Train loss 7.006364 on epoch=14
06/19/2022 21:46:54 - INFO - __main__ - Step 40 Global step 40 Train loss 5.298869 on epoch=19
06/19/2022 21:46:56 - INFO - __main__ - Step 50 Global step 50 Train loss 2.732635 on epoch=24
06/19/2022 21:46:57 - INFO - __main__ - Global step 50 Train loss 8.814397 ACC 0.53125 on epoch=24
06/19/2022 21:46:59 - INFO - __main__ - Step 60 Global step 60 Train loss 2.166094 on epoch=29
06/19/2022 21:47:02 - INFO - __main__ - Step 70 Global step 70 Train loss 1.258247 on epoch=34
06/19/2022 21:47:04 - INFO - __main__ - Step 80 Global step 80 Train loss 1.186198 on epoch=39
06/19/2022 21:47:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.795285 on epoch=44
06/19/2022 21:47:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.738268 on epoch=49
06/19/2022 21:47:09 - INFO - __main__ - Global step 100 Train loss 1.228819 ACC 0.4375 on epoch=49
06/19/2022 21:47:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.808724 on epoch=54
06/19/2022 21:47:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.571278 on epoch=59
06/19/2022 21:47:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.566275 on epoch=64
06/19/2022 21:47:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.493760 on epoch=69
06/19/2022 21:47:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.409211 on epoch=74
06/19/2022 21:47:22 - INFO - __main__ - Global step 150 Train loss 0.569850 ACC 0.5 on epoch=74
06/19/2022 21:47:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.566741 on epoch=79
06/19/2022 21:47:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.559090 on epoch=84
06/19/2022 21:47:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.480430 on epoch=89
06/19/2022 21:47:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.410914 on epoch=94
06/19/2022 21:47:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.278462 on epoch=99
06/19/2022 21:47:35 - INFO - __main__ - Global step 200 Train loss 0.459127 ACC 0.5 on epoch=99
06/19/2022 21:47:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.323705 on epoch=104
06/19/2022 21:47:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.345549 on epoch=109
06/19/2022 21:47:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.204587 on epoch=114
06/19/2022 21:47:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.468489 on epoch=119
06/19/2022 21:47:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.209646 on epoch=124
06/19/2022 21:47:48 - INFO - __main__ - Global step 250 Train loss 0.310395 ACC 0.46875 on epoch=124
06/19/2022 21:47:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.169084 on epoch=129
06/19/2022 21:47:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.209017 on epoch=134
06/19/2022 21:47:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.171909 on epoch=139
06/19/2022 21:47:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.142410 on epoch=144
06/19/2022 21:48:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.097104 on epoch=149
06/19/2022 21:48:01 - INFO - __main__ - Global step 300 Train loss 0.157905 ACC 0.5 on epoch=149
06/19/2022 21:48:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.044965 on epoch=154
06/19/2022 21:48:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.062610 on epoch=159
06/19/2022 21:48:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.052645 on epoch=164
06/19/2022 21:48:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.161819 on epoch=169
06/19/2022 21:48:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.053467 on epoch=174
06/19/2022 21:48:13 - INFO - __main__ - Global step 350 Train loss 0.075101 ACC 0.5625 on epoch=174
06/19/2022 21:48:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.626814 on epoch=179
06/19/2022 21:48:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.100811 on epoch=184
06/19/2022 21:48:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.085802 on epoch=189
06/19/2022 21:48:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.024225 on epoch=194
06/19/2022 21:48:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.070366 on epoch=199
06/19/2022 21:48:26 - INFO - __main__ - Global step 400 Train loss 0.181604 ACC 0.5 on epoch=199
06/19/2022 21:48:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.100018 on epoch=204
06/19/2022 21:48:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.223529 on epoch=209
06/19/2022 21:48:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.110511 on epoch=214
06/19/2022 21:48:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.074925 on epoch=219
06/19/2022 21:48:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.146650 on epoch=224
06/19/2022 21:48:39 - INFO - __main__ - Global step 450 Train loss 0.131127 ACC 0.53125 on epoch=224
06/19/2022 21:48:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.035425 on epoch=229
06/19/2022 21:48:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.034022 on epoch=234
06/19/2022 21:48:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.093702 on epoch=239
06/19/2022 21:48:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.151162 on epoch=244
06/19/2022 21:48:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.035184 on epoch=249
06/19/2022 21:48:52 - INFO - __main__ - Global step 500 Train loss 0.069899 ACC 0.5625 on epoch=249
06/19/2022 21:48:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.067140 on epoch=254
06/19/2022 21:48:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.029003 on epoch=259
06/19/2022 21:48:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.042477 on epoch=264
06/19/2022 21:49:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.011313 on epoch=269
06/19/2022 21:49:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.015043 on epoch=274
06/19/2022 21:49:05 - INFO - __main__ - Global step 550 Train loss 0.032995 ACC 0.53125 on epoch=274
06/19/2022 21:49:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.046789 on epoch=279
06/19/2022 21:49:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.015395 on epoch=284
06/19/2022 21:49:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.024087 on epoch=289
06/19/2022 21:49:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.042629 on epoch=294
06/19/2022 21:49:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.041104 on epoch=299
06/19/2022 21:49:18 - INFO - __main__ - Global step 600 Train loss 0.034001 ACC 0.5625 on epoch=299
06/19/2022 21:49:18 - INFO - __main__ - save last model!
06/19/2022 21:49:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:49:18 - INFO - __main__ - Printing 3 examples
06/19/2022 21:49:18 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/19/2022 21:49:18 - INFO - __main__ - ['Similar']
06/19/2022 21:49:18 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/19/2022 21:49:18 - INFO - __main__ - ['Similar']
06/19/2022 21:49:18 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/19/2022 21:49:18 - INFO - __main__ - ['Similar']
06/19/2022 21:49:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:49:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:49:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:49:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:49:18 - INFO - __main__ - Printing 3 examples
06/19/2022 21:49:18 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/19/2022 21:49:18 - INFO - __main__ - ['Similar']
06/19/2022 21:49:18 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/19/2022 21:49:18 - INFO - __main__ - ['Similar']
06/19/2022 21:49:18 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/19/2022 21:49:18 - INFO - __main__ - ['Similar']
06/19/2022 21:49:18 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:49:18 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:49:18 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:49:20 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 21:49:20 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 21:49:20 - INFO - __main__ - Printing 3 examples
06/19/2022 21:49:20 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 21:49:20 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:49:20 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 21:49:20 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:49:20 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 21:49:20 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:49:20 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:49:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:49:21 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 21:49:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:49:23 - INFO - __main__ - Starting training!
06/19/2022 21:49:26 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.0005_8_predictions.txt
06/19/2022 21:49:26 - INFO - __main__ - ACC on test data: 0.5607
06/19/2022 21:49:26 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.0005, bsz=8, dev_performance=0.5625, test_performance=0.5606557377049181
06/19/2022 21:49:26 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.0003, bsz=8 ...
06/19/2022 21:49:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:49:27 - INFO - __main__ - Printing 3 examples
06/19/2022 21:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/19/2022 21:49:27 - INFO - __main__ - ['Similar']
06/19/2022 21:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/19/2022 21:49:27 - INFO - __main__ - ['Similar']
06/19/2022 21:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/19/2022 21:49:27 - INFO - __main__ - ['Similar']
06/19/2022 21:49:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:49:27 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:49:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:49:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:49:27 - INFO - __main__ - Printing 3 examples
06/19/2022 21:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/19/2022 21:49:27 - INFO - __main__ - ['Similar']
06/19/2022 21:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/19/2022 21:49:27 - INFO - __main__ - ['Similar']
06/19/2022 21:49:27 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/19/2022 21:49:27 - INFO - __main__ - ['Similar']
06/19/2022 21:49:27 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:49:27 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:49:27 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:49:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:49:31 - INFO - __main__ - Starting training!
06/19/2022 21:49:33 - INFO - __main__ - Step 10 Global step 10 Train loss 17.924284 on epoch=4
06/19/2022 21:49:36 - INFO - __main__ - Step 20 Global step 20 Train loss 13.275493 on epoch=9
06/19/2022 21:49:38 - INFO - __main__ - Step 30 Global step 30 Train loss 8.351721 on epoch=14
06/19/2022 21:49:41 - INFO - __main__ - Step 40 Global step 40 Train loss 6.344050 on epoch=19
06/19/2022 21:49:43 - INFO - __main__ - Step 50 Global step 50 Train loss 4.842376 on epoch=24
06/19/2022 21:49:45 - INFO - __main__ - Global step 50 Train loss 10.147585 ACC 0.0 on epoch=24
06/19/2022 21:49:48 - INFO - __main__ - Step 60 Global step 60 Train loss 5.268292 on epoch=29
06/19/2022 21:49:50 - INFO - __main__ - Step 70 Global step 70 Train loss 2.946805 on epoch=34
06/19/2022 21:49:53 - INFO - __main__ - Step 80 Global step 80 Train loss 1.473765 on epoch=39
06/19/2022 21:49:55 - INFO - __main__ - Step 90 Global step 90 Train loss 1.840880 on epoch=44
06/19/2022 21:49:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.877876 on epoch=49
06/19/2022 21:49:58 - INFO - __main__ - Global step 100 Train loss 2.481524 ACC 0.5 on epoch=49
06/19/2022 21:50:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.142376 on epoch=54
06/19/2022 21:50:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.598720 on epoch=59
06/19/2022 21:50:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.580652 on epoch=64
06/19/2022 21:50:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.771167 on epoch=69
06/19/2022 21:50:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.725707 on epoch=74
06/19/2022 21:50:11 - INFO - __main__ - Global step 150 Train loss 0.763724 ACC 0.5 on epoch=74
06/19/2022 21:50:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.716130 on epoch=79
06/19/2022 21:50:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.579284 on epoch=84
06/19/2022 21:50:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.667934 on epoch=89
06/19/2022 21:50:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.514468 on epoch=94
06/19/2022 21:50:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.748785 on epoch=99
06/19/2022 21:50:24 - INFO - __main__ - Global step 200 Train loss 0.645320 ACC 0.46875 on epoch=99
06/19/2022 21:50:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.632611 on epoch=104
06/19/2022 21:50:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.727111 on epoch=109
06/19/2022 21:50:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.588828 on epoch=114
06/19/2022 21:50:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.469771 on epoch=119
06/19/2022 21:50:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.423510 on epoch=124
06/19/2022 21:50:37 - INFO - __main__ - Global step 250 Train loss 0.568366 ACC 0.5 on epoch=124
06/19/2022 21:50:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.507113 on epoch=129
06/19/2022 21:50:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.440493 on epoch=134
06/19/2022 21:50:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.482143 on epoch=139
06/19/2022 21:50:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.553277 on epoch=144
06/19/2022 21:50:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.431429 on epoch=149
06/19/2022 21:50:49 - INFO - __main__ - Global step 300 Train loss 0.482891 ACC 0.5625 on epoch=149
06/19/2022 21:50:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.472908 on epoch=154
06/19/2022 21:50:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.412311 on epoch=159
06/19/2022 21:50:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.370337 on epoch=164
06/19/2022 21:51:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.434194 on epoch=169
06/19/2022 21:51:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.485385 on epoch=174
06/19/2022 21:51:02 - INFO - __main__ - Global step 350 Train loss 0.435027 ACC 0.53125 on epoch=174
06/19/2022 21:51:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.414553 on epoch=179
06/19/2022 21:51:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.379088 on epoch=184
06/19/2022 21:51:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.431762 on epoch=189
06/19/2022 21:51:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.342448 on epoch=194
06/19/2022 21:51:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.386814 on epoch=199
06/19/2022 21:51:15 - INFO - __main__ - Global step 400 Train loss 0.390933 ACC 0.5 on epoch=199
06/19/2022 21:51:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.362945 on epoch=204
06/19/2022 21:51:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.417479 on epoch=209
06/19/2022 21:51:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.349373 on epoch=214
06/19/2022 21:51:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.332044 on epoch=219
06/19/2022 21:51:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.389674 on epoch=224
06/19/2022 21:51:28 - INFO - __main__ - Global step 450 Train loss 0.370303 ACC 0.59375 on epoch=224
06/19/2022 21:51:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.391228 on epoch=229
06/19/2022 21:51:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.282639 on epoch=234
06/19/2022 21:51:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.332979 on epoch=239
06/19/2022 21:51:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.411582 on epoch=244
06/19/2022 21:51:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.274404 on epoch=249
06/19/2022 21:51:41 - INFO - __main__ - Global step 500 Train loss 0.338566 ACC 0.5 on epoch=249
06/19/2022 21:51:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.348747 on epoch=254
06/19/2022 21:51:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.377781 on epoch=259
06/19/2022 21:51:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.312571 on epoch=264
06/19/2022 21:51:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.375691 on epoch=269
06/19/2022 21:51:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.349515 on epoch=274
06/19/2022 21:51:54 - INFO - __main__ - Global step 550 Train loss 0.352861 ACC 0.59375 on epoch=274
06/19/2022 21:51:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.335116 on epoch=279
06/19/2022 21:51:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.309525 on epoch=284
06/19/2022 21:52:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.316544 on epoch=289
06/19/2022 21:52:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.312939 on epoch=294
06/19/2022 21:52:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.336254 on epoch=299
06/19/2022 21:52:07 - INFO - __main__ - Global step 600 Train loss 0.322075 ACC 0.53125 on epoch=299
06/19/2022 21:52:07 - INFO - __main__ - save last model!
06/19/2022 21:52:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:52:07 - INFO - __main__ - Printing 3 examples
06/19/2022 21:52:07 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/19/2022 21:52:07 - INFO - __main__ - ['Similar']
06/19/2022 21:52:07 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/19/2022 21:52:07 - INFO - __main__ - ['Similar']
06/19/2022 21:52:07 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/19/2022 21:52:07 - INFO - __main__ - ['Similar']
06/19/2022 21:52:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:52:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:52:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:52:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:52:07 - INFO - __main__ - Printing 3 examples
06/19/2022 21:52:07 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/19/2022 21:52:07 - INFO - __main__ - ['Similar']
06/19/2022 21:52:07 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/19/2022 21:52:07 - INFO - __main__ - ['Similar']
06/19/2022 21:52:07 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/19/2022 21:52:07 - INFO - __main__ - ['Similar']
06/19/2022 21:52:07 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:52:07 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:52:08 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:52:10 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 21:52:10 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 21:52:10 - INFO - __main__ - Printing 3 examples
06/19/2022 21:52:10 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 21:52:10 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:52:10 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 21:52:10 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:52:10 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 21:52:10 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:52:10 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:52:10 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:52:11 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 21:52:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:52:11 - INFO - __main__ - Starting training!
06/19/2022 21:52:15 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.0003_8_predictions.txt
06/19/2022 21:52:15 - INFO - __main__ - ACC on test data: 0.5410
06/19/2022 21:52:15 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.5409836065573771
06/19/2022 21:52:16 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.0002, bsz=8 ...
06/19/2022 21:52:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:52:16 - INFO - __main__ - Printing 3 examples
06/19/2022 21:52:16 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/19/2022 21:52:16 - INFO - __main__ - ['Similar']
06/19/2022 21:52:16 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/19/2022 21:52:16 - INFO - __main__ - ['Similar']
06/19/2022 21:52:16 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/19/2022 21:52:16 - INFO - __main__ - ['Similar']
06/19/2022 21:52:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:52:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:52:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:52:16 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:52:16 - INFO - __main__ - Printing 3 examples
06/19/2022 21:52:16 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/19/2022 21:52:16 - INFO - __main__ - ['Similar']
06/19/2022 21:52:16 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/19/2022 21:52:16 - INFO - __main__ - ['Similar']
06/19/2022 21:52:16 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/19/2022 21:52:16 - INFO - __main__ - ['Similar']
06/19/2022 21:52:16 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:52:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:52:17 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:52:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:52:20 - INFO - __main__ - Starting training!
06/19/2022 21:52:22 - INFO - __main__ - Step 10 Global step 10 Train loss 19.682741 on epoch=4
06/19/2022 21:52:25 - INFO - __main__ - Step 20 Global step 20 Train loss 13.492709 on epoch=9
06/19/2022 21:52:27 - INFO - __main__ - Step 30 Global step 30 Train loss 9.288360 on epoch=14
06/19/2022 21:52:30 - INFO - __main__ - Step 40 Global step 40 Train loss 7.208647 on epoch=19
06/19/2022 21:52:32 - INFO - __main__ - Step 50 Global step 50 Train loss 4.941666 on epoch=24
06/19/2022 21:52:34 - INFO - __main__ - Global step 50 Train loss 10.922825 ACC 0.0 on epoch=24
06/19/2022 21:52:36 - INFO - __main__ - Step 60 Global step 60 Train loss 5.410664 on epoch=29
06/19/2022 21:52:39 - INFO - __main__ - Step 70 Global step 70 Train loss 3.762475 on epoch=34
06/19/2022 21:52:41 - INFO - __main__ - Step 80 Global step 80 Train loss 2.627687 on epoch=39
06/19/2022 21:52:44 - INFO - __main__ - Step 90 Global step 90 Train loss 2.269985 on epoch=44
06/19/2022 21:52:47 - INFO - __main__ - Step 100 Global step 100 Train loss 1.578277 on epoch=49
06/19/2022 21:52:47 - INFO - __main__ - Global step 100 Train loss 3.129817 ACC 0.53125 on epoch=49
06/19/2022 21:52:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.932884 on epoch=54
06/19/2022 21:52:53 - INFO - __main__ - Step 120 Global step 120 Train loss 1.105725 on epoch=59
06/19/2022 21:52:55 - INFO - __main__ - Step 130 Global step 130 Train loss 1.224160 on epoch=64
06/19/2022 21:52:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.813686 on epoch=69
06/19/2022 21:53:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.771600 on epoch=74
06/19/2022 21:53:00 - INFO - __main__ - Global step 150 Train loss 0.969611 ACC 0.5 on epoch=74
06/19/2022 21:53:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.684528 on epoch=79
06/19/2022 21:53:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.781051 on epoch=84
06/19/2022 21:53:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.532825 on epoch=89
06/19/2022 21:53:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.507623 on epoch=94
06/19/2022 21:53:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.585097 on epoch=99
06/19/2022 21:53:13 - INFO - __main__ - Global step 200 Train loss 0.618225 ACC 0.53125 on epoch=99
06/19/2022 21:53:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.492887 on epoch=104
06/19/2022 21:53:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.596474 on epoch=109
06/19/2022 21:53:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.564930 on epoch=114
06/19/2022 21:53:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.524511 on epoch=119
06/19/2022 21:53:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.569750 on epoch=124
06/19/2022 21:53:26 - INFO - __main__ - Global step 250 Train loss 0.549710 ACC 0.53125 on epoch=124
06/19/2022 21:53:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.389267 on epoch=129
06/19/2022 21:53:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.340658 on epoch=134
06/19/2022 21:53:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.617800 on epoch=139
06/19/2022 21:53:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.678938 on epoch=144
06/19/2022 21:53:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.405357 on epoch=149
06/19/2022 21:53:38 - INFO - __main__ - Global step 300 Train loss 0.486404 ACC 0.4375 on epoch=149
06/19/2022 21:53:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.599077 on epoch=154
06/19/2022 21:53:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.405718 on epoch=159
06/19/2022 21:53:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.335553 on epoch=164
06/19/2022 21:53:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.513146 on epoch=169
06/19/2022 21:53:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.383430 on epoch=174
06/19/2022 21:53:51 - INFO - __main__ - Global step 350 Train loss 0.447384 ACC 0.59375 on epoch=174
06/19/2022 21:53:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.549532 on epoch=179
06/19/2022 21:53:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.456885 on epoch=184
06/19/2022 21:53:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.420833 on epoch=189
06/19/2022 21:54:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.448006 on epoch=194
06/19/2022 21:54:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.639728 on epoch=199
06/19/2022 21:54:04 - INFO - __main__ - Global step 400 Train loss 0.502997 ACC 0.5 on epoch=199
06/19/2022 21:54:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.311383 on epoch=204
06/19/2022 21:54:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.383047 on epoch=209
06/19/2022 21:54:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.403102 on epoch=214
06/19/2022 21:54:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.336082 on epoch=219
06/19/2022 21:54:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.417867 on epoch=224
06/19/2022 21:54:17 - INFO - __main__ - Global step 450 Train loss 0.370296 ACC 0.4375 on epoch=224
06/19/2022 21:54:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.372648 on epoch=229
06/19/2022 21:54:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.468271 on epoch=234
06/19/2022 21:54:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.451384 on epoch=239
06/19/2022 21:54:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.420683 on epoch=244
06/19/2022 21:54:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.351951 on epoch=249
06/19/2022 21:54:29 - INFO - __main__ - Global step 500 Train loss 0.412987 ACC 0.59375 on epoch=249
06/19/2022 21:54:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.291916 on epoch=254
06/19/2022 21:54:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.479914 on epoch=259
06/19/2022 21:54:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.328509 on epoch=264
06/19/2022 21:54:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.318320 on epoch=269
06/19/2022 21:54:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.327685 on epoch=274
06/19/2022 21:54:42 - INFO - __main__ - Global step 550 Train loss 0.349269 ACC 0.59375 on epoch=274
06/19/2022 21:54:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.358176 on epoch=279
06/19/2022 21:54:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.370271 on epoch=284
06/19/2022 21:54:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.323581 on epoch=289
06/19/2022 21:54:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.352941 on epoch=294
06/19/2022 21:54:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.327108 on epoch=299
06/19/2022 21:54:55 - INFO - __main__ - Global step 600 Train loss 0.346415 ACC 0.5625 on epoch=299
06/19/2022 21:54:55 - INFO - __main__ - save last model!
06/19/2022 21:54:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:54:56 - INFO - __main__ - Printing 3 examples
06/19/2022 21:54:56 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/19/2022 21:54:56 - INFO - __main__ - ['Similar']
06/19/2022 21:54:56 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/19/2022 21:54:56 - INFO - __main__ - ['Similar']
06/19/2022 21:54:56 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/19/2022 21:54:56 - INFO - __main__ - ['Similar']
06/19/2022 21:54:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:54:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:54:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:54:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:54:56 - INFO - __main__ - Printing 3 examples
06/19/2022 21:54:56 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/19/2022 21:54:56 - INFO - __main__ - ['Similar']
06/19/2022 21:54:56 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/19/2022 21:54:56 - INFO - __main__ - ['Similar']
06/19/2022 21:54:56 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/19/2022 21:54:56 - INFO - __main__ - ['Similar']
06/19/2022 21:54:56 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:54:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:54:56 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:54:58 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 21:54:58 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 21:54:58 - INFO - __main__ - Printing 3 examples
06/19/2022 21:54:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 21:54:58 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:54:58 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 21:54:58 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:54:58 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 21:54:58 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:54:58 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:54:58 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:54:59 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 21:55:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:55:00 - INFO - __main__ - Starting training!
06/19/2022 21:55:03 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.0002_8_predictions.txt
06/19/2022 21:55:03 - INFO - __main__ - ACC on test data: 0.4934
06/19/2022 21:55:03 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.0002, bsz=8, dev_performance=0.59375, test_performance=0.4934426229508197
06/19/2022 21:55:03 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_13, lr=0.0001, bsz=8 ...
06/19/2022 21:55:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:55:04 - INFO - __main__ - Printing 3 examples
06/19/2022 21:55:04 - INFO - __main__ -  [medical_questions_pairs] question 1: Hi I just found out I contracted an std. My doctor prescribed me Azithromicyn 500, they are too big to swallow, am I able to crush them up with food? [SEP] question 2: I have been given Azithromycin as  I am diagnosed with Chlamydia. Should my partner also get treatment for the same?
06/19/2022 21:55:04 - INFO - __main__ - ['Similar']
06/19/2022 21:55:04 - INFO - __main__ -  [medical_questions_pairs] question 1: My 5 yr. Old daughter had a urine test last week and they found a trace of blood, calcium is 2.4 and creatinine is 3.3, should I be concerned? [SEP] question 2: What is the management of blood in urine for my 5 year old daughter?
06/19/2022 21:55:04 - INFO - __main__ - ['Similar']
06/19/2022 21:55:04 - INFO - __main__ -  [medical_questions_pairs] question 1: Side effect of prolixin (fluphenazine)? [SEP] question 2: What is Fluphenazine used for?
06/19/2022 21:55:04 - INFO - __main__ - ['Similar']
06/19/2022 21:55:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:55:04 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:55:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:55:04 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:55:04 - INFO - __main__ - Printing 3 examples
06/19/2022 21:55:04 - INFO - __main__ -  [medical_questions_pairs] question 1: If I go to ER with a tooth ache will it be pulled? [SEP] question 2: Hello doctor, I have tooth decay and loose tooth along with pain. My dentist advised me to get it removed. I am getting tooth removed on next Monday. Will they give me anaesthesia? Is there any side effect?
06/19/2022 21:55:04 - INFO - __main__ - ['Similar']
06/19/2022 21:55:04 - INFO - __main__ -  [medical_questions_pairs] question 1: Is hypno-therapy dangeorus? [SEP] question 2: I have anxiety and my doctor advised breathing exercises. I am considering hypno-therapy.
06/19/2022 21:55:04 - INFO - __main__ - ['Similar']
06/19/2022 21:55:04 - INFO - __main__ -  [medical_questions_pairs] question 1: Why does my back hurt when I breathe or bend over? Its kind of in my upper middle back but just on the right side. [SEP] question 2: I was lifting weights at the gym and seemed to have hurt my upper back. It seems like a pulled muscle. How long does this last?
06/19/2022 21:55:04 - INFO - __main__ - ['Similar']
06/19/2022 21:55:04 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:55:04 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:55:04 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:55:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:55:08 - INFO - __main__ - Starting training!
06/19/2022 21:55:10 - INFO - __main__ - Step 10 Global step 10 Train loss 18.278038 on epoch=4
06/19/2022 21:55:13 - INFO - __main__ - Step 20 Global step 20 Train loss 14.903661 on epoch=9
06/19/2022 21:55:15 - INFO - __main__ - Step 30 Global step 30 Train loss 10.964771 on epoch=14
06/19/2022 21:55:18 - INFO - __main__ - Step 40 Global step 40 Train loss 9.879000 on epoch=19
06/19/2022 21:55:20 - INFO - __main__ - Step 50 Global step 50 Train loss 9.608874 on epoch=24
06/19/2022 21:55:22 - INFO - __main__ - Global step 50 Train loss 12.726869 ACC 0.0 on epoch=24
06/19/2022 21:55:25 - INFO - __main__ - Step 60 Global step 60 Train loss 8.619262 on epoch=29
06/19/2022 21:55:27 - INFO - __main__ - Step 70 Global step 70 Train loss 7.144987 on epoch=34
06/19/2022 21:55:30 - INFO - __main__ - Step 80 Global step 80 Train loss 5.844779 on epoch=39
06/19/2022 21:55:32 - INFO - __main__ - Step 90 Global step 90 Train loss 5.319704 on epoch=44
06/19/2022 21:55:35 - INFO - __main__ - Step 100 Global step 100 Train loss 5.779228 on epoch=49
06/19/2022 21:55:36 - INFO - __main__ - Global step 100 Train loss 6.541592 ACC 0.0 on epoch=49
06/19/2022 21:55:38 - INFO - __main__ - Step 110 Global step 110 Train loss 4.242386 on epoch=54
06/19/2022 21:55:41 - INFO - __main__ - Step 120 Global step 120 Train loss 3.699949 on epoch=59
06/19/2022 21:55:43 - INFO - __main__ - Step 130 Global step 130 Train loss 3.745583 on epoch=64
06/19/2022 21:55:46 - INFO - __main__ - Step 140 Global step 140 Train loss 2.707087 on epoch=69
06/19/2022 21:55:48 - INFO - __main__ - Step 150 Global step 150 Train loss 2.430984 on epoch=74
06/19/2022 21:55:48 - INFO - __main__ - Global step 150 Train loss 3.365198 ACC 0.5 on epoch=74
06/19/2022 21:55:51 - INFO - __main__ - Step 160 Global step 160 Train loss 2.539950 on epoch=79
06/19/2022 21:55:54 - INFO - __main__ - Step 170 Global step 170 Train loss 1.526165 on epoch=84
06/19/2022 21:55:56 - INFO - __main__ - Step 180 Global step 180 Train loss 1.988950 on epoch=89
06/19/2022 21:55:59 - INFO - __main__ - Step 190 Global step 190 Train loss 1.933303 on epoch=94
06/19/2022 21:56:01 - INFO - __main__ - Step 200 Global step 200 Train loss 1.547338 on epoch=99
06/19/2022 21:56:02 - INFO - __main__ - Global step 200 Train loss 1.907141 ACC 0.5 on epoch=99
06/19/2022 21:56:04 - INFO - __main__ - Step 210 Global step 210 Train loss 1.358816 on epoch=104
06/19/2022 21:56:07 - INFO - __main__ - Step 220 Global step 220 Train loss 1.100975 on epoch=109
06/19/2022 21:56:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.860793 on epoch=114
06/19/2022 21:56:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.936492 on epoch=119
06/19/2022 21:56:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.738376 on epoch=124
06/19/2022 21:56:14 - INFO - __main__ - Global step 250 Train loss 0.999090 ACC 0.5 on epoch=124
06/19/2022 21:56:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.565332 on epoch=129
06/19/2022 21:56:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.735053 on epoch=134
06/19/2022 21:56:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.758218 on epoch=139
06/19/2022 21:56:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.753237 on epoch=144
06/19/2022 21:56:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.664735 on epoch=149
06/19/2022 21:56:27 - INFO - __main__ - Global step 300 Train loss 0.695315 ACC 0.5 on epoch=149
06/19/2022 21:56:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.680291 on epoch=154
06/19/2022 21:56:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.608618 on epoch=159
06/19/2022 21:56:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.455861 on epoch=164
06/19/2022 21:56:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.652208 on epoch=169
06/19/2022 21:56:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.602944 on epoch=174
06/19/2022 21:56:40 - INFO - __main__ - Global step 350 Train loss 0.599984 ACC 0.5 on epoch=174
06/19/2022 21:56:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.662256 on epoch=179
06/19/2022 21:56:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.701811 on epoch=184
06/19/2022 21:56:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.601441 on epoch=189
06/19/2022 21:56:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.577416 on epoch=194
06/19/2022 21:56:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.443165 on epoch=199
06/19/2022 21:56:53 - INFO - __main__ - Global step 400 Train loss 0.597218 ACC 0.5 on epoch=199
06/19/2022 21:56:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.876150 on epoch=204
06/19/2022 21:56:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.553254 on epoch=209
06/19/2022 21:57:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.590446 on epoch=214
06/19/2022 21:57:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.615062 on epoch=219
06/19/2022 21:57:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.472269 on epoch=224
06/19/2022 21:57:06 - INFO - __main__ - Global step 450 Train loss 0.621437 ACC 0.5 on epoch=224
06/19/2022 21:57:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.544786 on epoch=229
06/19/2022 21:57:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.613339 on epoch=234
06/19/2022 21:57:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.444531 on epoch=239
06/19/2022 21:57:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.608501 on epoch=244
06/19/2022 21:57:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.542703 on epoch=249
06/19/2022 21:57:18 - INFO - __main__ - Global step 500 Train loss 0.550772 ACC 0.53125 on epoch=249
06/19/2022 21:57:21 - INFO - __main__ - Step 510 Global step 510 Train loss 1.064286 on epoch=254
06/19/2022 21:57:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.514524 on epoch=259
06/19/2022 21:57:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.786698 on epoch=264
06/19/2022 21:57:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.584300 on epoch=269
06/19/2022 21:57:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.504918 on epoch=274
06/19/2022 21:57:31 - INFO - __main__ - Global step 550 Train loss 0.690945 ACC 0.5 on epoch=274
06/19/2022 21:57:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.376171 on epoch=279
06/19/2022 21:57:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.509008 on epoch=284
06/19/2022 21:57:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.520775 on epoch=289
06/19/2022 21:57:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.548779 on epoch=294
06/19/2022 21:57:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.588860 on epoch=299
06/19/2022 21:57:44 - INFO - __main__ - Global step 600 Train loss 0.508719 ACC 0.5 on epoch=299
06/19/2022 21:57:44 - INFO - __main__ - save last model!
06/19/2022 21:57:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:57:45 - INFO - __main__ - Printing 3 examples
06/19/2022 21:57:45 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/19/2022 21:57:45 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:45 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/19/2022 21:57:45 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:45 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/19/2022 21:57:45 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 21:57:45 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:57:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:57:45 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:57:45 - INFO - __main__ - Printing 3 examples
06/19/2022 21:57:45 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/19/2022 21:57:45 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:45 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/19/2022 21:57:45 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:45 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/19/2022 21:57:45 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:45 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:57:45 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:57:45 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:57:47 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 21:57:47 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 21:57:47 - INFO - __main__ - Printing 3 examples
06/19/2022 21:57:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 21:57:47 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 21:57:47 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:47 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 21:57:47 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:47 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:57:47 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:57:48 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 21:57:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:57:49 - INFO - __main__ - Starting training!
06/19/2022 21:57:53 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_13_0.0001_8_predictions.txt
06/19/2022 21:57:53 - INFO - __main__ - ACC on test data: 0.5393
06/19/2022 21:57:53 - INFO - __main__ - prefix=medical_questions_pairs_16_13, lr=0.0001, bsz=8, dev_performance=0.53125, test_performance=0.5393442622950819
06/19/2022 21:57:53 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.0005, bsz=8 ...
06/19/2022 21:57:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:57:54 - INFO - __main__ - Printing 3 examples
06/19/2022 21:57:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/19/2022 21:57:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/19/2022 21:57:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/19/2022 21:57:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 21:57:54 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:57:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 21:57:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 21:57:54 - INFO - __main__ - Printing 3 examples
06/19/2022 21:57:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/19/2022 21:57:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/19/2022 21:57:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/19/2022 21:57:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 21:57:54 - INFO - __main__ - Tokenizing Input ...
06/19/2022 21:57:54 - INFO - __main__ - Tokenizing Output ...
06/19/2022 21:57:54 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 21:57:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 21:57:58 - INFO - __main__ - Starting training!
06/19/2022 21:58:00 - INFO - __main__ - Step 10 Global step 10 Train loss 18.263361 on epoch=4
06/19/2022 21:58:02 - INFO - __main__ - Step 20 Global step 20 Train loss 14.034497 on epoch=9
06/19/2022 21:58:05 - INFO - __main__ - Step 30 Global step 30 Train loss 6.479559 on epoch=14
06/19/2022 21:58:07 - INFO - __main__ - Step 40 Global step 40 Train loss 4.363780 on epoch=19
06/19/2022 21:58:10 - INFO - __main__ - Step 50 Global step 50 Train loss 2.342908 on epoch=24
06/19/2022 21:58:10 - INFO - __main__ - Global step 50 Train loss 9.096821 ACC 0.5 on epoch=24
06/19/2022 21:58:13 - INFO - __main__ - Step 60 Global step 60 Train loss 1.596575 on epoch=29
06/19/2022 21:58:15 - INFO - __main__ - Step 70 Global step 70 Train loss 1.171517 on epoch=34
06/19/2022 21:58:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.752897 on epoch=39
06/19/2022 21:58:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.845210 on epoch=44
06/19/2022 21:58:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.488856 on epoch=49
06/19/2022 21:58:23 - INFO - __main__ - Global step 100 Train loss 0.971011 ACC 0.5 on epoch=49
06/19/2022 21:58:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.563414 on epoch=54
06/19/2022 21:58:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.514864 on epoch=59
06/19/2022 21:58:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.421917 on epoch=64
06/19/2022 21:58:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.525454 on epoch=69
06/19/2022 21:58:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.468731 on epoch=74
06/19/2022 21:58:36 - INFO - __main__ - Global step 150 Train loss 0.498876 ACC 0.5 on epoch=74
06/19/2022 21:58:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.503219 on epoch=79
06/19/2022 21:58:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.637239 on epoch=84
06/19/2022 21:58:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.713084 on epoch=89
06/19/2022 21:58:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.474673 on epoch=94
06/19/2022 21:58:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.540629 on epoch=99
06/19/2022 21:58:48 - INFO - __main__ - Global step 200 Train loss 0.573769 ACC 0.5 on epoch=99
06/19/2022 21:58:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.556621 on epoch=104
06/19/2022 21:58:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.430000 on epoch=109
06/19/2022 21:58:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.398162 on epoch=114
06/19/2022 21:58:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.427118 on epoch=119
06/19/2022 21:59:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.348531 on epoch=124
06/19/2022 21:59:01 - INFO - __main__ - Global step 250 Train loss 0.432086 ACC 0.5 on epoch=124
06/19/2022 21:59:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.359250 on epoch=129
06/19/2022 21:59:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.534651 on epoch=134
06/19/2022 21:59:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.377973 on epoch=139
06/19/2022 21:59:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.323877 on epoch=144
06/19/2022 21:59:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.445358 on epoch=149
06/19/2022 21:59:14 - INFO - __main__ - Global step 300 Train loss 0.408222 ACC 0.5 on epoch=149
06/19/2022 21:59:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.381192 on epoch=154
06/19/2022 21:59:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.389894 on epoch=159
06/19/2022 21:59:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.395334 on epoch=164
06/19/2022 21:59:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.342290 on epoch=169
06/19/2022 21:59:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.391013 on epoch=174
06/19/2022 21:59:27 - INFO - __main__ - Global step 350 Train loss 0.379945 ACC 0.4375 on epoch=174
06/19/2022 21:59:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.314263 on epoch=179
06/19/2022 21:59:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.397298 on epoch=184
06/19/2022 21:59:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.334125 on epoch=189
06/19/2022 21:59:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.364567 on epoch=194
06/19/2022 21:59:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.331093 on epoch=199
06/19/2022 21:59:40 - INFO - __main__ - Global step 400 Train loss 0.348269 ACC 0.5 on epoch=199
06/19/2022 21:59:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.314772 on epoch=204
06/19/2022 21:59:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.353407 on epoch=209
06/19/2022 21:59:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.324213 on epoch=214
06/19/2022 21:59:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.332473 on epoch=219
06/19/2022 21:59:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.305794 on epoch=224
06/19/2022 21:59:53 - INFO - __main__ - Global step 450 Train loss 0.326132 ACC 0.5 on epoch=224
06/19/2022 21:59:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.377612 on epoch=229
06/19/2022 21:59:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.333241 on epoch=234
06/19/2022 22:00:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.300635 on epoch=239
06/19/2022 22:00:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.331114 on epoch=244
06/19/2022 22:00:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.374797 on epoch=249
06/19/2022 22:00:05 - INFO - __main__ - Global step 500 Train loss 0.343480 ACC 0.5 on epoch=249
06/19/2022 22:00:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.293539 on epoch=254
06/19/2022 22:00:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.365738 on epoch=259
06/19/2022 22:00:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.332678 on epoch=264
06/19/2022 22:00:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.298177 on epoch=269
06/19/2022 22:00:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.308348 on epoch=274
06/19/2022 22:00:18 - INFO - __main__ - Global step 550 Train loss 0.319696 ACC 0.5 on epoch=274
06/19/2022 22:00:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.306472 on epoch=279
06/19/2022 22:00:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.311817 on epoch=284
06/19/2022 22:00:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.298542 on epoch=289
06/19/2022 22:00:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.351022 on epoch=294
06/19/2022 22:00:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.313188 on epoch=299
06/19/2022 22:00:31 - INFO - __main__ - Global step 600 Train loss 0.316208 ACC 0.5 on epoch=299
06/19/2022 22:00:31 - INFO - __main__ - save last model!
06/19/2022 22:00:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:00:32 - INFO - __main__ - Printing 3 examples
06/19/2022 22:00:32 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/19/2022 22:00:32 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:32 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/19/2022 22:00:32 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:32 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/19/2022 22:00:32 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:00:32 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:00:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:00:32 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:00:32 - INFO - __main__ - Printing 3 examples
06/19/2022 22:00:32 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/19/2022 22:00:32 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:32 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/19/2022 22:00:32 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:32 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/19/2022 22:00:32 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:32 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:00:32 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:00:32 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:00:34 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:00:34 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:00:34 - INFO - __main__ - Printing 3 examples
06/19/2022 22:00:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:00:34 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:34 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:00:34 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:34 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:00:34 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:34 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:00:34 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:00:35 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:00:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:00:35 - INFO - __main__ - Starting training!
06/19/2022 22:00:38 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.0005_8_predictions.txt
06/19/2022 22:00:38 - INFO - __main__ - ACC on test data: 0.4836
06/19/2022 22:00:38 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.0005, bsz=8, dev_performance=0.5, test_performance=0.48360655737704916
06/19/2022 22:00:38 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.0003, bsz=8 ...
06/19/2022 22:00:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:00:39 - INFO - __main__ - Printing 3 examples
06/19/2022 22:00:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/19/2022 22:00:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/19/2022 22:00:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/19/2022 22:00:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:00:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:00:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:00:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:00:39 - INFO - __main__ - Printing 3 examples
06/19/2022 22:00:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/19/2022 22:00:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/19/2022 22:00:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/19/2022 22:00:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:00:39 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:00:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:00:39 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:00:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:00:44 - INFO - __main__ - Starting training!
06/19/2022 22:00:45 - INFO - __main__ - Step 10 Global step 10 Train loss 18.263643 on epoch=4
06/19/2022 22:00:48 - INFO - __main__ - Step 20 Global step 20 Train loss 14.037119 on epoch=9
06/19/2022 22:00:50 - INFO - __main__ - Step 30 Global step 30 Train loss 8.006036 on epoch=14
06/19/2022 22:00:53 - INFO - __main__ - Step 40 Global step 40 Train loss 4.946561 on epoch=19
06/19/2022 22:00:55 - INFO - __main__ - Step 50 Global step 50 Train loss 3.834538 on epoch=24
06/19/2022 22:00:56 - INFO - __main__ - Global step 50 Train loss 9.817579 ACC 0.5 on epoch=24
06/19/2022 22:00:58 - INFO - __main__ - Step 60 Global step 60 Train loss 3.518132 on epoch=29
06/19/2022 22:01:01 - INFO - __main__ - Step 70 Global step 70 Train loss 2.115766 on epoch=34
06/19/2022 22:01:03 - INFO - __main__ - Step 80 Global step 80 Train loss 1.737482 on epoch=39
06/19/2022 22:01:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.932565 on epoch=44
06/19/2022 22:01:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.713135 on epoch=49
06/19/2022 22:01:08 - INFO - __main__ - Global step 100 Train loss 1.803416 ACC 0.5 on epoch=49
06/19/2022 22:01:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.820765 on epoch=54
06/19/2022 22:01:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.971918 on epoch=59
06/19/2022 22:01:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.557979 on epoch=64
06/19/2022 22:01:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.696874 on epoch=69
06/19/2022 22:01:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.503069 on epoch=74
06/19/2022 22:01:21 - INFO - __main__ - Global step 150 Train loss 0.710121 ACC 0.5 on epoch=74
06/19/2022 22:01:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.402096 on epoch=79
06/19/2022 22:01:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.727361 on epoch=84
06/19/2022 22:01:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.557570 on epoch=89
06/19/2022 22:01:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.455483 on epoch=94
06/19/2022 22:01:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.582960 on epoch=99
06/19/2022 22:01:33 - INFO - __main__ - Global step 200 Train loss 0.545094 ACC 0.625 on epoch=99
06/19/2022 22:01:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.486173 on epoch=104
06/19/2022 22:01:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.546660 on epoch=109
06/19/2022 22:01:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.514707 on epoch=114
06/19/2022 22:01:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.384908 on epoch=119
06/19/2022 22:01:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.368492 on epoch=124
06/19/2022 22:01:46 - INFO - __main__ - Global step 250 Train loss 0.460188 ACC 0.5625 on epoch=124
06/19/2022 22:01:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.364124 on epoch=129
06/19/2022 22:01:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.314649 on epoch=134
06/19/2022 22:01:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.260090 on epoch=139
06/19/2022 22:01:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.351116 on epoch=144
06/19/2022 22:01:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.267535 on epoch=149
06/19/2022 22:01:58 - INFO - __main__ - Global step 300 Train loss 0.311503 ACC 0.5 on epoch=149
06/19/2022 22:02:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.200758 on epoch=154
06/19/2022 22:02:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.226322 on epoch=159
06/19/2022 22:02:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.195642 on epoch=164
06/19/2022 22:02:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.158240 on epoch=169
06/19/2022 22:02:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.186647 on epoch=174
06/19/2022 22:02:11 - INFO - __main__ - Global step 350 Train loss 0.193522 ACC 0.5625 on epoch=174
06/19/2022 22:02:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.236325 on epoch=179
06/19/2022 22:02:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.243482 on epoch=184
06/19/2022 22:02:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.153677 on epoch=189
06/19/2022 22:02:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.137101 on epoch=194
06/19/2022 22:02:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.169841 on epoch=199
06/19/2022 22:02:23 - INFO - __main__ - Global step 400 Train loss 0.188085 ACC 0.46875 on epoch=199
06/19/2022 22:02:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.143465 on epoch=204
06/19/2022 22:02:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.157666 on epoch=209
06/19/2022 22:02:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.070675 on epoch=214
06/19/2022 22:02:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.110813 on epoch=219
06/19/2022 22:02:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.102770 on epoch=224
06/19/2022 22:02:36 - INFO - __main__ - Global step 450 Train loss 0.117077 ACC 0.5625 on epoch=224
06/19/2022 22:02:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.031396 on epoch=229
06/19/2022 22:02:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.059014 on epoch=234
06/19/2022 22:02:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.048769 on epoch=239
06/19/2022 22:02:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.049157 on epoch=244
06/19/2022 22:02:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.024348 on epoch=249
06/19/2022 22:02:48 - INFO - __main__ - Global step 500 Train loss 0.042537 ACC 0.5625 on epoch=249
06/19/2022 22:02:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.053218 on epoch=254
06/19/2022 22:02:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.022232 on epoch=259
06/19/2022 22:02:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.044093 on epoch=264
06/19/2022 22:02:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.027691 on epoch=269
06/19/2022 22:03:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.044003 on epoch=274
06/19/2022 22:03:01 - INFO - __main__ - Global step 550 Train loss 0.038247 ACC 0.5625 on epoch=274
06/19/2022 22:03:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.035635 on epoch=279
06/19/2022 22:03:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.027963 on epoch=284
06/19/2022 22:03:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.018374 on epoch=289
06/19/2022 22:03:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.018945 on epoch=294
06/19/2022 22:03:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.041791 on epoch=299
06/19/2022 22:03:13 - INFO - __main__ - Global step 600 Train loss 0.028542 ACC 0.59375 on epoch=299
06/19/2022 22:03:13 - INFO - __main__ - save last model!
06/19/2022 22:03:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:03:14 - INFO - __main__ - Printing 3 examples
06/19/2022 22:03:14 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/19/2022 22:03:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:14 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/19/2022 22:03:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:14 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/19/2022 22:03:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:03:14 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:03:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:03:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:03:14 - INFO - __main__ - Printing 3 examples
06/19/2022 22:03:14 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/19/2022 22:03:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:14 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/19/2022 22:03:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:14 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/19/2022 22:03:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:14 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:03:14 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:03:14 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:03:16 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:03:16 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:03:16 - INFO - __main__ - Printing 3 examples
06/19/2022 22:03:16 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:03:16 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:16 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:03:16 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:16 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:03:16 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:16 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:03:16 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:03:17 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:03:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:03:18 - INFO - __main__ - Starting training!
06/19/2022 22:03:21 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.0003_8_predictions.txt
06/19/2022 22:03:21 - INFO - __main__ - ACC on test data: 0.5820
06/19/2022 22:03:21 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.0003, bsz=8, dev_performance=0.625, test_performance=0.5819672131147541
06/19/2022 22:03:21 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.0002, bsz=8 ...
06/19/2022 22:03:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:03:22 - INFO - __main__ - Printing 3 examples
06/19/2022 22:03:22 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/19/2022 22:03:22 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:22 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/19/2022 22:03:22 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:22 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/19/2022 22:03:22 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:03:22 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:03:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:03:22 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:03:22 - INFO - __main__ - Printing 3 examples
06/19/2022 22:03:22 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/19/2022 22:03:22 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:22 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/19/2022 22:03:22 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:22 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/19/2022 22:03:22 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:03:22 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:03:22 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:03:22 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:03:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:03:27 - INFO - __main__ - Starting training!
06/19/2022 22:03:29 - INFO - __main__ - Step 10 Global step 10 Train loss 18.562302 on epoch=4
06/19/2022 22:03:31 - INFO - __main__ - Step 20 Global step 20 Train loss 15.307414 on epoch=9
06/19/2022 22:03:33 - INFO - __main__ - Step 30 Global step 30 Train loss 8.822958 on epoch=14
06/19/2022 22:03:36 - INFO - __main__ - Step 40 Global step 40 Train loss 6.952077 on epoch=19
06/19/2022 22:03:38 - INFO - __main__ - Step 50 Global step 50 Train loss 5.845090 on epoch=24
06/19/2022 22:03:40 - INFO - __main__ - Global step 50 Train loss 11.097969 ACC 0.0 on epoch=24
06/19/2022 22:03:43 - INFO - __main__ - Step 60 Global step 60 Train loss 5.000064 on epoch=29
06/19/2022 22:03:45 - INFO - __main__ - Step 70 Global step 70 Train loss 3.558408 on epoch=34
06/19/2022 22:03:48 - INFO - __main__ - Step 80 Global step 80 Train loss 3.453797 on epoch=39
06/19/2022 22:03:50 - INFO - __main__ - Step 90 Global step 90 Train loss 1.755476 on epoch=44
06/19/2022 22:03:53 - INFO - __main__ - Step 100 Global step 100 Train loss 1.932778 on epoch=49
06/19/2022 22:03:53 - INFO - __main__ - Global step 100 Train loss 3.140105 ACC 0.53125 on epoch=49
06/19/2022 22:03:56 - INFO - __main__ - Step 110 Global step 110 Train loss 1.734237 on epoch=54
06/19/2022 22:03:58 - INFO - __main__ - Step 120 Global step 120 Train loss 1.735260 on epoch=59
06/19/2022 22:04:01 - INFO - __main__ - Step 130 Global step 130 Train loss 1.665289 on epoch=64
06/19/2022 22:04:03 - INFO - __main__ - Step 140 Global step 140 Train loss 1.019130 on epoch=69
06/19/2022 22:04:06 - INFO - __main__ - Step 150 Global step 150 Train loss 1.048283 on epoch=74
06/19/2022 22:04:06 - INFO - __main__ - Global step 150 Train loss 1.440440 ACC 0.46875 on epoch=74
06/19/2022 22:04:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.752028 on epoch=79
06/19/2022 22:04:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.888602 on epoch=84
06/19/2022 22:04:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.739160 on epoch=89
06/19/2022 22:04:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.740384 on epoch=94
06/19/2022 22:04:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.505623 on epoch=99
06/19/2022 22:04:18 - INFO - __main__ - Global step 200 Train loss 0.725159 ACC 0.5 on epoch=99
06/19/2022 22:04:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.495626 on epoch=104
06/19/2022 22:04:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.564620 on epoch=109
06/19/2022 22:04:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.610662 on epoch=114
06/19/2022 22:04:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.490737 on epoch=119
06/19/2022 22:04:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.497882 on epoch=124
06/19/2022 22:04:31 - INFO - __main__ - Global step 250 Train loss 0.531905 ACC 0.46875 on epoch=124
06/19/2022 22:04:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.266937 on epoch=129
06/19/2022 22:04:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.470829 on epoch=134
06/19/2022 22:04:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.521633 on epoch=139
06/19/2022 22:04:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.452012 on epoch=144
06/19/2022 22:04:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.412301 on epoch=149
06/19/2022 22:04:43 - INFO - __main__ - Global step 300 Train loss 0.424742 ACC 0.5 on epoch=149
06/19/2022 22:04:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.323077 on epoch=154
06/19/2022 22:04:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.342232 on epoch=159
06/19/2022 22:04:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.378777 on epoch=164
06/19/2022 22:04:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.434085 on epoch=169
06/19/2022 22:04:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.326291 on epoch=174
06/19/2022 22:04:56 - INFO - __main__ - Global step 350 Train loss 0.360893 ACC 0.5625 on epoch=174
06/19/2022 22:04:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.202781 on epoch=179
06/19/2022 22:05:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.331332 on epoch=184
06/19/2022 22:05:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.373250 on epoch=189
06/19/2022 22:05:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.288604 on epoch=194
06/19/2022 22:05:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.256142 on epoch=199
06/19/2022 22:05:09 - INFO - __main__ - Global step 400 Train loss 0.290422 ACC 0.59375 on epoch=199
06/19/2022 22:05:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.223511 on epoch=204
06/19/2022 22:05:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.195553 on epoch=209
06/19/2022 22:05:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.205600 on epoch=214
06/19/2022 22:05:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.157791 on epoch=219
06/19/2022 22:05:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.165927 on epoch=224
06/19/2022 22:05:22 - INFO - __main__ - Global step 450 Train loss 0.189676 ACC 0.5625 on epoch=224
06/19/2022 22:05:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.172393 on epoch=229
06/19/2022 22:05:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.154651 on epoch=234
06/19/2022 22:05:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.161771 on epoch=239
06/19/2022 22:05:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.149664 on epoch=244
06/19/2022 22:05:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.112104 on epoch=249
06/19/2022 22:05:34 - INFO - __main__ - Global step 500 Train loss 0.150117 ACC 0.5625 on epoch=249
06/19/2022 22:05:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.066059 on epoch=254
06/19/2022 22:05:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.246545 on epoch=259
06/19/2022 22:05:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.094135 on epoch=264
06/19/2022 22:05:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.167022 on epoch=269
06/19/2022 22:05:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.428264 on epoch=274
06/19/2022 22:05:47 - INFO - __main__ - Global step 550 Train loss 0.200405 ACC 0.5 on epoch=274
06/19/2022 22:05:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.452737 on epoch=279
06/19/2022 22:05:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.182069 on epoch=284
06/19/2022 22:05:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.181526 on epoch=289
06/19/2022 22:05:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.150584 on epoch=294
06/19/2022 22:05:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.061458 on epoch=299
06/19/2022 22:05:59 - INFO - __main__ - Global step 600 Train loss 0.205675 ACC 0.5625 on epoch=299
06/19/2022 22:05:59 - INFO - __main__ - save last model!
06/19/2022 22:06:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:06:00 - INFO - __main__ - Printing 3 examples
06/19/2022 22:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/19/2022 22:06:00 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/19/2022 22:06:00 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/19/2022 22:06:00 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:06:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:06:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:06:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:06:00 - INFO - __main__ - Printing 3 examples
06/19/2022 22:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/19/2022 22:06:00 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/19/2022 22:06:00 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:00 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/19/2022 22:06:00 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:00 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:06:00 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:06:00 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:06:02 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:06:02 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:06:02 - INFO - __main__ - Printing 3 examples
06/19/2022 22:06:02 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:06:02 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:02 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:06:02 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:02 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:06:02 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:02 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:06:02 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:06:03 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:06:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:06:04 - INFO - __main__ - Starting training!
06/19/2022 22:06:08 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.0002_8_predictions.txt
06/19/2022 22:06:08 - INFO - __main__ - ACC on test data: 0.5443
06/19/2022 22:06:08 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.0002, bsz=8, dev_performance=0.59375, test_performance=0.5442622950819672
06/19/2022 22:06:08 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_21, lr=0.0001, bsz=8 ...
06/19/2022 22:06:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:06:09 - INFO - __main__ - Printing 3 examples
06/19/2022 22:06:09 - INFO - __main__ -  [medical_questions_pairs] question 1: Is having a cycle important? I haven't had a decent period in almost my whole life....What can be wrong with me?...  I'm 23 yrs old... [SEP] question 2: I am 23 years old and still have not started getting normal period. Why should a female get her cycles? Is something wrong with me?
06/19/2022 22:06:09 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:09 - INFO - __main__ -  [medical_questions_pairs] question 1: Ive noticed that after eating my resting heart rate goes from the 60s close to the 80s. Is this normal after eating? [SEP] question 2: My resting heart rate seems to go from 60s to 80s after eating. Is that expected after eating?
06/19/2022 22:06:09 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:09 - INFO - __main__ -  [medical_questions_pairs] question 1: Always use birth control, always use condoms with spermicide, always pull out with condom, but my period is late. Stress from college or pregnancy? [SEP] question 2: I always have protected sex but my period is still late, could I be pregnant?
06/19/2022 22:06:09 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:06:09 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:06:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:06:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:06:09 - INFO - __main__ - Printing 3 examples
06/19/2022 22:06:09 - INFO - __main__ -  [medical_questions_pairs] question 1: Does taking 5 htp help reduce the adrenaline hormone in the adrenal glands? [SEP] question 2: Can the  adrenalin levels come down when one takes 5htp?
06/19/2022 22:06:09 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:09 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having lightheadedness for a while. Would a CT of head and an MRI with contrast of brain detect blockages in arteries? [SEP] question 2: I am worried about blockage in my brain arteries because I have been having lightheadedness for some time. Can a CT of scan or MRI with contrast may help diagnose it?
06/19/2022 22:06:09 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:09 - INFO - __main__ -  [medical_questions_pairs] question 1: I reached over a counter and something popped on my left side above rib cage. It gets sore if I pull on something, etc What is this injury? IA weekno [SEP] question 2: I notice soreness above the left rib cage if I pull on something and happened after it popped as I reached over a counter. What kind of an injury is this?
06/19/2022 22:06:09 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:06:09 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:06:09 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:06:09 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:06:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:06:12 - INFO - __main__ - Starting training!
06/19/2022 22:06:14 - INFO - __main__ - Step 10 Global step 10 Train loss 18.439373 on epoch=4
06/19/2022 22:06:17 - INFO - __main__ - Step 20 Global step 20 Train loss 16.662292 on epoch=9
06/19/2022 22:06:19 - INFO - __main__ - Step 30 Global step 30 Train loss 11.408463 on epoch=14
06/19/2022 22:06:22 - INFO - __main__ - Step 40 Global step 40 Train loss 10.213072 on epoch=19
06/19/2022 22:06:24 - INFO - __main__ - Step 50 Global step 50 Train loss 8.507877 on epoch=24
06/19/2022 22:06:26 - INFO - __main__ - Global step 50 Train loss 13.046217 ACC 0.0 on epoch=24
06/19/2022 22:06:29 - INFO - __main__ - Step 60 Global step 60 Train loss 7.948514 on epoch=29
06/19/2022 22:06:31 - INFO - __main__ - Step 70 Global step 70 Train loss 7.483582 on epoch=34
06/19/2022 22:06:33 - INFO - __main__ - Step 80 Global step 80 Train loss 6.426381 on epoch=39
06/19/2022 22:06:36 - INFO - __main__ - Step 90 Global step 90 Train loss 5.832387 on epoch=44
06/19/2022 22:06:38 - INFO - __main__ - Step 100 Global step 100 Train loss 5.257264 on epoch=49
06/19/2022 22:06:40 - INFO - __main__ - Global step 100 Train loss 6.589625 ACC 0.0 on epoch=49
06/19/2022 22:06:42 - INFO - __main__ - Step 110 Global step 110 Train loss 5.508944 on epoch=54
06/19/2022 22:06:45 - INFO - __main__ - Step 120 Global step 120 Train loss 5.040240 on epoch=59
06/19/2022 22:06:47 - INFO - __main__ - Step 130 Global step 130 Train loss 3.638742 on epoch=64
06/19/2022 22:06:50 - INFO - __main__ - Step 140 Global step 140 Train loss 4.450998 on epoch=69
06/19/2022 22:06:52 - INFO - __main__ - Step 150 Global step 150 Train loss 3.419750 on epoch=74
06/19/2022 22:06:52 - INFO - __main__ - Global step 150 Train loss 4.411735 ACC 0.5 on epoch=74
06/19/2022 22:06:55 - INFO - __main__ - Step 160 Global step 160 Train loss 2.708899 on epoch=79
06/19/2022 22:06:58 - INFO - __main__ - Step 170 Global step 170 Train loss 2.158772 on epoch=84
06/19/2022 22:07:00 - INFO - __main__ - Step 180 Global step 180 Train loss 3.330396 on epoch=89
06/19/2022 22:07:03 - INFO - __main__ - Step 190 Global step 190 Train loss 1.817253 on epoch=94
06/19/2022 22:07:05 - INFO - __main__ - Step 200 Global step 200 Train loss 1.485292 on epoch=99
06/19/2022 22:07:05 - INFO - __main__ - Global step 200 Train loss 2.300122 ACC 0.4375 on epoch=99
06/19/2022 22:07:08 - INFO - __main__ - Step 210 Global step 210 Train loss 1.682502 on epoch=104
06/19/2022 22:07:10 - INFO - __main__ - Step 220 Global step 220 Train loss 1.908651 on epoch=109
06/19/2022 22:07:13 - INFO - __main__ - Step 230 Global step 230 Train loss 1.070876 on epoch=114
06/19/2022 22:07:15 - INFO - __main__ - Step 240 Global step 240 Train loss 1.173329 on epoch=119
06/19/2022 22:07:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.849381 on epoch=124
06/19/2022 22:07:18 - INFO - __main__ - Global step 250 Train loss 1.336948 ACC 0.46875 on epoch=124
06/19/2022 22:07:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.975271 on epoch=129
06/19/2022 22:07:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.646057 on epoch=134
06/19/2022 22:07:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.912192 on epoch=139
06/19/2022 22:07:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.538576 on epoch=144
06/19/2022 22:07:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.486090 on epoch=149
06/19/2022 22:07:31 - INFO - __main__ - Global step 300 Train loss 0.711637 ACC 0.53125 on epoch=149
06/19/2022 22:07:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.701052 on epoch=154
06/19/2022 22:07:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.682732 on epoch=159
06/19/2022 22:07:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.460010 on epoch=164
06/19/2022 22:07:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.680157 on epoch=169
06/19/2022 22:07:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.690994 on epoch=174
06/19/2022 22:07:44 - INFO - __main__ - Global step 350 Train loss 0.642989 ACC 0.5 on epoch=174
06/19/2022 22:07:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.546833 on epoch=179
06/19/2022 22:07:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.683377 on epoch=184
06/19/2022 22:07:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.558180 on epoch=189
06/19/2022 22:07:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.484317 on epoch=194
06/19/2022 22:07:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.549232 on epoch=199
06/19/2022 22:07:56 - INFO - __main__ - Global step 400 Train loss 0.564388 ACC 0.46875 on epoch=199
06/19/2022 22:07:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.405371 on epoch=204
06/19/2022 22:08:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.562581 on epoch=209
06/19/2022 22:08:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.551699 on epoch=214
06/19/2022 22:08:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.609054 on epoch=219
06/19/2022 22:08:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.529339 on epoch=224
06/19/2022 22:08:09 - INFO - __main__ - Global step 450 Train loss 0.531609 ACC 0.5625 on epoch=224
06/19/2022 22:08:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.472601 on epoch=229
06/19/2022 22:08:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.464552 on epoch=234
06/19/2022 22:08:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.416030 on epoch=239
06/19/2022 22:08:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.423940 on epoch=244
06/19/2022 22:08:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.468995 on epoch=249
06/19/2022 22:08:22 - INFO - __main__ - Global step 500 Train loss 0.449224 ACC 0.59375 on epoch=249
06/19/2022 22:08:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.455058 on epoch=254
06/19/2022 22:08:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.407140 on epoch=259
06/19/2022 22:08:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.364872 on epoch=264
06/19/2022 22:08:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.397310 on epoch=269
06/19/2022 22:08:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.475642 on epoch=274
06/19/2022 22:08:35 - INFO - __main__ - Global step 550 Train loss 0.420004 ACC 0.59375 on epoch=274
06/19/2022 22:08:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.375009 on epoch=279
06/19/2022 22:08:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.335675 on epoch=284
06/19/2022 22:08:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.381958 on epoch=289
06/19/2022 22:08:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.325211 on epoch=294
06/19/2022 22:08:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.310335 on epoch=299
06/19/2022 22:08:47 - INFO - __main__ - Global step 600 Train loss 0.345637 ACC 0.59375 on epoch=299
06/19/2022 22:08:47 - INFO - __main__ - save last model!
06/19/2022 22:08:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:08:48 - INFO - __main__ - Printing 3 examples
06/19/2022 22:08:48 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/19/2022 22:08:48 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:48 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/19/2022 22:08:48 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:48 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/19/2022 22:08:48 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:08:48 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:08:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:08:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:08:48 - INFO - __main__ - Printing 3 examples
06/19/2022 22:08:48 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/19/2022 22:08:48 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:48 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/19/2022 22:08:48 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:48 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/19/2022 22:08:48 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:48 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:08:48 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:08:48 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:08:50 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:08:50 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:08:50 - INFO - __main__ - Printing 3 examples
06/19/2022 22:08:50 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:08:50 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:50 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:08:50 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:50 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:08:50 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:50 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:08:51 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:08:51 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:08:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:08:53 - INFO - __main__ - Starting training!
06/19/2022 22:08:56 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_21_0.0001_8_predictions.txt
06/19/2022 22:08:56 - INFO - __main__ - ACC on test data: 0.5475
06/19/2022 22:08:56 - INFO - __main__ - prefix=medical_questions_pairs_16_21, lr=0.0001, bsz=8, dev_performance=0.59375, test_performance=0.5475409836065573
06/19/2022 22:08:56 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.0005, bsz=8 ...
06/19/2022 22:08:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:08:57 - INFO - __main__ - Printing 3 examples
06/19/2022 22:08:57 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/19/2022 22:08:57 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:57 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/19/2022 22:08:57 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:57 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/19/2022 22:08:57 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:08:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:08:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:08:57 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:08:57 - INFO - __main__ - Printing 3 examples
06/19/2022 22:08:57 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/19/2022 22:08:57 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:57 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/19/2022 22:08:57 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:57 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/19/2022 22:08:57 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:08:57 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:08:57 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:08:57 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:09:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:09:01 - INFO - __main__ - Starting training!
06/19/2022 22:09:03 - INFO - __main__ - Step 10 Global step 10 Train loss 18.961933 on epoch=4
06/19/2022 22:09:05 - INFO - __main__ - Step 20 Global step 20 Train loss 12.575649 on epoch=9
06/19/2022 22:09:08 - INFO - __main__ - Step 30 Global step 30 Train loss 6.084550 on epoch=14
06/19/2022 22:09:10 - INFO - __main__ - Step 40 Global step 40 Train loss 3.695277 on epoch=19
06/19/2022 22:09:13 - INFO - __main__ - Step 50 Global step 50 Train loss 1.685230 on epoch=24
06/19/2022 22:09:13 - INFO - __main__ - Global step 50 Train loss 8.600528 ACC 0.46875 on epoch=24
06/19/2022 22:09:16 - INFO - __main__ - Step 60 Global step 60 Train loss 0.869144 on epoch=29
06/19/2022 22:09:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.801650 on epoch=34
06/19/2022 22:09:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.610696 on epoch=39
06/19/2022 22:09:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.517510 on epoch=44
06/19/2022 22:09:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.504199 on epoch=49
06/19/2022 22:09:26 - INFO - __main__ - Global step 100 Train loss 0.660640 ACC 0.53125 on epoch=49
06/19/2022 22:09:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.523091 on epoch=54
06/19/2022 22:09:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.514627 on epoch=59
06/19/2022 22:09:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.502278 on epoch=64
06/19/2022 22:09:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.424822 on epoch=69
06/19/2022 22:09:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.445343 on epoch=74
06/19/2022 22:09:40 - INFO - __main__ - Global step 150 Train loss 0.482032 ACC 0.46875 on epoch=74
06/19/2022 22:09:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.486935 on epoch=79
06/19/2022 22:09:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.530465 on epoch=84
06/19/2022 22:09:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.558376 on epoch=89
06/19/2022 22:09:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.482993 on epoch=94
06/19/2022 22:09:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.463036 on epoch=99
06/19/2022 22:09:53 - INFO - __main__ - Global step 200 Train loss 0.504361 ACC 0.5 on epoch=99
06/19/2022 22:09:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.396967 on epoch=104
06/19/2022 22:09:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.453045 on epoch=109
06/19/2022 22:10:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.474347 on epoch=114
06/19/2022 22:10:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.444363 on epoch=119
06/19/2022 22:10:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.397991 on epoch=124
06/19/2022 22:10:06 - INFO - __main__ - Global step 250 Train loss 0.433343 ACC 0.5 on epoch=124
06/19/2022 22:10:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.348556 on epoch=129
06/19/2022 22:10:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.341064 on epoch=134
06/19/2022 22:10:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.352123 on epoch=139
06/19/2022 22:10:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.267600 on epoch=144
06/19/2022 22:10:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.373072 on epoch=149
06/19/2022 22:10:19 - INFO - __main__ - Global step 300 Train loss 0.336483 ACC 0.5 on epoch=149
06/19/2022 22:10:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.438447 on epoch=154
06/19/2022 22:10:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.433199 on epoch=159
06/19/2022 22:10:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.366780 on epoch=164
06/19/2022 22:10:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.354309 on epoch=169
06/19/2022 22:10:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.329256 on epoch=174
06/19/2022 22:10:32 - INFO - __main__ - Global step 350 Train loss 0.384398 ACC 0.5 on epoch=174
06/19/2022 22:10:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.326118 on epoch=179
06/19/2022 22:10:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.367766 on epoch=184
06/19/2022 22:10:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.327120 on epoch=189
06/19/2022 22:10:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.322407 on epoch=194
06/19/2022 22:10:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.395563 on epoch=199
06/19/2022 22:10:45 - INFO - __main__ - Global step 400 Train loss 0.347795 ACC 0.5 on epoch=199
06/19/2022 22:10:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.336543 on epoch=204
06/19/2022 22:10:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.346210 on epoch=209
06/19/2022 22:10:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.356292 on epoch=214
06/19/2022 22:10:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.368002 on epoch=219
06/19/2022 22:10:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.361147 on epoch=224
06/19/2022 22:10:58 - INFO - __main__ - Global step 450 Train loss 0.353639 ACC 0.40625 on epoch=224
06/19/2022 22:11:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.326821 on epoch=229
06/19/2022 22:11:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.309720 on epoch=234
06/19/2022 22:11:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.318469 on epoch=239
06/19/2022 22:11:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.372709 on epoch=244
06/19/2022 22:11:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.362169 on epoch=249
06/19/2022 22:11:11 - INFO - __main__ - Global step 500 Train loss 0.337978 ACC 0.40625 on epoch=249
06/19/2022 22:11:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.314677 on epoch=254
06/19/2022 22:11:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.347817 on epoch=259
06/19/2022 22:11:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.352889 on epoch=264
06/19/2022 22:11:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.345101 on epoch=269
06/19/2022 22:11:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.287447 on epoch=274
06/19/2022 22:11:24 - INFO - __main__ - Global step 550 Train loss 0.329586 ACC 0.5 on epoch=274
06/19/2022 22:11:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.315179 on epoch=279
06/19/2022 22:11:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.338615 on epoch=284
06/19/2022 22:11:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.299081 on epoch=289
06/19/2022 22:11:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.349373 on epoch=294
06/19/2022 22:11:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.301313 on epoch=299
06/19/2022 22:11:37 - INFO - __main__ - Global step 600 Train loss 0.320713 ACC 0.4375 on epoch=299
06/19/2022 22:11:37 - INFO - __main__ - save last model!
06/19/2022 22:11:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:11:37 - INFO - __main__ - Printing 3 examples
06/19/2022 22:11:37 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/19/2022 22:11:37 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:37 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/19/2022 22:11:37 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:37 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/19/2022 22:11:37 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:11:37 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:11:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:11:37 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:11:37 - INFO - __main__ - Printing 3 examples
06/19/2022 22:11:37 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/19/2022 22:11:37 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:37 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/19/2022 22:11:37 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:37 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/19/2022 22:11:37 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:37 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:11:37 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:11:37 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:11:39 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:11:39 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:11:39 - INFO - __main__ - Printing 3 examples
06/19/2022 22:11:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:11:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:11:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:11:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:39 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:11:40 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:11:40 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:11:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:11:41 - INFO - __main__ - Starting training!
06/19/2022 22:11:45 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.0005_8_predictions.txt
06/19/2022 22:11:45 - INFO - __main__ - ACC on test data: 0.5246
06/19/2022 22:11:45 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.0005, bsz=8, dev_performance=0.53125, test_performance=0.5245901639344263
06/19/2022 22:11:45 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.0003, bsz=8 ...
06/19/2022 22:11:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:11:46 - INFO - __main__ - Printing 3 examples
06/19/2022 22:11:46 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/19/2022 22:11:46 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:46 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/19/2022 22:11:46 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/19/2022 22:11:46 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:11:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:11:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:11:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:11:46 - INFO - __main__ - Printing 3 examples
06/19/2022 22:11:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/19/2022 22:11:46 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/19/2022 22:11:46 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:46 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/19/2022 22:11:46 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:11:46 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:11:46 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:11:46 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:11:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:11:50 - INFO - __main__ - Starting training!
06/19/2022 22:11:52 - INFO - __main__ - Step 10 Global step 10 Train loss 19.066418 on epoch=4
06/19/2022 22:11:55 - INFO - __main__ - Step 20 Global step 20 Train loss 13.150807 on epoch=9
06/19/2022 22:11:57 - INFO - __main__ - Step 30 Global step 30 Train loss 8.661226 on epoch=14
06/19/2022 22:12:00 - INFO - __main__ - Step 40 Global step 40 Train loss 6.657041 on epoch=19
06/19/2022 22:12:02 - INFO - __main__ - Step 50 Global step 50 Train loss 5.034283 on epoch=24
06/19/2022 22:12:04 - INFO - __main__ - Global step 50 Train loss 10.513954 ACC 0.03125 on epoch=24
06/19/2022 22:12:06 - INFO - __main__ - Step 60 Global step 60 Train loss 3.616277 on epoch=29
06/19/2022 22:12:09 - INFO - __main__ - Step 70 Global step 70 Train loss 2.737835 on epoch=34
06/19/2022 22:12:11 - INFO - __main__ - Step 80 Global step 80 Train loss 2.284883 on epoch=39
06/19/2022 22:12:14 - INFO - __main__ - Step 90 Global step 90 Train loss 1.535193 on epoch=44
06/19/2022 22:12:16 - INFO - __main__ - Step 100 Global step 100 Train loss 1.252273 on epoch=49
06/19/2022 22:12:17 - INFO - __main__ - Global step 100 Train loss 2.285293 ACC 0.5 on epoch=49
06/19/2022 22:12:20 - INFO - __main__ - Step 110 Global step 110 Train loss 1.178820 on epoch=54
06/19/2022 22:12:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.921846 on epoch=59
06/19/2022 22:12:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.712031 on epoch=64
06/19/2022 22:12:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.617359 on epoch=69
06/19/2022 22:12:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.609783 on epoch=74
06/19/2022 22:12:30 - INFO - __main__ - Global step 150 Train loss 0.807968 ACC 0.46875 on epoch=74
06/19/2022 22:12:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.665826 on epoch=79
06/19/2022 22:12:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.471776 on epoch=84
06/19/2022 22:12:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.662234 on epoch=89
06/19/2022 22:12:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.473611 on epoch=94
06/19/2022 22:12:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.606719 on epoch=99
06/19/2022 22:12:43 - INFO - __main__ - Global step 200 Train loss 0.576033 ACC 0.46875 on epoch=99
06/19/2022 22:12:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.621628 on epoch=104
06/19/2022 22:12:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.500779 on epoch=109
06/19/2022 22:12:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.833501 on epoch=114
06/19/2022 22:12:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.413164 on epoch=119
06/19/2022 22:12:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.465381 on epoch=124
06/19/2022 22:12:56 - INFO - __main__ - Global step 250 Train loss 0.566891 ACC 0.46875 on epoch=124
06/19/2022 22:12:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.559025 on epoch=129
06/19/2022 22:13:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.397349 on epoch=134
06/19/2022 22:13:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.422440 on epoch=139
06/19/2022 22:13:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.448508 on epoch=144
06/19/2022 22:13:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.402397 on epoch=149
06/19/2022 22:13:09 - INFO - __main__ - Global step 300 Train loss 0.445944 ACC 0.5 on epoch=149
06/19/2022 22:13:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.392339 on epoch=154
06/19/2022 22:13:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.342200 on epoch=159
06/19/2022 22:13:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.377033 on epoch=164
06/19/2022 22:13:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.398965 on epoch=169
06/19/2022 22:13:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.315247 on epoch=174
06/19/2022 22:13:22 - INFO - __main__ - Global step 350 Train loss 0.365157 ACC 0.46875 on epoch=174
06/19/2022 22:13:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.464740 on epoch=179
06/19/2022 22:13:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.311458 on epoch=184
06/19/2022 22:13:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.390749 on epoch=189
06/19/2022 22:13:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.371069 on epoch=194
06/19/2022 22:13:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.396120 on epoch=199
06/19/2022 22:13:34 - INFO - __main__ - Global step 400 Train loss 0.386827 ACC 0.5 on epoch=199
06/19/2022 22:13:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.400960 on epoch=204
06/19/2022 22:13:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.343007 on epoch=209
06/19/2022 22:13:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.347875 on epoch=214
06/19/2022 22:13:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.297755 on epoch=219
06/19/2022 22:13:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.294800 on epoch=224
06/19/2022 22:13:47 - INFO - __main__ - Global step 450 Train loss 0.336880 ACC 0.5 on epoch=224
06/19/2022 22:13:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.286742 on epoch=229
06/19/2022 22:13:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.329251 on epoch=234
06/19/2022 22:13:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.415634 on epoch=239
06/19/2022 22:13:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.302130 on epoch=244
06/19/2022 22:14:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.333902 on epoch=249
06/19/2022 22:14:00 - INFO - __main__ - Global step 500 Train loss 0.333532 ACC 0.53125 on epoch=249
06/19/2022 22:14:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.407145 on epoch=254
06/19/2022 22:14:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.358394 on epoch=259
06/19/2022 22:14:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.355564 on epoch=264
06/19/2022 22:14:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.324464 on epoch=269
06/19/2022 22:14:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.332156 on epoch=274
06/19/2022 22:14:13 - INFO - __main__ - Global step 550 Train loss 0.355545 ACC 0.4375 on epoch=274
06/19/2022 22:14:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.386496 on epoch=279
06/19/2022 22:14:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.298065 on epoch=284
06/19/2022 22:14:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.313871 on epoch=289
06/19/2022 22:14:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.357980 on epoch=294
06/19/2022 22:14:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.358923 on epoch=299
06/19/2022 22:14:26 - INFO - __main__ - Global step 600 Train loss 0.343067 ACC 0.53125 on epoch=299
06/19/2022 22:14:26 - INFO - __main__ - save last model!
06/19/2022 22:14:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:14:27 - INFO - __main__ - Printing 3 examples
06/19/2022 22:14:27 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/19/2022 22:14:27 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:27 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/19/2022 22:14:27 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/19/2022 22:14:27 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:14:27 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:14:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:14:27 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:14:27 - INFO - __main__ - Printing 3 examples
06/19/2022 22:14:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/19/2022 22:14:27 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/19/2022 22:14:27 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:27 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/19/2022 22:14:27 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:27 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:14:27 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:14:27 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:14:29 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:14:29 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:14:29 - INFO - __main__ - Printing 3 examples
06/19/2022 22:14:29 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:14:29 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:29 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:14:29 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:29 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:14:29 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:29 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:14:30 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:14:30 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:14:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:14:32 - INFO - __main__ - Starting training!
06/19/2022 22:14:35 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.0003_8_predictions.txt
06/19/2022 22:14:35 - INFO - __main__ - ACC on test data: 0.5164
06/19/2022 22:14:35 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.0003, bsz=8, dev_performance=0.53125, test_performance=0.5163934426229508
06/19/2022 22:14:35 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.0002, bsz=8 ...
06/19/2022 22:14:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:14:36 - INFO - __main__ - Printing 3 examples
06/19/2022 22:14:36 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/19/2022 22:14:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:36 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/19/2022 22:14:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/19/2022 22:14:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:14:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:14:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:14:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:14:36 - INFO - __main__ - Printing 3 examples
06/19/2022 22:14:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/19/2022 22:14:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/19/2022 22:14:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/19/2022 22:14:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:14:36 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:14:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:14:36 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:14:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:14:40 - INFO - __main__ - Starting training!
06/19/2022 22:14:42 - INFO - __main__ - Step 10 Global step 10 Train loss 18.834593 on epoch=4
06/19/2022 22:14:44 - INFO - __main__ - Step 20 Global step 20 Train loss 12.742158 on epoch=9
06/19/2022 22:14:47 - INFO - __main__ - Step 30 Global step 30 Train loss 9.119041 on epoch=14
06/19/2022 22:14:49 - INFO - __main__ - Step 40 Global step 40 Train loss 6.506831 on epoch=19
06/19/2022 22:14:52 - INFO - __main__ - Step 50 Global step 50 Train loss 5.365610 on epoch=24
06/19/2022 22:14:53 - INFO - __main__ - Global step 50 Train loss 10.513645 ACC 0.0 on epoch=24
06/19/2022 22:14:56 - INFO - __main__ - Step 60 Global step 60 Train loss 4.927352 on epoch=29
06/19/2022 22:14:58 - INFO - __main__ - Step 70 Global step 70 Train loss 4.146152 on epoch=34
06/19/2022 22:15:01 - INFO - __main__ - Step 80 Global step 80 Train loss 3.565250 on epoch=39
06/19/2022 22:15:04 - INFO - __main__ - Step 90 Global step 90 Train loss 3.559921 on epoch=44
06/19/2022 22:15:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.718338 on epoch=49
06/19/2022 22:15:06 - INFO - __main__ - Global step 100 Train loss 3.583403 ACC 0.46875 on epoch=49
06/19/2022 22:15:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.706620 on epoch=54
06/19/2022 22:15:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.233156 on epoch=59
06/19/2022 22:15:14 - INFO - __main__ - Step 130 Global step 130 Train loss 1.393503 on epoch=64
06/19/2022 22:15:17 - INFO - __main__ - Step 140 Global step 140 Train loss 1.279847 on epoch=69
06/19/2022 22:15:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.932258 on epoch=74
06/19/2022 22:15:20 - INFO - __main__ - Global step 150 Train loss 1.309077 ACC 0.375 on epoch=74
06/19/2022 22:15:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.829624 on epoch=79
06/19/2022 22:15:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.562093 on epoch=84
06/19/2022 22:15:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.728968 on epoch=89
06/19/2022 22:15:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.808906 on epoch=94
06/19/2022 22:15:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.836460 on epoch=99
06/19/2022 22:15:33 - INFO - __main__ - Global step 200 Train loss 0.753210 ACC 0.375 on epoch=99
06/19/2022 22:15:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.641238 on epoch=104
06/19/2022 22:15:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.543882 on epoch=109
06/19/2022 22:15:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.773020 on epoch=114
06/19/2022 22:15:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.626340 on epoch=119
06/19/2022 22:15:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.499077 on epoch=124
06/19/2022 22:15:45 - INFO - __main__ - Global step 250 Train loss 0.616711 ACC 0.4375 on epoch=124
06/19/2022 22:15:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.467683 on epoch=129
06/19/2022 22:15:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.484771 on epoch=134
06/19/2022 22:15:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.637066 on epoch=139
06/19/2022 22:15:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.556505 on epoch=144
06/19/2022 22:15:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.443672 on epoch=149
06/19/2022 22:15:58 - INFO - __main__ - Global step 300 Train loss 0.517939 ACC 0.5 on epoch=149
06/19/2022 22:16:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.453857 on epoch=154
06/19/2022 22:16:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.403648 on epoch=159
06/19/2022 22:16:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.505191 on epoch=164
06/19/2022 22:16:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.455766 on epoch=169
06/19/2022 22:16:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.422730 on epoch=174
06/19/2022 22:16:11 - INFO - __main__ - Global step 350 Train loss 0.448239 ACC 0.46875 on epoch=174
06/19/2022 22:16:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.362104 on epoch=179
06/19/2022 22:16:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.551945 on epoch=184
06/19/2022 22:16:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.339837 on epoch=189
06/19/2022 22:16:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.494955 on epoch=194
06/19/2022 22:16:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.396682 on epoch=199
06/19/2022 22:16:24 - INFO - __main__ - Global step 400 Train loss 0.429105 ACC 0.5625 on epoch=199
06/19/2022 22:16:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.411655 on epoch=204
06/19/2022 22:16:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.334580 on epoch=209
06/19/2022 22:16:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.414032 on epoch=214
06/19/2022 22:16:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.420239 on epoch=219
06/19/2022 22:16:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.307533 on epoch=224
06/19/2022 22:16:38 - INFO - __main__ - Global step 450 Train loss 0.377608 ACC 0.46875 on epoch=224
06/19/2022 22:16:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.367273 on epoch=229
06/19/2022 22:16:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.428434 on epoch=234
06/19/2022 22:16:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.377663 on epoch=239
06/19/2022 22:16:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.373400 on epoch=244
06/19/2022 22:16:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.344655 on epoch=249
06/19/2022 22:16:50 - INFO - __main__ - Global step 500 Train loss 0.378285 ACC 0.53125 on epoch=249
06/19/2022 22:16:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.310999 on epoch=254
06/19/2022 22:16:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.412067 on epoch=259
06/19/2022 22:16:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.337864 on epoch=264
06/19/2022 22:17:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.383795 on epoch=269
06/19/2022 22:17:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.472202 on epoch=274
06/19/2022 22:17:03 - INFO - __main__ - Global step 550 Train loss 0.383386 ACC 0.46875 on epoch=274
06/19/2022 22:17:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.338203 on epoch=279
06/19/2022 22:17:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.339034 on epoch=284
06/19/2022 22:17:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.514689 on epoch=289
06/19/2022 22:17:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.352785 on epoch=294
06/19/2022 22:17:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.358937 on epoch=299
06/19/2022 22:17:16 - INFO - __main__ - Global step 600 Train loss 0.380729 ACC 0.5 on epoch=299
06/19/2022 22:17:16 - INFO - __main__ - save last model!
06/19/2022 22:17:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:17:17 - INFO - __main__ - Printing 3 examples
06/19/2022 22:17:17 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/19/2022 22:17:17 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:17 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/19/2022 22:17:17 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:17 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/19/2022 22:17:17 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:17:17 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:17:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:17:17 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:17:17 - INFO - __main__ - Printing 3 examples
06/19/2022 22:17:17 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/19/2022 22:17:17 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:17 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/19/2022 22:17:17 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:17 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/19/2022 22:17:17 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:17 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:17:17 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:17:17 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:17:19 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:17:19 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:17:19 - INFO - __main__ - Printing 3 examples
06/19/2022 22:17:19 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:17:19 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:19 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:17:19 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:19 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:17:19 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:19 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:17:19 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:17:20 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:17:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:17:21 - INFO - __main__ - Starting training!
06/19/2022 22:17:24 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.0002_8_predictions.txt
06/19/2022 22:17:24 - INFO - __main__ - ACC on test data: 0.5475
06/19/2022 22:17:24 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.0002, bsz=8, dev_performance=0.5625, test_performance=0.5475409836065573
06/19/2022 22:17:25 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_42, lr=0.0001, bsz=8 ...
06/19/2022 22:17:25 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:17:25 - INFO - __main__ - Printing 3 examples
06/19/2022 22:17:25 - INFO - __main__ -  [medical_questions_pairs] question 1: For the past few months, around the same time every month I get moody, irritable, bloated, fatigue cramps. These symptoms go on for about a week? [SEP] question 2: I have noticed that my sister has symptoms like being moody, irritable, bloated before her period occurs. Why is that so?
06/19/2022 22:17:25 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:25 - INFO - __main__ -  [medical_questions_pairs] question 1: When you lose weight, when does it plateau? [SEP] question 2: Once you start a weight loss regimen, do you lose weight consistently or does the rate of weight loss slow down over time?
06/19/2022 22:17:25 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:25 - INFO - __main__ -  [medical_questions_pairs] question 1: I discovered I get this weakness in my hand whenever I try to snap my fingers, slight pain runs across elbow and wrist? [SEP] question 2: When I try to snap my fingers there is weakness and pain across elbow and wrist? May I know what are the causes?
06/19/2022 22:17:25 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:17:25 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:17:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:17:26 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:17:26 - INFO - __main__ - Printing 3 examples
06/19/2022 22:17:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I had a cold about a week and a half ago. Now I'm having very strong headaches and a pain on my left nasal, feels like the bone hurts on my left nasal? [SEP] question 2: I had cold 1.5 weeks ago but I started experiencing severe headache and pain in left nostril yesterday. Is it related to bone in my left nostril?
06/19/2022 22:17:26 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I've had a cold for 1week. I just stopped antibiotics. Coughing up grey mucus sometimes. Dr isn't concerned. Should l take 2nd lot of antibiotics? [SEP] question 2: I've been taking antibiotics for my cold for almost a week now, but I still am coughing up some phlegm with grey mucous membranes. Should I continue with the antibiotics? 
06/19/2022 22:17:26 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:26 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm bleeding from penis for no reason. What could this be? [SEP] question 2: What are some causes of bleeding from the penis?
06/19/2022 22:17:26 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:17:26 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:17:26 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:17:26 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:17:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:17:29 - INFO - __main__ - Starting training!
06/19/2022 22:17:31 - INFO - __main__ - Step 10 Global step 10 Train loss 18.562811 on epoch=4
06/19/2022 22:17:33 - INFO - __main__ - Step 20 Global step 20 Train loss 17.588703 on epoch=9
06/19/2022 22:17:36 - INFO - __main__ - Step 30 Global step 30 Train loss 12.405912 on epoch=14
06/19/2022 22:17:38 - INFO - __main__ - Step 40 Global step 40 Train loss 9.968210 on epoch=19
06/19/2022 22:17:41 - INFO - __main__ - Step 50 Global step 50 Train loss 9.422848 on epoch=24
06/19/2022 22:17:43 - INFO - __main__ - Global step 50 Train loss 13.589697 ACC 0.0 on epoch=24
06/19/2022 22:17:46 - INFO - __main__ - Step 60 Global step 60 Train loss 8.298203 on epoch=29
06/19/2022 22:17:48 - INFO - __main__ - Step 70 Global step 70 Train loss 7.544498 on epoch=34
06/19/2022 22:17:51 - INFO - __main__ - Step 80 Global step 80 Train loss 7.275489 on epoch=39
06/19/2022 22:17:53 - INFO - __main__ - Step 90 Global step 90 Train loss 6.113791 on epoch=44
06/19/2022 22:17:56 - INFO - __main__ - Step 100 Global step 100 Train loss 5.188893 on epoch=49
06/19/2022 22:17:57 - INFO - __main__ - Global step 100 Train loss 6.884175 ACC 0.0 on epoch=49
06/19/2022 22:18:00 - INFO - __main__ - Step 110 Global step 110 Train loss 5.142051 on epoch=54
06/19/2022 22:18:02 - INFO - __main__ - Step 120 Global step 120 Train loss 5.106194 on epoch=59
06/19/2022 22:18:05 - INFO - __main__ - Step 130 Global step 130 Train loss 3.894529 on epoch=64
06/19/2022 22:18:07 - INFO - __main__ - Step 140 Global step 140 Train loss 3.269058 on epoch=69
06/19/2022 22:18:10 - INFO - __main__ - Step 150 Global step 150 Train loss 3.560943 on epoch=74
06/19/2022 22:18:10 - INFO - __main__ - Global step 150 Train loss 4.194555 ACC 0.5 on epoch=74
06/19/2022 22:18:13 - INFO - __main__ - Step 160 Global step 160 Train loss 2.360893 on epoch=79
06/19/2022 22:18:16 - INFO - __main__ - Step 170 Global step 170 Train loss 2.389047 on epoch=84
06/19/2022 22:18:18 - INFO - __main__ - Step 180 Global step 180 Train loss 1.828407 on epoch=89
06/19/2022 22:18:21 - INFO - __main__ - Step 190 Global step 190 Train loss 1.766296 on epoch=94
06/19/2022 22:18:23 - INFO - __main__ - Step 200 Global step 200 Train loss 1.622331 on epoch=99
06/19/2022 22:18:23 - INFO - __main__ - Global step 200 Train loss 1.993395 ACC 0.53125 on epoch=99
06/19/2022 22:18:26 - INFO - __main__ - Step 210 Global step 210 Train loss 1.208053 on epoch=104
06/19/2022 22:18:29 - INFO - __main__ - Step 220 Global step 220 Train loss 1.206966 on epoch=109
06/19/2022 22:18:31 - INFO - __main__ - Step 230 Global step 230 Train loss 1.037651 on epoch=114
06/19/2022 22:18:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.939018 on epoch=119
06/19/2022 22:18:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.745821 on epoch=124
06/19/2022 22:18:36 - INFO - __main__ - Global step 250 Train loss 1.027502 ACC 0.5625 on epoch=124
06/19/2022 22:18:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.902901 on epoch=129
06/19/2022 22:18:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.948832 on epoch=134
06/19/2022 22:18:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.741285 on epoch=139
06/19/2022 22:18:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.780971 on epoch=144
06/19/2022 22:18:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.792251 on epoch=149
06/19/2022 22:18:50 - INFO - __main__ - Global step 300 Train loss 0.833248 ACC 0.5 on epoch=149
06/19/2022 22:18:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.843359 on epoch=154
06/19/2022 22:18:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.778508 on epoch=159
06/19/2022 22:18:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.622054 on epoch=164
06/19/2022 22:18:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.488532 on epoch=169
06/19/2022 22:19:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.566319 on epoch=174
06/19/2022 22:19:02 - INFO - __main__ - Global step 350 Train loss 0.659755 ACC 0.5 on epoch=174
06/19/2022 22:19:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.553197 on epoch=179
06/19/2022 22:19:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.386755 on epoch=184
06/19/2022 22:19:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.510384 on epoch=189
06/19/2022 22:19:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.463568 on epoch=194
06/19/2022 22:19:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.450525 on epoch=199
06/19/2022 22:19:15 - INFO - __main__ - Global step 400 Train loss 0.472886 ACC 0.5 on epoch=199
06/19/2022 22:19:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.430169 on epoch=204
06/19/2022 22:19:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.427223 on epoch=209
06/19/2022 22:19:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.520932 on epoch=214
06/19/2022 22:19:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.446476 on epoch=219
06/19/2022 22:19:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.502495 on epoch=224
06/19/2022 22:19:27 - INFO - __main__ - Global step 450 Train loss 0.465459 ACC 0.5 on epoch=224
06/19/2022 22:19:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.585123 on epoch=229
06/19/2022 22:19:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.584479 on epoch=234
06/19/2022 22:19:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.427983 on epoch=239
06/19/2022 22:19:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.445800 on epoch=244
06/19/2022 22:19:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.587322 on epoch=249
06/19/2022 22:19:40 - INFO - __main__ - Global step 500 Train loss 0.526142 ACC 0.46875 on epoch=249
06/19/2022 22:19:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.376328 on epoch=254
06/19/2022 22:19:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.426703 on epoch=259
06/19/2022 22:19:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.520669 on epoch=264
06/19/2022 22:19:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.403046 on epoch=269
06/19/2022 22:19:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.525376 on epoch=274
06/19/2022 22:19:52 - INFO - __main__ - Global step 550 Train loss 0.450424 ACC 0.5 on epoch=274
06/19/2022 22:19:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.342869 on epoch=279
06/19/2022 22:19:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.361286 on epoch=284
06/19/2022 22:20:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.512545 on epoch=289
06/19/2022 22:20:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.428887 on epoch=294
06/19/2022 22:20:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.353433 on epoch=299
06/19/2022 22:20:05 - INFO - __main__ - Global step 600 Train loss 0.399804 ACC 0.5625 on epoch=299
06/19/2022 22:20:05 - INFO - __main__ - save last model!
06/19/2022 22:20:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:20:06 - INFO - __main__ - Printing 3 examples
06/19/2022 22:20:06 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/19/2022 22:20:06 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:06 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/19/2022 22:20:06 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:06 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/19/2022 22:20:06 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:20:06 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:20:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:20:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:20:06 - INFO - __main__ - Printing 3 examples
06/19/2022 22:20:06 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/19/2022 22:20:06 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:06 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/19/2022 22:20:06 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:06 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/19/2022 22:20:06 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:06 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:20:06 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:20:06 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:20:07 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:20:08 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:20:08 - INFO - __main__ - Printing 3 examples
06/19/2022 22:20:08 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:20:08 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:08 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:20:08 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:08 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:20:08 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:08 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:20:08 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:20:08 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:20:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:20:09 - INFO - __main__ - Starting training!
06/19/2022 22:20:13 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_42_0.0001_8_predictions.txt
06/19/2022 22:20:13 - INFO - __main__ - ACC on test data: 0.5295
06/19/2022 22:20:13 - INFO - __main__ - prefix=medical_questions_pairs_16_42, lr=0.0001, bsz=8, dev_performance=0.5625, test_performance=0.5295081967213114
06/19/2022 22:20:13 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.0005, bsz=8 ...
06/19/2022 22:20:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:20:14 - INFO - __main__ - Printing 3 examples
06/19/2022 22:20:14 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/19/2022 22:20:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:14 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/19/2022 22:20:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:14 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/19/2022 22:20:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:20:14 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:20:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:20:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:20:14 - INFO - __main__ - Printing 3 examples
06/19/2022 22:20:14 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/19/2022 22:20:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:14 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/19/2022 22:20:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:14 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/19/2022 22:20:14 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:20:14 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:20:14 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:20:14 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:20:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:20:19 - INFO - __main__ - Starting training!
06/19/2022 22:20:21 - INFO - __main__ - Step 10 Global step 10 Train loss 17.920025 on epoch=4
06/19/2022 22:20:23 - INFO - __main__ - Step 20 Global step 20 Train loss 11.766829 on epoch=9
06/19/2022 22:20:25 - INFO - __main__ - Step 30 Global step 30 Train loss 6.230873 on epoch=14
06/19/2022 22:20:28 - INFO - __main__ - Step 40 Global step 40 Train loss 3.218684 on epoch=19
06/19/2022 22:20:30 - INFO - __main__ - Step 50 Global step 50 Train loss 1.796926 on epoch=24
06/19/2022 22:20:30 - INFO - __main__ - Global step 50 Train loss 8.186666 ACC 0.5 on epoch=24
06/19/2022 22:20:33 - INFO - __main__ - Step 60 Global step 60 Train loss 0.898172 on epoch=29
06/19/2022 22:20:36 - INFO - __main__ - Step 70 Global step 70 Train loss 1.322638 on epoch=34
06/19/2022 22:20:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.694838 on epoch=39
06/19/2022 22:20:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.848734 on epoch=44
06/19/2022 22:20:43 - INFO - __main__ - Step 100 Global step 100 Train loss 1.087901 on epoch=49
06/19/2022 22:20:43 - INFO - __main__ - Global step 100 Train loss 0.970457 ACC 0.5625 on epoch=49
06/19/2022 22:20:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.622989 on epoch=54
06/19/2022 22:20:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.653820 on epoch=59
06/19/2022 22:20:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.690055 on epoch=64
06/19/2022 22:20:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.620346 on epoch=69
06/19/2022 22:20:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.520343 on epoch=74
06/19/2022 22:20:57 - INFO - __main__ - Global step 150 Train loss 0.621511 ACC 0.5 on epoch=74
06/19/2022 22:21:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.560075 on epoch=79
06/19/2022 22:21:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.546418 on epoch=84
06/19/2022 22:21:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.406938 on epoch=89
06/19/2022 22:21:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.451675 on epoch=94
06/19/2022 22:21:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.489769 on epoch=99
06/19/2022 22:21:10 - INFO - __main__ - Global step 200 Train loss 0.490975 ACC 0.5 on epoch=99
06/19/2022 22:21:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.438331 on epoch=104
06/19/2022 22:21:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.436592 on epoch=109
06/19/2022 22:21:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.370192 on epoch=114
06/19/2022 22:21:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.357316 on epoch=119
06/19/2022 22:21:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.408101 on epoch=124
06/19/2022 22:21:23 - INFO - __main__ - Global step 250 Train loss 0.402107 ACC 0.5 on epoch=124
06/19/2022 22:21:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.442725 on epoch=129
06/19/2022 22:21:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.393109 on epoch=134
06/19/2022 22:21:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.446769 on epoch=139
06/19/2022 22:21:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.473394 on epoch=144
06/19/2022 22:21:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.435363 on epoch=149
06/19/2022 22:21:36 - INFO - __main__ - Global step 300 Train loss 0.438272 ACC 0.4375 on epoch=149
06/19/2022 22:21:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.497428 on epoch=154
06/19/2022 22:21:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.382843 on epoch=159
06/19/2022 22:21:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.442223 on epoch=164
06/19/2022 22:21:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.426356 on epoch=169
06/19/2022 22:21:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.421568 on epoch=174
06/19/2022 22:21:48 - INFO - __main__ - Global step 350 Train loss 0.434084 ACC 0.375 on epoch=174
06/19/2022 22:21:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.316172 on epoch=179
06/19/2022 22:21:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.354214 on epoch=184
06/19/2022 22:21:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.342788 on epoch=189
06/19/2022 22:21:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.390802 on epoch=194
06/19/2022 22:22:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.391589 on epoch=199
06/19/2022 22:22:01 - INFO - __main__ - Global step 400 Train loss 0.359113 ACC 0.5 on epoch=199
06/19/2022 22:22:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.362500 on epoch=204
06/19/2022 22:22:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.340375 on epoch=209
06/19/2022 22:22:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.279989 on epoch=214
06/19/2022 22:22:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.295504 on epoch=219
06/19/2022 22:22:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.362294 on epoch=224
06/19/2022 22:22:14 - INFO - __main__ - Global step 450 Train loss 0.328132 ACC 0.5 on epoch=224
06/19/2022 22:22:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.272496 on epoch=229
06/19/2022 22:22:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.337376 on epoch=234
06/19/2022 22:22:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.315464 on epoch=239
06/19/2022 22:22:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.307328 on epoch=244
06/19/2022 22:22:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.306854 on epoch=249
06/19/2022 22:22:27 - INFO - __main__ - Global step 500 Train loss 0.307903 ACC 0.5 on epoch=249
06/19/2022 22:22:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.330066 on epoch=254
06/19/2022 22:22:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.327094 on epoch=259
06/19/2022 22:22:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.317921 on epoch=264
06/19/2022 22:22:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.320449 on epoch=269
06/19/2022 22:22:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.296080 on epoch=274
06/19/2022 22:22:40 - INFO - __main__ - Global step 550 Train loss 0.318322 ACC 0.4375 on epoch=274
06/19/2022 22:22:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.285292 on epoch=279
06/19/2022 22:22:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.320262 on epoch=284
06/19/2022 22:22:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.303695 on epoch=289
06/19/2022 22:22:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.286744 on epoch=294
06/19/2022 22:22:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.277322 on epoch=299
06/19/2022 22:22:53 - INFO - __main__ - Global step 600 Train loss 0.294663 ACC 0.5 on epoch=299
06/19/2022 22:22:53 - INFO - __main__ - save last model!
06/19/2022 22:22:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:22:54 - INFO - __main__ - Printing 3 examples
06/19/2022 22:22:54 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/19/2022 22:22:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:22:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/19/2022 22:22:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:22:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/19/2022 22:22:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:22:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:22:54 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:22:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:22:54 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:22:54 - INFO - __main__ - Printing 3 examples
06/19/2022 22:22:54 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/19/2022 22:22:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:22:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/19/2022 22:22:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:22:54 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/19/2022 22:22:54 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:22:54 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:22:54 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:22:54 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:22:55 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:22:55 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:22:55 - INFO - __main__ - Printing 3 examples
06/19/2022 22:22:55 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:22:55 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:22:55 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:22:55 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:22:55 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:22:55 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:22:55 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:22:56 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:22:56 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:22:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:22:57 - INFO - __main__ - Starting training!
06/19/2022 22:23:01 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.0005_8_predictions.txt
06/19/2022 22:23:01 - INFO - __main__ - ACC on test data: 0.4984
06/19/2022 22:23:01 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.0005, bsz=8, dev_performance=0.5625, test_performance=0.49836065573770494
06/19/2022 22:23:01 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.0003, bsz=8 ...
06/19/2022 22:23:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:23:02 - INFO - __main__ - Printing 3 examples
06/19/2022 22:23:02 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/19/2022 22:23:02 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:23:02 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/19/2022 22:23:02 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:23:02 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/19/2022 22:23:02 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:23:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:23:02 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:23:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:23:02 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:23:02 - INFO - __main__ - Printing 3 examples
06/19/2022 22:23:02 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/19/2022 22:23:02 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:23:02 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/19/2022 22:23:02 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:23:02 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/19/2022 22:23:02 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:23:02 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:23:02 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:23:02 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:23:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:23:06 - INFO - __main__ - Starting training!
06/19/2022 22:23:08 - INFO - __main__ - Step 10 Global step 10 Train loss 18.821100 on epoch=4
06/19/2022 22:23:10 - INFO - __main__ - Step 20 Global step 20 Train loss 14.745058 on epoch=9
06/19/2022 22:23:12 - INFO - __main__ - Step 30 Global step 30 Train loss 9.716303 on epoch=14
06/19/2022 22:23:15 - INFO - __main__ - Step 40 Global step 40 Train loss 6.937131 on epoch=19
06/19/2022 22:23:17 - INFO - __main__ - Step 50 Global step 50 Train loss 4.869977 on epoch=24
06/19/2022 22:23:19 - INFO - __main__ - Global step 50 Train loss 11.017914 ACC 0.34375 on epoch=24
06/19/2022 22:23:21 - INFO - __main__ - Step 60 Global step 60 Train loss 3.488509 on epoch=29
06/19/2022 22:23:24 - INFO - __main__ - Step 70 Global step 70 Train loss 2.586156 on epoch=34
06/19/2022 22:23:26 - INFO - __main__ - Step 80 Global step 80 Train loss 1.857976 on epoch=39
06/19/2022 22:23:29 - INFO - __main__ - Step 90 Global step 90 Train loss 1.371373 on epoch=44
06/19/2022 22:23:31 - INFO - __main__ - Step 100 Global step 100 Train loss 1.151046 on epoch=49
06/19/2022 22:23:32 - INFO - __main__ - Global step 100 Train loss 2.091012 ACC 0.53125 on epoch=49
06/19/2022 22:23:35 - INFO - __main__ - Step 110 Global step 110 Train loss 1.334499 on epoch=54
06/19/2022 22:23:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.948972 on epoch=59
06/19/2022 22:23:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.751886 on epoch=64
06/19/2022 22:23:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.641573 on epoch=69
06/19/2022 22:23:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.673128 on epoch=74
06/19/2022 22:23:45 - INFO - __main__ - Global step 150 Train loss 0.870012 ACC 0.4375 on epoch=74
06/19/2022 22:23:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.466465 on epoch=79
06/19/2022 22:23:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.998029 on epoch=84
06/19/2022 22:23:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.637455 on epoch=89
06/19/2022 22:23:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.567941 on epoch=94
06/19/2022 22:23:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.606375 on epoch=99
06/19/2022 22:23:57 - INFO - __main__ - Global step 200 Train loss 0.655253 ACC 0.4375 on epoch=99
06/19/2022 22:24:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.445043 on epoch=104
06/19/2022 22:24:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.450402 on epoch=109
06/19/2022 22:24:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.559782 on epoch=114
06/19/2022 22:24:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.497239 on epoch=119
06/19/2022 22:24:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.590935 on epoch=124
06/19/2022 22:24:10 - INFO - __main__ - Global step 250 Train loss 0.508680 ACC 0.4375 on epoch=124
06/19/2022 22:24:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.535409 on epoch=129
06/19/2022 22:24:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.334390 on epoch=134
06/19/2022 22:24:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.494930 on epoch=139
06/19/2022 22:24:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.374430 on epoch=144
06/19/2022 22:24:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.496227 on epoch=149
06/19/2022 22:24:22 - INFO - __main__ - Global step 300 Train loss 0.447077 ACC 0.5 on epoch=149
06/19/2022 22:24:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.479455 on epoch=154
06/19/2022 22:24:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.345795 on epoch=159
06/19/2022 22:24:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.293321 on epoch=164
06/19/2022 22:24:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.389675 on epoch=169
06/19/2022 22:24:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.354846 on epoch=174
06/19/2022 22:24:35 - INFO - __main__ - Global step 350 Train loss 0.372618 ACC 0.46875 on epoch=174
06/19/2022 22:24:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.327195 on epoch=179
06/19/2022 22:24:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.253262 on epoch=184
06/19/2022 22:24:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.280916 on epoch=189
06/19/2022 22:24:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.326692 on epoch=194
06/19/2022 22:24:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.295488 on epoch=199
06/19/2022 22:24:47 - INFO - __main__ - Global step 400 Train loss 0.296711 ACC 0.59375 on epoch=199
06/19/2022 22:24:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.250063 on epoch=204
06/19/2022 22:24:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.284283 on epoch=209
06/19/2022 22:24:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.276925 on epoch=214
06/19/2022 22:24:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.282813 on epoch=219
06/19/2022 22:25:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.305707 on epoch=224
06/19/2022 22:25:00 - INFO - __main__ - Global step 450 Train loss 0.279958 ACC 0.53125 on epoch=224
06/19/2022 22:25:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.279039 on epoch=229
06/19/2022 22:25:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.173456 on epoch=234
06/19/2022 22:25:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.242912 on epoch=239
06/19/2022 22:25:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.184761 on epoch=244
06/19/2022 22:25:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.182638 on epoch=249
06/19/2022 22:25:13 - INFO - __main__ - Global step 500 Train loss 0.212561 ACC 0.53125 on epoch=249
06/19/2022 22:25:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.199103 on epoch=254
06/19/2022 22:25:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.225319 on epoch=259
06/19/2022 22:25:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.178884 on epoch=264
06/19/2022 22:25:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.166742 on epoch=269
06/19/2022 22:25:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.239903 on epoch=274
06/19/2022 22:25:25 - INFO - __main__ - Global step 550 Train loss 0.201990 ACC 0.53125 on epoch=274
06/19/2022 22:25:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.183556 on epoch=279
06/19/2022 22:25:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.252097 on epoch=284
06/19/2022 22:25:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.187334 on epoch=289
06/19/2022 22:25:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.130376 on epoch=294
06/19/2022 22:25:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.211305 on epoch=299
06/19/2022 22:25:38 - INFO - __main__ - Global step 600 Train loss 0.192934 ACC 0.53125 on epoch=299
06/19/2022 22:25:38 - INFO - __main__ - save last model!
06/19/2022 22:25:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:25:39 - INFO - __main__ - Printing 3 examples
06/19/2022 22:25:39 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/19/2022 22:25:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/19/2022 22:25:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/19/2022 22:25:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:25:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:25:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:25:39 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:25:39 - INFO - __main__ - Printing 3 examples
06/19/2022 22:25:39 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/19/2022 22:25:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/19/2022 22:25:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:39 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/19/2022 22:25:39 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:39 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:25:39 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:25:39 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:25:41 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:25:41 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:25:41 - INFO - __main__ - Printing 3 examples
06/19/2022 22:25:41 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:25:41 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:41 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:25:41 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:41 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:25:41 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:41 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:25:41 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:25:42 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:25:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:25:43 - INFO - __main__ - Starting training!
06/19/2022 22:25:46 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.0003_8_predictions.txt
06/19/2022 22:25:46 - INFO - __main__ - ACC on test data: 0.5393
06/19/2022 22:25:47 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.5393442622950819
06/19/2022 22:25:47 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.0002, bsz=8 ...
06/19/2022 22:25:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:25:47 - INFO - __main__ - Printing 3 examples
06/19/2022 22:25:47 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/19/2022 22:25:47 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:47 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/19/2022 22:25:47 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:47 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/19/2022 22:25:47 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:25:47 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:25:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:25:48 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:25:48 - INFO - __main__ - Printing 3 examples
06/19/2022 22:25:48 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/19/2022 22:25:48 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:48 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/19/2022 22:25:48 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:48 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/19/2022 22:25:48 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:25:48 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:25:48 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:25:48 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:25:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:25:52 - INFO - __main__ - Starting training!
06/19/2022 22:25:54 - INFO - __main__ - Step 10 Global step 10 Train loss 18.086256 on epoch=4
06/19/2022 22:25:56 - INFO - __main__ - Step 20 Global step 20 Train loss 15.279223 on epoch=9
06/19/2022 22:25:59 - INFO - __main__ - Step 30 Global step 30 Train loss 10.153497 on epoch=14
06/19/2022 22:26:01 - INFO - __main__ - Step 40 Global step 40 Train loss 8.655247 on epoch=19
06/19/2022 22:26:04 - INFO - __main__ - Step 50 Global step 50 Train loss 5.895544 on epoch=24
06/19/2022 22:26:06 - INFO - __main__ - Global step 50 Train loss 11.613954 ACC 0.0 on epoch=24
06/19/2022 22:26:08 - INFO - __main__ - Step 60 Global step 60 Train loss 5.115770 on epoch=29
06/19/2022 22:26:11 - INFO - __main__ - Step 70 Global step 70 Train loss 4.735017 on epoch=34
06/19/2022 22:26:13 - INFO - __main__ - Step 80 Global step 80 Train loss 3.710901 on epoch=39
06/19/2022 22:26:16 - INFO - __main__ - Step 90 Global step 90 Train loss 3.222229 on epoch=44
06/19/2022 22:26:18 - INFO - __main__ - Step 100 Global step 100 Train loss 2.702595 on epoch=49
06/19/2022 22:26:18 - INFO - __main__ - Global step 100 Train loss 3.897302 ACC 0.5 on epoch=49
06/19/2022 22:26:21 - INFO - __main__ - Step 110 Global step 110 Train loss 2.224216 on epoch=54
06/19/2022 22:26:24 - INFO - __main__ - Step 120 Global step 120 Train loss 2.305759 on epoch=59
06/19/2022 22:26:27 - INFO - __main__ - Step 130 Global step 130 Train loss 1.116160 on epoch=64
06/19/2022 22:26:29 - INFO - __main__ - Step 140 Global step 140 Train loss 1.358167 on epoch=69
06/19/2022 22:26:32 - INFO - __main__ - Step 150 Global step 150 Train loss 1.419963 on epoch=74
06/19/2022 22:26:32 - INFO - __main__ - Global step 150 Train loss 1.684853 ACC 0.40625 on epoch=74
06/19/2022 22:26:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.866685 on epoch=79
06/19/2022 22:26:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.262867 on epoch=84
06/19/2022 22:26:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.866479 on epoch=89
06/19/2022 22:26:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.932430 on epoch=94
06/19/2022 22:26:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.765912 on epoch=99
06/19/2022 22:26:45 - INFO - __main__ - Global step 200 Train loss 0.938875 ACC 0.4375 on epoch=99
06/19/2022 22:26:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.580103 on epoch=104
06/19/2022 22:26:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.858506 on epoch=109
06/19/2022 22:26:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.906668 on epoch=114
06/19/2022 22:26:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.765738 on epoch=119
06/19/2022 22:26:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.592945 on epoch=124
06/19/2022 22:26:57 - INFO - __main__ - Global step 250 Train loss 0.740792 ACC 0.5 on epoch=124
06/19/2022 22:27:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.504193 on epoch=129
06/19/2022 22:27:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.547251 on epoch=134
06/19/2022 22:27:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.617583 on epoch=139
06/19/2022 22:27:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.471276 on epoch=144
06/19/2022 22:27:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.523454 on epoch=149
06/19/2022 22:27:10 - INFO - __main__ - Global step 300 Train loss 0.532751 ACC 0.5 on epoch=149
06/19/2022 22:27:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.450311 on epoch=154
06/19/2022 22:27:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.632990 on epoch=159
06/19/2022 22:27:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.484370 on epoch=164
06/19/2022 22:27:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.429331 on epoch=169
06/19/2022 22:27:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.404880 on epoch=174
06/19/2022 22:27:23 - INFO - __main__ - Global step 350 Train loss 0.480376 ACC 0.5 on epoch=174
06/19/2022 22:27:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.496269 on epoch=179
06/19/2022 22:27:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.341636 on epoch=184
06/19/2022 22:27:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.320308 on epoch=189
06/19/2022 22:27:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.422876 on epoch=194
06/19/2022 22:27:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.474894 on epoch=199
06/19/2022 22:27:36 - INFO - __main__ - Global step 400 Train loss 0.411197 ACC 0.5 on epoch=199
06/19/2022 22:27:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.479643 on epoch=204
06/19/2022 22:27:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.355010 on epoch=209
06/19/2022 22:27:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.366633 on epoch=214
06/19/2022 22:27:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.243061 on epoch=219
06/19/2022 22:27:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.266638 on epoch=224
06/19/2022 22:27:49 - INFO - __main__ - Global step 450 Train loss 0.342197 ACC 0.375 on epoch=224
06/19/2022 22:27:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.275746 on epoch=229
06/19/2022 22:27:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.261771 on epoch=234
06/19/2022 22:27:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.151529 on epoch=239
06/19/2022 22:27:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.198294 on epoch=244
06/19/2022 22:28:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.252483 on epoch=249
06/19/2022 22:28:01 - INFO - __main__ - Global step 500 Train loss 0.227965 ACC 0.46875 on epoch=249
06/19/2022 22:28:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.168207 on epoch=254
06/19/2022 22:28:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.154031 on epoch=259
06/19/2022 22:28:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.138211 on epoch=264
06/19/2022 22:28:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.317247 on epoch=269
06/19/2022 22:28:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.322699 on epoch=274
06/19/2022 22:28:15 - INFO - __main__ - Global step 550 Train loss 0.220079 ACC 0.40625 on epoch=274
06/19/2022 22:28:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.212347 on epoch=279
06/19/2022 22:28:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.120139 on epoch=284
06/19/2022 22:28:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.148459 on epoch=289
06/19/2022 22:28:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.172851 on epoch=294
06/19/2022 22:28:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.070342 on epoch=299
06/19/2022 22:28:28 - INFO - __main__ - Global step 600 Train loss 0.144828 ACC 0.4375 on epoch=299
06/19/2022 22:28:28 - INFO - __main__ - save last model!
06/19/2022 22:28:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:28:29 - INFO - __main__ - Printing 3 examples
06/19/2022 22:28:29 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/19/2022 22:28:29 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:29 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/19/2022 22:28:29 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:29 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/19/2022 22:28:29 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/19/2022 22:28:29 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:28:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:28:29 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:28:29 - INFO - __main__ - Printing 3 examples
06/19/2022 22:28:29 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/19/2022 22:28:29 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:29 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/19/2022 22:28:29 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:29 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/19/2022 22:28:29 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:29 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:28:29 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:28:29 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:28:31 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:28:31 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:28:31 - INFO - __main__ - Printing 3 examples
06/19/2022 22:28:31 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:28:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:31 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:28:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:31 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:28:31 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:31 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:28:31 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:28:32 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:28:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:28:33 - INFO - __main__ - Starting training!
06/19/2022 22:28:35 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.0002_8_predictions.txt
06/19/2022 22:28:35 - INFO - __main__ - ACC on test data: 0.4836
06/19/2022 22:28:35 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.0002, bsz=8, dev_performance=0.5, test_performance=0.48360655737704916
06/19/2022 22:28:35 - INFO - __main__ - Running ... prefix=medical_questions_pairs_16_87, lr=0.0001, bsz=8 ...
06/19/2022 22:28:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:28:36 - INFO - __main__ - Printing 3 examples
06/19/2022 22:28:36 - INFO - __main__ -  [medical_questions_pairs] question 1: After a hysterectomy do you start menopause imediately? [SEP] question 2: I am scheduled for hysterectomy next week. I have a lot of questions and anxiety about it. Does menopause start immediately after the surgery?
06/19/2022 22:28:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:36 - INFO - __main__ -  [medical_questions_pairs] question 1: Is it normal for the area where you got your stitches to be hard? [SEP] question 2: My stitches from the surgery feel hard, is it normal?
06/19/2022 22:28:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I think I have a bunion that is very painful and it's too late for me to go to a store. Are there any common household items I can use to help for now? [SEP] question 2: What at home remedies can help with pain due to a bunion? 
06/19/2022 22:28:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/19/2022 22:28:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:28:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/19/2022 22:28:36 - INFO - __main__ - Start tokenizing ... 32 instances
06/19/2022 22:28:36 - INFO - __main__ - Printing 3 examples
06/19/2022 22:28:36 - INFO - __main__ -  [medical_questions_pairs] question 1: Eyes feeling dry last couple of days. One eye became mildly red with clear mucus, next day the mucous was yellow with eye irritation. Mucous in throat? [SEP] question 2: My eyes have been feeling dry for the past couple of days. Then, I noticed mild redness, irritation in eye, clear mucus which turned yellow. I have mucus in my throat also, not sure if it might be related to my eye issue. 
06/19/2022 22:28:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm having bad headaches with cold sweats neck pain and nausae and pain behind the eyes [SEP] question 2: What causes recurrent headaches, cold sweats, neck pain and pain behind the eyes?
06/19/2022 22:28:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:36 - INFO - __main__ -  [medical_questions_pairs] question 1: I'm a young MTF and I want to start hormone replacement therapy. Is aldactone (spironolactone) 100-mg/day a good choice? [SEP] question 2: I am a MTF and looking to start HRT. Can I start off with aldactone 100 mg/day?
06/19/2022 22:28:36 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:28:36 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:28:36 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:28:36 - INFO - __main__ - Loaded 32 examples from dev data
06/19/2022 22:28:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
06/19/2022 22:28:41 - INFO - __main__ - Starting training!
06/19/2022 22:28:43 - INFO - __main__ - Step 10 Global step 10 Train loss 18.151890 on epoch=4
06/19/2022 22:28:45 - INFO - __main__ - Step 20 Global step 20 Train loss 16.951363 on epoch=9
06/19/2022 22:28:47 - INFO - __main__ - Step 30 Global step 30 Train loss 12.102943 on epoch=14
06/19/2022 22:28:50 - INFO - __main__ - Step 40 Global step 40 Train loss 9.925978 on epoch=19
06/19/2022 22:28:52 - INFO - __main__ - Step 50 Global step 50 Train loss 8.258475 on epoch=24
06/19/2022 22:28:55 - INFO - __main__ - Global step 50 Train loss 13.078130 ACC 0.0 on epoch=24
06/19/2022 22:28:58 - INFO - __main__ - Step 60 Global step 60 Train loss 7.005045 on epoch=29
06/19/2022 22:29:00 - INFO - __main__ - Step 70 Global step 70 Train loss 6.313018 on epoch=34
06/19/2022 22:29:03 - INFO - __main__ - Step 80 Global step 80 Train loss 5.952629 on epoch=39
06/19/2022 22:29:05 - INFO - __main__ - Step 90 Global step 90 Train loss 5.691124 on epoch=44
06/19/2022 22:29:08 - INFO - __main__ - Step 100 Global step 100 Train loss 4.523479 on epoch=49
06/19/2022 22:29:09 - INFO - __main__ - Global step 100 Train loss 5.897058 ACC 0.0 on epoch=49
06/19/2022 22:29:11 - INFO - __main__ - Step 110 Global step 110 Train loss 5.014492 on epoch=54
06/19/2022 22:29:14 - INFO - __main__ - Step 120 Global step 120 Train loss 3.374057 on epoch=59
06/19/2022 22:29:16 - INFO - __main__ - Step 130 Global step 130 Train loss 3.929295 on epoch=64
06/19/2022 22:29:19 - INFO - __main__ - Step 140 Global step 140 Train loss 3.558249 on epoch=69
06/19/2022 22:29:21 - INFO - __main__ - Step 150 Global step 150 Train loss 2.342378 on epoch=74
06/19/2022 22:29:21 - INFO - __main__ - Global step 150 Train loss 3.643694 ACC 0.5 on epoch=74
06/19/2022 22:29:25 - INFO - __main__ - Step 160 Global step 160 Train loss 2.202863 on epoch=79
06/19/2022 22:29:27 - INFO - __main__ - Step 170 Global step 170 Train loss 1.451550 on epoch=84
06/19/2022 22:29:30 - INFO - __main__ - Step 180 Global step 180 Train loss 2.377009 on epoch=89
06/19/2022 22:29:32 - INFO - __main__ - Step 190 Global step 190 Train loss 1.222351 on epoch=94
06/19/2022 22:29:35 - INFO - __main__ - Step 200 Global step 200 Train loss 1.365089 on epoch=99
06/19/2022 22:29:35 - INFO - __main__ - Global step 200 Train loss 1.723772 ACC 0.3125 on epoch=99
06/19/2022 22:29:37 - INFO - __main__ - Step 210 Global step 210 Train loss 1.187913 on epoch=104
06/19/2022 22:29:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.740759 on epoch=109
06/19/2022 22:29:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.939870 on epoch=114
06/19/2022 22:29:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.707799 on epoch=119
06/19/2022 22:29:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.725273 on epoch=124
06/19/2022 22:29:48 - INFO - __main__ - Global step 250 Train loss 0.860323 ACC 0.5 on epoch=124
06/19/2022 22:29:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.972792 on epoch=129
06/19/2022 22:29:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.854484 on epoch=134
06/19/2022 22:29:55 - INFO - __main__ - Step 280 Global step 280 Train loss 1.128851 on epoch=139
06/19/2022 22:29:58 - INFO - __main__ - Step 290 Global step 290 Train loss 1.169111 on epoch=144
06/19/2022 22:30:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.998678 on epoch=149
06/19/2022 22:30:01 - INFO - __main__ - Global step 300 Train loss 1.024783 ACC 0.46875 on epoch=149
06/19/2022 22:30:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.831976 on epoch=154
06/19/2022 22:30:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.580855 on epoch=159
06/19/2022 22:30:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.649603 on epoch=164
06/19/2022 22:30:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.623985 on epoch=169
06/19/2022 22:30:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.678319 on epoch=174
06/19/2022 22:30:13 - INFO - __main__ - Global step 350 Train loss 0.672948 ACC 0.5 on epoch=174
06/19/2022 22:30:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.577933 on epoch=179
06/19/2022 22:30:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.778366 on epoch=184
06/19/2022 22:30:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.580592 on epoch=189
06/19/2022 22:30:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.641571 on epoch=194
06/19/2022 22:30:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.523416 on epoch=199
06/19/2022 22:30:26 - INFO - __main__ - Global step 400 Train loss 0.620376 ACC 0.5 on epoch=199
06/19/2022 22:30:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.426716 on epoch=204
06/19/2022 22:30:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.478717 on epoch=209
06/19/2022 22:30:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.474622 on epoch=214
06/19/2022 22:30:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.497303 on epoch=219
06/19/2022 22:30:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.358406 on epoch=224
06/19/2022 22:30:39 - INFO - __main__ - Global step 450 Train loss 0.447153 ACC 0.5 on epoch=224
06/19/2022 22:30:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.568171 on epoch=229
06/19/2022 22:30:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.644406 on epoch=234
06/19/2022 22:30:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.481678 on epoch=239
06/19/2022 22:30:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.512976 on epoch=244
06/19/2022 22:30:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.537400 on epoch=249
06/19/2022 22:30:52 - INFO - __main__ - Global step 500 Train loss 0.548926 ACC 0.46875 on epoch=249
06/19/2022 22:30:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.524198 on epoch=254
06/19/2022 22:30:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.470055 on epoch=259
06/19/2022 22:31:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.467881 on epoch=264
06/19/2022 22:31:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.463273 on epoch=269
06/19/2022 22:31:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.565627 on epoch=274
06/19/2022 22:31:05 - INFO - __main__ - Global step 550 Train loss 0.498207 ACC 0.5 on epoch=274
06/19/2022 22:31:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.450928 on epoch=279
06/19/2022 22:31:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.414077 on epoch=284
06/19/2022 22:31:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.438960 on epoch=289
06/19/2022 22:31:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.559992 on epoch=294
06/19/2022 22:31:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.491731 on epoch=299
06/19/2022 22:31:18 - INFO - __main__ - Global step 600 Train loss 0.471137 ACC 0.59375 on epoch=299
06/19/2022 22:31:18 - INFO - __main__ - save last model!
06/19/2022 22:31:21 - INFO - __main__ - Loading checkpoint on the fly
06/19/2022 22:31:21 - INFO - __main__ - Start tokenizing ... 610 instances
06/19/2022 22:31:21 - INFO - __main__ - Printing 3 examples
06/19/2022 22:31:21 - INFO - __main__ -  [medical_questions_pairs] question 1: I have been having problems every time i eat i go poop and it is yellow and runny and my tummy feels weird in the middle it has I have diabetes [SEP] question 2: My son has been complaining of abdominal pain and runny stools for the past 2 weeks. This seems to occur whenever he eats and I also think he stomach ache. He has a history of diabetes.
06/19/2022 22:31:21 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:31:21 - INFO - __main__ -  [medical_questions_pairs] question 1: I have a positive ANA 1:160 Homo pattern & RNP of. 3. Joint pain, limb numbness, skin indents stay for hours, bruising. No diagnosis. More testing? [SEP] question 2: My friend has some symptoms like limb numbness, joint pain, bruising and skin indents. The doctor asked him to get an ANA and RNP and the values are 1:160 and 3. The doctor is not able to come to a conclusion on what it is and I would like your opinion on additional tests that can be taken.
06/19/2022 22:31:21 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:31:21 - INFO - __main__ -  [medical_questions_pairs] question 1: Which body parts can be donated after you die? [SEP] question 2: I want to donate my organs after I die, which body parts can be donated?
06/19/2022 22:31:21 - INFO - __main__ - ['Dissimilar']
06/19/2022 22:31:21 - INFO - __main__ - Tokenizing Input ...
06/19/2022 22:31:21 - INFO - __main__ - Tokenizing Output ...
06/19/2022 22:31:22 - INFO - __main__ - Loaded 610 examples from test data
06/19/2022 22:31:27 - INFO - __main__ - Saved prediction in models/T5-base-ft-nopara2para/singletask-medical_questions_pairs/medical_questions_pairs_16_87_0.0001_8_predictions.txt
06/19/2022 22:31:27 - INFO - __main__ - ACC on test data: 0.5246
06/19/2022 22:31:27 - INFO - __main__ - prefix=medical_questions_pairs_16_87, lr=0.0001, bsz=8, dev_performance=0.59375, test_performance=0.5245901639344263
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (88154): No such process
Task: paws, Checkpoint: None, Identifier: T5-base-ft-nopara2para
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFitFT/tune_singletask_nopara2para.py", line 225, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFitFT/tune_singletask_nopara2para.py", line 137, in main
    handlers=[logging.FileHandler(os.path.join(args.output_dir, log_filename)),
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1146, in __init__
    StreamHandler.__init__(self, self._open())
  File "/opt/conda/envs/meta/lib/python3.9/logging/__init__.py", line 1175, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding,
FileNotFoundError: [Errno 2] No such file or directory: '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFitFT/models/T5-base-ft-nopara2para/singletask-paws/log.txt'
06/19/2022 22:31:32 - INFO - __main__ - Namespace(task_dir='data/paws/', task_name='paws', identifier='T5-base-ft-nopara2para', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-nopara2para/singletask-paws', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='6,7')
06/19/2022 22:31:32 - INFO - __main__ - models/T5-base-ft-nopara2para/singletask-paws
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'tune_singletask_nopara2para.py', '--local_rank=1', '--task_dir', 'data/paws/', '--task_name', 'paws', '--identifier', 'T5-base-ft-nopara2para', '--checkpoint', 'None', '--do_train', '--do_predict', '--learning_rate_list', '5e-4', '3e-4', '2e-4', '1e-4', '--bsz_list', '8', '--predict_batch_size', '32', '--total_steps', '1000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '300.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-base-ft-nopara2para/singletask-paws', '--cuda', '6,7', '--model', 'google/t5-v1_1-base']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 88159
Killing subprocess 88160
++++++++++++++++++++++++++++++
kill: (88166): No such process
