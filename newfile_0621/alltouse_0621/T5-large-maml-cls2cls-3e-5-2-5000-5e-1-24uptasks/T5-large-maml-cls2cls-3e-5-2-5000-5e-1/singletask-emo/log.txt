03/14/2022 15:18:30 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/14/2022 15:18:30 - INFO - __main__ - models/T5-large-cls2cls/singletask-emo
03/14/2022 15:30:37 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/14/2022 15:30:37 - INFO - __main__ - models/T5-large-cls2cls/singletask-emo
03/14/2022 15:30:37 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/14/2022 15:30:37 - INFO - __main__ - models/T5-large-cls2cls/singletask-emo
03/14/2022 15:31:29 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/14/2022 15:31:29 - INFO - __main__ - models/T5-large-cls2cls/singletask-emo
03/14/2022 15:31:29 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/14/2022 15:31:29 - INFO - __main__ - models/T5-large-cls2cls/singletask-emo
03/16/2022 07:31:45 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
03/16/2022 07:31:45 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo
03/16/2022 07:31:45 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
03/16/2022 07:31:45 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo
03/16/2022 07:31:47 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/16/2022 07:31:47 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/16/2022 07:31:47 - INFO - __main__ - args.device: cuda:0
03/16/2022 07:31:47 - INFO - __main__ - Using 2 gpus
03/16/2022 07:31:47 - INFO - __main__ - args.device: cuda:1
03/16/2022 07:31:47 - INFO - __main__ - Using 2 gpus
03/16/2022 07:31:47 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
03/16/2022 07:31:47 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
03/16/2022 07:31:54 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
03/16/2022 07:31:55 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 07:31:55 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 07:31:55 - INFO - __main__ - Printing 3 examples
03/16/2022 07:31:55 - INFO - __main__ -  [emo] how cause yes am listening
03/16/2022 07:31:55 - INFO - __main__ - Printing 3 examples
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ -  [emo] how cause yes am listening
03/16/2022 07:31:55 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/16/2022 07:31:55 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/16/2022 07:31:55 - INFO - __main__ - Tokenizing Input ...
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ - Tokenizing Input ...
03/16/2022 07:31:55 - INFO - __main__ - Tokenizing Output ...
03/16/2022 07:31:55 - INFO - __main__ - Tokenizing Output ...
03/16/2022 07:31:55 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 07:31:55 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 07:31:55 - INFO - __main__ - Printing 3 examples
03/16/2022 07:31:55 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ - Tokenizing Input ...
03/16/2022 07:31:55 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 07:31:55 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 07:31:55 - INFO - __main__ - Printing 3 examples
03/16/2022 07:31:55 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/16/2022 07:31:55 - INFO - __main__ - ['others']
03/16/2022 07:31:55 - INFO - __main__ - Tokenizing Input ...
03/16/2022 07:31:55 - INFO - __main__ - Tokenizing Output ...
03/16/2022 07:31:55 - INFO - __main__ - Tokenizing Output ...
03/16/2022 07:31:56 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 07:31:56 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 07:32:13 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 07:32:13 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 07:32:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 07:32:14 - INFO - __main__ - Starting training!
03/16/2022 07:32:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 07:32:23 - INFO - __main__ - Starting training!
03/16/2022 07:32:26 - INFO - __main__ - Step 10 Global step 10 Train loss 2.25 on epoch=2
03/16/2022 07:32:28 - INFO - __main__ - Step 20 Global step 20 Train loss 1.21 on epoch=4
03/16/2022 07:32:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.97 on epoch=7
03/16/2022 07:32:33 - INFO - __main__ - Step 40 Global step 40 Train loss 1.04 on epoch=9
03/16/2022 07:32:36 - INFO - __main__ - Step 50 Global step 50 Train loss 2.11 on epoch=12
03/16/2022 07:32:37 - INFO - __main__ - Global step 50 Train loss 1.51 Classification-F1 0.08205128205128205 on epoch=12
03/16/2022 07:32:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.08205128205128205 on epoch=12, global_step=50
03/16/2022 07:32:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=14
03/16/2022 07:32:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=17
03/16/2022 07:32:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.83 on epoch=19
03/16/2022 07:32:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=22
03/16/2022 07:32:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=24
03/16/2022 07:32:50 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.36330409356725146 on epoch=24
03/16/2022 07:32:50 - INFO - __main__ - Saving model with best Classification-F1: 0.08205128205128205 -> 0.36330409356725146 on epoch=24, global_step=100
03/16/2022 07:32:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.85 on epoch=27
03/16/2022 07:32:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.84 on epoch=29
03/16/2022 07:32:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=32
03/16/2022 07:32:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
03/16/2022 07:33:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.80 on epoch=37
03/16/2022 07:33:03 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.33093635923824605 on epoch=37
03/16/2022 07:33:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=39
03/16/2022 07:33:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.78 on epoch=42
03/16/2022 07:33:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=44
03/16/2022 07:33:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.72 on epoch=47
03/16/2022 07:33:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.82 on epoch=49
03/16/2022 07:33:16 - INFO - __main__ - Global step 200 Train loss 0.81 Classification-F1 0.3673334569560984 on epoch=49
03/16/2022 07:33:16 - INFO - __main__ - Saving model with best Classification-F1: 0.36330409356725146 -> 0.3673334569560984 on epoch=49, global_step=200
03/16/2022 07:33:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.80 on epoch=52
03/16/2022 07:33:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.85 on epoch=54
03/16/2022 07:33:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.81 on epoch=57
03/16/2022 07:33:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=59
03/16/2022 07:33:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=62
03/16/2022 07:33:29 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.2603826918895412 on epoch=62
03/16/2022 07:33:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=64
03/16/2022 07:33:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.76 on epoch=67
03/16/2022 07:33:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.80 on epoch=69
03/16/2022 07:33:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.76 on epoch=72
03/16/2022 07:33:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.75 on epoch=74
03/16/2022 07:33:42 - INFO - __main__ - Global step 300 Train loss 0.77 Classification-F1 0.43674649346918254 on epoch=74
03/16/2022 07:33:42 - INFO - __main__ - Saving model with best Classification-F1: 0.3673334569560984 -> 0.43674649346918254 on epoch=74, global_step=300
03/16/2022 07:33:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.69 on epoch=77
03/16/2022 07:33:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.78 on epoch=79
03/16/2022 07:33:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.77 on epoch=82
03/16/2022 07:33:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.73 on epoch=84
03/16/2022 07:33:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=87
03/16/2022 07:33:55 - INFO - __main__ - Global step 350 Train loss 0.73 Classification-F1 0.6064104307678359 on epoch=87
03/16/2022 07:33:55 - INFO - __main__ - Saving model with best Classification-F1: 0.43674649346918254 -> 0.6064104307678359 on epoch=87, global_step=350
03/16/2022 07:33:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.72 on epoch=89
03/16/2022 07:33:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.73 on epoch=92
03/16/2022 07:34:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.64 on epoch=94
03/16/2022 07:34:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.62 on epoch=97
03/16/2022 07:34:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.71 on epoch=99
03/16/2022 07:34:07 - INFO - __main__ - Global step 400 Train loss 0.68 Classification-F1 0.5945640616693249 on epoch=99
03/16/2022 07:34:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.65 on epoch=102
03/16/2022 07:34:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.67 on epoch=104
03/16/2022 07:34:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.63 on epoch=107
03/16/2022 07:34:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.66 on epoch=109
03/16/2022 07:34:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.59 on epoch=112
03/16/2022 07:34:20 - INFO - __main__ - Global step 450 Train loss 0.64 Classification-F1 0.549020135311445 on epoch=112
03/16/2022 07:34:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.67 on epoch=114
03/16/2022 07:34:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.65 on epoch=117
03/16/2022 07:34:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.60 on epoch=119
03/16/2022 07:34:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.71 on epoch=122
03/16/2022 07:34:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.63 on epoch=124
03/16/2022 07:34:33 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.6676587301587301 on epoch=124
03/16/2022 07:34:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6064104307678359 -> 0.6676587301587301 on epoch=124, global_step=500
03/16/2022 07:34:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=127
03/16/2022 07:34:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.59 on epoch=129
03/16/2022 07:34:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=132
03/16/2022 07:34:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=134
03/16/2022 07:34:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=137
03/16/2022 07:34:46 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.6086336336336337 on epoch=137
03/16/2022 07:34:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=139
03/16/2022 07:34:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=142
03/16/2022 07:34:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=144
03/16/2022 07:34:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.55 on epoch=147
03/16/2022 07:34:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=149
03/16/2022 07:34:59 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.5161849014790192 on epoch=149
03/16/2022 07:35:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=152
03/16/2022 07:35:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=154
03/16/2022 07:35:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=157
03/16/2022 07:35:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=159
03/16/2022 07:35:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=162
03/16/2022 07:35:12 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.692918771043771 on epoch=162
03/16/2022 07:35:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6676587301587301 -> 0.692918771043771 on epoch=162, global_step=650
03/16/2022 07:35:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=164
03/16/2022 07:35:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=167
03/16/2022 07:35:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=169
03/16/2022 07:35:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=172
03/16/2022 07:35:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=174
03/16/2022 07:35:25 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.6971464059878694 on epoch=174
03/16/2022 07:35:25 - INFO - __main__ - Saving model with best Classification-F1: 0.692918771043771 -> 0.6971464059878694 on epoch=174, global_step=700
03/16/2022 07:35:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=177
03/16/2022 07:35:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=179
03/16/2022 07:35:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=182
03/16/2022 07:35:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=184
03/16/2022 07:35:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=187
03/16/2022 07:35:38 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.7162903225806452 on epoch=187
03/16/2022 07:35:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6971464059878694 -> 0.7162903225806452 on epoch=187, global_step=750
03/16/2022 07:35:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=189
03/16/2022 07:35:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=192
03/16/2022 07:35:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=194
03/16/2022 07:35:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=197
03/16/2022 07:35:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=199
03/16/2022 07:35:51 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.707615245904166 on epoch=199
03/16/2022 07:35:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=202
03/16/2022 07:35:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=204
03/16/2022 07:35:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=207
03/16/2022 07:36:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=209
03/16/2022 07:36:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=212
03/16/2022 07:36:04 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.6882985452245414 on epoch=212
03/16/2022 07:36:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=214
03/16/2022 07:36:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=217
03/16/2022 07:36:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=219
03/16/2022 07:36:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=222
03/16/2022 07:36:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=224
03/16/2022 07:36:17 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.6701807111733582 on epoch=224
03/16/2022 07:36:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=227
03/16/2022 07:36:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=229
03/16/2022 07:36:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=232
03/16/2022 07:36:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=234
03/16/2022 07:36:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=237
03/16/2022 07:36:30 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.599426961926962 on epoch=237
03/16/2022 07:36:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=239
03/16/2022 07:36:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=242
03/16/2022 07:36:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=244
03/16/2022 07:36:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=247
03/16/2022 07:36:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=249
03/16/2022 07:36:43 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.6421286905566824 on epoch=249
03/16/2022 07:36:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=252
03/16/2022 07:36:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=254
03/16/2022 07:36:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=257
03/16/2022 07:36:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
03/16/2022 07:36:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=262
03/16/2022 07:36:56 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.5593557422969189 on epoch=262
03/16/2022 07:36:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=264
03/16/2022 07:37:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=267
03/16/2022 07:37:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=269
03/16/2022 07:37:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
03/16/2022 07:37:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=274
03/16/2022 07:37:09 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.6452302631578948 on epoch=274
03/16/2022 07:37:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=277
03/16/2022 07:37:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=279
03/16/2022 07:37:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=282
03/16/2022 07:37:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=284
03/16/2022 07:37:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=287
03/16/2022 07:37:22 - INFO - __main__ - Global step 1150 Train loss 0.14 Classification-F1 0.5148287385129491 on epoch=287
03/16/2022 07:37:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=289
03/16/2022 07:37:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=292
03/16/2022 07:37:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=294
03/16/2022 07:37:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=297
03/16/2022 07:37:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=299
03/16/2022 07:37:35 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.5922082542694497 on epoch=299
03/16/2022 07:37:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=302
03/16/2022 07:37:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
03/16/2022 07:37:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=307
03/16/2022 07:37:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=309
03/16/2022 07:37:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=312
03/16/2022 07:37:48 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.6579501915708812 on epoch=312
03/16/2022 07:37:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=314
03/16/2022 07:37:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=317
03/16/2022 07:37:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=319
03/16/2022 07:37:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=322
03/16/2022 07:38:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
03/16/2022 07:38:01 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.5274453024453024 on epoch=324
03/16/2022 07:38:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
03/16/2022 07:38:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
03/16/2022 07:38:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=332
03/16/2022 07:38:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=334
03/16/2022 07:38:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=337
03/16/2022 07:38:15 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.5117862529627236 on epoch=337
03/16/2022 07:38:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=339
03/16/2022 07:38:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
03/16/2022 07:38:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=344
03/16/2022 07:38:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=347
03/16/2022 07:38:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=349
03/16/2022 07:38:28 - INFO - __main__ - Global step 1400 Train loss 0.11 Classification-F1 0.5125785852593416 on epoch=349
03/16/2022 07:38:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=352
03/16/2022 07:38:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=354
03/16/2022 07:38:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
03/16/2022 07:38:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=359
03/16/2022 07:38:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
03/16/2022 07:38:41 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.5169786535303776 on epoch=362
03/16/2022 07:38:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
03/16/2022 07:38:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=367
03/16/2022 07:38:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
03/16/2022 07:38:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
03/16/2022 07:38:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=374
03/16/2022 07:38:54 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.513816402609506 on epoch=374
03/16/2022 07:38:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
03/16/2022 07:38:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
03/16/2022 07:39:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=382
03/16/2022 07:39:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=384
03/16/2022 07:39:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
03/16/2022 07:39:08 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.6361580474483701 on epoch=387
03/16/2022 07:39:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
03/16/2022 07:39:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=392
03/16/2022 07:39:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
03/16/2022 07:39:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=397
03/16/2022 07:39:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
03/16/2022 07:39:21 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.5756609566903684 on epoch=399
03/16/2022 07:39:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
03/16/2022 07:39:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=404
03/16/2022 07:39:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
03/16/2022 07:39:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
03/16/2022 07:39:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
03/16/2022 07:39:34 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.5703795286800795 on epoch=412
03/16/2022 07:39:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
03/16/2022 07:39:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
03/16/2022 07:39:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=419
03/16/2022 07:39:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
03/16/2022 07:39:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
03/16/2022 07:39:48 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.5242689075630252 on epoch=424
03/16/2022 07:39:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
03/16/2022 07:39:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=429
03/16/2022 07:39:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
03/16/2022 07:39:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
03/16/2022 07:40:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
03/16/2022 07:40:01 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.5460824122114445 on epoch=437
03/16/2022 07:40:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
03/16/2022 07:40:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
03/16/2022 07:40:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
03/16/2022 07:40:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
03/16/2022 07:40:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=449
03/16/2022 07:40:15 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.5645727221279004 on epoch=449
03/16/2022 07:40:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
03/16/2022 07:40:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
03/16/2022 07:40:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
03/16/2022 07:40:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
03/16/2022 07:40:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=462
03/16/2022 07:40:28 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.6871974151173261 on epoch=462
03/16/2022 07:40:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
03/16/2022 07:40:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
03/16/2022 07:40:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
03/16/2022 07:40:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/16/2022 07:40:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=474
03/16/2022 07:40:41 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7537314269694422 on epoch=474
03/16/2022 07:40:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7162903225806452 -> 0.7537314269694422 on epoch=474, global_step=1900
03/16/2022 07:40:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
03/16/2022 07:40:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
03/16/2022 07:40:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
03/16/2022 07:40:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=484
03/16/2022 07:40:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/16/2022 07:40:54 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6871329121329122 on epoch=487
03/16/2022 07:40:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
03/16/2022 07:40:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=492
03/16/2022 07:41:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
03/16/2022 07:41:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=497
03/16/2022 07:41:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
03/16/2022 07:41:08 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.6545596686950643 on epoch=499
03/16/2022 07:41:10 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
03/16/2022 07:41:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 07:41:15 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
03/16/2022 07:41:17 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
03/16/2022 07:41:20 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/16/2022 07:41:21 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7033365152693647 on epoch=512
03/16/2022 07:41:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/16/2022 07:41:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
03/16/2022 07:41:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/16/2022 07:41:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
03/16/2022 07:41:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
03/16/2022 07:41:34 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.6654855001629196 on epoch=524
03/16/2022 07:41:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/16/2022 07:41:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=529
03/16/2022 07:41:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=532
03/16/2022 07:41:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/16/2022 07:41:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/16/2022 07:41:47 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6860913562950914 on epoch=537
03/16/2022 07:41:50 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/16/2022 07:41:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/16/2022 07:41:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
03/16/2022 07:41:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/16/2022 07:41:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=549
03/16/2022 07:42:00 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6846050294326156 on epoch=549
03/16/2022 07:42:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
03/16/2022 07:42:05 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
03/16/2022 07:42:08 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
03/16/2022 07:42:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
03/16/2022 07:42:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/16/2022 07:42:14 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.6993120393120393 on epoch=562
03/16/2022 07:42:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=564
03/16/2022 07:42:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
03/16/2022 07:42:21 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
03/16/2022 07:42:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 07:42:26 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
03/16/2022 07:42:27 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7001557360854457 on epoch=574
03/16/2022 07:42:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
03/16/2022 07:42:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/16/2022 07:42:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
03/16/2022 07:42:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
03/16/2022 07:42:39 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=587
03/16/2022 07:42:40 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6964373941276115 on epoch=587
03/16/2022 07:42:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=589
03/16/2022 07:42:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/16/2022 07:42:47 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/16/2022 07:42:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
03/16/2022 07:42:52 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
03/16/2022 07:42:53 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7060792976021782 on epoch=599
03/16/2022 07:42:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/16/2022 07:42:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
03/16/2022 07:43:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/16/2022 07:43:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/16/2022 07:43:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=612
03/16/2022 07:43:07 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7171676362852833 on epoch=612
03/16/2022 07:43:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
03/16/2022 07:43:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=617
03/16/2022 07:43:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
03/16/2022 07:43:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
03/16/2022 07:43:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
03/16/2022 07:43:20 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.6993844118844119 on epoch=624
03/16/2022 07:43:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/16/2022 07:43:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
03/16/2022 07:43:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/16/2022 07:43:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
03/16/2022 07:43:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/16/2022 07:43:33 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6873418572528696 on epoch=637
03/16/2022 07:43:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/16/2022 07:43:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/16/2022 07:43:40 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/16/2022 07:43:43 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
03/16/2022 07:43:45 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/16/2022 07:43:46 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7193205430813614 on epoch=649
03/16/2022 07:43:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/16/2022 07:43:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=654
03/16/2022 07:43:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/16/2022 07:43:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/16/2022 07:43:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/16/2022 07:44:00 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7335232509426057 on epoch=662
03/16/2022 07:44:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=664
03/16/2022 07:44:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
03/16/2022 07:44:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.33 on epoch=669
03/16/2022 07:44:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
03/16/2022 07:44:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/16/2022 07:44:13 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.5598621877691644 on epoch=674
03/16/2022 07:44:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/16/2022 07:44:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
03/16/2022 07:44:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/16/2022 07:44:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/16/2022 07:44:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=687
03/16/2022 07:44:26 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7012043189368771 on epoch=687
03/16/2022 07:44:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=689
03/16/2022 07:44:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=692
03/16/2022 07:44:33 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=694
03/16/2022 07:44:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/16/2022 07:44:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/16/2022 07:44:39 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.5413274449564771 on epoch=699
03/16/2022 07:44:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/16/2022 07:44:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/16/2022 07:44:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/16/2022 07:44:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
03/16/2022 07:44:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/16/2022 07:44:53 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6881451881451882 on epoch=712
03/16/2022 07:44:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
03/16/2022 07:44:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
03/16/2022 07:45:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/16/2022 07:45:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=722
03/16/2022 07:45:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/16/2022 07:45:06 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7304268292682927 on epoch=724
03/16/2022 07:45:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/16/2022 07:45:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
03/16/2022 07:45:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 07:45:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
03/16/2022 07:45:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
03/16/2022 07:45:19 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6886165577342048 on epoch=737
03/16/2022 07:45:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
03/16/2022 07:45:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
03/16/2022 07:45:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
03/16/2022 07:45:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
03/16/2022 07:45:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 07:45:32 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 07:45:32 - INFO - __main__ - Printing 3 examples
03/16/2022 07:45:32 - INFO - __main__ -  [emo] how cause yes am listening
03/16/2022 07:45:32 - INFO - __main__ - ['others']
03/16/2022 07:45:32 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/16/2022 07:45:32 - INFO - __main__ - ['others']
03/16/2022 07:45:32 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/16/2022 07:45:32 - INFO - __main__ - ['others']
03/16/2022 07:45:32 - INFO - __main__ - Tokenizing Input ...
03/16/2022 07:45:32 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6862433862433863 on epoch=749
03/16/2022 07:45:32 - INFO - __main__ - save last model!
03/16/2022 07:45:32 - INFO - __main__ - Tokenizing Output ...
03/16/2022 07:45:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 07:45:33 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 07:45:33 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 07:45:33 - INFO - __main__ - Printing 3 examples
03/16/2022 07:45:33 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/16/2022 07:45:33 - INFO - __main__ - ['others']
03/16/2022 07:45:33 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/16/2022 07:45:33 - INFO - __main__ - ['others']
03/16/2022 07:45:33 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/16/2022 07:45:33 - INFO - __main__ - ['others']
03/16/2022 07:45:33 - INFO - __main__ - Tokenizing Input ...
03/16/2022 07:45:33 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 07:45:33 - INFO - __main__ - Printing 3 examples
03/16/2022 07:45:33 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 07:45:33 - INFO - __main__ - ['others']
03/16/2022 07:45:33 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 07:45:33 - INFO - __main__ - ['others']
03/16/2022 07:45:33 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 07:45:33 - INFO - __main__ - ['others']
03/16/2022 07:45:33 - INFO - __main__ - Tokenizing Input ...
03/16/2022 07:45:33 - INFO - __main__ - Tokenizing Output ...
03/16/2022 07:45:33 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 07:45:35 - INFO - __main__ - Tokenizing Output ...
03/16/2022 07:45:40 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 07:45:51 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 07:45:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 07:45:52 - INFO - __main__ - Starting training!
03/16/2022 07:47:19 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_100_0.5_8_predictions.txt
03/16/2022 07:47:19 - INFO - __main__ - Classification-F1 on test data: 0.3325
03/16/2022 07:47:20 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.7537314269694422, test_performance=0.33245963406173784
03/16/2022 07:47:20 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
03/16/2022 07:47:21 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 07:47:21 - INFO - __main__ - Printing 3 examples
03/16/2022 07:47:21 - INFO - __main__ -  [emo] how cause yes am listening
03/16/2022 07:47:21 - INFO - __main__ - ['others']
03/16/2022 07:47:21 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/16/2022 07:47:21 - INFO - __main__ - ['others']
03/16/2022 07:47:21 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/16/2022 07:47:21 - INFO - __main__ - ['others']
03/16/2022 07:47:21 - INFO - __main__ - Tokenizing Input ...
03/16/2022 07:47:21 - INFO - __main__ - Tokenizing Output ...
03/16/2022 07:47:21 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 07:47:21 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 07:47:21 - INFO - __main__ - Printing 3 examples
03/16/2022 07:47:21 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/16/2022 07:47:21 - INFO - __main__ - ['others']
03/16/2022 07:47:21 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/16/2022 07:47:21 - INFO - __main__ - ['others']
03/16/2022 07:47:21 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/16/2022 07:47:21 - INFO - __main__ - ['others']
03/16/2022 07:47:21 - INFO - __main__ - Tokenizing Input ...
03/16/2022 07:47:21 - INFO - __main__ - Tokenizing Output ...
03/16/2022 07:47:21 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 07:47:40 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 07:47:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 07:47:41 - INFO - __main__ - Starting training!
03/16/2022 07:47:44 - INFO - __main__ - Step 10 Global step 10 Train loss 2.50 on epoch=2
03/16/2022 07:47:46 - INFO - __main__ - Step 20 Global step 20 Train loss 1.31 on epoch=4
03/16/2022 07:47:49 - INFO - __main__ - Step 30 Global step 30 Train loss 1.77 on epoch=7
03/16/2022 07:47:51 - INFO - __main__ - Step 40 Global step 40 Train loss 1.24 on epoch=9
03/16/2022 07:47:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=12
03/16/2022 07:47:54 - INFO - __main__ - Global step 50 Train loss 1.55 Classification-F1 0.1 on epoch=12
03/16/2022 07:47:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/16/2022 07:47:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.91 on epoch=14
03/16/2022 07:47:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.90 on epoch=17
03/16/2022 07:48:02 - INFO - __main__ - Step 80 Global step 80 Train loss 1.01 on epoch=19
03/16/2022 07:48:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.86 on epoch=22
03/16/2022 07:48:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=24
03/16/2022 07:48:07 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.25898078529657476 on epoch=24
03/16/2022 07:48:07 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.25898078529657476 on epoch=24, global_step=100
03/16/2022 07:48:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=27
03/16/2022 07:48:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.89 on epoch=29
03/16/2022 07:48:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=32
03/16/2022 07:48:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.86 on epoch=34
03/16/2022 07:48:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.84 on epoch=37
03/16/2022 07:48:20 - INFO - __main__ - Global step 150 Train loss 0.90 Classification-F1 0.13067758749069247 on epoch=37
03/16/2022 07:48:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=39
03/16/2022 07:48:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=42
03/16/2022 07:48:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.73 on epoch=44
03/16/2022 07:48:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.88 on epoch=47
03/16/2022 07:48:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.83 on epoch=49
03/16/2022 07:48:33 - INFO - __main__ - Global step 200 Train loss 0.84 Classification-F1 0.4164502164502164 on epoch=49
03/16/2022 07:48:33 - INFO - __main__ - Saving model with best Classification-F1: 0.25898078529657476 -> 0.4164502164502164 on epoch=49, global_step=200
03/16/2022 07:48:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.84 on epoch=52
03/16/2022 07:48:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=54
03/16/2022 07:48:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.76 on epoch=57
03/16/2022 07:48:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=59
03/16/2022 07:48:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
03/16/2022 07:48:45 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.2710362863680941 on epoch=62
03/16/2022 07:48:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.82 on epoch=64
03/16/2022 07:48:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.82 on epoch=67
03/16/2022 07:48:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.79 on epoch=69
03/16/2022 07:48:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=72
03/16/2022 07:48:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=74
03/16/2022 07:48:58 - INFO - __main__ - Global step 300 Train loss 0.80 Classification-F1 0.4332898344943321 on epoch=74
03/16/2022 07:48:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4164502164502164 -> 0.4332898344943321 on epoch=74, global_step=300
03/16/2022 07:49:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=77
03/16/2022 07:49:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.78 on epoch=79
03/16/2022 07:49:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.67 on epoch=82
03/16/2022 07:49:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.74 on epoch=84
03/16/2022 07:49:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=87
03/16/2022 07:49:11 - INFO - __main__ - Global step 350 Train loss 0.73 Classification-F1 0.482588190907376 on epoch=87
03/16/2022 07:49:11 - INFO - __main__ - Saving model with best Classification-F1: 0.4332898344943321 -> 0.482588190907376 on epoch=87, global_step=350
03/16/2022 07:49:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.66 on epoch=89
03/16/2022 07:49:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.67 on epoch=92
03/16/2022 07:49:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.61 on epoch=94
03/16/2022 07:49:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.62 on epoch=97
03/16/2022 07:49:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.67 on epoch=99
03/16/2022 07:49:24 - INFO - __main__ - Global step 400 Train loss 0.64 Classification-F1 0.4566252587991718 on epoch=99
03/16/2022 07:49:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=102
03/16/2022 07:49:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.60 on epoch=104
03/16/2022 07:49:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.56 on epoch=107
03/16/2022 07:49:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.59 on epoch=109
03/16/2022 07:49:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=112
03/16/2022 07:49:37 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.49605263157894736 on epoch=112
03/16/2022 07:49:37 - INFO - __main__ - Saving model with best Classification-F1: 0.482588190907376 -> 0.49605263157894736 on epoch=112, global_step=450
03/16/2022 07:49:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=114
03/16/2022 07:49:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=117
03/16/2022 07:49:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=119
03/16/2022 07:49:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=122
03/16/2022 07:49:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=124
03/16/2022 07:49:50 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.5448051948051947 on epoch=124
03/16/2022 07:49:50 - INFO - __main__ - Saving model with best Classification-F1: 0.49605263157894736 -> 0.5448051948051947 on epoch=124, global_step=500
03/16/2022 07:49:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.42 on epoch=127
03/16/2022 07:49:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=129
03/16/2022 07:49:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=132
03/16/2022 07:49:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=134
03/16/2022 07:50:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=137
03/16/2022 07:50:03 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.5734930274403958 on epoch=137
03/16/2022 07:50:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5448051948051947 -> 0.5734930274403958 on epoch=137, global_step=550
03/16/2022 07:50:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=139
03/16/2022 07:50:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=142
03/16/2022 07:50:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=144
03/16/2022 07:50:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=147
03/16/2022 07:50:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=149
03/16/2022 07:50:15 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.5493925535846268 on epoch=149
03/16/2022 07:50:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.28 on epoch=152
03/16/2022 07:50:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=154
03/16/2022 07:50:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=157
03/16/2022 07:50:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=159
03/16/2022 07:50:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=162
03/16/2022 07:50:28 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.6019221481696128 on epoch=162
03/16/2022 07:50:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5734930274403958 -> 0.6019221481696128 on epoch=162, global_step=650
03/16/2022 07:50:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=164
03/16/2022 07:50:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=167
03/16/2022 07:50:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=169
03/16/2022 07:50:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=172
03/16/2022 07:50:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=174
03/16/2022 07:50:41 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.602124183006536 on epoch=174
03/16/2022 07:50:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6019221481696128 -> 0.602124183006536 on epoch=174, global_step=700
03/16/2022 07:50:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=177
03/16/2022 07:50:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=179
03/16/2022 07:50:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=182
03/16/2022 07:50:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
03/16/2022 07:50:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=187
03/16/2022 07:50:54 - INFO - __main__ - Global step 750 Train loss 0.24 Classification-F1 0.645350188898576 on epoch=187
03/16/2022 07:50:54 - INFO - __main__ - Saving model with best Classification-F1: 0.602124183006536 -> 0.645350188898576 on epoch=187, global_step=750
03/16/2022 07:50:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=189
03/16/2022 07:50:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=192
03/16/2022 07:51:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=194
03/16/2022 07:51:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=197
03/16/2022 07:51:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=199
03/16/2022 07:51:07 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.6856460882461994 on epoch=199
03/16/2022 07:51:07 - INFO - __main__ - Saving model with best Classification-F1: 0.645350188898576 -> 0.6856460882461994 on epoch=199, global_step=800
03/16/2022 07:51:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=202
03/16/2022 07:51:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=204
03/16/2022 07:51:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=207
03/16/2022 07:51:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=209
03/16/2022 07:51:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=212
03/16/2022 07:51:20 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.664918414918415 on epoch=212
03/16/2022 07:51:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=214
03/16/2022 07:51:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=217
03/16/2022 07:51:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=219
03/16/2022 07:51:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=222
03/16/2022 07:51:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=224
03/16/2022 07:51:33 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.6603764478764479 on epoch=224
03/16/2022 07:51:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=227
03/16/2022 07:51:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=229
03/16/2022 07:51:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=232
03/16/2022 07:51:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=234
03/16/2022 07:51:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=237
03/16/2022 07:51:46 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.6960232145350111 on epoch=237
03/16/2022 07:51:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6856460882461994 -> 0.6960232145350111 on epoch=237, global_step=950
03/16/2022 07:51:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=239
03/16/2022 07:51:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=242
03/16/2022 07:51:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
03/16/2022 07:51:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=247
03/16/2022 07:51:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=249
03/16/2022 07:51:58 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.646366919915307 on epoch=249
03/16/2022 07:52:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=252
03/16/2022 07:52:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
03/16/2022 07:52:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=257
03/16/2022 07:52:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
03/16/2022 07:52:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
03/16/2022 07:52:11 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.6733193277310925 on epoch=262
03/16/2022 07:52:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
03/16/2022 07:52:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
03/16/2022 07:52:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=269
03/16/2022 07:52:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
03/16/2022 07:52:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
03/16/2022 07:52:24 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.6250418935902807 on epoch=274
03/16/2022 07:52:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
03/16/2022 07:52:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
03/16/2022 07:52:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=282
03/16/2022 07:52:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
03/16/2022 07:52:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
03/16/2022 07:52:37 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.6629817943947465 on epoch=287
03/16/2022 07:52:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
03/16/2022 07:52:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
03/16/2022 07:52:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
03/16/2022 07:52:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
03/16/2022 07:52:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
03/16/2022 07:52:50 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.6087605377079062 on epoch=299
03/16/2022 07:52:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=302
03/16/2022 07:52:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
03/16/2022 07:52:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
03/16/2022 07:52:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=309
03/16/2022 07:53:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=312
03/16/2022 07:53:03 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.6926389746303319 on epoch=312
03/16/2022 07:53:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=314
03/16/2022 07:53:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
03/16/2022 07:53:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
03/16/2022 07:53:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=322
03/16/2022 07:53:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
03/16/2022 07:53:15 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.7498788845563039 on epoch=324
03/16/2022 07:53:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6960232145350111 -> 0.7498788845563039 on epoch=324, global_step=1300
03/16/2022 07:53:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
03/16/2022 07:53:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
03/16/2022 07:53:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
03/16/2022 07:53:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
03/16/2022 07:53:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
03/16/2022 07:53:28 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.6375650434917677 on epoch=337
03/16/2022 07:53:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
03/16/2022 07:53:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
03/16/2022 07:53:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
03/16/2022 07:53:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
03/16/2022 07:53:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
03/16/2022 07:53:41 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6460526315789474 on epoch=349
03/16/2022 07:53:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=352
03/16/2022 07:53:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
03/16/2022 07:53:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
03/16/2022 07:53:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
03/16/2022 07:53:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
03/16/2022 07:53:54 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6470018405502277 on epoch=362
03/16/2022 07:53:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
03/16/2022 07:53:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
03/16/2022 07:54:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
03/16/2022 07:54:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=372
03/16/2022 07:54:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
03/16/2022 07:54:07 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7028874675565443 on epoch=374
03/16/2022 07:54:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
03/16/2022 07:54:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=379
03/16/2022 07:54:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
03/16/2022 07:54:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=384
03/16/2022 07:54:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
03/16/2022 07:54:20 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.718348089468779 on epoch=387
03/16/2022 07:54:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
03/16/2022 07:54:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
03/16/2022 07:54:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
03/16/2022 07:54:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
03/16/2022 07:54:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
03/16/2022 07:54:33 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.7033802177858439 on epoch=399
03/16/2022 07:54:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/16/2022 07:54:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
03/16/2022 07:54:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
03/16/2022 07:54:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=409
03/16/2022 07:54:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
03/16/2022 07:54:46 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7107759511993383 on epoch=412
03/16/2022 07:54:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
03/16/2022 07:54:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
03/16/2022 07:54:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 07:54:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=422
03/16/2022 07:54:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
03/16/2022 07:54:59 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7152062056295928 on epoch=424
03/16/2022 07:55:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=427
03/16/2022 07:55:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
03/16/2022 07:55:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
03/16/2022 07:55:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/16/2022 07:55:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
03/16/2022 07:55:12 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6607142857142858 on epoch=437
03/16/2022 07:55:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=439
03/16/2022 07:55:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
03/16/2022 07:55:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=444
03/16/2022 07:55:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
03/16/2022 07:55:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=449
03/16/2022 07:55:25 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.6995812056295928 on epoch=449
03/16/2022 07:55:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/16/2022 07:55:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
03/16/2022 07:55:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
03/16/2022 07:55:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
03/16/2022 07:55:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
03/16/2022 07:55:38 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6608386327503974 on epoch=462
03/16/2022 07:55:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
03/16/2022 07:55:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
03/16/2022 07:55:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
03/16/2022 07:55:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/16/2022 07:55:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/16/2022 07:55:51 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6709265299782541 on epoch=474
03/16/2022 07:55:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
03/16/2022 07:55:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=479
03/16/2022 07:55:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
03/16/2022 07:56:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
03/16/2022 07:56:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
03/16/2022 07:56:03 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.6609903381642512 on epoch=487
03/16/2022 07:56:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/16/2022 07:56:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/16/2022 07:56:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/16/2022 07:56:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/16/2022 07:56:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 07:56:16 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.679694357113712 on epoch=499
03/16/2022 07:56:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/16/2022 07:56:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.13 on epoch=504
03/16/2022 07:56:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
03/16/2022 07:56:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/16/2022 07:56:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=512
03/16/2022 07:56:30 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.7285747642284146 on epoch=512
03/16/2022 07:56:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/16/2022 07:56:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
03/16/2022 07:56:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/16/2022 07:56:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/16/2022 07:56:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
03/16/2022 07:56:43 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7100052635536507 on epoch=524
03/16/2022 07:56:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/16/2022 07:56:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=529
03/16/2022 07:56:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=532
03/16/2022 07:56:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/16/2022 07:56:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/16/2022 07:56:56 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.6345314315855379 on epoch=537
03/16/2022 07:56:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
03/16/2022 07:57:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
03/16/2022 07:57:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
03/16/2022 07:57:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/16/2022 07:57:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/16/2022 07:57:09 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6767800944138473 on epoch=549
03/16/2022 07:57:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/16/2022 07:57:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
03/16/2022 07:57:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
03/16/2022 07:57:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
03/16/2022 07:57:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/16/2022 07:57:22 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6324110671936759 on epoch=562
03/16/2022 07:57:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/16/2022 07:57:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
03/16/2022 07:57:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/16/2022 07:57:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=572
03/16/2022 07:57:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/16/2022 07:57:35 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.6901596402937051 on epoch=574
03/16/2022 07:57:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/16/2022 07:57:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/16/2022 07:57:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/16/2022 07:57:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/16/2022 07:57:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
03/16/2022 07:57:48 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.667480725239346 on epoch=587
03/16/2022 07:57:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
03/16/2022 07:57:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/16/2022 07:57:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
03/16/2022 07:57:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
03/16/2022 07:58:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/16/2022 07:58:01 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7332481015462106 on epoch=599
03/16/2022 07:58:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/16/2022 07:58:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/16/2022 07:58:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=607
03/16/2022 07:58:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
03/16/2022 07:58:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/16/2022 07:58:14 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.71012012012012 on epoch=612
03/16/2022 07:58:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/16/2022 07:58:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/16/2022 07:58:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
03/16/2022 07:58:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/16/2022 07:58:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=624
03/16/2022 07:58:28 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6961861002608032 on epoch=624
03/16/2022 07:58:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=627
03/16/2022 07:58:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
03/16/2022 07:58:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/16/2022 07:58:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
03/16/2022 07:58:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
03/16/2022 07:58:41 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.5928071928071927 on epoch=637
03/16/2022 07:58:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/16/2022 07:58:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/16/2022 07:58:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=644
03/16/2022 07:58:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/16/2022 07:58:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/16/2022 07:58:54 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7320031990819342 on epoch=649
03/16/2022 07:58:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=652
03/16/2022 07:58:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/16/2022 07:59:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/16/2022 07:59:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/16/2022 07:59:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
03/16/2022 07:59:07 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6660957285957285 on epoch=662
03/16/2022 07:59:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/16/2022 07:59:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
03/16/2022 07:59:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/16/2022 07:59:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/16/2022 07:59:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/16/2022 07:59:21 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6961861002608032 on epoch=674
03/16/2022 07:59:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/16/2022 07:59:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/16/2022 07:59:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/16/2022 07:59:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/16/2022 07:59:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/16/2022 07:59:34 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6961861002608032 on epoch=687
03/16/2022 07:59:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/16/2022 07:59:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/16/2022 07:59:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/16/2022 07:59:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/16/2022 07:59:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/16/2022 07:59:48 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6812472437472438 on epoch=699
03/16/2022 07:59:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/16/2022 07:59:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/16/2022 07:59:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/16/2022 07:59:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
03/16/2022 08:00:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/16/2022 08:00:01 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6864055299539171 on epoch=712
03/16/2022 08:00:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/16/2022 08:00:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
03/16/2022 08:00:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/16/2022 08:00:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/16/2022 08:00:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
03/16/2022 08:00:14 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6512107247591119 on epoch=724
03/16/2022 08:00:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/16/2022 08:00:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 08:00:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 08:00:24 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/16/2022 08:00:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/16/2022 08:00:27 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6967121588089331 on epoch=737
03/16/2022 08:00:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/16/2022 08:00:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 08:00:34 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
03/16/2022 08:00:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 08:00:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 08:00:40 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6715717610805141 on epoch=749
03/16/2022 08:00:40 - INFO - __main__ - save last model!
03/16/2022 08:00:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 08:00:40 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 08:00:40 - INFO - __main__ - Printing 3 examples
03/16/2022 08:00:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 08:00:40 - INFO - __main__ - ['others']
03/16/2022 08:00:40 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 08:00:40 - INFO - __main__ - ['others']
03/16/2022 08:00:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 08:00:40 - INFO - __main__ - ['others']
03/16/2022 08:00:40 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:00:40 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:00:40 - INFO - __main__ - Printing 3 examples
03/16/2022 08:00:40 - INFO - __main__ -  [emo] how cause yes am listening
03/16/2022 08:00:40 - INFO - __main__ - ['others']
03/16/2022 08:00:40 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/16/2022 08:00:40 - INFO - __main__ - ['others']
03/16/2022 08:00:40 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/16/2022 08:00:40 - INFO - __main__ - ['others']
03/16/2022 08:00:40 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:00:40 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:00:40 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 08:00:40 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:00:40 - INFO - __main__ - Printing 3 examples
03/16/2022 08:00:40 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/16/2022 08:00:40 - INFO - __main__ - ['others']
03/16/2022 08:00:40 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/16/2022 08:00:40 - INFO - __main__ - ['others']
03/16/2022 08:00:40 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/16/2022 08:00:40 - INFO - __main__ - ['others']
03/16/2022 08:00:40 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:00:40 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:00:40 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 08:00:42 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:00:48 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 08:00:56 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 08:00:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 08:00:57 - INFO - __main__ - Starting training!
03/16/2022 08:02:22 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_100_0.4_8_predictions.txt
03/16/2022 08:02:22 - INFO - __main__ - Classification-F1 on test data: 0.3240
03/16/2022 08:02:22 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.7498788845563039, test_performance=0.3239814704286255
03/16/2022 08:02:22 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
03/16/2022 08:02:23 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:02:23 - INFO - __main__ - Printing 3 examples
03/16/2022 08:02:23 - INFO - __main__ -  [emo] how cause yes am listening
03/16/2022 08:02:23 - INFO - __main__ - ['others']
03/16/2022 08:02:23 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/16/2022 08:02:23 - INFO - __main__ - ['others']
03/16/2022 08:02:23 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/16/2022 08:02:23 - INFO - __main__ - ['others']
03/16/2022 08:02:23 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:02:23 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:02:23 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 08:02:23 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:02:23 - INFO - __main__ - Printing 3 examples
03/16/2022 08:02:23 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/16/2022 08:02:23 - INFO - __main__ - ['others']
03/16/2022 08:02:23 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/16/2022 08:02:23 - INFO - __main__ - ['others']
03/16/2022 08:02:23 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/16/2022 08:02:23 - INFO - __main__ - ['others']
03/16/2022 08:02:23 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:02:23 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:02:23 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 08:02:42 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 08:02:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 08:02:43 - INFO - __main__ - Starting training!
03/16/2022 08:02:46 - INFO - __main__ - Step 10 Global step 10 Train loss 2.81 on epoch=2
03/16/2022 08:02:49 - INFO - __main__ - Step 20 Global step 20 Train loss 1.30 on epoch=4
03/16/2022 08:02:51 - INFO - __main__ - Step 30 Global step 30 Train loss 1.15 on epoch=7
03/16/2022 08:02:53 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=9
03/16/2022 08:02:56 - INFO - __main__ - Step 50 Global step 50 Train loss 1.06 on epoch=12
03/16/2022 08:02:57 - INFO - __main__ - Global step 50 Train loss 1.46 Classification-F1 0.13067758749069247 on epoch=12
03/16/2022 08:02:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=12, global_step=50
03/16/2022 08:02:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.89 on epoch=14
03/16/2022 08:03:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.85 on epoch=17
03/16/2022 08:03:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=19
03/16/2022 08:03:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.86 on epoch=22
03/16/2022 08:03:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.92 on epoch=24
03/16/2022 08:03:10 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.35574860574860573 on epoch=24
03/16/2022 08:03:10 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.35574860574860573 on epoch=24, global_step=100
03/16/2022 08:03:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.89 on epoch=27
03/16/2022 08:03:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.77 on epoch=29
03/16/2022 08:03:17 - INFO - __main__ - Step 130 Global step 130 Train loss 1.17 on epoch=32
03/16/2022 08:03:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=34
03/16/2022 08:03:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=37
03/16/2022 08:03:23 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.3162982584338083 on epoch=37
03/16/2022 08:03:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.85 on epoch=39
03/16/2022 08:03:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.80 on epoch=42
03/16/2022 08:03:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.77 on epoch=44
03/16/2022 08:03:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.85 on epoch=47
03/16/2022 08:03:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.73 on epoch=49
03/16/2022 08:03:36 - INFO - __main__ - Global step 200 Train loss 0.80 Classification-F1 0.29239319033369376 on epoch=49
03/16/2022 08:03:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.84 on epoch=52
03/16/2022 08:03:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.82 on epoch=54
03/16/2022 08:03:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=57
03/16/2022 08:03:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=59
03/16/2022 08:03:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=62
03/16/2022 08:03:49 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.3717377860235003 on epoch=62
03/16/2022 08:03:49 - INFO - __main__ - Saving model with best Classification-F1: 0.35574860574860573 -> 0.3717377860235003 on epoch=62, global_step=250
03/16/2022 08:03:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.76 on epoch=64
03/16/2022 08:03:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.73 on epoch=67
03/16/2022 08:03:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.74 on epoch=69
03/16/2022 08:03:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.78 on epoch=72
03/16/2022 08:04:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=74
03/16/2022 08:04:01 - INFO - __main__ - Global step 300 Train loss 0.76 Classification-F1 0.37277777777777776 on epoch=74
03/16/2022 08:04:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3717377860235003 -> 0.37277777777777776 on epoch=74, global_step=300
03/16/2022 08:04:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.71 on epoch=77
03/16/2022 08:04:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.73 on epoch=79
03/16/2022 08:04:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.74 on epoch=82
03/16/2022 08:04:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.73 on epoch=84
03/16/2022 08:04:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=87
03/16/2022 08:04:14 - INFO - __main__ - Global step 350 Train loss 0.75 Classification-F1 0.5570289378023299 on epoch=87
03/16/2022 08:04:14 - INFO - __main__ - Saving model with best Classification-F1: 0.37277777777777776 -> 0.5570289378023299 on epoch=87, global_step=350
03/16/2022 08:04:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.71 on epoch=89
03/16/2022 08:04:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.84 on epoch=92
03/16/2022 08:04:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.65 on epoch=94
03/16/2022 08:04:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.76 on epoch=97
03/16/2022 08:04:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=99
03/16/2022 08:04:27 - INFO - __main__ - Global step 400 Train loss 0.76 Classification-F1 0.4088888888888889 on epoch=99
03/16/2022 08:04:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.80 on epoch=102
03/16/2022 08:04:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.70 on epoch=104
03/16/2022 08:04:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.73 on epoch=107
03/16/2022 08:04:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.69 on epoch=109
03/16/2022 08:04:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.72 on epoch=112
03/16/2022 08:04:40 - INFO - __main__ - Global step 450 Train loss 0.73 Classification-F1 0.4944222444222444 on epoch=112
03/16/2022 08:04:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=114
03/16/2022 08:04:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.70 on epoch=117
03/16/2022 08:04:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=119
03/16/2022 08:04:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.68 on epoch=122
03/16/2022 08:04:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.61 on epoch=124
03/16/2022 08:04:53 - INFO - __main__ - Global step 500 Train loss 0.67 Classification-F1 0.40815649867374004 on epoch=124
03/16/2022 08:04:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.71 on epoch=127
03/16/2022 08:04:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=129
03/16/2022 08:05:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.71 on epoch=132
03/16/2022 08:05:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.69 on epoch=134
03/16/2022 08:05:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.68 on epoch=137
03/16/2022 08:05:06 - INFO - __main__ - Global step 550 Train loss 0.69 Classification-F1 0.5565095476455321 on epoch=137
03/16/2022 08:05:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.60 on epoch=139
03/16/2022 08:05:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.62 on epoch=142
03/16/2022 08:05:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.64 on epoch=144
03/16/2022 08:05:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.65 on epoch=147
03/16/2022 08:05:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.63 on epoch=149
03/16/2022 08:05:19 - INFO - __main__ - Global step 600 Train loss 0.63 Classification-F1 0.49387532790894134 on epoch=149
03/16/2022 08:05:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.57 on epoch=152
03/16/2022 08:05:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.60 on epoch=154
03/16/2022 08:05:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.55 on epoch=157
03/16/2022 08:05:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=159
03/16/2022 08:05:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.63 on epoch=162
03/16/2022 08:05:32 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.4438827838827839 on epoch=162
03/16/2022 08:05:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=164
03/16/2022 08:05:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=167
03/16/2022 08:05:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=169
03/16/2022 08:05:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=172
03/16/2022 08:05:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=174
03/16/2022 08:05:44 - INFO - __main__ - Global step 700 Train loss 0.53 Classification-F1 0.5839847463486302 on epoch=174
03/16/2022 08:05:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5570289378023299 -> 0.5839847463486302 on epoch=174, global_step=700
03/16/2022 08:05:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=177
03/16/2022 08:05:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.53 on epoch=179
03/16/2022 08:05:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=182
03/16/2022 08:05:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=184
03/16/2022 08:05:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=187
03/16/2022 08:05:57 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.5568848776968982 on epoch=187
03/16/2022 08:06:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=189
03/16/2022 08:06:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=192
03/16/2022 08:06:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=194
03/16/2022 08:06:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=197
03/16/2022 08:06:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=199
03/16/2022 08:06:10 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.5642439431913117 on epoch=199
03/16/2022 08:06:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.50 on epoch=202
03/16/2022 08:06:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=204
03/16/2022 08:06:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=207
03/16/2022 08:06:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=209
03/16/2022 08:06:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=212
03/16/2022 08:06:23 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.619925379058506 on epoch=212
03/16/2022 08:06:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5839847463486302 -> 0.619925379058506 on epoch=212, global_step=850
03/16/2022 08:06:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=214
03/16/2022 08:06:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=217
03/16/2022 08:06:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.55 on epoch=219
03/16/2022 08:06:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=222
03/16/2022 08:06:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=224
03/16/2022 08:06:36 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.5040187678498572 on epoch=224
03/16/2022 08:06:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.31 on epoch=227
03/16/2022 08:06:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=229
03/16/2022 08:06:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=232
03/16/2022 08:06:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=234
03/16/2022 08:06:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=237
03/16/2022 08:06:48 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.5906125608137992 on epoch=237
03/16/2022 08:06:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=239
03/16/2022 08:06:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=242
03/16/2022 08:06:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=244
03/16/2022 08:06:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=247
03/16/2022 08:07:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=249
03/16/2022 08:07:01 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.6374475375434454 on epoch=249
03/16/2022 08:07:01 - INFO - __main__ - Saving model with best Classification-F1: 0.619925379058506 -> 0.6374475375434454 on epoch=249, global_step=1000
03/16/2022 08:07:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=252
03/16/2022 08:07:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=254
03/16/2022 08:07:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=257
03/16/2022 08:07:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=259
03/16/2022 08:07:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=262
03/16/2022 08:07:14 - INFO - __main__ - Global step 1050 Train loss 0.27 Classification-F1 0.5380076445293838 on epoch=262
03/16/2022 08:07:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=264
03/16/2022 08:07:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.15 on epoch=267
03/16/2022 08:07:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=269
03/16/2022 08:07:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=272
03/16/2022 08:07:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=274
03/16/2022 08:07:27 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.650462962962963 on epoch=274
03/16/2022 08:07:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6374475375434454 -> 0.650462962962963 on epoch=274, global_step=1100
03/16/2022 08:07:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=277
03/16/2022 08:07:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=279
03/16/2022 08:07:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=282
03/16/2022 08:07:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=284
03/16/2022 08:07:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=287
03/16/2022 08:07:40 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.6135057471264367 on epoch=287
03/16/2022 08:07:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=289
03/16/2022 08:07:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=292
03/16/2022 08:07:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=294
03/16/2022 08:07:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=297
03/16/2022 08:07:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
03/16/2022 08:07:53 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.6434121621621621 on epoch=299
03/16/2022 08:07:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
03/16/2022 08:07:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=304
03/16/2022 08:08:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=307
03/16/2022 08:08:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=309
03/16/2022 08:08:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=312
03/16/2022 08:08:06 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.6022394651087779 on epoch=312
03/16/2022 08:08:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=314
03/16/2022 08:08:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=317
03/16/2022 08:08:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
03/16/2022 08:08:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=322
03/16/2022 08:08:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
03/16/2022 08:08:19 - INFO - __main__ - Global step 1300 Train loss 0.11 Classification-F1 0.6535714285714286 on epoch=324
03/16/2022 08:08:19 - INFO - __main__ - Saving model with best Classification-F1: 0.650462962962963 -> 0.6535714285714286 on epoch=324, global_step=1300
03/16/2022 08:08:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
03/16/2022 08:08:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
03/16/2022 08:08:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
03/16/2022 08:08:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=334
03/16/2022 08:08:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=337
03/16/2022 08:08:32 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.6265814777327936 on epoch=337
03/16/2022 08:08:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
03/16/2022 08:08:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=342
03/16/2022 08:08:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=344
03/16/2022 08:08:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=347
03/16/2022 08:08:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=349
03/16/2022 08:08:45 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.5817706683560342 on epoch=349
03/16/2022 08:08:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=352
03/16/2022 08:08:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=354
03/16/2022 08:08:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
03/16/2022 08:08:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=359
03/16/2022 08:08:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
03/16/2022 08:08:57 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.5784500052792736 on epoch=362
03/16/2022 08:09:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=364
03/16/2022 08:09:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
03/16/2022 08:09:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
03/16/2022 08:09:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=372
03/16/2022 08:09:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=374
03/16/2022 08:09:10 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.6697140676973341 on epoch=374
03/16/2022 08:09:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6535714285714286 -> 0.6697140676973341 on epoch=374, global_step=1500
03/16/2022 08:09:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=377
03/16/2022 08:09:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
03/16/2022 08:09:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
03/16/2022 08:09:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=384
03/16/2022 08:09:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
03/16/2022 08:09:23 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.733973089468779 on epoch=387
03/16/2022 08:09:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6697140676973341 -> 0.733973089468779 on epoch=387, global_step=1550
03/16/2022 08:09:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=389
03/16/2022 08:09:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=392
03/16/2022 08:09:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.11 on epoch=394
03/16/2022 08:09:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
03/16/2022 08:09:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=399
03/16/2022 08:09:36 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.6709280303030304 on epoch=399
03/16/2022 08:09:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
03/16/2022 08:09:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=404
03/16/2022 08:09:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
03/16/2022 08:09:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
03/16/2022 08:09:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
03/16/2022 08:09:49 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.670145845855874 on epoch=412
03/16/2022 08:09:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
03/16/2022 08:09:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=417
03/16/2022 08:09:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=419
03/16/2022 08:09:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
03/16/2022 08:10:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=424
03/16/2022 08:10:03 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.5965524133514442 on epoch=424
03/16/2022 08:10:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
03/16/2022 08:10:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
03/16/2022 08:10:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
03/16/2022 08:10:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=434
03/16/2022 08:10:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
03/16/2022 08:10:16 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.7003267858265736 on epoch=437
03/16/2022 08:10:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
03/16/2022 08:10:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
03/16/2022 08:10:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
03/16/2022 08:10:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=447
03/16/2022 08:10:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=449
03/16/2022 08:10:28 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.6401069518716578 on epoch=449
03/16/2022 08:10:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=452
03/16/2022 08:10:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=454
03/16/2022 08:10:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
03/16/2022 08:10:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
03/16/2022 08:10:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
03/16/2022 08:10:41 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.6726533968656999 on epoch=462
03/16/2022 08:10:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
03/16/2022 08:10:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
03/16/2022 08:10:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
03/16/2022 08:10:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
03/16/2022 08:10:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/16/2022 08:10:54 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.610034917134456 on epoch=474
03/16/2022 08:10:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
03/16/2022 08:10:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/16/2022 08:11:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=482
03/16/2022 08:11:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=484
03/16/2022 08:11:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=487
03/16/2022 08:11:07 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.622616236445475 on epoch=487
03/16/2022 08:11:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
03/16/2022 08:11:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
03/16/2022 08:11:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
03/16/2022 08:11:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
03/16/2022 08:11:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
03/16/2022 08:11:20 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.6944124423963134 on epoch=499
03/16/2022 08:11:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=502
03/16/2022 08:11:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 08:11:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=507
03/16/2022 08:11:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/16/2022 08:11:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
03/16/2022 08:11:34 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.6978914240755312 on epoch=512
03/16/2022 08:11:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
03/16/2022 08:11:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/16/2022 08:11:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
03/16/2022 08:11:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
03/16/2022 08:11:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
03/16/2022 08:11:48 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.6623543123543123 on epoch=524
03/16/2022 08:11:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
03/16/2022 08:11:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 08:11:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=532
03/16/2022 08:11:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
03/16/2022 08:12:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
03/16/2022 08:12:01 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.5977257745550428 on epoch=537
03/16/2022 08:12:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/16/2022 08:12:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/16/2022 08:12:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
03/16/2022 08:12:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/16/2022 08:12:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/16/2022 08:12:15 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6297445797445798 on epoch=549
03/16/2022 08:12:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
03/16/2022 08:12:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
03/16/2022 08:12:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/16/2022 08:12:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
03/16/2022 08:12:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/16/2022 08:12:28 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6431502525252526 on epoch=562
03/16/2022 08:12:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
03/16/2022 08:12:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/16/2022 08:12:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
03/16/2022 08:12:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 08:12:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
03/16/2022 08:12:42 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6343390804597702 on epoch=574
03/16/2022 08:12:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=577
03/16/2022 08:12:46 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/16/2022 08:12:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/16/2022 08:12:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 08:12:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
03/16/2022 08:12:55 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6290509259259259 on epoch=587
03/16/2022 08:12:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 08:13:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/16/2022 08:13:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/16/2022 08:13:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
03/16/2022 08:13:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
03/16/2022 08:13:08 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.5726972486607376 on epoch=599
03/16/2022 08:13:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=602
03/16/2022 08:13:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
03/16/2022 08:13:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/16/2022 08:13:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
03/16/2022 08:13:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
03/16/2022 08:13:22 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.6481947423417478 on epoch=612
03/16/2022 08:13:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
03/16/2022 08:13:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 08:13:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 08:13:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/16/2022 08:13:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=624
03/16/2022 08:13:35 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.5913138332255979 on epoch=624
03/16/2022 08:13:37 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
03/16/2022 08:13:40 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
03/16/2022 08:13:42 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
03/16/2022 08:13:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=634
03/16/2022 08:13:47 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/16/2022 08:13:48 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.6528030303030303 on epoch=637
03/16/2022 08:13:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/16/2022 08:13:53 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
03/16/2022 08:13:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
03/16/2022 08:13:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
03/16/2022 08:14:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=649
03/16/2022 08:14:01 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.6699485915281244 on epoch=649
03/16/2022 08:14:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/16/2022 08:14:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=654
03/16/2022 08:14:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
03/16/2022 08:14:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=659
03/16/2022 08:14:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/16/2022 08:14:15 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.63264221158958 on epoch=662
03/16/2022 08:14:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/16/2022 08:14:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=667
03/16/2022 08:14:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/16/2022 08:14:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
03/16/2022 08:14:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
03/16/2022 08:14:28 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.642837918660287 on epoch=674
03/16/2022 08:14:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/16/2022 08:14:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/16/2022 08:14:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/16/2022 08:14:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/16/2022 08:14:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/16/2022 08:14:41 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6565058479532164 on epoch=687
03/16/2022 08:14:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/16/2022 08:14:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
03/16/2022 08:14:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/16/2022 08:14:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/16/2022 08:14:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/16/2022 08:14:55 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5972160707987828 on epoch=699
03/16/2022 08:14:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
03/16/2022 08:15:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/16/2022 08:15:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/16/2022 08:15:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
03/16/2022 08:15:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/16/2022 08:15:08 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6494659982563208 on epoch=712
03/16/2022 08:15:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/16/2022 08:15:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
03/16/2022 08:15:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
03/16/2022 08:15:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=722
03/16/2022 08:15:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/16/2022 08:15:22 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6958994708994709 on epoch=724
03/16/2022 08:15:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/16/2022 08:15:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 08:15:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=732
03/16/2022 08:15:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
03/16/2022 08:15:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
03/16/2022 08:15:35 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6639819245082403 on epoch=737
03/16/2022 08:15:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/16/2022 08:15:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 08:15:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=744
03/16/2022 08:15:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 08:15:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 08:15:48 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6504928315412186 on epoch=749
03/16/2022 08:15:48 - INFO - __main__ - save last model!
03/16/2022 08:15:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 08:15:48 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 08:15:48 - INFO - __main__ - Printing 3 examples
03/16/2022 08:15:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 08:15:48 - INFO - __main__ - ['others']
03/16/2022 08:15:48 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 08:15:48 - INFO - __main__ - ['others']
03/16/2022 08:15:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 08:15:48 - INFO - __main__ - ['others']
03/16/2022 08:15:48 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:15:50 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:15:50 - INFO - __main__ - Printing 3 examples
03/16/2022 08:15:50 - INFO - __main__ -  [emo] how cause yes am listening
03/16/2022 08:15:50 - INFO - __main__ - ['others']
03/16/2022 08:15:50 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/16/2022 08:15:50 - INFO - __main__ - ['others']
03/16/2022 08:15:50 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/16/2022 08:15:50 - INFO - __main__ - ['others']
03/16/2022 08:15:50 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:15:50 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:15:50 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 08:15:50 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:15:50 - INFO - __main__ - Printing 3 examples
03/16/2022 08:15:50 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/16/2022 08:15:50 - INFO - __main__ - ['others']
03/16/2022 08:15:50 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/16/2022 08:15:50 - INFO - __main__ - ['others']
03/16/2022 08:15:50 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/16/2022 08:15:50 - INFO - __main__ - ['others']
03/16/2022 08:15:50 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:15:50 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:15:50 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 08:15:51 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:15:56 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 08:16:08 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 08:16:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 08:16:09 - INFO - __main__ - Starting training!
03/16/2022 08:17:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_100_0.3_8_predictions.txt
03/16/2022 08:17:30 - INFO - __main__ - Classification-F1 on test data: 0.3103
03/16/2022 08:17:31 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.733973089468779, test_performance=0.310295641697495
03/16/2022 08:17:31 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
03/16/2022 08:17:31 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:17:31 - INFO - __main__ - Printing 3 examples
03/16/2022 08:17:31 - INFO - __main__ -  [emo] how cause yes am listening
03/16/2022 08:17:31 - INFO - __main__ - ['others']
03/16/2022 08:17:31 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/16/2022 08:17:31 - INFO - __main__ - ['others']
03/16/2022 08:17:31 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/16/2022 08:17:31 - INFO - __main__ - ['others']
03/16/2022 08:17:31 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:17:32 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:17:32 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 08:17:32 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:17:32 - INFO - __main__ - Printing 3 examples
03/16/2022 08:17:32 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/16/2022 08:17:32 - INFO - __main__ - ['others']
03/16/2022 08:17:32 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/16/2022 08:17:32 - INFO - __main__ - ['others']
03/16/2022 08:17:32 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/16/2022 08:17:32 - INFO - __main__ - ['others']
03/16/2022 08:17:32 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:17:32 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:17:32 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 08:17:51 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 08:17:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 08:17:52 - INFO - __main__ - Starting training!
03/16/2022 08:17:54 - INFO - __main__ - Step 10 Global step 10 Train loss 3.09 on epoch=2
03/16/2022 08:17:57 - INFO - __main__ - Step 20 Global step 20 Train loss 1.57 on epoch=4
03/16/2022 08:17:59 - INFO - __main__ - Step 30 Global step 30 Train loss 1.30 on epoch=7
03/16/2022 08:18:01 - INFO - __main__ - Step 40 Global step 40 Train loss 1.07 on epoch=9
03/16/2022 08:18:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.98 on epoch=12
03/16/2022 08:18:05 - INFO - __main__ - Global step 50 Train loss 1.60 Classification-F1 0.1 on epoch=12
03/16/2022 08:18:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/16/2022 08:18:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.94 on epoch=14
03/16/2022 08:18:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=17
03/16/2022 08:18:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.90 on epoch=19
03/16/2022 08:18:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.91 on epoch=22
03/16/2022 08:18:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.84 on epoch=24
03/16/2022 08:18:17 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.2393048128342246 on epoch=24
03/16/2022 08:18:17 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.2393048128342246 on epoch=24, global_step=100
03/16/2022 08:18:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=27
03/16/2022 08:18:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.89 on epoch=29
03/16/2022 08:18:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.89 on epoch=32
03/16/2022 08:18:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.86 on epoch=34
03/16/2022 08:18:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.84 on epoch=37
03/16/2022 08:18:30 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.15295815295815296 on epoch=37
03/16/2022 08:18:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=39
03/16/2022 08:18:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.90 on epoch=42
03/16/2022 08:18:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=44
03/16/2022 08:18:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.79 on epoch=47
03/16/2022 08:18:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.87 on epoch=49
03/16/2022 08:18:42 - INFO - __main__ - Global step 200 Train loss 0.85 Classification-F1 0.3865012106537531 on epoch=49
03/16/2022 08:18:42 - INFO - __main__ - Saving model with best Classification-F1: 0.2393048128342246 -> 0.3865012106537531 on epoch=49, global_step=200
03/16/2022 08:18:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.87 on epoch=52
03/16/2022 08:18:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.88 on epoch=54
03/16/2022 08:18:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=57
03/16/2022 08:18:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.72 on epoch=59
03/16/2022 08:18:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.67 on epoch=62
03/16/2022 08:18:55 - INFO - __main__ - Global step 250 Train loss 0.80 Classification-F1 0.4546440527430484 on epoch=62
03/16/2022 08:18:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3865012106537531 -> 0.4546440527430484 on epoch=62, global_step=250
03/16/2022 08:18:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.80 on epoch=64
03/16/2022 08:19:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.83 on epoch=67
03/16/2022 08:19:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.81 on epoch=69
03/16/2022 08:19:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=72
03/16/2022 08:19:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.68 on epoch=74
03/16/2022 08:19:08 - INFO - __main__ - Global step 300 Train loss 0.78 Classification-F1 0.3985119047619048 on epoch=74
03/16/2022 08:19:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.66 on epoch=77
03/16/2022 08:19:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.71 on epoch=79
03/16/2022 08:19:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.62 on epoch=82
03/16/2022 08:19:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.62 on epoch=84
03/16/2022 08:19:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.64 on epoch=87
03/16/2022 08:19:20 - INFO - __main__ - Global step 350 Train loss 0.65 Classification-F1 0.5630442354580285 on epoch=87
03/16/2022 08:19:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4546440527430484 -> 0.5630442354580285 on epoch=87, global_step=350
03/16/2022 08:19:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.60 on epoch=89
03/16/2022 08:19:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.62 on epoch=92
03/16/2022 08:19:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=94
03/16/2022 08:19:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.58 on epoch=97
03/16/2022 08:19:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.60 on epoch=99
03/16/2022 08:19:33 - INFO - __main__ - Global step 400 Train loss 0.59 Classification-F1 0.38903225806451613 on epoch=99
03/16/2022 08:19:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.56 on epoch=102
03/16/2022 08:19:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=104
03/16/2022 08:19:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=107
03/16/2022 08:19:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=109
03/16/2022 08:19:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=112
03/16/2022 08:19:45 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.50979532555151 on epoch=112
03/16/2022 08:19:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=114
03/16/2022 08:19:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=117
03/16/2022 08:19:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=119
03/16/2022 08:19:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=122
03/16/2022 08:19:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=124
03/16/2022 08:19:58 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.5057653457653458 on epoch=124
03/16/2022 08:20:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=127
03/16/2022 08:20:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=129
03/16/2022 08:20:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=132
03/16/2022 08:20:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=134
03/16/2022 08:20:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=137
03/16/2022 08:20:10 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.616214667685256 on epoch=137
03/16/2022 08:20:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5630442354580285 -> 0.616214667685256 on epoch=137, global_step=550
03/16/2022 08:20:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=139
03/16/2022 08:20:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=142
03/16/2022 08:20:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=144
03/16/2022 08:20:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=147
03/16/2022 08:20:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=149
03/16/2022 08:20:23 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.6072375899962106 on epoch=149
03/16/2022 08:20:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=152
03/16/2022 08:20:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=154
03/16/2022 08:20:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=157
03/16/2022 08:20:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=159
03/16/2022 08:20:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=162
03/16/2022 08:20:35 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.7300610269360269 on epoch=162
03/16/2022 08:20:35 - INFO - __main__ - Saving model with best Classification-F1: 0.616214667685256 -> 0.7300610269360269 on epoch=162, global_step=650
03/16/2022 08:20:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.31 on epoch=164
03/16/2022 08:20:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
03/16/2022 08:20:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=169
03/16/2022 08:20:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=172
03/16/2022 08:20:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=174
03/16/2022 08:20:48 - INFO - __main__ - Global step 700 Train loss 0.25 Classification-F1 0.6745112781954887 on epoch=174
03/16/2022 08:20:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=177
03/16/2022 08:20:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=179
03/16/2022 08:20:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=182
03/16/2022 08:20:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=184
03/16/2022 08:21:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=187
03/16/2022 08:21:01 - INFO - __main__ - Global step 750 Train loss 0.26 Classification-F1 0.7040895341802782 on epoch=187
03/16/2022 08:21:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=189
03/16/2022 08:21:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=192
03/16/2022 08:21:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=194
03/16/2022 08:21:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=197
03/16/2022 08:21:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=199
03/16/2022 08:21:13 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.635266049739734 on epoch=199
03/16/2022 08:21:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=202
03/16/2022 08:21:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=204
03/16/2022 08:21:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=207
03/16/2022 08:21:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=209
03/16/2022 08:21:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=212
03/16/2022 08:21:26 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.6575616760734728 on epoch=212
03/16/2022 08:21:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=214
03/16/2022 08:21:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=217
03/16/2022 08:21:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=219
03/16/2022 08:21:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=222
03/16/2022 08:21:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=224
03/16/2022 08:21:38 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.6613667582417583 on epoch=224
03/16/2022 08:21:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=227
03/16/2022 08:21:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=229
03/16/2022 08:21:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
03/16/2022 08:21:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=234
03/16/2022 08:21:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=237
03/16/2022 08:21:51 - INFO - __main__ - Global step 950 Train loss 0.14 Classification-F1 0.6690089829035062 on epoch=237
03/16/2022 08:21:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=239
03/16/2022 08:21:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=242
03/16/2022 08:21:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
03/16/2022 08:22:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=247
03/16/2022 08:22:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
03/16/2022 08:22:04 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.6162760611036473 on epoch=249
03/16/2022 08:22:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=252
03/16/2022 08:22:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
03/16/2022 08:22:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=257
03/16/2022 08:22:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=259
03/16/2022 08:22:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
03/16/2022 08:22:16 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.6166003351487224 on epoch=262
03/16/2022 08:22:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
03/16/2022 08:22:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=267
03/16/2022 08:22:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
03/16/2022 08:22:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=272
03/16/2022 08:22:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=274
03/16/2022 08:22:29 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.697265209855954 on epoch=274
03/16/2022 08:22:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=277
03/16/2022 08:22:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
03/16/2022 08:22:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
03/16/2022 08:22:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=284
03/16/2022 08:22:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=287
03/16/2022 08:22:42 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.6962962962962962 on epoch=287
03/16/2022 08:22:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
03/16/2022 08:22:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=292
03/16/2022 08:22:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
03/16/2022 08:22:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
03/16/2022 08:22:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
03/16/2022 08:22:55 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.637024756852343 on epoch=299
03/16/2022 08:22:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.09 on epoch=302
03/16/2022 08:23:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=304
03/16/2022 08:23:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
03/16/2022 08:23:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
03/16/2022 08:23:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
03/16/2022 08:23:08 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.6621212121212121 on epoch=312
03/16/2022 08:23:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
03/16/2022 08:23:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
03/16/2022 08:23:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=319
03/16/2022 08:23:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
03/16/2022 08:23:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
03/16/2022 08:23:21 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.6990454404247508 on epoch=324
03/16/2022 08:23:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
03/16/2022 08:23:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.11 on epoch=329
03/16/2022 08:23:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=332
03/16/2022 08:23:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=334
03/16/2022 08:23:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
03/16/2022 08:23:34 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.7119307400379508 on epoch=337
03/16/2022 08:23:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
03/16/2022 08:23:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=342
03/16/2022 08:23:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
03/16/2022 08:23:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
03/16/2022 08:23:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
03/16/2022 08:23:47 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.59176155466271 on epoch=349
03/16/2022 08:23:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=352
03/16/2022 08:23:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=354
03/16/2022 08:23:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
03/16/2022 08:23:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=359
03/16/2022 08:23:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
03/16/2022 08:24:00 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.6554753490237362 on epoch=362
03/16/2022 08:24:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
03/16/2022 08:24:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
03/16/2022 08:24:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
03/16/2022 08:24:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
03/16/2022 08:24:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
03/16/2022 08:24:13 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.6657451386351642 on epoch=374
03/16/2022 08:24:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
03/16/2022 08:24:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
03/16/2022 08:24:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
03/16/2022 08:24:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
03/16/2022 08:24:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
03/16/2022 08:24:26 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6002673796791445 on epoch=387
03/16/2022 08:24:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
03/16/2022 08:24:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=392
03/16/2022 08:24:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
03/16/2022 08:24:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
03/16/2022 08:24:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=399
03/16/2022 08:24:39 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.6324266537681171 on epoch=399
03/16/2022 08:24:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
03/16/2022 08:24:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
03/16/2022 08:24:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=407
03/16/2022 08:24:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
03/16/2022 08:24:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
03/16/2022 08:24:53 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7293660640434834 on epoch=412
03/16/2022 08:24:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
03/16/2022 08:24:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
03/16/2022 08:25:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
03/16/2022 08:25:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
03/16/2022 08:25:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
03/16/2022 08:25:06 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7132211538461538 on epoch=424
03/16/2022 08:25:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
03/16/2022 08:25:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
03/16/2022 08:25:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
03/16/2022 08:25:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=434
03/16/2022 08:25:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=437
03/16/2022 08:25:19 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6700375773651636 on epoch=437
03/16/2022 08:25:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
03/16/2022 08:25:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
03/16/2022 08:25:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
03/16/2022 08:25:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=447
03/16/2022 08:25:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/16/2022 08:25:32 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6951699969526794 on epoch=449
03/16/2022 08:25:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
03/16/2022 08:25:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
03/16/2022 08:25:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
03/16/2022 08:25:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
03/16/2022 08:25:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=462
03/16/2022 08:25:45 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6959907834101382 on epoch=462
03/16/2022 08:25:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=464
03/16/2022 08:25:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/16/2022 08:25:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
03/16/2022 08:25:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
03/16/2022 08:25:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
03/16/2022 08:25:59 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.6696073876423652 on epoch=474
03/16/2022 08:26:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
03/16/2022 08:26:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/16/2022 08:26:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=482
03/16/2022 08:26:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/16/2022 08:26:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
03/16/2022 08:26:12 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7136814233588427 on epoch=487
03/16/2022 08:26:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/16/2022 08:26:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
03/16/2022 08:26:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
03/16/2022 08:26:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
03/16/2022 08:26:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
03/16/2022 08:26:25 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.694915514592934 on epoch=499
03/16/2022 08:26:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
03/16/2022 08:26:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
03/16/2022 08:26:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=507
03/16/2022 08:26:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/16/2022 08:26:37 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
03/16/2022 08:26:38 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6919020897832817 on epoch=512
03/16/2022 08:26:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/16/2022 08:26:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
03/16/2022 08:26:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/16/2022 08:26:48 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
03/16/2022 08:26:50 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=524
03/16/2022 08:26:51 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7498559907834101 on epoch=524
03/16/2022 08:26:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7300610269360269 -> 0.7498559907834101 on epoch=524, global_step=2100
03/16/2022 08:26:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
03/16/2022 08:26:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 08:26:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
03/16/2022 08:27:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
03/16/2022 08:27:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/16/2022 08:27:05 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7452876984126985 on epoch=537
03/16/2022 08:27:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
03/16/2022 08:27:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/16/2022 08:27:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
03/16/2022 08:27:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
03/16/2022 08:27:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/16/2022 08:27:18 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7069702208160735 on epoch=549
03/16/2022 08:27:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
03/16/2022 08:27:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
03/16/2022 08:27:25 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
03/16/2022 08:27:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/16/2022 08:27:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
03/16/2022 08:27:31 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7104787014061207 on epoch=562
03/16/2022 08:27:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/16/2022 08:27:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
03/16/2022 08:27:38 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/16/2022 08:27:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
03/16/2022 08:27:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.14 on epoch=574
03/16/2022 08:27:44 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7213937731764557 on epoch=574
03/16/2022 08:27:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/16/2022 08:27:49 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
03/16/2022 08:27:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/16/2022 08:27:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 08:27:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/16/2022 08:27:57 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7107356204130397 on epoch=587
03/16/2022 08:28:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 08:28:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/16/2022 08:28:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=594
03/16/2022 08:28:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=597
03/16/2022 08:28:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=599
03/16/2022 08:28:11 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.6424317456043285 on epoch=599
03/16/2022 08:28:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
03/16/2022 08:28:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
03/16/2022 08:28:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/16/2022 08:28:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
03/16/2022 08:28:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
03/16/2022 08:28:24 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6905332752106945 on epoch=612
03/16/2022 08:28:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/16/2022 08:28:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/16/2022 08:28:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
03/16/2022 08:28:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/16/2022 08:28:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
03/16/2022 08:28:37 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6658385093167701 on epoch=624
03/16/2022 08:28:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/16/2022 08:28:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
03/16/2022 08:28:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/16/2022 08:28:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/16/2022 08:28:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/16/2022 08:28:50 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6951699969526794 on epoch=637
03/16/2022 08:28:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/16/2022 08:28:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/16/2022 08:28:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
03/16/2022 08:29:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/16/2022 08:29:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/16/2022 08:29:03 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7320215862236236 on epoch=649
03/16/2022 08:29:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/16/2022 08:29:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/16/2022 08:29:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/16/2022 08:29:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
03/16/2022 08:29:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/16/2022 08:29:17 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6678030303030302 on epoch=662
03/16/2022 08:29:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/16/2022 08:29:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=667
03/16/2022 08:29:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=669
03/16/2022 08:29:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
03/16/2022 08:29:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=674
03/16/2022 08:29:30 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.6619669669669669 on epoch=674
03/16/2022 08:29:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/16/2022 08:29:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/16/2022 08:29:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/16/2022 08:29:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=684
03/16/2022 08:29:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=687
03/16/2022 08:29:43 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6995103276353276 on epoch=687
03/16/2022 08:29:46 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=689
03/16/2022 08:29:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
03/16/2022 08:29:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
03/16/2022 08:29:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/16/2022 08:29:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
03/16/2022 08:29:56 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7100298183789645 on epoch=699
03/16/2022 08:29:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
03/16/2022 08:30:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/16/2022 08:30:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/16/2022 08:30:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
03/16/2022 08:30:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/16/2022 08:30:10 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6758558558558557 on epoch=712
03/16/2022 08:30:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/16/2022 08:30:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
03/16/2022 08:30:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/16/2022 08:30:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/16/2022 08:30:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/16/2022 08:30:23 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5983262179615616 on epoch=724
03/16/2022 08:30:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
03/16/2022 08:30:28 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 08:30:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 08:30:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
03/16/2022 08:30:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
03/16/2022 08:30:36 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7438191438191438 on epoch=737
03/16/2022 08:30:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
03/16/2022 08:30:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 08:30:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/16/2022 08:30:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
03/16/2022 08:30:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 08:30:50 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.69817023583627 on epoch=749
03/16/2022 08:30:50 - INFO - __main__ - save last model!
03/16/2022 08:30:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 08:30:50 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 08:30:50 - INFO - __main__ - Printing 3 examples
03/16/2022 08:30:50 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 08:30:50 - INFO - __main__ - ['others']
03/16/2022 08:30:50 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 08:30:50 - INFO - __main__ - ['others']
03/16/2022 08:30:50 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 08:30:50 - INFO - __main__ - ['others']
03/16/2022 08:30:50 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:30:51 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:30:51 - INFO - __main__ - Printing 3 examples
03/16/2022 08:30:51 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/16/2022 08:30:51 - INFO - __main__ - ['others']
03/16/2022 08:30:51 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/16/2022 08:30:51 - INFO - __main__ - ['others']
03/16/2022 08:30:51 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/16/2022 08:30:51 - INFO - __main__ - ['others']
03/16/2022 08:30:51 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:30:51 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:30:51 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 08:30:51 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:30:51 - INFO - __main__ - Printing 3 examples
03/16/2022 08:30:51 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/16/2022 08:30:51 - INFO - __main__ - ['others']
03/16/2022 08:30:51 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/16/2022 08:30:51 - INFO - __main__ - ['others']
03/16/2022 08:30:51 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/16/2022 08:30:51 - INFO - __main__ - ['others']
03/16/2022 08:30:51 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:30:51 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:30:51 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 08:30:52 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:30:57 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 08:31:09 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 08:31:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 08:31:10 - INFO - __main__ - Starting training!
03/16/2022 08:32:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_100_0.2_8_predictions.txt
03/16/2022 08:32:35 - INFO - __main__ - Classification-F1 on test data: 0.3417
03/16/2022 08:32:35 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.7498559907834101, test_performance=0.3417333603978749
03/16/2022 08:32:35 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
03/16/2022 08:32:38 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:32:38 - INFO - __main__ - Printing 3 examples
03/16/2022 08:32:38 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/16/2022 08:32:38 - INFO - __main__ - ['others']
03/16/2022 08:32:38 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/16/2022 08:32:38 - INFO - __main__ - ['others']
03/16/2022 08:32:38 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/16/2022 08:32:38 - INFO - __main__ - ['others']
03/16/2022 08:32:38 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:32:38 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:32:38 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 08:32:38 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:32:38 - INFO - __main__ - Printing 3 examples
03/16/2022 08:32:38 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/16/2022 08:32:38 - INFO - __main__ - ['others']
03/16/2022 08:32:38 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/16/2022 08:32:38 - INFO - __main__ - ['others']
03/16/2022 08:32:38 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/16/2022 08:32:38 - INFO - __main__ - ['others']
03/16/2022 08:32:38 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:32:39 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:32:39 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 08:32:54 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 08:32:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 08:32:54 - INFO - __main__ - Starting training!
03/16/2022 08:32:57 - INFO - __main__ - Step 10 Global step 10 Train loss 2.39 on epoch=2
03/16/2022 08:33:00 - INFO - __main__ - Step 20 Global step 20 Train loss 1.27 on epoch=4
03/16/2022 08:33:03 - INFO - __main__ - Step 30 Global step 30 Train loss 1.06 on epoch=7
03/16/2022 08:33:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.95 on epoch=9
03/16/2022 08:33:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.99 on epoch=12
03/16/2022 08:33:08 - INFO - __main__ - Global step 50 Train loss 1.33 Classification-F1 0.1 on epoch=12
03/16/2022 08:33:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/16/2022 08:33:11 - INFO - __main__ - Step 60 Global step 60 Train loss 1.00 on epoch=14
03/16/2022 08:33:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=17
03/16/2022 08:33:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=19
03/16/2022 08:33:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.88 on epoch=22
03/16/2022 08:33:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.78 on epoch=24
03/16/2022 08:33:21 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.2681318681318681 on epoch=24
03/16/2022 08:33:21 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.2681318681318681 on epoch=24, global_step=100
03/16/2022 08:33:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.75 on epoch=27
03/16/2022 08:33:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=29
03/16/2022 08:33:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.71 on epoch=32
03/16/2022 08:33:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.68 on epoch=34
03/16/2022 08:33:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.69 on epoch=37
03/16/2022 08:33:34 - INFO - __main__ - Global step 150 Train loss 0.72 Classification-F1 0.43244474823422185 on epoch=37
03/16/2022 08:33:34 - INFO - __main__ - Saving model with best Classification-F1: 0.2681318681318681 -> 0.43244474823422185 on epoch=37, global_step=150
03/16/2022 08:33:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.72 on epoch=39
03/16/2022 08:33:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=42
03/16/2022 08:33:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.64 on epoch=44
03/16/2022 08:33:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.66 on epoch=47
03/16/2022 08:33:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.69 on epoch=49
03/16/2022 08:33:47 - INFO - __main__ - Global step 200 Train loss 0.68 Classification-F1 0.5555892579886388 on epoch=49
03/16/2022 08:33:47 - INFO - __main__ - Saving model with best Classification-F1: 0.43244474823422185 -> 0.5555892579886388 on epoch=49, global_step=200
03/16/2022 08:33:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.56 on epoch=52
03/16/2022 08:33:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.66 on epoch=54
03/16/2022 08:33:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=57
03/16/2022 08:33:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=59
03/16/2022 08:34:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=62
03/16/2022 08:34:00 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.7168624686716792 on epoch=62
03/16/2022 08:34:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5555892579886388 -> 0.7168624686716792 on epoch=62, global_step=250
03/16/2022 08:34:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.56 on epoch=64
03/16/2022 08:34:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=67
03/16/2022 08:34:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=69
03/16/2022 08:34:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=72
03/16/2022 08:34:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=74
03/16/2022 08:34:13 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.5714654282765738 on epoch=74
03/16/2022 08:34:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=77
03/16/2022 08:34:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=79
03/16/2022 08:34:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=82
03/16/2022 08:34:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=84
03/16/2022 08:34:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=87
03/16/2022 08:34:26 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.627550839964633 on epoch=87
03/16/2022 08:34:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=89
03/16/2022 08:34:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=92
03/16/2022 08:34:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=94
03/16/2022 08:34:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=97
03/16/2022 08:34:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=99
03/16/2022 08:34:39 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.6554179566563467 on epoch=99
03/16/2022 08:34:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=102
03/16/2022 08:34:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=104
03/16/2022 08:34:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=107
03/16/2022 08:34:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=109
03/16/2022 08:34:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=112
03/16/2022 08:34:52 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.8413752913752913 on epoch=112
03/16/2022 08:34:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7168624686716792 -> 0.8413752913752913 on epoch=112, global_step=450
03/16/2022 08:34:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=114
03/16/2022 08:34:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=117
03/16/2022 08:35:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=119
03/16/2022 08:35:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=122
03/16/2022 08:35:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=124
03/16/2022 08:35:05 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.4935907335907336 on epoch=124
03/16/2022 08:35:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=127
03/16/2022 08:35:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.16 on epoch=129
03/16/2022 08:35:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=132
03/16/2022 08:35:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=134
03/16/2022 08:35:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=137
03/16/2022 08:35:19 - INFO - __main__ - Global step 550 Train loss 0.18 Classification-F1 0.7493045494059692 on epoch=137
03/16/2022 08:35:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=139
03/16/2022 08:35:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=142
03/16/2022 08:35:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=144
03/16/2022 08:35:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=147
03/16/2022 08:35:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=149
03/16/2022 08:35:32 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.7326747284279564 on epoch=149
03/16/2022 08:35:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=152
03/16/2022 08:35:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=154
03/16/2022 08:35:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=157
03/16/2022 08:35:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=159
03/16/2022 08:35:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=162
03/16/2022 08:35:45 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.7330200501253132 on epoch=162
03/16/2022 08:35:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=164
03/16/2022 08:35:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=167
03/16/2022 08:35:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=169
03/16/2022 08:35:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=172
03/16/2022 08:35:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=174
03/16/2022 08:35:58 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.7955492424242424 on epoch=174
03/16/2022 08:36:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=177
03/16/2022 08:36:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=179
03/16/2022 08:36:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=182
03/16/2022 08:36:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=184
03/16/2022 08:36:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=187
03/16/2022 08:36:11 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.795033345033345 on epoch=187
03/16/2022 08:36:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=189
03/16/2022 08:36:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=192
03/16/2022 08:36:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=194
03/16/2022 08:36:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=197
03/16/2022 08:36:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=199
03/16/2022 08:36:24 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.7256004864700517 on epoch=199
03/16/2022 08:36:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=202
03/16/2022 08:36:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=204
03/16/2022 08:36:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=207
03/16/2022 08:36:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
03/16/2022 08:36:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=212
03/16/2022 08:36:37 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.7443181818181818 on epoch=212
03/16/2022 08:36:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=214
03/16/2022 08:36:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=217
03/16/2022 08:36:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=219
03/16/2022 08:36:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=222
03/16/2022 08:36:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=224
03/16/2022 08:36:49 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.7938176406926407 on epoch=224
03/16/2022 08:36:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=227
03/16/2022 08:36:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
03/16/2022 08:36:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=232
03/16/2022 08:36:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=234
03/16/2022 08:37:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
03/16/2022 08:37:02 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.794890873015873 on epoch=237
03/16/2022 08:37:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
03/16/2022 08:37:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
03/16/2022 08:37:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
03/16/2022 08:37:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=247
03/16/2022 08:37:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
03/16/2022 08:37:15 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.7657696489593041 on epoch=249
03/16/2022 08:37:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
03/16/2022 08:37:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
03/16/2022 08:37:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=257
03/16/2022 08:37:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=259
03/16/2022 08:37:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=262
03/16/2022 08:37:28 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.7647279822926374 on epoch=262
03/16/2022 08:37:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
03/16/2022 08:37:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
03/16/2022 08:37:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=269
03/16/2022 08:37:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
03/16/2022 08:37:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=274
03/16/2022 08:37:41 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.7166818688557819 on epoch=274
03/16/2022 08:37:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
03/16/2022 08:37:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
03/16/2022 08:37:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
03/16/2022 08:37:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
03/16/2022 08:37:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
03/16/2022 08:37:54 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.7522091073271414 on epoch=287
03/16/2022 08:37:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
03/16/2022 08:37:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=292
03/16/2022 08:38:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=294
03/16/2022 08:38:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=297
03/16/2022 08:38:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
03/16/2022 08:38:08 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.7418389853873725 on epoch=299
03/16/2022 08:38:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
03/16/2022 08:38:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
03/16/2022 08:38:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
03/16/2022 08:38:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
03/16/2022 08:38:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
03/16/2022 08:38:21 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.772336265884653 on epoch=312
03/16/2022 08:38:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
03/16/2022 08:38:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
03/16/2022 08:38:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
03/16/2022 08:38:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
03/16/2022 08:38:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
03/16/2022 08:38:34 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.6256783216783217 on epoch=324
03/16/2022 08:38:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
03/16/2022 08:38:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
03/16/2022 08:38:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=332
03/16/2022 08:38:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
03/16/2022 08:38:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
03/16/2022 08:38:47 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.6494207908097851 on epoch=337
03/16/2022 08:38:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
03/16/2022 08:38:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
03/16/2022 08:38:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
03/16/2022 08:38:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
03/16/2022 08:38:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
03/16/2022 08:39:01 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6256783216783217 on epoch=349
03/16/2022 08:39:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
03/16/2022 08:39:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
03/16/2022 08:39:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
03/16/2022 08:39:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
03/16/2022 08:39:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
03/16/2022 08:39:14 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.7488666085440279 on epoch=362
03/16/2022 08:39:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=364
03/16/2022 08:39:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
03/16/2022 08:39:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=369
03/16/2022 08:39:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
03/16/2022 08:39:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
03/16/2022 08:39:27 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.764977359804946 on epoch=374
03/16/2022 08:39:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
03/16/2022 08:39:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
03/16/2022 08:39:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
03/16/2022 08:39:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=384
03/16/2022 08:39:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
03/16/2022 08:39:40 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.7723801220575415 on epoch=387
03/16/2022 08:39:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
03/16/2022 08:39:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
03/16/2022 08:39:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
03/16/2022 08:39:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
03/16/2022 08:39:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
03/16/2022 08:39:53 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7717936117936118 on epoch=399
03/16/2022 08:39:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/16/2022 08:39:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
03/16/2022 08:40:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=407
03/16/2022 08:40:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
03/16/2022 08:40:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
03/16/2022 08:40:06 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.7804659498207885 on epoch=412
03/16/2022 08:40:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
03/16/2022 08:40:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
03/16/2022 08:40:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 08:40:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
03/16/2022 08:40:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
03/16/2022 08:40:19 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.7072558744884722 on epoch=424
03/16/2022 08:40:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
03/16/2022 08:40:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/16/2022 08:40:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
03/16/2022 08:40:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/16/2022 08:40:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/16/2022 08:40:32 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7584522332506204 on epoch=437
03/16/2022 08:40:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
03/16/2022 08:40:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
03/16/2022 08:40:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
03/16/2022 08:40:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/16/2022 08:40:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
03/16/2022 08:40:45 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7787660256410256 on epoch=449
03/16/2022 08:40:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
03/16/2022 08:40:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
03/16/2022 08:40:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
03/16/2022 08:40:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
03/16/2022 08:40:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
03/16/2022 08:40:58 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7715050434435234 on epoch=462
03/16/2022 08:41:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
03/16/2022 08:41:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/16/2022 08:41:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
03/16/2022 08:41:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
03/16/2022 08:41:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
03/16/2022 08:41:12 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7510358557684534 on epoch=474
03/16/2022 08:41:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
03/16/2022 08:41:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/16/2022 08:41:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
03/16/2022 08:41:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/16/2022 08:41:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/16/2022 08:41:25 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7601508295625943 on epoch=487
03/16/2022 08:41:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/16/2022 08:41:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/16/2022 08:41:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
03/16/2022 08:41:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/16/2022 08:41:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
03/16/2022 08:41:38 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6816919191919192 on epoch=499
03/16/2022 08:41:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/16/2022 08:41:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 08:41:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
03/16/2022 08:41:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
03/16/2022 08:41:50 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/16/2022 08:41:52 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7145015316067947 on epoch=512
03/16/2022 08:41:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/16/2022 08:41:57 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/16/2022 08:41:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/16/2022 08:42:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/16/2022 08:42:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
03/16/2022 08:42:05 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7567380536130536 on epoch=524
03/16/2022 08:42:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/16/2022 08:42:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 08:42:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
03/16/2022 08:42:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
03/16/2022 08:42:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=537
03/16/2022 08:42:19 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7578616912233211 on epoch=537
03/16/2022 08:42:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
03/16/2022 08:42:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
03/16/2022 08:42:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
03/16/2022 08:42:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/16/2022 08:42:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/16/2022 08:42:32 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7365841073271414 on epoch=549
03/16/2022 08:42:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
03/16/2022 08:42:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
03/16/2022 08:42:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
03/16/2022 08:42:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/16/2022 08:42:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/16/2022 08:42:45 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7222601959444065 on epoch=562
03/16/2022 08:42:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/16/2022 08:42:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/16/2022 08:42:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
03/16/2022 08:42:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
03/16/2022 08:42:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/16/2022 08:42:58 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7377622377622378 on epoch=574
03/16/2022 08:43:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
03/16/2022 08:43:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/16/2022 08:43:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=582
03/16/2022 08:43:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 08:43:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
03/16/2022 08:43:11 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7235042735042735 on epoch=587
03/16/2022 08:43:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/16/2022 08:43:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/16/2022 08:43:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/16/2022 08:43:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/16/2022 08:43:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/16/2022 08:43:25 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7373931623931623 on epoch=599
03/16/2022 08:43:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/16/2022 08:43:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/16/2022 08:43:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/16/2022 08:43:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/16/2022 08:43:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/16/2022 08:43:38 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.7258935405994229 on epoch=612
03/16/2022 08:43:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/16/2022 08:43:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/16/2022 08:43:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 08:43:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/16/2022 08:43:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
03/16/2022 08:43:52 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.7094509109311741 on epoch=624
03/16/2022 08:43:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/16/2022 08:43:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 08:43:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/16/2022 08:44:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/16/2022 08:44:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
03/16/2022 08:44:05 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7032980304719435 on epoch=637
03/16/2022 08:44:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/16/2022 08:44:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/16/2022 08:44:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/16/2022 08:44:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/16/2022 08:44:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/16/2022 08:44:19 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7667124542124542 on epoch=649
03/16/2022 08:44:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/16/2022 08:44:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/16/2022 08:44:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/16/2022 08:44:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/16/2022 08:44:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/16/2022 08:44:33 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7667124542124542 on epoch=662
03/16/2022 08:44:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/16/2022 08:44:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/16/2022 08:44:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/16/2022 08:44:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
03/16/2022 08:44:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
03/16/2022 08:44:46 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6805594050258684 on epoch=674
03/16/2022 08:44:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/16/2022 08:44:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/16/2022 08:44:54 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/16/2022 08:44:56 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/16/2022 08:44:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
03/16/2022 08:45:00 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7383471487066814 on epoch=687
03/16/2022 08:45:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/16/2022 08:45:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=692
03/16/2022 08:45:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
03/16/2022 08:45:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/16/2022 08:45:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=699
03/16/2022 08:45:13 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7217146652630524 on epoch=699
03/16/2022 08:45:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
03/16/2022 08:45:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/16/2022 08:45:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/16/2022 08:45:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
03/16/2022 08:45:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/16/2022 08:45:27 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7730132063748363 on epoch=712
03/16/2022 08:45:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/16/2022 08:45:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
03/16/2022 08:45:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/16/2022 08:45:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/16/2022 08:45:39 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/16/2022 08:45:40 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7522926634768741 on epoch=724
03/16/2022 08:45:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
03/16/2022 08:45:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 08:45:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
03/16/2022 08:45:51 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/16/2022 08:45:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
03/16/2022 08:45:54 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7669149896800825 on epoch=737
03/16/2022 08:45:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/16/2022 08:45:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 08:46:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/16/2022 08:46:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
03/16/2022 08:46:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/16/2022 08:46:08 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7483325098814229 on epoch=749
03/16/2022 08:46:08 - INFO - __main__ - save last model!
03/16/2022 08:46:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 08:46:08 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 08:46:08 - INFO - __main__ - Printing 3 examples
03/16/2022 08:46:08 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 08:46:08 - INFO - __main__ - ['others']
03/16/2022 08:46:08 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 08:46:08 - INFO - __main__ - ['others']
03/16/2022 08:46:08 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 08:46:08 - INFO - __main__ - ['others']
03/16/2022 08:46:08 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:46:08 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:46:08 - INFO - __main__ - Printing 3 examples
03/16/2022 08:46:08 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/16/2022 08:46:08 - INFO - __main__ - ['others']
03/16/2022 08:46:08 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/16/2022 08:46:08 - INFO - __main__ - ['others']
03/16/2022 08:46:08 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/16/2022 08:46:08 - INFO - __main__ - ['others']
03/16/2022 08:46:08 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:46:08 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:46:08 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 08:46:08 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:46:08 - INFO - __main__ - Printing 3 examples
03/16/2022 08:46:08 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/16/2022 08:46:08 - INFO - __main__ - ['others']
03/16/2022 08:46:08 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/16/2022 08:46:08 - INFO - __main__ - ['others']
03/16/2022 08:46:08 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/16/2022 08:46:08 - INFO - __main__ - ['others']
03/16/2022 08:46:08 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:46:08 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:46:08 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 08:46:10 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:46:15 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 08:46:26 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 08:46:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 08:46:27 - INFO - __main__ - Starting training!
03/16/2022 08:47:51 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_13_0.5_8_predictions.txt
03/16/2022 08:47:51 - INFO - __main__ - Classification-F1 on test data: 0.1121
03/16/2022 08:47:52 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.8413752913752913, test_performance=0.11213086319655437
03/16/2022 08:47:52 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
03/16/2022 08:47:53 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:47:53 - INFO - __main__ - Printing 3 examples
03/16/2022 08:47:53 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/16/2022 08:47:53 - INFO - __main__ - ['others']
03/16/2022 08:47:53 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/16/2022 08:47:53 - INFO - __main__ - ['others']
03/16/2022 08:47:53 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/16/2022 08:47:53 - INFO - __main__ - ['others']
03/16/2022 08:47:53 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:47:53 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:47:53 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 08:47:53 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 08:47:53 - INFO - __main__ - Printing 3 examples
03/16/2022 08:47:53 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/16/2022 08:47:53 - INFO - __main__ - ['others']
03/16/2022 08:47:53 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/16/2022 08:47:53 - INFO - __main__ - ['others']
03/16/2022 08:47:53 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/16/2022 08:47:53 - INFO - __main__ - ['others']
03/16/2022 08:47:53 - INFO - __main__ - Tokenizing Input ...
03/16/2022 08:47:53 - INFO - __main__ - Tokenizing Output ...
03/16/2022 08:47:53 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 08:48:08 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 08:48:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 08:48:08 - INFO - __main__ - Starting training!
03/16/2022 08:48:11 - INFO - __main__ - Step 10 Global step 10 Train loss 2.67 on epoch=2
03/16/2022 08:48:14 - INFO - __main__ - Step 20 Global step 20 Train loss 1.23 on epoch=4
03/16/2022 08:48:16 - INFO - __main__ - Step 30 Global step 30 Train loss 1.06 on epoch=7
03/16/2022 08:48:19 - INFO - __main__ - Step 40 Global step 40 Train loss 1.02 on epoch=9
03/16/2022 08:48:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.93 on epoch=12
03/16/2022 08:48:22 - INFO - __main__ - Global step 50 Train loss 1.38 Classification-F1 0.1 on epoch=12
03/16/2022 08:48:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/16/2022 08:48:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.93 on epoch=14
03/16/2022 08:48:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=17
03/16/2022 08:48:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.88 on epoch=19
03/16/2022 08:48:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.89 on epoch=22
03/16/2022 08:48:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=24
03/16/2022 08:48:35 - INFO - __main__ - Global step 100 Train loss 0.91 Classification-F1 0.1581196581196581 on epoch=24
03/16/2022 08:48:35 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1581196581196581 on epoch=24, global_step=100
03/16/2022 08:48:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.88 on epoch=27
03/16/2022 08:48:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=29
03/16/2022 08:48:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.88 on epoch=32
03/16/2022 08:48:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=34
03/16/2022 08:48:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=37
03/16/2022 08:48:48 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.1 on epoch=37
03/16/2022 08:48:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.92 on epoch=39
03/16/2022 08:48:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.85 on epoch=42
03/16/2022 08:48:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.78 on epoch=44
03/16/2022 08:48:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.81 on epoch=47
03/16/2022 08:49:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.79 on epoch=49
03/16/2022 08:49:01 - INFO - __main__ - Global step 200 Train loss 0.83 Classification-F1 0.42560224089635856 on epoch=49
03/16/2022 08:49:01 - INFO - __main__ - Saving model with best Classification-F1: 0.1581196581196581 -> 0.42560224089635856 on epoch=49, global_step=200
03/16/2022 08:49:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.74 on epoch=52
03/16/2022 08:49:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.67 on epoch=54
03/16/2022 08:49:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.69 on epoch=57
03/16/2022 08:49:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.73 on epoch=59
03/16/2022 08:49:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.67 on epoch=62
03/16/2022 08:49:14 - INFO - __main__ - Global step 250 Train loss 0.70 Classification-F1 0.4220071455233829 on epoch=62
03/16/2022 08:49:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.66 on epoch=64
03/16/2022 08:49:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.60 on epoch=67
03/16/2022 08:49:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.61 on epoch=69
03/16/2022 08:49:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.62 on epoch=72
03/16/2022 08:49:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.58 on epoch=74
03/16/2022 08:49:27 - INFO - __main__ - Global step 300 Train loss 0.62 Classification-F1 0.35668449197860963 on epoch=74
03/16/2022 08:49:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=77
03/16/2022 08:49:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=79
03/16/2022 08:49:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=82
03/16/2022 08:49:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=84
03/16/2022 08:49:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=87
03/16/2022 08:49:40 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.7514724310776942 on epoch=87
03/16/2022 08:49:40 - INFO - __main__ - Saving model with best Classification-F1: 0.42560224089635856 -> 0.7514724310776942 on epoch=87, global_step=350
03/16/2022 08:49:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=89
03/16/2022 08:49:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=92
03/16/2022 08:49:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=94
03/16/2022 08:49:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=97
03/16/2022 08:49:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=99
03/16/2022 08:49:53 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.3462895129561796 on epoch=99
03/16/2022 08:49:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=102
03/16/2022 08:49:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=104
03/16/2022 08:50:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=107
03/16/2022 08:50:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.34 on epoch=109
03/16/2022 08:50:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=112
03/16/2022 08:50:06 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.6060992138791 on epoch=112
03/16/2022 08:50:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=114
03/16/2022 08:50:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=117
03/16/2022 08:50:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=119
03/16/2022 08:50:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=122
03/16/2022 08:50:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=124
03/16/2022 08:50:19 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.655952380952381 on epoch=124
03/16/2022 08:50:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=127
03/16/2022 08:50:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=129
03/16/2022 08:50:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=132
03/16/2022 08:50:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=134
03/16/2022 08:50:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=137
03/16/2022 08:50:32 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.7613122171945701 on epoch=137
03/16/2022 08:50:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7514724310776942 -> 0.7613122171945701 on epoch=137, global_step=550
03/16/2022 08:50:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=139
03/16/2022 08:50:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=142
03/16/2022 08:50:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=144
03/16/2022 08:50:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
03/16/2022 08:50:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=149
03/16/2022 08:50:45 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.6162366701066392 on epoch=149
03/16/2022 08:50:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=152
03/16/2022 08:50:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=154
03/16/2022 08:50:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=157
03/16/2022 08:50:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=159
03/16/2022 08:50:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=162
03/16/2022 08:50:58 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.7585591206179443 on epoch=162
03/16/2022 08:51:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=164
03/16/2022 08:51:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
03/16/2022 08:51:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=169
03/16/2022 08:51:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=172
03/16/2022 08:51:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=174
03/16/2022 08:51:11 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.6695548489666135 on epoch=174
03/16/2022 08:51:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=177
03/16/2022 08:51:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=179
03/16/2022 08:51:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
03/16/2022 08:51:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=184
03/16/2022 08:51:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=187
03/16/2022 08:51:24 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.7578354978354979 on epoch=187
03/16/2022 08:51:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=189
03/16/2022 08:51:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=192
03/16/2022 08:51:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=194
03/16/2022 08:51:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=197
03/16/2022 08:51:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=199
03/16/2022 08:51:37 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.7229816966659072 on epoch=199
03/16/2022 08:51:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=202
03/16/2022 08:51:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=204
03/16/2022 08:51:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=207
03/16/2022 08:51:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=209
03/16/2022 08:51:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=212
03/16/2022 08:51:50 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.6552606177606177 on epoch=212
03/16/2022 08:51:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=214
03/16/2022 08:51:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.11 on epoch=217
03/16/2022 08:51:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=219
03/16/2022 08:52:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=222
03/16/2022 08:52:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=224
03/16/2022 08:52:03 - INFO - __main__ - Global step 900 Train loss 0.14 Classification-F1 0.7102219198993392 on epoch=224
03/16/2022 08:52:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=227
03/16/2022 08:52:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=229
03/16/2022 08:52:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=232
03/16/2022 08:52:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=234
03/16/2022 08:52:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=237
03/16/2022 08:52:16 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.7160556976413752 on epoch=237
03/16/2022 08:52:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
03/16/2022 08:52:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=242
03/16/2022 08:52:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
03/16/2022 08:52:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
03/16/2022 08:52:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
03/16/2022 08:52:30 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.7457565606971379 on epoch=249
03/16/2022 08:52:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
03/16/2022 08:52:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
03/16/2022 08:52:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=257
03/16/2022 08:52:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
03/16/2022 08:52:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
03/16/2022 08:52:43 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.780278488206877 on epoch=262
03/16/2022 08:52:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7613122171945701 -> 0.780278488206877 on epoch=262, global_step=1050
03/16/2022 08:52:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=264
03/16/2022 08:52:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
03/16/2022 08:52:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=269
03/16/2022 08:52:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
03/16/2022 08:52:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
03/16/2022 08:52:56 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.743993993993994 on epoch=274
03/16/2022 08:52:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
03/16/2022 08:53:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=279
03/16/2022 08:53:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
03/16/2022 08:53:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=284
03/16/2022 08:53:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=287
03/16/2022 08:53:09 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.7231200172376643 on epoch=287
03/16/2022 08:53:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
03/16/2022 08:53:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
03/16/2022 08:53:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=294
03/16/2022 08:53:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
03/16/2022 08:53:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
03/16/2022 08:53:23 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.7714583333333334 on epoch=299
03/16/2022 08:53:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
03/16/2022 08:53:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
03/16/2022 08:53:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
03/16/2022 08:53:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
03/16/2022 08:53:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
03/16/2022 08:53:36 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.7382149961433849 on epoch=312
03/16/2022 08:53:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
03/16/2022 08:53:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
03/16/2022 08:53:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
03/16/2022 08:53:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
03/16/2022 08:53:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
03/16/2022 08:53:49 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7090728715728717 on epoch=324
03/16/2022 08:53:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
03/16/2022 08:53:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
03/16/2022 08:53:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=332
03/16/2022 08:53:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
03/16/2022 08:54:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
03/16/2022 08:54:02 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.7231442577030813 on epoch=337
03/16/2022 08:54:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
03/16/2022 08:54:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
03/16/2022 08:54:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
03/16/2022 08:54:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
03/16/2022 08:54:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
03/16/2022 08:54:15 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.7293349815088945 on epoch=349
03/16/2022 08:54:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=352
03/16/2022 08:54:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
03/16/2022 08:54:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
03/16/2022 08:54:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/16/2022 08:54:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
03/16/2022 08:54:28 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.7148256605479608 on epoch=362
03/16/2022 08:54:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
03/16/2022 08:54:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
03/16/2022 08:54:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
03/16/2022 08:54:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
03/16/2022 08:54:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
03/16/2022 08:54:42 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7077466121583769 on epoch=374
03/16/2022 08:54:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
03/16/2022 08:54:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=379
03/16/2022 08:54:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
03/16/2022 08:54:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
03/16/2022 08:54:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
03/16/2022 08:54:55 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.7174594696333827 on epoch=387
03/16/2022 08:54:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
03/16/2022 08:55:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
03/16/2022 08:55:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
03/16/2022 08:55:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
03/16/2022 08:55:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
03/16/2022 08:55:08 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6799062049062049 on epoch=399
03/16/2022 08:55:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
03/16/2022 08:55:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
03/16/2022 08:55:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
03/16/2022 08:55:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
03/16/2022 08:55:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
03/16/2022 08:55:21 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7025869768197135 on epoch=412
03/16/2022 08:55:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
03/16/2022 08:55:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
03/16/2022 08:55:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 08:55:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
03/16/2022 08:55:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
03/16/2022 08:55:35 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6846218814968815 on epoch=424
03/16/2022 08:55:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
03/16/2022 08:55:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
03/16/2022 08:55:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
03/16/2022 08:55:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/16/2022 08:55:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/16/2022 08:55:48 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7110653574068209 on epoch=437
03/16/2022 08:55:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
03/16/2022 08:55:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
03/16/2022 08:55:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
03/16/2022 08:55:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
03/16/2022 08:56:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/16/2022 08:56:01 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6939510939510939 on epoch=449
03/16/2022 08:56:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/16/2022 08:56:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/16/2022 08:56:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
03/16/2022 08:56:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
03/16/2022 08:56:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
03/16/2022 08:56:14 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7164289958407605 on epoch=462
03/16/2022 08:56:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
03/16/2022 08:56:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=467
03/16/2022 08:56:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
03/16/2022 08:56:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
03/16/2022 08:56:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/16/2022 08:56:28 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7182295932295932 on epoch=474
03/16/2022 08:56:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
03/16/2022 08:56:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/16/2022 08:56:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
03/16/2022 08:56:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
03/16/2022 08:56:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/16/2022 08:56:41 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6944346159516438 on epoch=487
03/16/2022 08:56:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/16/2022 08:56:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/16/2022 08:56:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
03/16/2022 08:56:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/16/2022 08:56:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 08:56:54 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6886297182349814 on epoch=499
03/16/2022 08:56:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/16/2022 08:56:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 08:57:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
03/16/2022 08:57:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
03/16/2022 08:57:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/16/2022 08:57:07 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7082070707070707 on epoch=512
03/16/2022 08:57:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/16/2022 08:57:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
03/16/2022 08:57:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/16/2022 08:57:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
03/16/2022 08:57:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 08:57:21 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6555555555555556 on epoch=524
03/16/2022 08:57:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/16/2022 08:57:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 08:57:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=532
03/16/2022 08:57:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/16/2022 08:57:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
03/16/2022 08:57:34 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7477150537634408 on epoch=537
03/16/2022 08:57:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
03/16/2022 08:57:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
03/16/2022 08:57:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
03/16/2022 08:57:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/16/2022 08:57:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/16/2022 08:57:48 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6992647058823529 on epoch=549
03/16/2022 08:57:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/16/2022 08:57:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/16/2022 08:57:55 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/16/2022 08:57:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/16/2022 08:58:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/16/2022 08:58:01 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6936507936507936 on epoch=562
03/16/2022 08:58:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/16/2022 08:58:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/16/2022 08:58:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/16/2022 08:58:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 08:58:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
03/16/2022 08:58:14 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7025869768197135 on epoch=574
03/16/2022 08:58:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
03/16/2022 08:58:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=579
03/16/2022 08:58:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
03/16/2022 08:58:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 08:58:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/16/2022 08:58:28 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6876050420168067 on epoch=587
03/16/2022 08:58:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 08:58:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
03/16/2022 08:58:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/16/2022 08:58:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/16/2022 08:58:40 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
03/16/2022 08:58:41 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6699454435574574 on epoch=599
03/16/2022 08:58:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=602
03/16/2022 08:58:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=604
03/16/2022 08:58:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/16/2022 08:58:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
03/16/2022 08:58:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/16/2022 08:58:54 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6797163091280738 on epoch=612
03/16/2022 08:58:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/16/2022 08:58:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/16/2022 08:59:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 08:59:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/16/2022 08:59:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/16/2022 08:59:07 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.7073712367830015 on epoch=624
03/16/2022 08:59:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
03/16/2022 08:59:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 08:59:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/16/2022 08:59:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=634
03/16/2022 08:59:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
03/16/2022 08:59:21 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.701797385620915 on epoch=637
03/16/2022 08:59:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/16/2022 08:59:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
03/16/2022 08:59:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/16/2022 08:59:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/16/2022 08:59:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/16/2022 08:59:34 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6795093795093795 on epoch=649
03/16/2022 08:59:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/16/2022 08:59:39 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
03/16/2022 08:59:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/16/2022 08:59:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
03/16/2022 08:59:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/16/2022 08:59:48 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.715490426487869 on epoch=662
03/16/2022 08:59:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/16/2022 08:59:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
03/16/2022 08:59:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/16/2022 08:59:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/16/2022 09:00:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/16/2022 09:00:01 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.701769983355661 on epoch=674
03/16/2022 09:00:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/16/2022 09:00:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/16/2022 09:00:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/16/2022 09:00:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=684
03/16/2022 09:00:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/16/2022 09:00:14 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.7319690113807761 on epoch=687
03/16/2022 09:00:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/16/2022 09:00:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/16/2022 09:00:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/16/2022 09:00:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/16/2022 09:00:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/16/2022 09:00:28 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.71565887224455 on epoch=699
03/16/2022 09:00:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/16/2022 09:00:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/16/2022 09:00:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/16/2022 09:00:38 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
03/16/2022 09:00:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/16/2022 09:00:41 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7305294795783925 on epoch=712
03/16/2022 09:00:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/16/2022 09:00:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 09:00:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/16/2022 09:00:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/16/2022 09:00:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/16/2022 09:00:55 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7010174893756129 on epoch=724
03/16/2022 09:00:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/16/2022 09:01:00 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 09:01:02 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 09:01:05 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/16/2022 09:01:07 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
03/16/2022 09:01:08 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7014189455046231 on epoch=737
03/16/2022 09:01:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/16/2022 09:01:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 09:01:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/16/2022 09:01:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 09:01:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/16/2022 09:01:22 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6659239453357101 on epoch=749
03/16/2022 09:01:22 - INFO - __main__ - save last model!
03/16/2022 09:01:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 09:01:22 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 09:01:22 - INFO - __main__ - Printing 3 examples
03/16/2022 09:01:22 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 09:01:22 - INFO - __main__ - ['others']
03/16/2022 09:01:22 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 09:01:22 - INFO - __main__ - ['others']
03/16/2022 09:01:22 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 09:01:22 - INFO - __main__ - ['others']
03/16/2022 09:01:22 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:01:23 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:01:23 - INFO - __main__ - Printing 3 examples
03/16/2022 09:01:23 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/16/2022 09:01:23 - INFO - __main__ - ['others']
03/16/2022 09:01:23 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/16/2022 09:01:23 - INFO - __main__ - ['others']
03/16/2022 09:01:23 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/16/2022 09:01:23 - INFO - __main__ - ['others']
03/16/2022 09:01:23 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:01:23 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:01:23 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 09:01:23 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:01:23 - INFO - __main__ - Printing 3 examples
03/16/2022 09:01:23 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/16/2022 09:01:23 - INFO - __main__ - ['others']
03/16/2022 09:01:23 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/16/2022 09:01:23 - INFO - __main__ - ['others']
03/16/2022 09:01:23 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/16/2022 09:01:23 - INFO - __main__ - ['others']
03/16/2022 09:01:23 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:01:23 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:01:23 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 09:01:24 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:01:29 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 09:01:41 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 09:01:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 09:01:42 - INFO - __main__ - Starting training!
03/16/2022 09:03:04 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_13_0.4_8_predictions.txt
03/16/2022 09:03:04 - INFO - __main__ - Classification-F1 on test data: 0.1058
03/16/2022 09:03:04 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.780278488206877, test_performance=0.10579426919390328
03/16/2022 09:03:04 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
03/16/2022 09:03:05 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:03:05 - INFO - __main__ - Printing 3 examples
03/16/2022 09:03:05 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/16/2022 09:03:05 - INFO - __main__ - ['others']
03/16/2022 09:03:05 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/16/2022 09:03:05 - INFO - __main__ - ['others']
03/16/2022 09:03:05 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/16/2022 09:03:05 - INFO - __main__ - ['others']
03/16/2022 09:03:05 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:03:05 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:03:05 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 09:03:05 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:03:05 - INFO - __main__ - Printing 3 examples
03/16/2022 09:03:05 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/16/2022 09:03:05 - INFO - __main__ - ['others']
03/16/2022 09:03:05 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/16/2022 09:03:05 - INFO - __main__ - ['others']
03/16/2022 09:03:05 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/16/2022 09:03:05 - INFO - __main__ - ['others']
03/16/2022 09:03:05 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:03:05 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:03:05 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 09:03:20 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 09:03:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 09:03:21 - INFO - __main__ - Starting training!
03/16/2022 09:03:24 - INFO - __main__ - Step 10 Global step 10 Train loss 2.90 on epoch=2
03/16/2022 09:03:27 - INFO - __main__ - Step 20 Global step 20 Train loss 1.53 on epoch=4
03/16/2022 09:03:29 - INFO - __main__ - Step 30 Global step 30 Train loss 1.17 on epoch=7
03/16/2022 09:03:32 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=9
03/16/2022 09:03:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.94 on epoch=12
03/16/2022 09:03:35 - INFO - __main__ - Global step 50 Train loss 1.52 Classification-F1 0.1 on epoch=12
03/16/2022 09:03:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/16/2022 09:03:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.81 on epoch=14
03/16/2022 09:03:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.85 on epoch=17
03/16/2022 09:03:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=19
03/16/2022 09:03:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.93 on epoch=22
03/16/2022 09:03:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.80 on epoch=24
03/16/2022 09:03:48 - INFO - __main__ - Global step 100 Train loss 0.85 Classification-F1 0.3281297134238311 on epoch=24
03/16/2022 09:03:48 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.3281297134238311 on epoch=24, global_step=100
03/16/2022 09:03:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.80 on epoch=27
03/16/2022 09:03:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.89 on epoch=29
03/16/2022 09:03:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.87 on epoch=32
03/16/2022 09:03:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.78 on epoch=34
03/16/2022 09:04:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.78 on epoch=37
03/16/2022 09:04:01 - INFO - __main__ - Global step 150 Train loss 0.82 Classification-F1 0.39101117699532983 on epoch=37
03/16/2022 09:04:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3281297134238311 -> 0.39101117699532983 on epoch=37, global_step=150
03/16/2022 09:04:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.77 on epoch=39
03/16/2022 09:04:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.66 on epoch=42
03/16/2022 09:04:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.73 on epoch=44
03/16/2022 09:04:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.58 on epoch=47
03/16/2022 09:04:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.70 on epoch=49
03/16/2022 09:04:14 - INFO - __main__ - Global step 200 Train loss 0.69 Classification-F1 0.6785493517775996 on epoch=49
03/16/2022 09:04:14 - INFO - __main__ - Saving model with best Classification-F1: 0.39101117699532983 -> 0.6785493517775996 on epoch=49, global_step=200
03/16/2022 09:04:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.58 on epoch=52
03/16/2022 09:04:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.63 on epoch=54
03/16/2022 09:04:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.62 on epoch=57
03/16/2022 09:04:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.68 on epoch=59
03/16/2022 09:04:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=62
03/16/2022 09:04:27 - INFO - __main__ - Global step 250 Train loss 0.62 Classification-F1 0.5955072463768116 on epoch=62
03/16/2022 09:04:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.57 on epoch=64
03/16/2022 09:04:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=67
03/16/2022 09:04:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=69
03/16/2022 09:04:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=72
03/16/2022 09:04:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.58 on epoch=74
03/16/2022 09:04:40 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.4793896321070234 on epoch=74
03/16/2022 09:04:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=77
03/16/2022 09:04:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=79
03/16/2022 09:04:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=82
03/16/2022 09:04:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=84
03/16/2022 09:04:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.55 on epoch=87
03/16/2022 09:04:53 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.7735961768219832 on epoch=87
03/16/2022 09:04:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6785493517775996 -> 0.7735961768219832 on epoch=87, global_step=350
03/16/2022 09:04:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=89
03/16/2022 09:04:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=92
03/16/2022 09:05:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=94
03/16/2022 09:05:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=97
03/16/2022 09:05:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=99
03/16/2022 09:05:06 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.7131251066734938 on epoch=99
03/16/2022 09:05:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=102
03/16/2022 09:05:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=104
03/16/2022 09:05:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=107
03/16/2022 09:05:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=109
03/16/2022 09:05:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=112
03/16/2022 09:05:19 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.8182073840138356 on epoch=112
03/16/2022 09:05:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7735961768219832 -> 0.8182073840138356 on epoch=112, global_step=450
03/16/2022 09:05:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=114
03/16/2022 09:05:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=117
03/16/2022 09:05:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=119
03/16/2022 09:05:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=122
03/16/2022 09:05:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=124
03/16/2022 09:05:32 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.6534539473684211 on epoch=124
03/16/2022 09:05:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=127
03/16/2022 09:05:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=129
03/16/2022 09:05:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=132
03/16/2022 09:05:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=134
03/16/2022 09:05:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=137
03/16/2022 09:05:45 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.7768139923846127 on epoch=137
03/16/2022 09:05:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=139
03/16/2022 09:05:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=142
03/16/2022 09:05:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.31 on epoch=144
03/16/2022 09:05:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=147
03/16/2022 09:05:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=149
03/16/2022 09:05:58 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.7602418414918415 on epoch=149
03/16/2022 09:06:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=152
03/16/2022 09:06:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=154
03/16/2022 09:06:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=157
03/16/2022 09:06:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=159
03/16/2022 09:06:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=162
03/16/2022 09:06:11 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.8100849265163781 on epoch=162
03/16/2022 09:06:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=164
03/16/2022 09:06:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
03/16/2022 09:06:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=169
03/16/2022 09:06:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=172
03/16/2022 09:06:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=174
03/16/2022 09:06:24 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.6958168642951252 on epoch=174
03/16/2022 09:06:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=177
03/16/2022 09:06:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=179
03/16/2022 09:06:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=182
03/16/2022 09:06:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=184
03/16/2022 09:06:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=187
03/16/2022 09:06:37 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.7773882619217304 on epoch=187
03/16/2022 09:06:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=189
03/16/2022 09:06:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=192
03/16/2022 09:06:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=194
03/16/2022 09:06:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=197
03/16/2022 09:06:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=199
03/16/2022 09:06:50 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.5668521723694138 on epoch=199
03/16/2022 09:06:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=202
03/16/2022 09:06:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=204
03/16/2022 09:06:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=207
03/16/2022 09:07:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=209
03/16/2022 09:07:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=212
03/16/2022 09:07:03 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.873662068174719 on epoch=212
03/16/2022 09:07:03 - INFO - __main__ - Saving model with best Classification-F1: 0.8182073840138356 -> 0.873662068174719 on epoch=212, global_step=850
03/16/2022 09:07:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=214
03/16/2022 09:07:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=217
03/16/2022 09:07:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=219
03/16/2022 09:07:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=222
03/16/2022 09:07:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=224
03/16/2022 09:07:16 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.742858122269887 on epoch=224
03/16/2022 09:07:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=227
03/16/2022 09:07:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=229
03/16/2022 09:07:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
03/16/2022 09:07:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=234
03/16/2022 09:07:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=237
03/16/2022 09:07:29 - INFO - __main__ - Global step 950 Train loss 0.14 Classification-F1 0.77609418767507 on epoch=237
03/16/2022 09:07:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=239
03/16/2022 09:07:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=242
03/16/2022 09:07:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=244
03/16/2022 09:07:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=247
03/16/2022 09:07:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=249
03/16/2022 09:07:42 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.7785576923076922 on epoch=249
03/16/2022 09:07:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=252
03/16/2022 09:07:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
03/16/2022 09:07:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=257
03/16/2022 09:07:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=259
03/16/2022 09:07:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=262
03/16/2022 09:07:55 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.5996741238846501 on epoch=262
03/16/2022 09:07:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=264
03/16/2022 09:08:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
03/16/2022 09:08:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=269
03/16/2022 09:08:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=272
03/16/2022 09:08:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=274
03/16/2022 09:08:08 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.6705025499143147 on epoch=274
03/16/2022 09:08:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=277
03/16/2022 09:08:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
03/16/2022 09:08:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
03/16/2022 09:08:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=284
03/16/2022 09:08:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=287
03/16/2022 09:08:21 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.7453380800154994 on epoch=287
03/16/2022 09:08:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=289
03/16/2022 09:08:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=292
03/16/2022 09:08:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=294
03/16/2022 09:08:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=297
03/16/2022 09:08:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=299
03/16/2022 09:08:34 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.6148340548340548 on epoch=299
03/16/2022 09:08:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
03/16/2022 09:08:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=304
03/16/2022 09:08:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=307
03/16/2022 09:08:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=309
03/16/2022 09:08:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
03/16/2022 09:08:47 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.7477726574500768 on epoch=312
03/16/2022 09:08:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=314
03/16/2022 09:08:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=317
03/16/2022 09:08:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
03/16/2022 09:08:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
03/16/2022 09:08:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=324
03/16/2022 09:09:00 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.6296814296814297 on epoch=324
03/16/2022 09:09:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
03/16/2022 09:09:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
03/16/2022 09:09:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=332
03/16/2022 09:09:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=334
03/16/2022 09:09:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
03/16/2022 09:09:13 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.7635127364728883 on epoch=337
03/16/2022 09:09:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=339
03/16/2022 09:09:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
03/16/2022 09:09:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=344
03/16/2022 09:09:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
03/16/2022 09:09:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
03/16/2022 09:09:26 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.7166827953272965 on epoch=349
03/16/2022 09:09:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
03/16/2022 09:09:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=354
03/16/2022 09:09:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
03/16/2022 09:09:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/16/2022 09:09:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
03/16/2022 09:09:40 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.7093702865761689 on epoch=362
03/16/2022 09:09:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
03/16/2022 09:09:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
03/16/2022 09:09:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
03/16/2022 09:09:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
03/16/2022 09:09:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=374
03/16/2022 09:09:53 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.7384962406015038 on epoch=374
03/16/2022 09:09:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
03/16/2022 09:09:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
03/16/2022 09:10:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
03/16/2022 09:10:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
03/16/2022 09:10:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=387
03/16/2022 09:10:06 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.717044627698278 on epoch=387
03/16/2022 09:10:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
03/16/2022 09:10:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
03/16/2022 09:10:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
03/16/2022 09:10:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
03/16/2022 09:10:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
03/16/2022 09:10:19 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.7618580765639589 on epoch=399
03/16/2022 09:10:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
03/16/2022 09:10:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
03/16/2022 09:10:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
03/16/2022 09:10:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=409
03/16/2022 09:10:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=412
03/16/2022 09:10:32 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.7303030303030303 on epoch=412
03/16/2022 09:10:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
03/16/2022 09:10:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
03/16/2022 09:10:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 09:10:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
03/16/2022 09:10:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
03/16/2022 09:10:45 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.7641553887034978 on epoch=424
03/16/2022 09:10:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
03/16/2022 09:10:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=429
03/16/2022 09:10:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=432
03/16/2022 09:10:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
03/16/2022 09:10:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
03/16/2022 09:10:58 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.7372501885369532 on epoch=437
03/16/2022 09:11:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
03/16/2022 09:11:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=442
03/16/2022 09:11:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=444
03/16/2022 09:11:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=447
03/16/2022 09:11:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
03/16/2022 09:11:11 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.7124011007911937 on epoch=449
03/16/2022 09:11:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=452
03/16/2022 09:11:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=454
03/16/2022 09:11:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=457
03/16/2022 09:11:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=459
03/16/2022 09:11:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=462
03/16/2022 09:11:24 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.7720023767082591 on epoch=462
03/16/2022 09:11:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
03/16/2022 09:11:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=467
03/16/2022 09:11:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=469
03/16/2022 09:11:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=472
03/16/2022 09:11:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
03/16/2022 09:11:37 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.7423412698412699 on epoch=474
03/16/2022 09:11:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
03/16/2022 09:11:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=479
03/16/2022 09:11:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=482
03/16/2022 09:11:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
03/16/2022 09:11:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
03/16/2022 09:11:50 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.7391443863218057 on epoch=487
03/16/2022 09:11:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
03/16/2022 09:11:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=492
03/16/2022 09:11:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=494
03/16/2022 09:12:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
03/16/2022 09:12:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
03/16/2022 09:12:04 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.7301822573561705 on epoch=499
03/16/2022 09:12:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/16/2022 09:12:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
03/16/2022 09:12:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
03/16/2022 09:12:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
03/16/2022 09:12:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
03/16/2022 09:12:17 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7637310606060606 on epoch=512
03/16/2022 09:12:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/16/2022 09:12:22 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
03/16/2022 09:12:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
03/16/2022 09:12:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/16/2022 09:12:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
03/16/2022 09:12:30 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6702380952380952 on epoch=524
03/16/2022 09:12:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
03/16/2022 09:12:35 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=529
03/16/2022 09:12:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 09:12:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=534
03/16/2022 09:12:42 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/16/2022 09:12:43 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.755868070808648 on epoch=537
03/16/2022 09:12:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
03/16/2022 09:12:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
03/16/2022 09:12:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/16/2022 09:12:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
03/16/2022 09:12:55 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
03/16/2022 09:12:56 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.683937088348853 on epoch=549
03/16/2022 09:12:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
03/16/2022 09:13:01 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
03/16/2022 09:13:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/16/2022 09:13:06 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/16/2022 09:13:09 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
03/16/2022 09:13:10 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7033045273534404 on epoch=562
03/16/2022 09:13:12 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/16/2022 09:13:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/16/2022 09:13:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/16/2022 09:13:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 09:13:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
03/16/2022 09:13:23 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7164758657086023 on epoch=574
03/16/2022 09:13:26 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
03/16/2022 09:13:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
03/16/2022 09:13:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
03/16/2022 09:13:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 09:13:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
03/16/2022 09:13:37 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7080126787557128 on epoch=587
03/16/2022 09:13:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
03/16/2022 09:13:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.13 on epoch=592
03/16/2022 09:13:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
03/16/2022 09:13:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/16/2022 09:13:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/16/2022 09:13:50 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7078869047619047 on epoch=599
03/16/2022 09:13:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=602
03/16/2022 09:13:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
03/16/2022 09:13:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
03/16/2022 09:14:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
03/16/2022 09:14:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=612
03/16/2022 09:14:03 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.7218782249742002 on epoch=612
03/16/2022 09:14:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
03/16/2022 09:14:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
03/16/2022 09:14:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=619
03/16/2022 09:14:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/16/2022 09:14:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
03/16/2022 09:14:16 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6970408207250313 on epoch=624
03/16/2022 09:14:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
03/16/2022 09:14:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/16/2022 09:14:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/16/2022 09:14:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
03/16/2022 09:14:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
03/16/2022 09:14:29 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6119205638730257 on epoch=637
03/16/2022 09:14:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/16/2022 09:14:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=642
03/16/2022 09:14:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=644
03/16/2022 09:14:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/16/2022 09:14:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/16/2022 09:14:43 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.7077466121583769 on epoch=649
03/16/2022 09:14:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/16/2022 09:14:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/16/2022 09:14:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/16/2022 09:14:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=659
03/16/2022 09:14:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
03/16/2022 09:14:56 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7081285831285832 on epoch=662
03/16/2022 09:14:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
03/16/2022 09:15:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
03/16/2022 09:15:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
03/16/2022 09:15:06 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
03/16/2022 09:15:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/16/2022 09:15:09 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7451903907496013 on epoch=674
03/16/2022 09:15:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
03/16/2022 09:15:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
03/16/2022 09:15:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
03/16/2022 09:15:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/16/2022 09:15:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=687
03/16/2022 09:15:22 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7465562987253755 on epoch=687
03/16/2022 09:15:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
03/16/2022 09:15:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
03/16/2022 09:15:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
03/16/2022 09:15:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/16/2022 09:15:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/16/2022 09:15:36 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7325539044289044 on epoch=699
03/16/2022 09:15:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
03/16/2022 09:15:40 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/16/2022 09:15:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/16/2022 09:15:45 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
03/16/2022 09:15:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=712
03/16/2022 09:15:49 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.736969696969697 on epoch=712
03/16/2022 09:15:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/16/2022 09:15:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 09:15:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/16/2022 09:15:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/16/2022 09:16:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=724
03/16/2022 09:16:03 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.736969696969697 on epoch=724
03/16/2022 09:16:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
03/16/2022 09:16:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
03/16/2022 09:16:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=732
03/16/2022 09:16:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
03/16/2022 09:16:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/16/2022 09:16:16 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7307069297558428 on epoch=737
03/16/2022 09:16:18 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
03/16/2022 09:16:21 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 09:16:23 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/16/2022 09:16:26 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 09:16:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 09:16:29 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6932298226415874 on epoch=749
03/16/2022 09:16:29 - INFO - __main__ - save last model!
03/16/2022 09:16:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 09:16:29 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 09:16:29 - INFO - __main__ - Printing 3 examples
03/16/2022 09:16:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 09:16:29 - INFO - __main__ - ['others']
03/16/2022 09:16:29 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 09:16:29 - INFO - __main__ - ['others']
03/16/2022 09:16:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 09:16:29 - INFO - __main__ - ['others']
03/16/2022 09:16:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:16:29 - INFO - __main__ - Printing 3 examples
03/16/2022 09:16:29 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/16/2022 09:16:29 - INFO - __main__ - ['others']
03/16/2022 09:16:29 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/16/2022 09:16:29 - INFO - __main__ - ['others']
03/16/2022 09:16:29 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/16/2022 09:16:29 - INFO - __main__ - ['others']
03/16/2022 09:16:29 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:16:29 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:16:29 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:16:30 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 09:16:30 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:16:30 - INFO - __main__ - Printing 3 examples
03/16/2022 09:16:30 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/16/2022 09:16:30 - INFO - __main__ - ['others']
03/16/2022 09:16:30 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/16/2022 09:16:30 - INFO - __main__ - ['others']
03/16/2022 09:16:30 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/16/2022 09:16:30 - INFO - __main__ - ['others']
03/16/2022 09:16:30 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:16:30 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:16:30 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 09:16:32 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:16:37 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 09:16:48 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 09:16:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 09:16:49 - INFO - __main__ - Starting training!
03/16/2022 09:18:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_13_0.3_8_predictions.txt
03/16/2022 09:18:10 - INFO - __main__ - Classification-F1 on test data: 0.0844
03/16/2022 09:18:10 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.873662068174719, test_performance=0.08436739427678028
03/16/2022 09:18:10 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
03/16/2022 09:18:11 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:18:11 - INFO - __main__ - Printing 3 examples
03/16/2022 09:18:11 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/16/2022 09:18:11 - INFO - __main__ - ['others']
03/16/2022 09:18:11 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/16/2022 09:18:11 - INFO - __main__ - ['others']
03/16/2022 09:18:11 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/16/2022 09:18:11 - INFO - __main__ - ['others']
03/16/2022 09:18:11 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:18:11 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:18:11 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 09:18:11 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:18:11 - INFO - __main__ - Printing 3 examples
03/16/2022 09:18:11 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/16/2022 09:18:11 - INFO - __main__ - ['others']
03/16/2022 09:18:11 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/16/2022 09:18:11 - INFO - __main__ - ['others']
03/16/2022 09:18:11 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/16/2022 09:18:11 - INFO - __main__ - ['others']
03/16/2022 09:18:11 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:18:11 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:18:11 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 09:18:26 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 09:18:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 09:18:27 - INFO - __main__ - Starting training!
03/16/2022 09:18:30 - INFO - __main__ - Step 10 Global step 10 Train loss 3.47 on epoch=2
03/16/2022 09:18:32 - INFO - __main__ - Step 20 Global step 20 Train loss 1.77 on epoch=4
03/16/2022 09:18:35 - INFO - __main__ - Step 30 Global step 30 Train loss 1.35 on epoch=7
03/16/2022 09:18:37 - INFO - __main__ - Step 40 Global step 40 Train loss 1.04 on epoch=9
03/16/2022 09:18:40 - INFO - __main__ - Step 50 Global step 50 Train loss 1.02 on epoch=12
03/16/2022 09:18:40 - INFO - __main__ - Global step 50 Train loss 1.73 Classification-F1 0.13067758749069247 on epoch=12
03/16/2022 09:18:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=12, global_step=50
03/16/2022 09:18:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.99 on epoch=14
03/16/2022 09:18:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=17
03/16/2022 09:18:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=19
03/16/2022 09:18:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=22
03/16/2022 09:18:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.78 on epoch=24
03/16/2022 09:18:54 - INFO - __main__ - Global step 100 Train loss 0.91 Classification-F1 0.42871174524400335 on epoch=24
03/16/2022 09:18:54 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.42871174524400335 on epoch=24, global_step=100
03/16/2022 09:18:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.86 on epoch=27
03/16/2022 09:18:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.83 on epoch=29
03/16/2022 09:19:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.75 on epoch=32
03/16/2022 09:19:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.84 on epoch=34
03/16/2022 09:19:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=37
03/16/2022 09:19:07 - INFO - __main__ - Global step 150 Train loss 0.82 Classification-F1 0.1 on epoch=37
03/16/2022 09:19:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.78 on epoch=39
03/16/2022 09:19:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.79 on epoch=42
03/16/2022 09:19:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.77 on epoch=44
03/16/2022 09:19:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.78 on epoch=47
03/16/2022 09:19:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.80 on epoch=49
03/16/2022 09:19:20 - INFO - __main__ - Global step 200 Train loss 0.78 Classification-F1 0.40767195767195774 on epoch=49
03/16/2022 09:19:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.81 on epoch=52
03/16/2022 09:19:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.71 on epoch=54
03/16/2022 09:19:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.70 on epoch=57
03/16/2022 09:19:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.76 on epoch=59
03/16/2022 09:19:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.68 on epoch=62
03/16/2022 09:19:33 - INFO - __main__ - Global step 250 Train loss 0.73 Classification-F1 0.3134674922600619 on epoch=62
03/16/2022 09:19:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.68 on epoch=64
03/16/2022 09:19:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.62 on epoch=67
03/16/2022 09:19:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.72 on epoch=69
03/16/2022 09:19:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.63 on epoch=72
03/16/2022 09:19:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.68 on epoch=74
03/16/2022 09:19:46 - INFO - __main__ - Global step 300 Train loss 0.67 Classification-F1 0.490064131286753 on epoch=74
03/16/2022 09:19:46 - INFO - __main__ - Saving model with best Classification-F1: 0.42871174524400335 -> 0.490064131286753 on epoch=74, global_step=300
03/16/2022 09:19:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.62 on epoch=77
03/16/2022 09:19:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=79
03/16/2022 09:19:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.60 on epoch=82
03/16/2022 09:19:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=84
03/16/2022 09:19:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.62 on epoch=87
03/16/2022 09:19:59 - INFO - __main__ - Global step 350 Train loss 0.61 Classification-F1 0.6305749805749805 on epoch=87
03/16/2022 09:19:59 - INFO - __main__ - Saving model with best Classification-F1: 0.490064131286753 -> 0.6305749805749805 on epoch=87, global_step=350
03/16/2022 09:20:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=89
03/16/2022 09:20:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.63 on epoch=92
03/16/2022 09:20:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.61 on epoch=94
03/16/2022 09:20:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.58 on epoch=97
03/16/2022 09:20:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=99
03/16/2022 09:20:12 - INFO - __main__ - Global step 400 Train loss 0.58 Classification-F1 0.75 on epoch=99
03/16/2022 09:20:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6305749805749805 -> 0.75 on epoch=99, global_step=400
03/16/2022 09:20:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.58 on epoch=102
03/16/2022 09:20:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.60 on epoch=104
03/16/2022 09:20:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=107
03/16/2022 09:20:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.70 on epoch=109
03/16/2022 09:20:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=112
03/16/2022 09:20:25 - INFO - __main__ - Global step 450 Train loss 0.60 Classification-F1 0.6024783918872589 on epoch=112
03/16/2022 09:20:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=114
03/16/2022 09:20:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=117
03/16/2022 09:20:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.55 on epoch=119
03/16/2022 09:20:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=122
03/16/2022 09:20:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=124
03/16/2022 09:20:38 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.7477671451355662 on epoch=124
03/16/2022 09:20:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=127
03/16/2022 09:20:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=129
03/16/2022 09:20:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=132
03/16/2022 09:20:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=134
03/16/2022 09:20:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=137
03/16/2022 09:20:51 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.6992466191576314 on epoch=137
03/16/2022 09:20:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=139
03/16/2022 09:20:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=142
03/16/2022 09:20:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=144
03/16/2022 09:21:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=147
03/16/2022 09:21:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=149
03/16/2022 09:21:03 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.7732323232323232 on epoch=149
03/16/2022 09:21:03 - INFO - __main__ - Saving model with best Classification-F1: 0.75 -> 0.7732323232323232 on epoch=149, global_step=600
03/16/2022 09:21:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=152
03/16/2022 09:21:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=154
03/16/2022 09:21:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=157
03/16/2022 09:21:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=159
03/16/2022 09:21:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=162
03/16/2022 09:21:17 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.7813060452512784 on epoch=162
03/16/2022 09:21:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7732323232323232 -> 0.7813060452512784 on epoch=162, global_step=650
03/16/2022 09:21:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=164
03/16/2022 09:21:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=167
03/16/2022 09:21:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=169
03/16/2022 09:21:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=172
03/16/2022 09:21:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=174
03/16/2022 09:21:30 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.7623724006002737 on epoch=174
03/16/2022 09:21:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=177
03/16/2022 09:21:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=179
03/16/2022 09:21:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=182
03/16/2022 09:21:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=184
03/16/2022 09:21:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=187
03/16/2022 09:21:42 - INFO - __main__ - Global step 750 Train loss 0.31 Classification-F1 0.7466745457916204 on epoch=187
03/16/2022 09:21:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=189
03/16/2022 09:21:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=192
03/16/2022 09:21:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=194
03/16/2022 09:21:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=197
03/16/2022 09:21:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=199
03/16/2022 09:21:55 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.7644275079758951 on epoch=199
03/16/2022 09:21:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=202
03/16/2022 09:22:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=204
03/16/2022 09:22:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=207
03/16/2022 09:22:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=209
03/16/2022 09:22:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=212
03/16/2022 09:22:09 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.7916666666666666 on epoch=212
03/16/2022 09:22:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7813060452512784 -> 0.7916666666666666 on epoch=212, global_step=850
03/16/2022 09:22:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=214
03/16/2022 09:22:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=217
03/16/2022 09:22:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=219
03/16/2022 09:22:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=222
03/16/2022 09:22:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=224
03/16/2022 09:22:22 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.5297438999798347 on epoch=224
03/16/2022 09:22:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=227
03/16/2022 09:22:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=229
03/16/2022 09:22:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=232
03/16/2022 09:22:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=234
03/16/2022 09:22:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=237
03/16/2022 09:22:35 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.7575694444444444 on epoch=237
03/16/2022 09:22:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=239
03/16/2022 09:22:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=242
03/16/2022 09:22:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=244
03/16/2022 09:22:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=247
03/16/2022 09:22:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=249
03/16/2022 09:22:48 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.7001461988304094 on epoch=249
03/16/2022 09:22:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=252
03/16/2022 09:22:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=254
03/16/2022 09:22:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=257
03/16/2022 09:22:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
03/16/2022 09:23:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=262
03/16/2022 09:23:01 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.5856060606060607 on epoch=262
03/16/2022 09:23:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=264
03/16/2022 09:23:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
03/16/2022 09:23:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=269
03/16/2022 09:23:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=272
03/16/2022 09:23:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=274
03/16/2022 09:23:14 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.7257687165775402 on epoch=274
03/16/2022 09:23:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=277
03/16/2022 09:23:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=279
03/16/2022 09:23:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=282
03/16/2022 09:23:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=284
03/16/2022 09:23:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=287
03/16/2022 09:23:27 - INFO - __main__ - Global step 1150 Train loss 0.16 Classification-F1 0.5664221294925292 on epoch=287
03/16/2022 09:23:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=289
03/16/2022 09:23:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.14 on epoch=292
03/16/2022 09:23:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=294
03/16/2022 09:23:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=297
03/16/2022 09:23:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=299
03/16/2022 09:23:40 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.7641329258976318 on epoch=299
03/16/2022 09:23:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=302
03/16/2022 09:23:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=304
03/16/2022 09:23:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=307
03/16/2022 09:23:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=309
03/16/2022 09:23:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=312
03/16/2022 09:23:53 - INFO - __main__ - Global step 1250 Train loss 0.13 Classification-F1 0.7778759779773978 on epoch=312
03/16/2022 09:23:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=314
03/16/2022 09:23:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=317
03/16/2022 09:24:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
03/16/2022 09:24:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=322
03/16/2022 09:24:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=324
03/16/2022 09:24:06 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.7632962862564381 on epoch=324
03/16/2022 09:24:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=327
03/16/2022 09:24:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=329
03/16/2022 09:24:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=332
03/16/2022 09:24:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=334
03/16/2022 09:24:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
03/16/2022 09:24:19 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.7514453477868112 on epoch=337
03/16/2022 09:24:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
03/16/2022 09:24:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=342
03/16/2022 09:24:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=344
03/16/2022 09:24:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
03/16/2022 09:24:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
03/16/2022 09:24:32 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.7774228459712331 on epoch=349
03/16/2022 09:24:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
03/16/2022 09:24:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
03/16/2022 09:24:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
03/16/2022 09:24:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
03/16/2022 09:24:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
03/16/2022 09:24:45 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.7535185185185185 on epoch=362
03/16/2022 09:24:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=364
03/16/2022 09:24:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
03/16/2022 09:24:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=369
03/16/2022 09:24:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
03/16/2022 09:24:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
03/16/2022 09:24:58 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.7708629605688428 on epoch=374
03/16/2022 09:25:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
03/16/2022 09:25:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
03/16/2022 09:25:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
03/16/2022 09:25:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
03/16/2022 09:25:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=387
03/16/2022 09:25:12 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.766278771284333 on epoch=387
03/16/2022 09:25:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=389
03/16/2022 09:25:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
03/16/2022 09:25:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
03/16/2022 09:25:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
03/16/2022 09:25:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=399
03/16/2022 09:25:25 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.6897435897435897 on epoch=399
03/16/2022 09:25:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
03/16/2022 09:25:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
03/16/2022 09:25:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
03/16/2022 09:25:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=409
03/16/2022 09:25:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
03/16/2022 09:25:38 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7604764474092968 on epoch=412
03/16/2022 09:25:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
03/16/2022 09:25:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
03/16/2022 09:25:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
03/16/2022 09:25:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=422
03/16/2022 09:25:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
03/16/2022 09:25:51 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.744674185463659 on epoch=424
03/16/2022 09:25:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
03/16/2022 09:25:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=429
03/16/2022 09:25:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
03/16/2022 09:26:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=434
03/16/2022 09:26:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=437
03/16/2022 09:26:05 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.7786674347158219 on epoch=437
03/16/2022 09:26:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=439
03/16/2022 09:26:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
03/16/2022 09:26:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
03/16/2022 09:26:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
03/16/2022 09:26:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/16/2022 09:26:18 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7394345238095238 on epoch=449
03/16/2022 09:26:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
03/16/2022 09:26:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/16/2022 09:26:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=457
03/16/2022 09:26:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
03/16/2022 09:26:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
03/16/2022 09:26:31 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6808188099188941 on epoch=462
03/16/2022 09:26:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=464
03/16/2022 09:26:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
03/16/2022 09:26:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/16/2022 09:26:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
03/16/2022 09:26:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=474
03/16/2022 09:26:44 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.744674185463659 on epoch=474
03/16/2022 09:26:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
03/16/2022 09:26:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
03/16/2022 09:26:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=482
03/16/2022 09:26:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=484
03/16/2022 09:26:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
03/16/2022 09:26:57 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7257718593925491 on epoch=487
03/16/2022 09:27:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
03/16/2022 09:27:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
03/16/2022 09:27:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=494
03/16/2022 09:27:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/16/2022 09:27:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 09:27:10 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.745687748783724 on epoch=499
03/16/2022 09:27:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
03/16/2022 09:27:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 09:27:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
03/16/2022 09:27:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
03/16/2022 09:27:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
03/16/2022 09:27:24 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7908126356402218 on epoch=512
03/16/2022 09:27:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/16/2022 09:27:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
03/16/2022 09:27:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
03/16/2022 09:27:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
03/16/2022 09:27:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 09:27:37 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7637254901960785 on epoch=524
03/16/2022 09:27:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/16/2022 09:27:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 09:27:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 09:27:46 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
03/16/2022 09:27:49 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=537
03/16/2022 09:27:50 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6931372549019609 on epoch=537
03/16/2022 09:27:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/16/2022 09:27:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
03/16/2022 09:27:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/16/2022 09:28:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/16/2022 09:28:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=549
03/16/2022 09:28:03 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7599316801619433 on epoch=549
03/16/2022 09:28:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
03/16/2022 09:28:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/16/2022 09:28:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
03/16/2022 09:28:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/16/2022 09:28:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
03/16/2022 09:28:16 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.776559934318555 on epoch=562
03/16/2022 09:28:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
03/16/2022 09:28:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/16/2022 09:28:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
03/16/2022 09:28:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 09:28:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
03/16/2022 09:28:30 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7451610938187763 on epoch=574
03/16/2022 09:28:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
03/16/2022 09:28:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
03/16/2022 09:28:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
03/16/2022 09:28:40 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 09:28:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
03/16/2022 09:28:43 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7630678708264915 on epoch=587
03/16/2022 09:28:45 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=589
03/16/2022 09:28:48 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=592
03/16/2022 09:28:50 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
03/16/2022 09:28:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/16/2022 09:28:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
03/16/2022 09:28:56 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.6808188099188941 on epoch=599
03/16/2022 09:28:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/16/2022 09:29:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/16/2022 09:29:03 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/16/2022 09:29:06 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/16/2022 09:29:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
03/16/2022 09:29:09 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7478491814698711 on epoch=612
03/16/2022 09:29:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/16/2022 09:29:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 09:29:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
03/16/2022 09:29:19 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/16/2022 09:29:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
03/16/2022 09:29:23 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7301608483683855 on epoch=624
03/16/2022 09:29:25 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/16/2022 09:29:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 09:29:30 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
03/16/2022 09:29:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=634
03/16/2022 09:29:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/16/2022 09:29:36 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7458986415882968 on epoch=637
03/16/2022 09:29:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/16/2022 09:29:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/16/2022 09:29:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=644
03/16/2022 09:29:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/16/2022 09:29:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/16/2022 09:29:49 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7641787658802177 on epoch=649
03/16/2022 09:29:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/16/2022 09:29:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/16/2022 09:29:56 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/16/2022 09:29:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=659
03/16/2022 09:30:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/16/2022 09:30:02 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6793311530153634 on epoch=662
03/16/2022 09:30:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
03/16/2022 09:30:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/16/2022 09:30:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/16/2022 09:30:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/16/2022 09:30:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
03/16/2022 09:30:16 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7644275079758951 on epoch=674
03/16/2022 09:30:18 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
03/16/2022 09:30:21 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/16/2022 09:30:23 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/16/2022 09:30:25 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
03/16/2022 09:30:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=687
03/16/2022 09:30:29 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.7450978075978076 on epoch=687
03/16/2022 09:30:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
03/16/2022 09:30:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/16/2022 09:30:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
03/16/2022 09:30:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
03/16/2022 09:30:41 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/16/2022 09:30:42 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.715629351155667 on epoch=699
03/16/2022 09:30:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/16/2022 09:30:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/16/2022 09:30:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/16/2022 09:30:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
03/16/2022 09:30:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
03/16/2022 09:30:56 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7271564457048328 on epoch=712
03/16/2022 09:30:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/16/2022 09:31:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 09:31:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/16/2022 09:31:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
03/16/2022 09:31:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
03/16/2022 09:31:09 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7291577291577291 on epoch=724
03/16/2022 09:31:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/16/2022 09:31:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 09:31:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
03/16/2022 09:31:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/16/2022 09:31:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
03/16/2022 09:31:22 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7818014705882352 on epoch=737
03/16/2022 09:31:25 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/16/2022 09:31:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=742
03/16/2022 09:31:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=744
03/16/2022 09:31:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
03/16/2022 09:31:35 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 09:31:36 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6894119394119393 on epoch=749
03/16/2022 09:31:36 - INFO - __main__ - save last model!
03/16/2022 09:31:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 09:31:36 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 09:31:36 - INFO - __main__ - Printing 3 examples
03/16/2022 09:31:36 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 09:31:36 - INFO - __main__ - ['others']
03/16/2022 09:31:36 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 09:31:36 - INFO - __main__ - ['others']
03/16/2022 09:31:36 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 09:31:36 - INFO - __main__ - ['others']
03/16/2022 09:31:36 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:31:36 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:31:36 - INFO - __main__ - Printing 3 examples
03/16/2022 09:31:36 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/16/2022 09:31:36 - INFO - __main__ - ['sad']
03/16/2022 09:31:36 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/16/2022 09:31:36 - INFO - __main__ - ['sad']
03/16/2022 09:31:36 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/16/2022 09:31:36 - INFO - __main__ - ['sad']
03/16/2022 09:31:36 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:31:36 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:31:36 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 09:31:36 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:31:36 - INFO - __main__ - Printing 3 examples
03/16/2022 09:31:36 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/16/2022 09:31:36 - INFO - __main__ - ['sad']
03/16/2022 09:31:36 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/16/2022 09:31:36 - INFO - __main__ - ['sad']
03/16/2022 09:31:36 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/16/2022 09:31:36 - INFO - __main__ - ['sad']
03/16/2022 09:31:36 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:31:36 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:31:36 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 09:31:38 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:31:43 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 09:31:55 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 09:31:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 09:31:55 - INFO - __main__ - Starting training!
03/16/2022 09:33:16 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_13_0.2_8_predictions.txt
03/16/2022 09:33:16 - INFO - __main__ - Classification-F1 on test data: 0.3834
03/16/2022 09:33:16 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.7916666666666666, test_performance=0.3833512756264965
03/16/2022 09:33:16 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
03/16/2022 09:33:17 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:33:17 - INFO - __main__ - Printing 3 examples
03/16/2022 09:33:17 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/16/2022 09:33:17 - INFO - __main__ - ['sad']
03/16/2022 09:33:17 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/16/2022 09:33:17 - INFO - __main__ - ['sad']
03/16/2022 09:33:17 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/16/2022 09:33:17 - INFO - __main__ - ['sad']
03/16/2022 09:33:17 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:33:17 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:33:17 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 09:33:17 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:33:17 - INFO - __main__ - Printing 3 examples
03/16/2022 09:33:17 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/16/2022 09:33:17 - INFO - __main__ - ['sad']
03/16/2022 09:33:17 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/16/2022 09:33:17 - INFO - __main__ - ['sad']
03/16/2022 09:33:17 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/16/2022 09:33:17 - INFO - __main__ - ['sad']
03/16/2022 09:33:17 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:33:17 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:33:17 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 09:33:36 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 09:33:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 09:33:37 - INFO - __main__ - Starting training!
03/16/2022 09:33:40 - INFO - __main__ - Step 10 Global step 10 Train loss 2.83 on epoch=2
03/16/2022 09:33:42 - INFO - __main__ - Step 20 Global step 20 Train loss 1.42 on epoch=4
03/16/2022 09:33:45 - INFO - __main__ - Step 30 Global step 30 Train loss 1.23 on epoch=7
03/16/2022 09:33:47 - INFO - __main__ - Step 40 Global step 40 Train loss 1.05 on epoch=9
03/16/2022 09:33:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.97 on epoch=12
03/16/2022 09:33:51 - INFO - __main__ - Global step 50 Train loss 1.50 Classification-F1 0.1581196581196581 on epoch=12
03/16/2022 09:33:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1581196581196581 on epoch=12, global_step=50
03/16/2022 09:33:54 - INFO - __main__ - Step 60 Global step 60 Train loss 0.91 on epoch=14
03/16/2022 09:33:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=17
03/16/2022 09:33:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.84 on epoch=19
03/16/2022 09:34:01 - INFO - __main__ - Step 90 Global step 90 Train loss 1.07 on epoch=22
03/16/2022 09:34:03 - INFO - __main__ - Step 100 Global step 100 Train loss 1.04 on epoch=24
03/16/2022 09:34:04 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.6049574694311537 on epoch=24
03/16/2022 09:34:04 - INFO - __main__ - Saving model with best Classification-F1: 0.1581196581196581 -> 0.6049574694311537 on epoch=24, global_step=100
03/16/2022 09:34:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.82 on epoch=27
03/16/2022 09:34:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=29
03/16/2022 09:34:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.70 on epoch=32
03/16/2022 09:34:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.71 on epoch=34
03/16/2022 09:34:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.72 on epoch=37
03/16/2022 09:34:17 - INFO - __main__ - Global step 150 Train loss 0.75 Classification-F1 0.6079423813928901 on epoch=37
03/16/2022 09:34:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6049574694311537 -> 0.6079423813928901 on epoch=37, global_step=150
03/16/2022 09:34:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.64 on epoch=39
03/16/2022 09:34:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.74 on epoch=42
03/16/2022 09:34:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.64 on epoch=44
03/16/2022 09:34:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.66 on epoch=47
03/16/2022 09:34:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.73 on epoch=49
03/16/2022 09:34:30 - INFO - __main__ - Global step 200 Train loss 0.68 Classification-F1 0.5028664072469238 on epoch=49
03/16/2022 09:34:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.67 on epoch=52
03/16/2022 09:34:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=54
03/16/2022 09:34:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.60 on epoch=57
03/16/2022 09:34:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=59
03/16/2022 09:34:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.56 on epoch=62
03/16/2022 09:34:43 - INFO - __main__ - Global step 250 Train loss 0.59 Classification-F1 0.5444444444444445 on epoch=62
03/16/2022 09:34:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=64
03/16/2022 09:34:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=67
03/16/2022 09:34:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=69
03/16/2022 09:34:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=72
03/16/2022 09:34:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=74
03/16/2022 09:34:56 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.6971497252747254 on epoch=74
03/16/2022 09:34:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6079423813928901 -> 0.6971497252747254 on epoch=74, global_step=300
03/16/2022 09:34:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=77
03/16/2022 09:35:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=79
03/16/2022 09:35:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=82
03/16/2022 09:35:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=84
03/16/2022 09:35:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=87
03/16/2022 09:35:09 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.6583005660188339 on epoch=87
03/16/2022 09:35:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=89
03/16/2022 09:35:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=92
03/16/2022 09:35:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=94
03/16/2022 09:35:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=97
03/16/2022 09:35:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=99
03/16/2022 09:35:21 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.7038796959434594 on epoch=99
03/16/2022 09:35:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6971497252747254 -> 0.7038796959434594 on epoch=99, global_step=400
03/16/2022 09:35:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=102
03/16/2022 09:35:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=104
03/16/2022 09:35:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=107
03/16/2022 09:35:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=109
03/16/2022 09:35:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=112
03/16/2022 09:35:34 - INFO - __main__ - Global step 450 Train loss 0.28 Classification-F1 0.6869824016563146 on epoch=112
03/16/2022 09:35:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=114
03/16/2022 09:35:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=117
03/16/2022 09:35:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=119
03/16/2022 09:35:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.17 on epoch=122
03/16/2022 09:35:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=124
03/16/2022 09:35:47 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.6790701415701416 on epoch=124
03/16/2022 09:35:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=127
03/16/2022 09:35:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=129
03/16/2022 09:35:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
03/16/2022 09:35:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
03/16/2022 09:35:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=137
03/16/2022 09:36:00 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.7214851597740798 on epoch=137
03/16/2022 09:36:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7038796959434594 -> 0.7214851597740798 on epoch=137, global_step=550
03/16/2022 09:36:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=139
03/16/2022 09:36:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=142
03/16/2022 09:36:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=144
03/16/2022 09:36:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=147
03/16/2022 09:36:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=149
03/16/2022 09:36:13 - INFO - __main__ - Global step 600 Train loss 0.16 Classification-F1 0.6808565531475749 on epoch=149
03/16/2022 09:36:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=152
03/16/2022 09:36:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=154
03/16/2022 09:36:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=157
03/16/2022 09:36:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=159
03/16/2022 09:36:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=162
03/16/2022 09:36:26 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.6842837342837342 on epoch=162
03/16/2022 09:36:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=164
03/16/2022 09:36:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=167
03/16/2022 09:36:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=169
03/16/2022 09:36:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=172
03/16/2022 09:36:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=174
03/16/2022 09:36:39 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.7228658833260352 on epoch=174
03/16/2022 09:36:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7214851597740798 -> 0.7228658833260352 on epoch=174, global_step=700
03/16/2022 09:36:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=177
03/16/2022 09:36:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=179
03/16/2022 09:36:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=182
03/16/2022 09:36:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=184
03/16/2022 09:36:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=187
03/16/2022 09:36:51 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.6877525252525253 on epoch=187
03/16/2022 09:36:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=189
03/16/2022 09:36:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=192
03/16/2022 09:36:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=194
03/16/2022 09:37:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=197
03/16/2022 09:37:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=199
03/16/2022 09:37:04 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.7035780684301511 on epoch=199
03/16/2022 09:37:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=202
03/16/2022 09:37:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=204
03/16/2022 09:37:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.13 on epoch=207
03/16/2022 09:37:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=209
03/16/2022 09:37:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=212
03/16/2022 09:37:17 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.6944871794871795 on epoch=212
03/16/2022 09:37:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=214
03/16/2022 09:37:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=217
03/16/2022 09:37:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=219
03/16/2022 09:37:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=222
03/16/2022 09:37:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
03/16/2022 09:37:30 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.6812187812187812 on epoch=224
03/16/2022 09:37:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=227
03/16/2022 09:37:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=229
03/16/2022 09:37:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
03/16/2022 09:37:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=234
03/16/2022 09:37:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=237
03/16/2022 09:37:43 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.7380299924479448 on epoch=237
03/16/2022 09:37:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7228658833260352 -> 0.7380299924479448 on epoch=237, global_step=950
03/16/2022 09:37:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
03/16/2022 09:37:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
03/16/2022 09:37:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
03/16/2022 09:37:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=247
03/16/2022 09:37:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=249
03/16/2022 09:37:56 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.7178670147420146 on epoch=249
03/16/2022 09:37:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=252
03/16/2022 09:38:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
03/16/2022 09:38:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=257
03/16/2022 09:38:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
03/16/2022 09:38:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=262
03/16/2022 09:38:09 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.7087193272677144 on epoch=262
03/16/2022 09:38:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
03/16/2022 09:38:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
03/16/2022 09:38:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
03/16/2022 09:38:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
03/16/2022 09:38:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
03/16/2022 09:38:22 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.6986788223630329 on epoch=274
03/16/2022 09:38:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
03/16/2022 09:38:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=279
03/16/2022 09:38:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
03/16/2022 09:38:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
03/16/2022 09:38:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
03/16/2022 09:38:35 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.7038796959434594 on epoch=287
03/16/2022 09:38:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
03/16/2022 09:38:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
03/16/2022 09:38:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
03/16/2022 09:38:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
03/16/2022 09:38:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
03/16/2022 09:38:48 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.7178670147420146 on epoch=299
03/16/2022 09:38:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
03/16/2022 09:38:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
03/16/2022 09:38:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
03/16/2022 09:38:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
03/16/2022 09:39:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
03/16/2022 09:39:01 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.7181436071238703 on epoch=312
03/16/2022 09:39:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
03/16/2022 09:39:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
03/16/2022 09:39:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
03/16/2022 09:39:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=322
03/16/2022 09:39:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=324
03/16/2022 09:39:14 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7302330022918259 on epoch=324
03/16/2022 09:39:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
03/16/2022 09:39:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
03/16/2022 09:39:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
03/16/2022 09:39:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
03/16/2022 09:39:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
03/16/2022 09:39:27 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.7049120234604105 on epoch=337
03/16/2022 09:39:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
03/16/2022 09:39:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
03/16/2022 09:39:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
03/16/2022 09:39:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
03/16/2022 09:39:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
03/16/2022 09:39:40 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6889568764568765 on epoch=349
03/16/2022 09:39:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
03/16/2022 09:39:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
03/16/2022 09:39:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=357
03/16/2022 09:39:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
03/16/2022 09:39:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
03/16/2022 09:39:53 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.7190128711867843 on epoch=362
03/16/2022 09:39:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
03/16/2022 09:39:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
03/16/2022 09:40:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
03/16/2022 09:40:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
03/16/2022 09:40:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
03/16/2022 09:40:06 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.7189293150370516 on epoch=374
03/16/2022 09:40:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
03/16/2022 09:40:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
03/16/2022 09:40:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
03/16/2022 09:40:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
03/16/2022 09:40:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
03/16/2022 09:40:19 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.724941724941725 on epoch=387
03/16/2022 09:40:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
03/16/2022 09:40:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
03/16/2022 09:40:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
03/16/2022 09:40:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
03/16/2022 09:40:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
03/16/2022 09:40:32 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.705040005040005 on epoch=399
03/16/2022 09:40:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/16/2022 09:40:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
03/16/2022 09:40:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
03/16/2022 09:40:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
03/16/2022 09:40:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
03/16/2022 09:40:45 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7241006785186308 on epoch=412
03/16/2022 09:40:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
03/16/2022 09:40:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
03/16/2022 09:40:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 09:40:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
03/16/2022 09:40:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
03/16/2022 09:40:58 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.685369532428356 on epoch=424
03/16/2022 09:41:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
03/16/2022 09:41:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
03/16/2022 09:41:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
03/16/2022 09:41:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
03/16/2022 09:41:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
03/16/2022 09:41:11 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7321184821184821 on epoch=437
03/16/2022 09:41:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
03/16/2022 09:41:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
03/16/2022 09:41:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
03/16/2022 09:41:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
03/16/2022 09:41:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
03/16/2022 09:41:24 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6842607313195548 on epoch=449
03/16/2022 09:41:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/16/2022 09:41:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/16/2022 09:41:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
03/16/2022 09:41:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
03/16/2022 09:41:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/16/2022 09:41:36 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6812723748207619 on epoch=462
03/16/2022 09:41:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
03/16/2022 09:41:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
03/16/2022 09:41:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
03/16/2022 09:41:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/16/2022 09:41:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
03/16/2022 09:41:49 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.713530439336891 on epoch=474
03/16/2022 09:41:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
03/16/2022 09:41:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=479
03/16/2022 09:41:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=482
03/16/2022 09:41:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/16/2022 09:42:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/16/2022 09:42:02 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7134951856431008 on epoch=487
03/16/2022 09:42:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/16/2022 09:42:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/16/2022 09:42:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/16/2022 09:42:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
03/16/2022 09:42:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 09:42:15 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.685369532428356 on epoch=499
03/16/2022 09:42:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/16/2022 09:42:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 09:42:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
03/16/2022 09:42:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
03/16/2022 09:42:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/16/2022 09:42:28 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7426565551565552 on epoch=512
03/16/2022 09:42:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7380299924479448 -> 0.7426565551565552 on epoch=512, global_step=2050
03/16/2022 09:42:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/16/2022 09:42:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/16/2022 09:42:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
03/16/2022 09:42:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/16/2022 09:42:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 09:42:41 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.711729242979243 on epoch=524
03/16/2022 09:42:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/16/2022 09:42:46 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 09:42:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 09:42:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
03/16/2022 09:42:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/16/2022 09:42:54 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6881944444444444 on epoch=537
03/16/2022 09:42:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/16/2022 09:42:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/16/2022 09:43:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/16/2022 09:43:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/16/2022 09:43:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/16/2022 09:43:07 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6881944444444444 on epoch=549
03/16/2022 09:43:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
03/16/2022 09:43:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
03/16/2022 09:43:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/16/2022 09:43:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=559
03/16/2022 09:43:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/16/2022 09:43:20 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6963702963702965 on epoch=562
03/16/2022 09:43:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/16/2022 09:43:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/16/2022 09:43:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
03/16/2022 09:43:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
03/16/2022 09:43:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/16/2022 09:43:32 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.7334920147420148 on epoch=574
03/16/2022 09:43:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/16/2022 09:43:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/16/2022 09:43:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/16/2022 09:43:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 09:43:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
03/16/2022 09:43:45 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7566353354166154 on epoch=587
03/16/2022 09:43:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7426565551565552 -> 0.7566353354166154 on epoch=587, global_step=2350
03/16/2022 09:43:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 09:43:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/16/2022 09:43:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
03/16/2022 09:43:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/16/2022 09:43:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/16/2022 09:43:58 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7177703089244851 on epoch=599
03/16/2022 09:44:01 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/16/2022 09:44:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/16/2022 09:44:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/16/2022 09:44:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/16/2022 09:44:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
03/16/2022 09:44:11 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.709140498304585 on epoch=612
03/16/2022 09:44:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/16/2022 09:44:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/16/2022 09:44:18 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 09:44:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/16/2022 09:44:23 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
03/16/2022 09:44:24 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7329053887034978 on epoch=624
03/16/2022 09:44:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/16/2022 09:44:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 09:44:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
03/16/2022 09:44:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
03/16/2022 09:44:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/16/2022 09:44:37 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7165936687675818 on epoch=637
03/16/2022 09:44:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/16/2022 09:44:42 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/16/2022 09:44:44 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/16/2022 09:44:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/16/2022 09:44:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/16/2022 09:44:50 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7079365079365079 on epoch=649
03/16/2022 09:44:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/16/2022 09:44:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/16/2022 09:44:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/16/2022 09:45:00 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/16/2022 09:45:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/16/2022 09:45:03 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7526578998353193 on epoch=662
03/16/2022 09:45:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
03/16/2022 09:45:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
03/16/2022 09:45:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/16/2022 09:45:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/16/2022 09:45:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/16/2022 09:45:16 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7171957671957672 on epoch=674
03/16/2022 09:45:19 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
03/16/2022 09:45:21 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/16/2022 09:45:23 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/16/2022 09:45:26 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/16/2022 09:45:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/16/2022 09:45:29 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6820187165775402 on epoch=687
03/16/2022 09:45:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/16/2022 09:45:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
03/16/2022 09:45:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/16/2022 09:45:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/16/2022 09:45:41 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/16/2022 09:45:42 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7186864283638478 on epoch=699
03/16/2022 09:45:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
03/16/2022 09:45:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/16/2022 09:45:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/16/2022 09:45:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/16/2022 09:45:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=712
03/16/2022 09:45:56 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6783045807239355 on epoch=712
03/16/2022 09:45:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/16/2022 09:46:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 09:46:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
03/16/2022 09:46:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
03/16/2022 09:46:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/16/2022 09:46:09 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7508976574500769 on epoch=724
03/16/2022 09:46:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/16/2022 09:46:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 09:46:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=732
03/16/2022 09:46:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/16/2022 09:46:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/16/2022 09:46:22 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7264114114114114 on epoch=737
03/16/2022 09:46:25 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=739
03/16/2022 09:46:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=742
03/16/2022 09:46:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
03/16/2022 09:46:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
03/16/2022 09:46:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 09:46:35 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7187671727237683 on epoch=749
03/16/2022 09:46:35 - INFO - __main__ - save last model!
03/16/2022 09:46:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 09:46:35 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 09:46:35 - INFO - __main__ - Printing 3 examples
03/16/2022 09:46:35 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 09:46:35 - INFO - __main__ - ['others']
03/16/2022 09:46:35 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 09:46:35 - INFO - __main__ - ['others']
03/16/2022 09:46:35 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 09:46:35 - INFO - __main__ - ['others']
03/16/2022 09:46:35 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:46:35 - INFO - __main__ - Printing 3 examples
03/16/2022 09:46:35 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/16/2022 09:46:35 - INFO - __main__ - ['sad']
03/16/2022 09:46:35 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/16/2022 09:46:35 - INFO - __main__ - ['sad']
03/16/2022 09:46:35 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/16/2022 09:46:35 - INFO - __main__ - ['sad']
03/16/2022 09:46:35 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:46:35 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:46:35 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:46:35 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 09:46:35 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:46:35 - INFO - __main__ - Printing 3 examples
03/16/2022 09:46:35 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/16/2022 09:46:35 - INFO - __main__ - ['sad']
03/16/2022 09:46:35 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/16/2022 09:46:35 - INFO - __main__ - ['sad']
03/16/2022 09:46:35 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/16/2022 09:46:35 - INFO - __main__ - ['sad']
03/16/2022 09:46:35 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:46:35 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:46:36 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 09:46:37 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:46:43 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 09:46:54 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 09:46:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 09:46:54 - INFO - __main__ - Starting training!
03/16/2022 09:48:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_21_0.5_8_predictions.txt
03/16/2022 09:48:10 - INFO - __main__ - Classification-F1 on test data: 0.1372
03/16/2022 09:48:10 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.7566353354166154, test_performance=0.13723549291062317
03/16/2022 09:48:10 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
03/16/2022 09:48:11 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:48:11 - INFO - __main__ - Printing 3 examples
03/16/2022 09:48:11 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/16/2022 09:48:11 - INFO - __main__ - ['sad']
03/16/2022 09:48:11 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/16/2022 09:48:11 - INFO - __main__ - ['sad']
03/16/2022 09:48:11 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/16/2022 09:48:11 - INFO - __main__ - ['sad']
03/16/2022 09:48:11 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:48:11 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:48:11 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 09:48:11 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 09:48:11 - INFO - __main__ - Printing 3 examples
03/16/2022 09:48:11 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/16/2022 09:48:11 - INFO - __main__ - ['sad']
03/16/2022 09:48:11 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/16/2022 09:48:11 - INFO - __main__ - ['sad']
03/16/2022 09:48:11 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/16/2022 09:48:11 - INFO - __main__ - ['sad']
03/16/2022 09:48:11 - INFO - __main__ - Tokenizing Input ...
03/16/2022 09:48:11 - INFO - __main__ - Tokenizing Output ...
03/16/2022 09:48:12 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 09:48:27 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 09:48:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 09:48:27 - INFO - __main__ - Starting training!
03/16/2022 09:48:30 - INFO - __main__ - Step 10 Global step 10 Train loss 2.89 on epoch=2
03/16/2022 09:48:33 - INFO - __main__ - Step 20 Global step 20 Train loss 1.45 on epoch=4
03/16/2022 09:48:35 - INFO - __main__ - Step 30 Global step 30 Train loss 1.14 on epoch=7
03/16/2022 09:48:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.94 on epoch=9
03/16/2022 09:48:40 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=12
03/16/2022 09:48:41 - INFO - __main__ - Global step 50 Train loss 1.48 Classification-F1 0.13067758749069247 on epoch=12
03/16/2022 09:48:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=12, global_step=50
03/16/2022 09:48:43 - INFO - __main__ - Step 60 Global step 60 Train loss 1.03 on epoch=14
03/16/2022 09:48:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=17
03/16/2022 09:48:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=19
03/16/2022 09:48:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=22
03/16/2022 09:48:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=24
03/16/2022 09:48:54 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.3394963057686843 on epoch=24
03/16/2022 09:48:54 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.3394963057686843 on epoch=24, global_step=100
03/16/2022 09:48:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.80 on epoch=27
03/16/2022 09:48:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=29
03/16/2022 09:49:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.81 on epoch=32
03/16/2022 09:49:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.80 on epoch=34
03/16/2022 09:49:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.69 on epoch=37
03/16/2022 09:49:07 - INFO - __main__ - Global step 150 Train loss 0.78 Classification-F1 0.3273861783519378 on epoch=37
03/16/2022 09:49:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.69 on epoch=39
03/16/2022 09:49:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.63 on epoch=42
03/16/2022 09:49:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=44
03/16/2022 09:49:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=47
03/16/2022 09:49:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.61 on epoch=49
03/16/2022 09:49:20 - INFO - __main__ - Global step 200 Train loss 0.60 Classification-F1 0.6603869310765863 on epoch=49
03/16/2022 09:49:20 - INFO - __main__ - Saving model with best Classification-F1: 0.3394963057686843 -> 0.6603869310765863 on epoch=49, global_step=200
03/16/2022 09:49:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.65 on epoch=52
03/16/2022 09:49:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=54
03/16/2022 09:49:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=57
03/16/2022 09:49:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=59
03/16/2022 09:49:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=62
03/16/2022 09:49:34 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.6510984535178084 on epoch=62
03/16/2022 09:49:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=64
03/16/2022 09:49:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=67
03/16/2022 09:49:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=69
03/16/2022 09:49:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=72
03/16/2022 09:49:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=74
03/16/2022 09:49:47 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.6592105263157895 on epoch=74
03/16/2022 09:49:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=77
03/16/2022 09:49:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=79
03/16/2022 09:49:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=82
03/16/2022 09:49:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=84
03/16/2022 09:49:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=87
03/16/2022 09:50:00 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.7313137313137313 on epoch=87
03/16/2022 09:50:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6603869310765863 -> 0.7313137313137313 on epoch=87, global_step=350
03/16/2022 09:50:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=89
03/16/2022 09:50:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=92
03/16/2022 09:50:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=94
03/16/2022 09:50:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=97
03/16/2022 09:50:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=99
03/16/2022 09:50:13 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.6422077922077921 on epoch=99
03/16/2022 09:50:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=102
03/16/2022 09:50:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=104
03/16/2022 09:50:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
03/16/2022 09:50:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.17 on epoch=109
03/16/2022 09:50:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.28 on epoch=112
03/16/2022 09:50:26 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.6371848739495799 on epoch=112
03/16/2022 09:50:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=114
03/16/2022 09:50:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=117
03/16/2022 09:50:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=119
03/16/2022 09:50:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=122
03/16/2022 09:50:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=124
03/16/2022 09:50:40 - INFO - __main__ - Global step 500 Train loss 0.18 Classification-F1 0.6958333333333334 on epoch=124
03/16/2022 09:50:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=127
03/16/2022 09:50:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=129
03/16/2022 09:50:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=132
03/16/2022 09:50:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=134
03/16/2022 09:50:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=137
03/16/2022 09:50:53 - INFO - __main__ - Global step 550 Train loss 0.15 Classification-F1 0.6675743611227483 on epoch=137
03/16/2022 09:50:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=139
03/16/2022 09:50:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=142
03/16/2022 09:51:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=144
03/16/2022 09:51:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=147
03/16/2022 09:51:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=149
03/16/2022 09:51:06 - INFO - __main__ - Global step 600 Train loss 0.15 Classification-F1 0.7173655226466464 on epoch=149
03/16/2022 09:51:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=152
03/16/2022 09:51:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=154
03/16/2022 09:51:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=157
03/16/2022 09:51:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=159
03/16/2022 09:51:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=162
03/16/2022 09:51:19 - INFO - __main__ - Global step 650 Train loss 0.11 Classification-F1 0.655921052631579 on epoch=162
03/16/2022 09:51:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=164
03/16/2022 09:51:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=167
03/16/2022 09:51:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.11 on epoch=169
03/16/2022 09:51:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=172
03/16/2022 09:51:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=174
03/16/2022 09:51:32 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.6703694124746756 on epoch=174
03/16/2022 09:51:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=177
03/16/2022 09:51:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=179
03/16/2022 09:51:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=182
03/16/2022 09:51:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=184
03/16/2022 09:51:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=187
03/16/2022 09:51:44 - INFO - __main__ - Global step 750 Train loss 0.09 Classification-F1 0.6444805194805195 on epoch=187
03/16/2022 09:51:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=189
03/16/2022 09:51:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=192
03/16/2022 09:51:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=194
03/16/2022 09:51:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=197
03/16/2022 09:51:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=199
03/16/2022 09:51:57 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.742189956297554 on epoch=199
03/16/2022 09:51:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7313137313137313 -> 0.742189956297554 on epoch=199, global_step=800
03/16/2022 09:52:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=202
03/16/2022 09:52:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=204
03/16/2022 09:52:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=207
03/16/2022 09:52:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=209
03/16/2022 09:52:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=212
03/16/2022 09:52:10 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.7005910755910756 on epoch=212
03/16/2022 09:52:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=214
03/16/2022 09:52:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=217
03/16/2022 09:52:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=219
03/16/2022 09:52:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=222
03/16/2022 09:52:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=224
03/16/2022 09:52:23 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.7406494121360373 on epoch=224
03/16/2022 09:52:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=227
03/16/2022 09:52:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=229
03/16/2022 09:52:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
03/16/2022 09:52:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
03/16/2022 09:52:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=237
03/16/2022 09:52:36 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.7300067204301076 on epoch=237
03/16/2022 09:52:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=239
03/16/2022 09:52:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=242
03/16/2022 09:52:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=244
03/16/2022 09:52:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
03/16/2022 09:52:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=249
03/16/2022 09:52:49 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.7636748120300751 on epoch=249
03/16/2022 09:52:49 - INFO - __main__ - Saving model with best Classification-F1: 0.742189956297554 -> 0.7636748120300751 on epoch=249, global_step=1000
03/16/2022 09:52:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
03/16/2022 09:52:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
03/16/2022 09:52:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=257
03/16/2022 09:52:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=259
03/16/2022 09:53:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
03/16/2022 09:53:03 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.7101437165775402 on epoch=262
03/16/2022 09:53:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=264
03/16/2022 09:53:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
03/16/2022 09:53:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
03/16/2022 09:53:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=272
03/16/2022 09:53:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
03/16/2022 09:53:16 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.753921568627451 on epoch=274
03/16/2022 09:53:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
03/16/2022 09:53:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
03/16/2022 09:53:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
03/16/2022 09:53:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
03/16/2022 09:53:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=287
03/16/2022 09:53:29 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.7164713064713065 on epoch=287
03/16/2022 09:53:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
03/16/2022 09:53:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
03/16/2022 09:53:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=294
03/16/2022 09:53:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=297
03/16/2022 09:53:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=299
03/16/2022 09:53:42 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6919191919191919 on epoch=299
03/16/2022 09:53:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
03/16/2022 09:53:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
03/16/2022 09:53:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
03/16/2022 09:53:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
03/16/2022 09:53:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
03/16/2022 09:53:55 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.7400754147812971 on epoch=312
03/16/2022 09:53:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
03/16/2022 09:54:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
03/16/2022 09:54:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
03/16/2022 09:54:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
03/16/2022 09:54:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
03/16/2022 09:54:08 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.7329542847369673 on epoch=324
03/16/2022 09:54:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
03/16/2022 09:54:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
03/16/2022 09:54:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
03/16/2022 09:54:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
03/16/2022 09:54:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
03/16/2022 09:54:21 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.7446570615607119 on epoch=337
03/16/2022 09:54:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
03/16/2022 09:54:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=342
03/16/2022 09:54:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
03/16/2022 09:54:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
03/16/2022 09:54:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
03/16/2022 09:54:34 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7481060606060607 on epoch=349
03/16/2022 09:54:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
03/16/2022 09:54:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=354
03/16/2022 09:54:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
03/16/2022 09:54:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/16/2022 09:54:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
03/16/2022 09:54:47 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.7192553414327608 on epoch=362
03/16/2022 09:54:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
03/16/2022 09:54:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
03/16/2022 09:54:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
03/16/2022 09:54:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
03/16/2022 09:54:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
03/16/2022 09:55:00 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.7327075098814229 on epoch=374
03/16/2022 09:55:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
03/16/2022 09:55:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
03/16/2022 09:55:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
03/16/2022 09:55:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
03/16/2022 09:55:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
03/16/2022 09:55:13 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7435873373373373 on epoch=387
03/16/2022 09:55:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=389
03/16/2022 09:55:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
03/16/2022 09:55:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
03/16/2022 09:55:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
03/16/2022 09:55:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
03/16/2022 09:55:26 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.7359660080248316 on epoch=399
03/16/2022 09:55:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
03/16/2022 09:55:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
03/16/2022 09:55:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
03/16/2022 09:55:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
03/16/2022 09:55:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
03/16/2022 09:55:39 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7229816966659072 on epoch=412
03/16/2022 09:55:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
03/16/2022 09:55:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
03/16/2022 09:55:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 09:55:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
03/16/2022 09:55:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
03/16/2022 09:55:52 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.7341643863382994 on epoch=424
03/16/2022 09:55:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
03/16/2022 09:55:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/16/2022 09:55:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=432
03/16/2022 09:56:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
03/16/2022 09:56:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/16/2022 09:56:05 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.721875 on epoch=437
03/16/2022 09:56:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
03/16/2022 09:56:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
03/16/2022 09:56:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
03/16/2022 09:56:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
03/16/2022 09:56:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/16/2022 09:56:18 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7471205265322912 on epoch=449
03/16/2022 09:56:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
03/16/2022 09:56:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
03/16/2022 09:56:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
03/16/2022 09:56:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
03/16/2022 09:56:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
03/16/2022 09:56:32 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.7175559947299077 on epoch=462
03/16/2022 09:56:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
03/16/2022 09:56:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/16/2022 09:56:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=469
03/16/2022 09:56:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/16/2022 09:56:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
03/16/2022 09:56:45 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7051042486231314 on epoch=474
03/16/2022 09:56:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
03/16/2022 09:56:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/16/2022 09:56:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
03/16/2022 09:56:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/16/2022 09:56:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=487
03/16/2022 09:56:58 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7397983870967743 on epoch=487
03/16/2022 09:57:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
03/16/2022 09:57:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/16/2022 09:57:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
03/16/2022 09:57:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/16/2022 09:57:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 09:57:11 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7320714049995876 on epoch=499
03/16/2022 09:57:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/16/2022 09:57:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 09:57:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/16/2022 09:57:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
03/16/2022 09:57:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
03/16/2022 09:57:24 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7090524193548388 on epoch=512
03/16/2022 09:57:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/16/2022 09:57:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
03/16/2022 09:57:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/16/2022 09:57:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/16/2022 09:57:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 09:57:37 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7377622377622378 on epoch=524
03/16/2022 09:57:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/16/2022 09:57:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 09:57:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
03/16/2022 09:57:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/16/2022 09:57:49 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/16/2022 09:57:50 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.8025244495832732 on epoch=537
03/16/2022 09:57:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7636748120300751 -> 0.8025244495832732 on epoch=537, global_step=2150
03/16/2022 09:57:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/16/2022 09:57:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=542
03/16/2022 09:57:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/16/2022 09:58:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/16/2022 09:58:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/16/2022 09:58:03 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7448940417690417 on epoch=549
03/16/2022 09:58:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/16/2022 09:58:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/16/2022 09:58:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/16/2022 09:58:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/16/2022 09:58:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/16/2022 09:58:16 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.7385576923076923 on epoch=562
03/16/2022 09:58:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
03/16/2022 09:58:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.11 on epoch=567
03/16/2022 09:58:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
03/16/2022 09:58:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 09:58:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/16/2022 09:58:30 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7538607226107227 on epoch=574
03/16/2022 09:58:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
03/16/2022 09:58:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/16/2022 09:58:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/16/2022 09:58:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 09:58:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
03/16/2022 09:58:43 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7538215421303657 on epoch=587
03/16/2022 09:58:45 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 09:58:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/16/2022 09:58:50 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=594
03/16/2022 09:58:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/16/2022 09:58:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/16/2022 09:58:56 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.74585326953748 on epoch=599
03/16/2022 09:58:58 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=602
03/16/2022 09:59:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
03/16/2022 09:59:03 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/16/2022 09:59:05 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/16/2022 09:59:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
03/16/2022 09:59:09 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7352366118769883 on epoch=612
03/16/2022 09:59:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
03/16/2022 09:59:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 09:59:16 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 09:59:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/16/2022 09:59:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/16/2022 09:59:22 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6830683624801271 on epoch=624
03/16/2022 09:59:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/16/2022 09:59:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 09:59:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=632
03/16/2022 09:59:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
03/16/2022 09:59:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/16/2022 09:59:35 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7910335794236723 on epoch=637
03/16/2022 09:59:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/16/2022 09:59:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/16/2022 09:59:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/16/2022 09:59:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/16/2022 09:59:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
03/16/2022 09:59:48 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.786945126945127 on epoch=649
03/16/2022 09:59:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/16/2022 09:59:53 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/16/2022 09:59:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
03/16/2022 09:59:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/16/2022 10:00:00 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/16/2022 10:00:01 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.786945126945127 on epoch=662
03/16/2022 10:00:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/16/2022 10:00:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/16/2022 10:00:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/16/2022 10:00:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
03/16/2022 10:00:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/16/2022 10:00:14 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7875995246583483 on epoch=674
03/16/2022 10:00:17 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/16/2022 10:00:19 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/16/2022 10:00:21 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/16/2022 10:00:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/16/2022 10:00:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/16/2022 10:00:27 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7326164874551971 on epoch=687
03/16/2022 10:00:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
03/16/2022 10:00:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/16/2022 10:00:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/16/2022 10:00:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/16/2022 10:00:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/16/2022 10:00:41 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7591156597774245 on epoch=699
03/16/2022 10:00:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
03/16/2022 10:00:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/16/2022 10:00:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/16/2022 10:00:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/16/2022 10:00:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/16/2022 10:00:53 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6990253411306042 on epoch=712
03/16/2022 10:00:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/16/2022 10:00:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 10:01:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/16/2022 10:01:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
03/16/2022 10:01:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
03/16/2022 10:01:06 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7007515473032715 on epoch=724
03/16/2022 10:01:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
03/16/2022 10:01:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
03/16/2022 10:01:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 10:01:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/16/2022 10:01:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/16/2022 10:01:19 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7436336336336336 on epoch=737
03/16/2022 10:01:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
03/16/2022 10:01:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
03/16/2022 10:01:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/16/2022 10:01:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 10:01:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=749
03/16/2022 10:01:31 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.74585326953748 on epoch=749
03/16/2022 10:01:31 - INFO - __main__ - save last model!
03/16/2022 10:01:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 10:01:31 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:01:31 - INFO - __main__ - Printing 3 examples
03/16/2022 10:01:31 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/16/2022 10:01:31 - INFO - __main__ - ['sad']
03/16/2022 10:01:31 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/16/2022 10:01:31 - INFO - __main__ - ['sad']
03/16/2022 10:01:31 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/16/2022 10:01:31 - INFO - __main__ - ['sad']
03/16/2022 10:01:31 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:01:31 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 10:01:31 - INFO - __main__ - Printing 3 examples
03/16/2022 10:01:31 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 10:01:31 - INFO - __main__ - ['others']
03/16/2022 10:01:31 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 10:01:31 - INFO - __main__ - ['others']
03/16/2022 10:01:31 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 10:01:31 - INFO - __main__ - ['others']
03/16/2022 10:01:31 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:01:31 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:01:31 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 10:01:31 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:01:31 - INFO - __main__ - Printing 3 examples
03/16/2022 10:01:31 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/16/2022 10:01:31 - INFO - __main__ - ['sad']
03/16/2022 10:01:31 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/16/2022 10:01:31 - INFO - __main__ - ['sad']
03/16/2022 10:01:31 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/16/2022 10:01:31 - INFO - __main__ - ['sad']
03/16/2022 10:01:31 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:01:31 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:01:32 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 10:01:33 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:01:39 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 10:01:47 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 10:01:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 10:01:48 - INFO - __main__ - Starting training!
03/16/2022 10:03:12 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_21_0.4_8_predictions.txt
03/16/2022 10:03:12 - INFO - __main__ - Classification-F1 on test data: 0.1157
03/16/2022 10:03:12 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.8025244495832732, test_performance=0.11567056899723996
03/16/2022 10:03:12 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
03/16/2022 10:03:13 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:03:13 - INFO - __main__ - Printing 3 examples
03/16/2022 10:03:13 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/16/2022 10:03:13 - INFO - __main__ - ['sad']
03/16/2022 10:03:13 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/16/2022 10:03:13 - INFO - __main__ - ['sad']
03/16/2022 10:03:13 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/16/2022 10:03:13 - INFO - __main__ - ['sad']
03/16/2022 10:03:13 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:03:13 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:03:13 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 10:03:13 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:03:13 - INFO - __main__ - Printing 3 examples
03/16/2022 10:03:13 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/16/2022 10:03:13 - INFO - __main__ - ['sad']
03/16/2022 10:03:13 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/16/2022 10:03:13 - INFO - __main__ - ['sad']
03/16/2022 10:03:13 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/16/2022 10:03:13 - INFO - __main__ - ['sad']
03/16/2022 10:03:13 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:03:13 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:03:13 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 10:03:28 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 10:03:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 10:03:29 - INFO - __main__ - Starting training!
03/16/2022 10:03:32 - INFO - __main__ - Step 10 Global step 10 Train loss 3.17 on epoch=2
03/16/2022 10:03:35 - INFO - __main__ - Step 20 Global step 20 Train loss 1.74 on epoch=4
03/16/2022 10:03:37 - INFO - __main__ - Step 30 Global step 30 Train loss 1.17 on epoch=7
03/16/2022 10:03:40 - INFO - __main__ - Step 40 Global step 40 Train loss 1.10 on epoch=9
03/16/2022 10:03:42 - INFO - __main__ - Step 50 Global step 50 Train loss 1.05 on epoch=12
03/16/2022 10:03:43 - INFO - __main__ - Global step 50 Train loss 1.65 Classification-F1 0.18284347231715653 on epoch=12
03/16/2022 10:03:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18284347231715653 on epoch=12, global_step=50
03/16/2022 10:03:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.90 on epoch=14
03/16/2022 10:03:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=17
03/16/2022 10:03:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=19
03/16/2022 10:03:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=22
03/16/2022 10:03:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=24
03/16/2022 10:03:56 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.38888888888888884 on epoch=24
03/16/2022 10:03:56 - INFO - __main__ - Saving model with best Classification-F1: 0.18284347231715653 -> 0.38888888888888884 on epoch=24, global_step=100
03/16/2022 10:03:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.95 on epoch=27
03/16/2022 10:04:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.89 on epoch=29
03/16/2022 10:04:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.79 on epoch=32
03/16/2022 10:04:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.78 on epoch=34
03/16/2022 10:04:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.74 on epoch=37
03/16/2022 10:04:09 - INFO - __main__ - Global step 150 Train loss 0.83 Classification-F1 0.3114630467571644 on epoch=37
03/16/2022 10:04:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.76 on epoch=39
03/16/2022 10:04:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.66 on epoch=42
03/16/2022 10:04:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=44
03/16/2022 10:04:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.76 on epoch=47
03/16/2022 10:04:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.75 on epoch=49
03/16/2022 10:04:22 - INFO - __main__ - Global step 200 Train loss 0.70 Classification-F1 0.480089355089355 on epoch=49
03/16/2022 10:04:22 - INFO - __main__ - Saving model with best Classification-F1: 0.38888888888888884 -> 0.480089355089355 on epoch=49, global_step=200
03/16/2022 10:04:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.65 on epoch=52
03/16/2022 10:04:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=54
03/16/2022 10:04:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.65 on epoch=57
03/16/2022 10:04:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=59
03/16/2022 10:04:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=62
03/16/2022 10:04:35 - INFO - __main__ - Global step 250 Train loss 0.59 Classification-F1 0.5752122554448136 on epoch=62
03/16/2022 10:04:35 - INFO - __main__ - Saving model with best Classification-F1: 0.480089355089355 -> 0.5752122554448136 on epoch=62, global_step=250
03/16/2022 10:04:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.63 on epoch=64
03/16/2022 10:04:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.56 on epoch=67
03/16/2022 10:04:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.60 on epoch=69
03/16/2022 10:04:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=72
03/16/2022 10:04:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=74
03/16/2022 10:04:48 - INFO - __main__ - Global step 300 Train loss 0.56 Classification-F1 0.5833333333333334 on epoch=74
03/16/2022 10:04:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5752122554448136 -> 0.5833333333333334 on epoch=74, global_step=300
03/16/2022 10:04:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=77
03/16/2022 10:04:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=79
03/16/2022 10:04:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=82
03/16/2022 10:04:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=84
03/16/2022 10:05:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=87
03/16/2022 10:05:01 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.641454442558008 on epoch=87
03/16/2022 10:05:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5833333333333334 -> 0.641454442558008 on epoch=87, global_step=350
03/16/2022 10:05:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=89
03/16/2022 10:05:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=92
03/16/2022 10:05:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=94
03/16/2022 10:05:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=97
03/16/2022 10:05:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=99
03/16/2022 10:05:14 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.6419358382773017 on epoch=99
03/16/2022 10:05:14 - INFO - __main__ - Saving model with best Classification-F1: 0.641454442558008 -> 0.6419358382773017 on epoch=99, global_step=400
03/16/2022 10:05:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=102
03/16/2022 10:05:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=104
03/16/2022 10:05:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=107
03/16/2022 10:05:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=109
03/16/2022 10:05:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=112
03/16/2022 10:05:27 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.6503762285012286 on epoch=112
03/16/2022 10:05:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6419358382773017 -> 0.6503762285012286 on epoch=112, global_step=450
03/16/2022 10:05:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.31 on epoch=114
03/16/2022 10:05:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=117
03/16/2022 10:05:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=119
03/16/2022 10:05:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=122
03/16/2022 10:05:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=124
03/16/2022 10:05:41 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.6266436784528889 on epoch=124
03/16/2022 10:05:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=127
03/16/2022 10:05:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.29 on epoch=129
03/16/2022 10:05:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=132
03/16/2022 10:05:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=134
03/16/2022 10:05:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=137
03/16/2022 10:05:54 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.6275027056277056 on epoch=137
03/16/2022 10:05:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=139
03/16/2022 10:05:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=142
03/16/2022 10:06:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=144
03/16/2022 10:06:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=147
03/16/2022 10:06:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=149
03/16/2022 10:06:07 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.6118046303887409 on epoch=149
03/16/2022 10:06:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=152
03/16/2022 10:06:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=154
03/16/2022 10:06:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=157
03/16/2022 10:06:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=159
03/16/2022 10:06:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=162
03/16/2022 10:06:20 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.6532064834390415 on epoch=162
03/16/2022 10:06:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6503762285012286 -> 0.6532064834390415 on epoch=162, global_step=650
03/16/2022 10:06:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=164
03/16/2022 10:06:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
03/16/2022 10:06:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=169
03/16/2022 10:06:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=172
03/16/2022 10:06:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=174
03/16/2022 10:06:33 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.6141117898083842 on epoch=174
03/16/2022 10:06:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=177
03/16/2022 10:06:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=179
03/16/2022 10:06:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=182
03/16/2022 10:06:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=184
03/16/2022 10:06:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=187
03/16/2022 10:06:46 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.6866313638052768 on epoch=187
03/16/2022 10:06:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6532064834390415 -> 0.6866313638052768 on epoch=187, global_step=750
03/16/2022 10:06:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=189
03/16/2022 10:06:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=192
03/16/2022 10:06:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=194
03/16/2022 10:06:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=197
03/16/2022 10:06:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=199
03/16/2022 10:07:00 - INFO - __main__ - Global step 800 Train loss 0.14 Classification-F1 0.6559577677224736 on epoch=199
03/16/2022 10:07:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=202
03/16/2022 10:07:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=204
03/16/2022 10:07:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=207
03/16/2022 10:07:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=209
03/16/2022 10:07:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=212
03/16/2022 10:07:13 - INFO - __main__ - Global step 850 Train loss 0.13 Classification-F1 0.6803990364474236 on epoch=212
03/16/2022 10:07:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=214
03/16/2022 10:07:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.15 on epoch=217
03/16/2022 10:07:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=219
03/16/2022 10:07:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
03/16/2022 10:07:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=224
03/16/2022 10:07:26 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.7436336336336336 on epoch=224
03/16/2022 10:07:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6866313638052768 -> 0.7436336336336336 on epoch=224, global_step=900
03/16/2022 10:07:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=227
03/16/2022 10:07:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
03/16/2022 10:07:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
03/16/2022 10:07:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=234
03/16/2022 10:07:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=237
03/16/2022 10:07:40 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.6721639471639472 on epoch=237
03/16/2022 10:07:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=239
03/16/2022 10:07:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
03/16/2022 10:07:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=244
03/16/2022 10:07:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=247
03/16/2022 10:07:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=249
03/16/2022 10:07:53 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.7382357226107227 on epoch=249
03/16/2022 10:07:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
03/16/2022 10:07:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
03/16/2022 10:08:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
03/16/2022 10:08:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=259
03/16/2022 10:08:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=262
03/16/2022 10:08:06 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.7024044795783926 on epoch=262
03/16/2022 10:08:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
03/16/2022 10:08:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=267
03/16/2022 10:08:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
03/16/2022 10:08:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
03/16/2022 10:08:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
03/16/2022 10:08:19 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.7028779644268774 on epoch=274
03/16/2022 10:08:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=277
03/16/2022 10:08:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=279
03/16/2022 10:08:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=282
03/16/2022 10:08:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
03/16/2022 10:08:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
03/16/2022 10:08:32 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.7020051477274478 on epoch=287
03/16/2022 10:08:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
03/16/2022 10:08:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
03/16/2022 10:08:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=294
03/16/2022 10:08:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
03/16/2022 10:08:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
03/16/2022 10:08:46 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.7528400168055573 on epoch=299
03/16/2022 10:08:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7436336336336336 -> 0.7528400168055573 on epoch=299, global_step=1200
03/16/2022 10:08:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
03/16/2022 10:08:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
03/16/2022 10:08:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=307
03/16/2022 10:08:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=309
03/16/2022 10:08:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
03/16/2022 10:08:59 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.7391443863218057 on epoch=312
03/16/2022 10:09:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
03/16/2022 10:09:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
03/16/2022 10:09:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
03/16/2022 10:09:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
03/16/2022 10:09:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
03/16/2022 10:09:12 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7426854395604396 on epoch=324
03/16/2022 10:09:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
03/16/2022 10:09:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
03/16/2022 10:09:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
03/16/2022 10:09:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
03/16/2022 10:09:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=337
03/16/2022 10:09:26 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.7015303537042668 on epoch=337
03/16/2022 10:09:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
03/16/2022 10:09:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
03/16/2022 10:09:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
03/16/2022 10:09:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
03/16/2022 10:09:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
03/16/2022 10:09:39 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7436823033597227 on epoch=349
03/16/2022 10:09:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=352
03/16/2022 10:09:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
03/16/2022 10:09:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
03/16/2022 10:09:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
03/16/2022 10:09:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
03/16/2022 10:09:52 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.7033117389653892 on epoch=362
03/16/2022 10:09:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
03/16/2022 10:09:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
03/16/2022 10:09:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
03/16/2022 10:10:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
03/16/2022 10:10:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
03/16/2022 10:10:05 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.7226399786883658 on epoch=374
03/16/2022 10:10:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
03/16/2022 10:10:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
03/16/2022 10:10:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
03/16/2022 10:10:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
03/16/2022 10:10:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
03/16/2022 10:10:19 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6993506493506494 on epoch=387
03/16/2022 10:10:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
03/16/2022 10:10:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
03/16/2022 10:10:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
03/16/2022 10:10:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=397
03/16/2022 10:10:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
03/16/2022 10:10:32 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.7091264651748523 on epoch=399
03/16/2022 10:10:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/16/2022 10:10:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
03/16/2022 10:10:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
03/16/2022 10:10:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
03/16/2022 10:10:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
03/16/2022 10:10:45 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7877027027027027 on epoch=412
03/16/2022 10:10:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7528400168055573 -> 0.7877027027027027 on epoch=412, global_step=1650
03/16/2022 10:10:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
03/16/2022 10:10:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
03/16/2022 10:10:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 10:10:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
03/16/2022 10:10:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
03/16/2022 10:10:59 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6878595946387709 on epoch=424
03/16/2022 10:11:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
03/16/2022 10:11:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/16/2022 10:11:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
03/16/2022 10:11:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
03/16/2022 10:11:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/16/2022 10:11:12 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7150243968555163 on epoch=437
03/16/2022 10:11:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=439
03/16/2022 10:11:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
03/16/2022 10:11:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
03/16/2022 10:11:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/16/2022 10:11:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=449
03/16/2022 10:11:25 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.6574092574092575 on epoch=449
03/16/2022 10:11:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
03/16/2022 10:11:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
03/16/2022 10:11:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
03/16/2022 10:11:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
03/16/2022 10:11:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/16/2022 10:11:39 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7167332044164431 on epoch=462
03/16/2022 10:11:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
03/16/2022 10:11:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
03/16/2022 10:11:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
03/16/2022 10:11:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
03/16/2022 10:11:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=474
03/16/2022 10:11:52 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7170439455046231 on epoch=474
03/16/2022 10:11:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
03/16/2022 10:11:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/16/2022 10:11:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=482
03/16/2022 10:12:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/16/2022 10:12:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
03/16/2022 10:12:05 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7238618082368082 on epoch=487
03/16/2022 10:12:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/16/2022 10:12:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
03/16/2022 10:12:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
03/16/2022 10:12:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/16/2022 10:12:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
03/16/2022 10:12:18 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7310737420663482 on epoch=499
03/16/2022 10:12:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
03/16/2022 10:12:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=504
03/16/2022 10:12:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
03/16/2022 10:12:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/16/2022 10:12:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
03/16/2022 10:12:32 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.6977513227513228 on epoch=512
03/16/2022 10:12:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/16/2022 10:12:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
03/16/2022 10:12:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/16/2022 10:12:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/16/2022 10:12:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 10:12:45 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7286816801619433 on epoch=524
03/16/2022 10:12:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/16/2022 10:12:50 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 10:12:52 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 10:12:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
03/16/2022 10:12:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/16/2022 10:12:58 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7131251066734938 on epoch=537
03/16/2022 10:13:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
03/16/2022 10:13:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
03/16/2022 10:13:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/16/2022 10:13:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
03/16/2022 10:13:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/16/2022 10:13:12 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7253530950305144 on epoch=549
03/16/2022 10:13:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/16/2022 10:13:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/16/2022 10:13:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/16/2022 10:13:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/16/2022 10:13:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/16/2022 10:13:25 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7281639044506691 on epoch=562
03/16/2022 10:13:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
03/16/2022 10:13:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/16/2022 10:13:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
03/16/2022 10:13:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 10:13:37 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
03/16/2022 10:13:38 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6709013209013209 on epoch=574
03/16/2022 10:13:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
03/16/2022 10:13:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
03/16/2022 10:13:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/16/2022 10:13:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 10:13:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
03/16/2022 10:13:52 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7035277255865492 on epoch=587
03/16/2022 10:13:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 10:13:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/16/2022 10:13:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/16/2022 10:14:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
03/16/2022 10:14:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
03/16/2022 10:14:05 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7179356776130971 on epoch=599
03/16/2022 10:14:08 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/16/2022 10:14:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/16/2022 10:14:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/16/2022 10:14:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/16/2022 10:14:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
03/16/2022 10:14:19 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7381134878193703 on epoch=612
03/16/2022 10:14:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
03/16/2022 10:14:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 10:14:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 10:14:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/16/2022 10:14:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
03/16/2022 10:14:32 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7242618492618493 on epoch=624
03/16/2022 10:14:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
03/16/2022 10:14:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 10:14:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/16/2022 10:14:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
03/16/2022 10:14:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/16/2022 10:14:45 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6793386793386793 on epoch=637
03/16/2022 10:14:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
03/16/2022 10:14:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/16/2022 10:14:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/16/2022 10:14:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=647
03/16/2022 10:14:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/16/2022 10:14:59 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6730994152046784 on epoch=649
03/16/2022 10:15:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/16/2022 10:15:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
03/16/2022 10:15:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/16/2022 10:15:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/16/2022 10:15:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/16/2022 10:15:12 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7035119969040248 on epoch=662
03/16/2022 10:15:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/16/2022 10:15:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
03/16/2022 10:15:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/16/2022 10:15:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
03/16/2022 10:15:24 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/16/2022 10:15:25 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7426872895622896 on epoch=674
03/16/2022 10:15:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/16/2022 10:15:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=679
03/16/2022 10:15:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/16/2022 10:15:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=684
03/16/2022 10:15:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/16/2022 10:15:39 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7762740580103009 on epoch=687
03/16/2022 10:15:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/16/2022 10:15:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
03/16/2022 10:15:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
03/16/2022 10:15:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/16/2022 10:15:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
03/16/2022 10:15:52 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7382800982800983 on epoch=699
03/16/2022 10:15:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
03/16/2022 10:15:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
03/16/2022 10:16:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/16/2022 10:16:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/16/2022 10:16:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/16/2022 10:16:06 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6987620343455583 on epoch=712
03/16/2022 10:16:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/16/2022 10:16:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 10:16:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/16/2022 10:16:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
03/16/2022 10:16:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
03/16/2022 10:16:20 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7403003003003003 on epoch=724
03/16/2022 10:16:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/16/2022 10:16:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
03/16/2022 10:16:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 10:16:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/16/2022 10:16:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/16/2022 10:16:33 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6730994152046784 on epoch=737
03/16/2022 10:16:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
03/16/2022 10:16:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
03/16/2022 10:16:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
03/16/2022 10:16:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 10:16:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 10:16:46 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:16:46 - INFO - __main__ - Printing 3 examples
03/16/2022 10:16:46 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/16/2022 10:16:46 - INFO - __main__ - ['sad']
03/16/2022 10:16:46 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/16/2022 10:16:46 - INFO - __main__ - ['sad']
03/16/2022 10:16:46 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/16/2022 10:16:46 - INFO - __main__ - ['sad']
03/16/2022 10:16:46 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:16:46 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:16:46 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6929487179487179 on epoch=749
03/16/2022 10:16:46 - INFO - __main__ - save last model!
03/16/2022 10:16:46 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 10:16:46 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:16:46 - INFO - __main__ - Printing 3 examples
03/16/2022 10:16:46 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/16/2022 10:16:46 - INFO - __main__ - ['sad']
03/16/2022 10:16:46 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/16/2022 10:16:46 - INFO - __main__ - ['sad']
03/16/2022 10:16:46 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/16/2022 10:16:46 - INFO - __main__ - ['sad']
03/16/2022 10:16:46 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:16:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 10:16:46 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:16:46 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 10:16:47 - INFO - __main__ - Printing 3 examples
03/16/2022 10:16:47 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 10:16:47 - INFO - __main__ - ['others']
03/16/2022 10:16:47 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 10:16:47 - INFO - __main__ - ['others']
03/16/2022 10:16:47 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 10:16:47 - INFO - __main__ - ['others']
03/16/2022 10:16:47 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:16:47 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 10:16:49 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:16:54 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 10:17:05 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 10:17:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 10:17:06 - INFO - __main__ - Starting training!
03/16/2022 10:18:42 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_21_0.3_8_predictions.txt
03/16/2022 10:18:42 - INFO - __main__ - Classification-F1 on test data: 0.2443
03/16/2022 10:18:42 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.7877027027027027, test_performance=0.2443444776060689
03/16/2022 10:18:42 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
03/16/2022 10:18:43 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:18:43 - INFO - __main__ - Printing 3 examples
03/16/2022 10:18:43 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/16/2022 10:18:43 - INFO - __main__ - ['sad']
03/16/2022 10:18:43 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/16/2022 10:18:43 - INFO - __main__ - ['sad']
03/16/2022 10:18:43 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/16/2022 10:18:43 - INFO - __main__ - ['sad']
03/16/2022 10:18:43 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:18:43 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:18:43 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 10:18:43 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:18:43 - INFO - __main__ - Printing 3 examples
03/16/2022 10:18:43 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/16/2022 10:18:43 - INFO - __main__ - ['sad']
03/16/2022 10:18:43 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/16/2022 10:18:43 - INFO - __main__ - ['sad']
03/16/2022 10:18:43 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/16/2022 10:18:43 - INFO - __main__ - ['sad']
03/16/2022 10:18:43 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:18:43 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:18:43 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 10:18:58 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 10:18:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 10:18:59 - INFO - __main__ - Starting training!
03/16/2022 10:19:02 - INFO - __main__ - Step 10 Global step 10 Train loss 3.26 on epoch=2
03/16/2022 10:19:04 - INFO - __main__ - Step 20 Global step 20 Train loss 2.09 on epoch=4
03/16/2022 10:19:06 - INFO - __main__ - Step 30 Global step 30 Train loss 1.55 on epoch=7
03/16/2022 10:19:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.14 on epoch=9
03/16/2022 10:19:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.08 on epoch=12
03/16/2022 10:19:12 - INFO - __main__ - Global step 50 Train loss 1.82 Classification-F1 0.1 on epoch=12
03/16/2022 10:19:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/16/2022 10:19:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=14
03/16/2022 10:19:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=17
03/16/2022 10:19:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=19
03/16/2022 10:19:22 - INFO - __main__ - Step 90 Global step 90 Train loss 1.01 on epoch=22
03/16/2022 10:19:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=24
03/16/2022 10:19:25 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.2753554821753487 on epoch=24
03/16/2022 10:19:25 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.2753554821753487 on epoch=24, global_step=100
03/16/2022 10:19:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=27
03/16/2022 10:19:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.85 on epoch=29
03/16/2022 10:19:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.91 on epoch=32
03/16/2022 10:19:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=34
03/16/2022 10:19:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=37
03/16/2022 10:19:39 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.2526371694381364 on epoch=37
03/16/2022 10:19:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.86 on epoch=39
03/16/2022 10:19:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=42
03/16/2022 10:19:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.82 on epoch=44
03/16/2022 10:19:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.87 on epoch=47
03/16/2022 10:19:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.82 on epoch=49
03/16/2022 10:19:52 - INFO - __main__ - Global step 200 Train loss 0.83 Classification-F1 0.32765151515151514 on epoch=49
03/16/2022 10:19:52 - INFO - __main__ - Saving model with best Classification-F1: 0.2753554821753487 -> 0.32765151515151514 on epoch=49, global_step=200
03/16/2022 10:19:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=52
03/16/2022 10:19:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.76 on epoch=54
03/16/2022 10:19:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.79 on epoch=57
03/16/2022 10:20:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.75 on epoch=59
03/16/2022 10:20:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.75 on epoch=62
03/16/2022 10:20:05 - INFO - __main__ - Global step 250 Train loss 0.77 Classification-F1 0.5181008075744917 on epoch=62
03/16/2022 10:20:05 - INFO - __main__ - Saving model with best Classification-F1: 0.32765151515151514 -> 0.5181008075744917 on epoch=62, global_step=250
03/16/2022 10:20:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.70 on epoch=64
03/16/2022 10:20:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=67
03/16/2022 10:20:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.67 on epoch=69
03/16/2022 10:20:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.77 on epoch=72
03/16/2022 10:20:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.65 on epoch=74
03/16/2022 10:20:18 - INFO - __main__ - Global step 300 Train loss 0.69 Classification-F1 0.7494068242707081 on epoch=74
03/16/2022 10:20:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5181008075744917 -> 0.7494068242707081 on epoch=74, global_step=300
03/16/2022 10:20:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=77
03/16/2022 10:20:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=79
03/16/2022 10:20:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.68 on epoch=82
03/16/2022 10:20:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=84
03/16/2022 10:20:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.59 on epoch=87
03/16/2022 10:20:31 - INFO - __main__ - Global step 350 Train loss 0.59 Classification-F1 0.6875653082549634 on epoch=87
03/16/2022 10:20:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=89
03/16/2022 10:20:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=92
03/16/2022 10:20:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=94
03/16/2022 10:20:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.55 on epoch=97
03/16/2022 10:20:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.54 on epoch=99
03/16/2022 10:20:44 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.5698642005157962 on epoch=99
03/16/2022 10:20:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.58 on epoch=102
03/16/2022 10:20:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=104
03/16/2022 10:20:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=107
03/16/2022 10:20:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=109
03/16/2022 10:20:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=112
03/16/2022 10:20:56 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.7474137931034482 on epoch=112
03/16/2022 10:20:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=114
03/16/2022 10:21:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=117
03/16/2022 10:21:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=119
03/16/2022 10:21:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=122
03/16/2022 10:21:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=124
03/16/2022 10:21:09 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.6993120393120393 on epoch=124
03/16/2022 10:21:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=127
03/16/2022 10:21:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=129
03/16/2022 10:21:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=132
03/16/2022 10:21:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=134
03/16/2022 10:21:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=137
03/16/2022 10:21:22 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.7653439153439152 on epoch=137
03/16/2022 10:21:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7494068242707081 -> 0.7653439153439152 on epoch=137, global_step=550
03/16/2022 10:21:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=139
03/16/2022 10:21:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=142
03/16/2022 10:21:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=144
03/16/2022 10:21:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=147
03/16/2022 10:21:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=149
03/16/2022 10:21:35 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.60489365706757 on epoch=149
03/16/2022 10:21:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=152
03/16/2022 10:21:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=154
03/16/2022 10:21:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=157
03/16/2022 10:21:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=159
03/16/2022 10:21:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=162
03/16/2022 10:21:48 - INFO - __main__ - Global step 650 Train loss 0.32 Classification-F1 0.6705412679425837 on epoch=162
03/16/2022 10:21:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=164
03/16/2022 10:21:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=167
03/16/2022 10:21:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=169
03/16/2022 10:21:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=172
03/16/2022 10:22:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.32 on epoch=174
03/16/2022 10:22:01 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.6180555555555556 on epoch=174
03/16/2022 10:22:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=177
03/16/2022 10:22:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=179
03/16/2022 10:22:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=182
03/16/2022 10:22:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=184
03/16/2022 10:22:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=187
03/16/2022 10:22:14 - INFO - __main__ - Global step 750 Train loss 0.24 Classification-F1 0.7034283205619413 on epoch=187
03/16/2022 10:22:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=189
03/16/2022 10:22:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=192
03/16/2022 10:22:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=194
03/16/2022 10:22:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=197
03/16/2022 10:22:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=199
03/16/2022 10:22:27 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.6033993783993784 on epoch=199
03/16/2022 10:22:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=202
03/16/2022 10:22:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=204
03/16/2022 10:22:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=207
03/16/2022 10:22:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=209
03/16/2022 10:22:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=212
03/16/2022 10:22:40 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.6628378378378378 on epoch=212
03/16/2022 10:22:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=214
03/16/2022 10:22:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.27 on epoch=217
03/16/2022 10:22:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=219
03/16/2022 10:22:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=222
03/16/2022 10:22:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=224
03/16/2022 10:22:53 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.623480685980686 on epoch=224
03/16/2022 10:22:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=227
03/16/2022 10:22:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
03/16/2022 10:23:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=232
03/16/2022 10:23:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
03/16/2022 10:23:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=237
03/16/2022 10:23:06 - INFO - __main__ - Global step 950 Train loss 0.13 Classification-F1 0.6666066066066066 on epoch=237
03/16/2022 10:23:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=239
03/16/2022 10:23:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=242
03/16/2022 10:23:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=244
03/16/2022 10:23:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
03/16/2022 10:23:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=249
03/16/2022 10:23:19 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.6733640755379885 on epoch=249
03/16/2022 10:23:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=252
03/16/2022 10:23:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=254
03/16/2022 10:23:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=257
03/16/2022 10:23:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=259
03/16/2022 10:23:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=262
03/16/2022 10:23:32 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.7095588235294118 on epoch=262
03/16/2022 10:23:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=264
03/16/2022 10:23:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
03/16/2022 10:23:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=269
03/16/2022 10:23:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
03/16/2022 10:23:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=274
03/16/2022 10:23:45 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.6391167312219944 on epoch=274
03/16/2022 10:23:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
03/16/2022 10:23:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=279
03/16/2022 10:23:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
03/16/2022 10:23:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=284
03/16/2022 10:23:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=287
03/16/2022 10:23:58 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.6715768461499011 on epoch=287
03/16/2022 10:24:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=289
03/16/2022 10:24:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=292
03/16/2022 10:24:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=294
03/16/2022 10:24:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=297
03/16/2022 10:24:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
03/16/2022 10:24:11 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.6340421303656598 on epoch=299
03/16/2022 10:24:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
03/16/2022 10:24:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=304
03/16/2022 10:24:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
03/16/2022 10:24:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
03/16/2022 10:24:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
03/16/2022 10:24:24 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.6736090225563911 on epoch=312
03/16/2022 10:24:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
03/16/2022 10:24:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
03/16/2022 10:24:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
03/16/2022 10:24:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=322
03/16/2022 10:24:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
03/16/2022 10:24:37 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.6679094272844273 on epoch=324
03/16/2022 10:24:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
03/16/2022 10:24:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
03/16/2022 10:24:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
03/16/2022 10:24:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=334
03/16/2022 10:24:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
03/16/2022 10:24:51 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.7017045454545454 on epoch=337
03/16/2022 10:24:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=339
03/16/2022 10:24:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=342
03/16/2022 10:24:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=344
03/16/2022 10:25:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
03/16/2022 10:25:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
03/16/2022 10:25:04 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.6684027777777777 on epoch=349
03/16/2022 10:25:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
03/16/2022 10:25:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=354
03/16/2022 10:25:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=357
03/16/2022 10:25:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
03/16/2022 10:25:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
03/16/2022 10:25:17 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.6889553752535498 on epoch=362
03/16/2022 10:25:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
03/16/2022 10:25:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=367
03/16/2022 10:25:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=369
03/16/2022 10:25:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
03/16/2022 10:25:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
03/16/2022 10:25:30 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.7028592375366569 on epoch=374
03/16/2022 10:25:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=377
03/16/2022 10:25:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
03/16/2022 10:25:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=382
03/16/2022 10:25:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
03/16/2022 10:25:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=387
03/16/2022 10:25:43 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.7124972437472438 on epoch=387
03/16/2022 10:25:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
03/16/2022 10:25:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=392
03/16/2022 10:25:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
03/16/2022 10:25:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
03/16/2022 10:25:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=399
03/16/2022 10:25:57 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.618150918904147 on epoch=399
03/16/2022 10:25:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=402
03/16/2022 10:26:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
03/16/2022 10:26:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=407
03/16/2022 10:26:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
03/16/2022 10:26:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
03/16/2022 10:26:10 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.6434334956074086 on epoch=412
03/16/2022 10:26:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
03/16/2022 10:26:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
03/16/2022 10:26:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
03/16/2022 10:26:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=422
03/16/2022 10:26:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
03/16/2022 10:26:23 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6662833554474422 on epoch=424
03/16/2022 10:26:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
03/16/2022 10:26:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
03/16/2022 10:26:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
03/16/2022 10:26:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
03/16/2022 10:26:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/16/2022 10:26:37 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.644952817650186 on epoch=437
03/16/2022 10:26:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=439
03/16/2022 10:26:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
03/16/2022 10:26:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=444
03/16/2022 10:26:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
03/16/2022 10:26:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
03/16/2022 10:26:50 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.6486925708699902 on epoch=449
03/16/2022 10:26:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
03/16/2022 10:26:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
03/16/2022 10:26:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
03/16/2022 10:27:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
03/16/2022 10:27:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/16/2022 10:27:03 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.6800792545574329 on epoch=462
03/16/2022 10:27:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
03/16/2022 10:27:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
03/16/2022 10:27:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
03/16/2022 10:27:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
03/16/2022 10:27:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=474
03/16/2022 10:27:17 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.681991216668636 on epoch=474
03/16/2022 10:27:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
03/16/2022 10:27:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
03/16/2022 10:27:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
03/16/2022 10:27:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=484
03/16/2022 10:27:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/16/2022 10:27:30 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7070279198635976 on epoch=487
03/16/2022 10:27:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/16/2022 10:27:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
03/16/2022 10:27:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
03/16/2022 10:27:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
03/16/2022 10:27:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
03/16/2022 10:27:43 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.6785624943611952 on epoch=499
03/16/2022 10:27:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
03/16/2022 10:27:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 10:27:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
03/16/2022 10:27:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/16/2022 10:27:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
03/16/2022 10:27:56 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6831860269360269 on epoch=512
03/16/2022 10:27:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=514
03/16/2022 10:28:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/16/2022 10:28:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/16/2022 10:28:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
03/16/2022 10:28:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 10:28:09 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.6631308728082922 on epoch=524
03/16/2022 10:28:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/16/2022 10:28:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 10:28:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 10:28:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/16/2022 10:28:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/16/2022 10:28:23 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6695259042033236 on epoch=537
03/16/2022 10:28:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
03/16/2022 10:28:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/16/2022 10:28:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
03/16/2022 10:28:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
03/16/2022 10:28:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
03/16/2022 10:28:36 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7127649039413746 on epoch=549
03/16/2022 10:28:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/16/2022 10:28:41 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/16/2022 10:28:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/16/2022 10:28:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
03/16/2022 10:28:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/16/2022 10:28:49 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6919025003264134 on epoch=562
03/16/2022 10:28:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/16/2022 10:28:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/16/2022 10:28:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
03/16/2022 10:28:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 10:29:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
03/16/2022 10:29:03 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6906498363721365 on epoch=574
03/16/2022 10:29:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/16/2022 10:29:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/16/2022 10:29:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=582
03/16/2022 10:29:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
03/16/2022 10:29:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
03/16/2022 10:29:16 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6536534701857283 on epoch=587
03/16/2022 10:29:18 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
03/16/2022 10:29:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
03/16/2022 10:29:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/16/2022 10:29:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/16/2022 10:29:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/16/2022 10:29:29 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7183333333333334 on epoch=599
03/16/2022 10:29:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/16/2022 10:29:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=604
03/16/2022 10:29:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/16/2022 10:29:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/16/2022 10:29:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/16/2022 10:29:43 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6631308728082922 on epoch=612
03/16/2022 10:29:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
03/16/2022 10:29:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 10:29:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
03/16/2022 10:29:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
03/16/2022 10:29:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=624
03/16/2022 10:29:56 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6531094501682737 on epoch=624
03/16/2022 10:29:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/16/2022 10:30:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 10:30:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/16/2022 10:30:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
03/16/2022 10:30:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/16/2022 10:30:10 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.685870870870871 on epoch=637
03/16/2022 10:30:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
03/16/2022 10:30:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
03/16/2022 10:30:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
03/16/2022 10:30:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/16/2022 10:30:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/16/2022 10:30:23 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6678784775558969 on epoch=649
03/16/2022 10:30:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/16/2022 10:30:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/16/2022 10:30:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
03/16/2022 10:30:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
03/16/2022 10:30:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/16/2022 10:30:36 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7124972437472438 on epoch=662
03/16/2022 10:30:39 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/16/2022 10:30:41 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/16/2022 10:30:44 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/16/2022 10:30:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
03/16/2022 10:30:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
03/16/2022 10:30:50 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.684469696969697 on epoch=674
03/16/2022 10:30:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/16/2022 10:30:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/16/2022 10:30:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=682
03/16/2022 10:31:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=684
03/16/2022 10:31:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/16/2022 10:31:03 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7345408544649532 on epoch=687
03/16/2022 10:31:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=689
03/16/2022 10:31:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
03/16/2022 10:31:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=694
03/16/2022 10:31:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/16/2022 10:31:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/16/2022 10:31:17 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7501571009635526 on epoch=699
03/16/2022 10:31:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
03/16/2022 10:31:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/16/2022 10:31:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
03/16/2022 10:31:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/16/2022 10:31:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/16/2022 10:31:30 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7476749340077015 on epoch=712
03/16/2022 10:31:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/16/2022 10:31:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 10:31:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/16/2022 10:31:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/16/2022 10:31:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/16/2022 10:31:44 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7056695992179864 on epoch=724
03/16/2022 10:31:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=727
03/16/2022 10:31:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 10:31:51 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
03/16/2022 10:31:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=734
03/16/2022 10:31:56 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
03/16/2022 10:31:57 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.6896557683002695 on epoch=737
03/16/2022 10:32:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/16/2022 10:32:02 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
03/16/2022 10:32:05 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
03/16/2022 10:32:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
03/16/2022 10:32:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 10:32:11 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7044081514669749 on epoch=749
03/16/2022 10:32:11 - INFO - __main__ - save last model!
03/16/2022 10:32:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 10:32:11 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 10:32:11 - INFO - __main__ - Printing 3 examples
03/16/2022 10:32:11 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 10:32:11 - INFO - __main__ - ['others']
03/16/2022 10:32:11 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 10:32:11 - INFO - __main__ - ['others']
03/16/2022 10:32:11 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 10:32:11 - INFO - __main__ - ['others']
03/16/2022 10:32:11 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:32:11 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:32:11 - INFO - __main__ - Printing 3 examples
03/16/2022 10:32:11 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/16/2022 10:32:11 - INFO - __main__ - ['happy']
03/16/2022 10:32:11 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/16/2022 10:32:11 - INFO - __main__ - ['happy']
03/16/2022 10:32:11 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/16/2022 10:32:11 - INFO - __main__ - ['happy']
03/16/2022 10:32:11 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:32:11 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:32:11 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 10:32:11 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:32:11 - INFO - __main__ - Printing 3 examples
03/16/2022 10:32:11 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/16/2022 10:32:11 - INFO - __main__ - ['happy']
03/16/2022 10:32:11 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/16/2022 10:32:11 - INFO - __main__ - ['happy']
03/16/2022 10:32:11 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/16/2022 10:32:11 - INFO - __main__ - ['happy']
03/16/2022 10:32:11 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:32:11 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:32:11 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 10:32:13 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:32:18 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 10:32:29 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 10:32:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 10:32:30 - INFO - __main__ - Starting training!
03/16/2022 10:33:56 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_21_0.2_8_predictions.txt
03/16/2022 10:33:56 - INFO - __main__ - Classification-F1 on test data: 0.1638
03/16/2022 10:33:57 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.7653439153439152, test_performance=0.1637805452145416
03/16/2022 10:33:57 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
03/16/2022 10:33:57 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:33:57 - INFO - __main__ - Printing 3 examples
03/16/2022 10:33:57 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/16/2022 10:33:57 - INFO - __main__ - ['happy']
03/16/2022 10:33:57 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/16/2022 10:33:57 - INFO - __main__ - ['happy']
03/16/2022 10:33:57 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/16/2022 10:33:57 - INFO - __main__ - ['happy']
03/16/2022 10:33:57 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:33:57 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:33:58 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 10:33:58 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:33:58 - INFO - __main__ - Printing 3 examples
03/16/2022 10:33:58 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/16/2022 10:33:58 - INFO - __main__ - ['happy']
03/16/2022 10:33:58 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/16/2022 10:33:58 - INFO - __main__ - ['happy']
03/16/2022 10:33:58 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/16/2022 10:33:58 - INFO - __main__ - ['happy']
03/16/2022 10:33:58 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:33:58 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:33:58 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 10:34:12 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 10:34:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 10:34:13 - INFO - __main__ - Starting training!
03/16/2022 10:34:16 - INFO - __main__ - Step 10 Global step 10 Train loss 2.37 on epoch=2
03/16/2022 10:34:18 - INFO - __main__ - Step 20 Global step 20 Train loss 1.33 on epoch=4
03/16/2022 10:34:21 - INFO - __main__ - Step 30 Global step 30 Train loss 0.99 on epoch=7
03/16/2022 10:34:23 - INFO - __main__ - Step 40 Global step 40 Train loss 0.95 on epoch=9
03/16/2022 10:34:26 - INFO - __main__ - Step 50 Global step 50 Train loss 0.93 on epoch=12
03/16/2022 10:34:27 - INFO - __main__ - Global step 50 Train loss 1.31 Classification-F1 0.1565276828434723 on epoch=12
03/16/2022 10:34:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1565276828434723 on epoch=12, global_step=50
03/16/2022 10:34:29 - INFO - __main__ - Step 60 Global step 60 Train loss 0.87 on epoch=14
03/16/2022 10:34:32 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=17
03/16/2022 10:34:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.89 on epoch=19
03/16/2022 10:34:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.83 on epoch=22
03/16/2022 10:34:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.76 on epoch=24
03/16/2022 10:34:40 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.3937432014057401 on epoch=24
03/16/2022 10:34:40 - INFO - __main__ - Saving model with best Classification-F1: 0.1565276828434723 -> 0.3937432014057401 on epoch=24, global_step=100
03/16/2022 10:34:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.78 on epoch=27
03/16/2022 10:34:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.77 on epoch=29
03/16/2022 10:34:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.68 on epoch=32
03/16/2022 10:34:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.73 on epoch=34
03/16/2022 10:34:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.67 on epoch=37
03/16/2022 10:34:53 - INFO - __main__ - Global step 150 Train loss 0.73 Classification-F1 0.4709446380178088 on epoch=37
03/16/2022 10:34:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3937432014057401 -> 0.4709446380178088 on epoch=37, global_step=150
03/16/2022 10:34:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.74 on epoch=39
03/16/2022 10:34:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=42
03/16/2022 10:35:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.73 on epoch=44
03/16/2022 10:35:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.64 on epoch=47
03/16/2022 10:35:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.72 on epoch=49
03/16/2022 10:35:06 - INFO - __main__ - Global step 200 Train loss 0.70 Classification-F1 0.5193023146546385 on epoch=49
03/16/2022 10:35:06 - INFO - __main__ - Saving model with best Classification-F1: 0.4709446380178088 -> 0.5193023146546385 on epoch=49, global_step=200
03/16/2022 10:35:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.56 on epoch=52
03/16/2022 10:35:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.63 on epoch=54
03/16/2022 10:35:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=57
03/16/2022 10:35:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=59
03/16/2022 10:35:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=62
03/16/2022 10:35:19 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.45046082949308763 on epoch=62
03/16/2022 10:35:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.61 on epoch=64
03/16/2022 10:35:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=67
03/16/2022 10:35:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.56 on epoch=69
03/16/2022 10:35:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=72
03/16/2022 10:35:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=74
03/16/2022 10:35:32 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.7032197429056511 on epoch=74
03/16/2022 10:35:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5193023146546385 -> 0.7032197429056511 on epoch=74, global_step=300
03/16/2022 10:35:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=77
03/16/2022 10:35:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=79
03/16/2022 10:35:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=82
03/16/2022 10:35:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=84
03/16/2022 10:35:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=87
03/16/2022 10:35:45 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.6262254484075565 on epoch=87
03/16/2022 10:35:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=89
03/16/2022 10:35:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=92
03/16/2022 10:35:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=94
03/16/2022 10:35:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=97
03/16/2022 10:35:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=99
03/16/2022 10:35:58 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.7287132222616094 on epoch=99
03/16/2022 10:35:58 - INFO - __main__ - Saving model with best Classification-F1: 0.7032197429056511 -> 0.7287132222616094 on epoch=99, global_step=400
03/16/2022 10:36:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=102
03/16/2022 10:36:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=104
03/16/2022 10:36:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=107
03/16/2022 10:36:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=109
03/16/2022 10:36:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=112
03/16/2022 10:36:11 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.6946386946386947 on epoch=112
03/16/2022 10:36:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=114
03/16/2022 10:36:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=117
03/16/2022 10:36:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=119
03/16/2022 10:36:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=122
03/16/2022 10:36:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=124
03/16/2022 10:36:25 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.660737812911726 on epoch=124
03/16/2022 10:36:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=127
03/16/2022 10:36:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=129
03/16/2022 10:36:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=132
03/16/2022 10:36:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=134
03/16/2022 10:36:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=137
03/16/2022 10:36:38 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.6731601731601732 on epoch=137
03/16/2022 10:36:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=139
03/16/2022 10:36:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=142
03/16/2022 10:36:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=144
03/16/2022 10:36:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=147
03/16/2022 10:36:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=149
03/16/2022 10:36:51 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.7655332752106945 on epoch=149
03/16/2022 10:36:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7287132222616094 -> 0.7655332752106945 on epoch=149, global_step=600
03/16/2022 10:36:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=152
03/16/2022 10:36:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=154
03/16/2022 10:36:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=157
03/16/2022 10:37:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=159
03/16/2022 10:37:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=162
03/16/2022 10:37:04 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.7319693616029822 on epoch=162
03/16/2022 10:37:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=164
03/16/2022 10:37:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=167
03/16/2022 10:37:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=169
03/16/2022 10:37:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=172
03/16/2022 10:37:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=174
03/16/2022 10:37:17 - INFO - __main__ - Global step 700 Train loss 0.17 Classification-F1 0.7967741935483871 on epoch=174
03/16/2022 10:37:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7655332752106945 -> 0.7967741935483871 on epoch=174, global_step=700
03/16/2022 10:37:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=177
03/16/2022 10:37:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=179
03/16/2022 10:37:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=182
03/16/2022 10:37:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
03/16/2022 10:37:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=187
03/16/2022 10:37:30 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.6955023205023205 on epoch=187
03/16/2022 10:37:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=189
03/16/2022 10:37:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=192
03/16/2022 10:37:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.14 on epoch=194
03/16/2022 10:37:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=197
03/16/2022 10:37:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=199
03/16/2022 10:37:43 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.6513250976665611 on epoch=199
03/16/2022 10:37:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=202
03/16/2022 10:37:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=204
03/16/2022 10:37:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=207
03/16/2022 10:37:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=209
03/16/2022 10:37:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=212
03/16/2022 10:37:56 - INFO - __main__ - Global step 850 Train loss 0.14 Classification-F1 0.6777192982456139 on epoch=212
03/16/2022 10:37:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=214
03/16/2022 10:38:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=217
03/16/2022 10:38:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=219
03/16/2022 10:38:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=222
03/16/2022 10:38:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=224
03/16/2022 10:38:09 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.7238618082368082 on epoch=224
03/16/2022 10:38:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=227
03/16/2022 10:38:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=229
03/16/2022 10:38:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
03/16/2022 10:38:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
03/16/2022 10:38:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=237
03/16/2022 10:38:22 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.6958998174168453 on epoch=237
03/16/2022 10:38:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=239
03/16/2022 10:38:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=242
03/16/2022 10:38:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.08 on epoch=244
03/16/2022 10:38:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=247
03/16/2022 10:38:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=249
03/16/2022 10:38:35 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.7787878787878787 on epoch=249
03/16/2022 10:38:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=252
03/16/2022 10:38:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
03/16/2022 10:38:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
03/16/2022 10:38:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
03/16/2022 10:38:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=262
03/16/2022 10:38:48 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.6666334396597555 on epoch=262
03/16/2022 10:38:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=264
03/16/2022 10:38:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
03/16/2022 10:38:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
03/16/2022 10:38:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
03/16/2022 10:39:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
03/16/2022 10:39:02 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.7115214506855374 on epoch=274
03/16/2022 10:39:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
03/16/2022 10:39:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
03/16/2022 10:39:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
03/16/2022 10:39:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
03/16/2022 10:39:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=287
03/16/2022 10:39:15 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.726696744116099 on epoch=287
03/16/2022 10:39:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
03/16/2022 10:39:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
03/16/2022 10:39:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
03/16/2022 10:39:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
03/16/2022 10:39:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
03/16/2022 10:39:28 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.6364734299516908 on epoch=299
03/16/2022 10:39:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.09 on epoch=302
03/16/2022 10:39:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=304
03/16/2022 10:39:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
03/16/2022 10:39:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=309
03/16/2022 10:39:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
03/16/2022 10:39:41 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.7788392331348557 on epoch=312
03/16/2022 10:39:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
03/16/2022 10:39:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
03/16/2022 10:39:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
03/16/2022 10:39:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
03/16/2022 10:39:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
03/16/2022 10:39:54 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7326799725993274 on epoch=324
03/16/2022 10:39:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=327
03/16/2022 10:39:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
03/16/2022 10:40:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
03/16/2022 10:40:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
03/16/2022 10:40:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
03/16/2022 10:40:07 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.744951923076923 on epoch=337
03/16/2022 10:40:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=339
03/16/2022 10:40:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
03/16/2022 10:40:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
03/16/2022 10:40:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
03/16/2022 10:40:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
03/16/2022 10:40:20 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6744123736451101 on epoch=349
03/16/2022 10:40:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
03/16/2022 10:40:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
03/16/2022 10:40:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
03/16/2022 10:40:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/16/2022 10:40:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
03/16/2022 10:40:33 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.6875776397515527 on epoch=362
03/16/2022 10:40:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
03/16/2022 10:40:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
03/16/2022 10:40:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
03/16/2022 10:40:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
03/16/2022 10:40:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=374
03/16/2022 10:40:47 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.7226839826839827 on epoch=374
03/16/2022 10:40:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
03/16/2022 10:40:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
03/16/2022 10:40:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
03/16/2022 10:40:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
03/16/2022 10:40:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=387
03/16/2022 10:41:00 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.7477359804946012 on epoch=387
03/16/2022 10:41:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
03/16/2022 10:41:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
03/16/2022 10:41:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
03/16/2022 10:41:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
03/16/2022 10:41:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=399
03/16/2022 10:41:13 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.7625564399757948 on epoch=399
03/16/2022 10:41:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
03/16/2022 10:41:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
03/16/2022 10:41:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
03/16/2022 10:41:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
03/16/2022 10:41:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
03/16/2022 10:41:26 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7088317384370015 on epoch=412
03/16/2022 10:41:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
03/16/2022 10:41:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
03/16/2022 10:41:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
03/16/2022 10:41:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=422
03/16/2022 10:41:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
03/16/2022 10:41:39 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6807261807261807 on epoch=424
03/16/2022 10:41:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
03/16/2022 10:41:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/16/2022 10:41:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
03/16/2022 10:41:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/16/2022 10:41:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
03/16/2022 10:41:52 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6822584396597555 on epoch=437
03/16/2022 10:41:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
03/16/2022 10:41:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
03/16/2022 10:42:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
03/16/2022 10:42:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/16/2022 10:42:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
03/16/2022 10:42:06 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6733318764568765 on epoch=449
03/16/2022 10:42:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
03/16/2022 10:42:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/16/2022 10:42:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
03/16/2022 10:42:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=459
03/16/2022 10:42:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=462
03/16/2022 10:42:19 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.6860641891891892 on epoch=462
03/16/2022 10:42:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
03/16/2022 10:42:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/16/2022 10:42:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/16/2022 10:42:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
03/16/2022 10:42:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/16/2022 10:42:32 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7114354395604396 on epoch=474
03/16/2022 10:42:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
03/16/2022 10:42:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/16/2022 10:42:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
03/16/2022 10:42:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=484
03/16/2022 10:42:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/16/2022 10:42:45 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.71188004000504 on epoch=487
03/16/2022 10:42:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
03/16/2022 10:42:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
03/16/2022 10:42:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
03/16/2022 10:42:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/16/2022 10:42:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
03/16/2022 10:42:59 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6845012626262625 on epoch=499
03/16/2022 10:43:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=502
03/16/2022 10:43:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
03/16/2022 10:43:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
03/16/2022 10:43:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/16/2022 10:43:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
03/16/2022 10:43:12 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7003205128205128 on epoch=512
03/16/2022 10:43:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=514
03/16/2022 10:43:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
03/16/2022 10:43:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
03/16/2022 10:43:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/16/2022 10:43:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 10:43:25 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7453431372549019 on epoch=524
03/16/2022 10:43:28 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=527
03/16/2022 10:43:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
03/16/2022 10:43:33 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 10:43:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/16/2022 10:43:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
03/16/2022 10:43:39 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7620895865677649 on epoch=537
03/16/2022 10:43:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
03/16/2022 10:43:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/16/2022 10:43:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/16/2022 10:43:48 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=547
03/16/2022 10:43:51 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/16/2022 10:43:52 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7152777777777777 on epoch=549
03/16/2022 10:43:54 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
03/16/2022 10:43:57 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/16/2022 10:43:59 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
03/16/2022 10:44:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/16/2022 10:44:04 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/16/2022 10:44:05 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7159455128205128 on epoch=562
03/16/2022 10:44:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
03/16/2022 10:44:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
03/16/2022 10:44:12 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
03/16/2022 10:44:15 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 10:44:17 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
03/16/2022 10:44:18 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.715686274509804 on epoch=574
03/16/2022 10:44:21 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
03/16/2022 10:44:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
03/16/2022 10:44:25 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
03/16/2022 10:44:28 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 10:44:30 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/16/2022 10:44:32 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7489814107461167 on epoch=587
03/16/2022 10:44:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 10:44:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/16/2022 10:44:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/16/2022 10:44:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/16/2022 10:44:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/16/2022 10:44:45 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7138665256312315 on epoch=599
03/16/2022 10:44:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/16/2022 10:44:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/16/2022 10:44:52 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/16/2022 10:44:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/16/2022 10:44:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/16/2022 10:44:58 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6993006993006993 on epoch=612
03/16/2022 10:45:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
03/16/2022 10:45:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 10:45:05 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 10:45:08 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=622
03/16/2022 10:45:10 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
03/16/2022 10:45:11 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7489814107461167 on epoch=624
03/16/2022 10:45:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
03/16/2022 10:45:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 10:45:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/16/2022 10:45:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/16/2022 10:45:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=637
03/16/2022 10:45:24 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7818454576187843 on epoch=637
03/16/2022 10:45:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
03/16/2022 10:45:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/16/2022 10:45:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/16/2022 10:45:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/16/2022 10:45:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/16/2022 10:45:37 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7482819264069265 on epoch=649
03/16/2022 10:45:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
03/16/2022 10:45:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
03/16/2022 10:45:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/16/2022 10:45:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/16/2022 10:45:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
03/16/2022 10:45:51 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7503694581280789 on epoch=662
03/16/2022 10:45:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/16/2022 10:45:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/16/2022 10:45:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/16/2022 10:46:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/16/2022 10:46:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/16/2022 10:46:04 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6000000000000001 on epoch=674
03/16/2022 10:46:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
03/16/2022 10:46:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/16/2022 10:46:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/16/2022 10:46:13 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/16/2022 10:46:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/16/2022 10:46:17 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7191240132416603 on epoch=687
03/16/2022 10:46:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
03/16/2022 10:46:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/16/2022 10:46:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/16/2022 10:46:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/16/2022 10:46:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/16/2022 10:46:30 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6848484848484848 on epoch=699
03/16/2022 10:46:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=702
03/16/2022 10:46:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/16/2022 10:46:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/16/2022 10:46:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/16/2022 10:46:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/16/2022 10:46:44 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7329386299974536 on epoch=712
03/16/2022 10:46:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/16/2022 10:46:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 10:46:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
03/16/2022 10:46:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
03/16/2022 10:46:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/16/2022 10:46:57 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6818318935965995 on epoch=724
03/16/2022 10:47:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/16/2022 10:47:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
03/16/2022 10:47:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 10:47:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
03/16/2022 10:47:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
03/16/2022 10:47:11 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6327473840548308 on epoch=737
03/16/2022 10:47:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/16/2022 10:47:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 10:47:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
03/16/2022 10:47:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
03/16/2022 10:47:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/16/2022 10:47:24 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7468023051514512 on epoch=749
03/16/2022 10:47:24 - INFO - __main__ - save last model!
03/16/2022 10:47:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 10:47:24 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:47:24 - INFO - __main__ - Printing 3 examples
03/16/2022 10:47:24 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/16/2022 10:47:24 - INFO - __main__ - ['happy']
03/16/2022 10:47:24 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/16/2022 10:47:24 - INFO - __main__ - ['happy']
03/16/2022 10:47:24 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/16/2022 10:47:24 - INFO - __main__ - ['happy']
03/16/2022 10:47:24 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:47:24 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:47:24 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 10:47:24 - INFO - __main__ - Printing 3 examples
03/16/2022 10:47:24 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 10:47:24 - INFO - __main__ - ['others']
03/16/2022 10:47:24 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 10:47:24 - INFO - __main__ - ['others']
03/16/2022 10:47:24 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 10:47:24 - INFO - __main__ - ['others']
03/16/2022 10:47:24 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:47:24 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 10:47:24 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:47:24 - INFO - __main__ - Printing 3 examples
03/16/2022 10:47:24 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/16/2022 10:47:24 - INFO - __main__ - ['happy']
03/16/2022 10:47:24 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/16/2022 10:47:24 - INFO - __main__ - ['happy']
03/16/2022 10:47:24 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/16/2022 10:47:24 - INFO - __main__ - ['happy']
03/16/2022 10:47:24 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:47:24 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:47:24 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 10:47:26 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:47:32 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 10:47:40 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 10:47:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 10:47:41 - INFO - __main__ - Starting training!
03/16/2022 10:49:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_42_0.5_8_predictions.txt
03/16/2022 10:49:09 - INFO - __main__ - Classification-F1 on test data: 0.1683
03/16/2022 10:49:10 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.7967741935483871, test_performance=0.168288337674344
03/16/2022 10:49:10 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
03/16/2022 10:49:11 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:49:11 - INFO - __main__ - Printing 3 examples
03/16/2022 10:49:11 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/16/2022 10:49:11 - INFO - __main__ - ['happy']
03/16/2022 10:49:11 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/16/2022 10:49:11 - INFO - __main__ - ['happy']
03/16/2022 10:49:11 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/16/2022 10:49:11 - INFO - __main__ - ['happy']
03/16/2022 10:49:11 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:49:11 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:49:11 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 10:49:11 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 10:49:11 - INFO - __main__ - Printing 3 examples
03/16/2022 10:49:11 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/16/2022 10:49:11 - INFO - __main__ - ['happy']
03/16/2022 10:49:11 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/16/2022 10:49:11 - INFO - __main__ - ['happy']
03/16/2022 10:49:11 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/16/2022 10:49:11 - INFO - __main__ - ['happy']
03/16/2022 10:49:11 - INFO - __main__ - Tokenizing Input ...
03/16/2022 10:49:11 - INFO - __main__ - Tokenizing Output ...
03/16/2022 10:49:11 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 10:49:30 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 10:49:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 10:49:31 - INFO - __main__ - Starting training!
03/16/2022 10:49:34 - INFO - __main__ - Step 10 Global step 10 Train loss 2.49 on epoch=2
03/16/2022 10:49:36 - INFO - __main__ - Step 20 Global step 20 Train loss 1.36 on epoch=4
03/16/2022 10:49:39 - INFO - __main__ - Step 30 Global step 30 Train loss 1.04 on epoch=7
03/16/2022 10:49:41 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=9
03/16/2022 10:49:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.99 on epoch=12
03/16/2022 10:49:45 - INFO - __main__ - Global step 50 Train loss 1.39 Classification-F1 0.1 on epoch=12
03/16/2022 10:49:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/16/2022 10:49:47 - INFO - __main__ - Step 60 Global step 60 Train loss 1.03 on epoch=14
03/16/2022 10:49:50 - INFO - __main__ - Step 70 Global step 70 Train loss 0.98 on epoch=17
03/16/2022 10:49:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.82 on epoch=19
03/16/2022 10:49:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=22
03/16/2022 10:49:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=24
03/16/2022 10:49:58 - INFO - __main__ - Global step 100 Train loss 0.91 Classification-F1 0.3373482726423903 on epoch=24
03/16/2022 10:49:58 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.3373482726423903 on epoch=24, global_step=100
03/16/2022 10:50:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.77 on epoch=27
03/16/2022 10:50:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.86 on epoch=29
03/16/2022 10:50:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.81 on epoch=32
03/16/2022 10:50:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.75 on epoch=34
03/16/2022 10:50:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.64 on epoch=37
03/16/2022 10:50:12 - INFO - __main__ - Global step 150 Train loss 0.77 Classification-F1 0.4535714285714286 on epoch=37
03/16/2022 10:50:12 - INFO - __main__ - Saving model with best Classification-F1: 0.3373482726423903 -> 0.4535714285714286 on epoch=37, global_step=150
03/16/2022 10:50:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.79 on epoch=39
03/16/2022 10:50:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.62 on epoch=42
03/16/2022 10:50:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.69 on epoch=44
03/16/2022 10:50:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.74 on epoch=47
03/16/2022 10:50:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.67 on epoch=49
03/16/2022 10:50:25 - INFO - __main__ - Global step 200 Train loss 0.70 Classification-F1 0.5797055946124269 on epoch=49
03/16/2022 10:50:25 - INFO - __main__ - Saving model with best Classification-F1: 0.4535714285714286 -> 0.5797055946124269 on epoch=49, global_step=200
03/16/2022 10:50:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=52
03/16/2022 10:50:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.64 on epoch=54
03/16/2022 10:50:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=57
03/16/2022 10:50:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=59
03/16/2022 10:50:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.66 on epoch=62
03/16/2022 10:50:39 - INFO - __main__ - Global step 250 Train loss 0.65 Classification-F1 0.498473849199802 on epoch=62
03/16/2022 10:50:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=64
03/16/2022 10:50:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=67
03/16/2022 10:50:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=69
03/16/2022 10:50:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=72
03/16/2022 10:50:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=74
03/16/2022 10:50:53 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.6472027972027973 on epoch=74
03/16/2022 10:50:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5797055946124269 -> 0.6472027972027973 on epoch=74, global_step=300
03/16/2022 10:50:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=77
03/16/2022 10:50:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=79
03/16/2022 10:51:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=82
03/16/2022 10:51:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=84
03/16/2022 10:51:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=87
03/16/2022 10:51:06 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.4318452380952381 on epoch=87
03/16/2022 10:51:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=89
03/16/2022 10:51:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=92
03/16/2022 10:51:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=94
03/16/2022 10:51:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=97
03/16/2022 10:51:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=99
03/16/2022 10:51:20 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.6304347826086957 on epoch=99
03/16/2022 10:51:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=102
03/16/2022 10:51:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=104
03/16/2022 10:51:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=107
03/16/2022 10:51:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=109
03/16/2022 10:51:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=112
03/16/2022 10:51:33 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.625460687960688 on epoch=112
03/16/2022 10:51:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=114
03/16/2022 10:51:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=117
03/16/2022 10:51:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=119
03/16/2022 10:51:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=122
03/16/2022 10:51:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=124
03/16/2022 10:51:47 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.672583559168925 on epoch=124
03/16/2022 10:51:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6472027972027973 -> 0.672583559168925 on epoch=124, global_step=500
03/16/2022 10:51:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=127
03/16/2022 10:51:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=129
03/16/2022 10:51:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=132
03/16/2022 10:51:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=134
03/16/2022 10:51:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=137
03/16/2022 10:52:00 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.588045288045288 on epoch=137
03/16/2022 10:52:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.28 on epoch=139
03/16/2022 10:52:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=142
03/16/2022 10:52:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=144
03/16/2022 10:52:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
03/16/2022 10:52:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=149
03/16/2022 10:52:14 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.6933470814284767 on epoch=149
03/16/2022 10:52:14 - INFO - __main__ - Saving model with best Classification-F1: 0.672583559168925 -> 0.6933470814284767 on epoch=149, global_step=600
03/16/2022 10:52:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=152
03/16/2022 10:52:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=154
03/16/2022 10:52:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=157
03/16/2022 10:52:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=159
03/16/2022 10:52:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=162
03/16/2022 10:52:28 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.6602440035943872 on epoch=162
03/16/2022 10:52:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=164
03/16/2022 10:52:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=167
03/16/2022 10:52:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=169
03/16/2022 10:52:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=172
03/16/2022 10:52:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=174
03/16/2022 10:52:41 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.6602440035943872 on epoch=174
03/16/2022 10:52:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
03/16/2022 10:52:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=179
03/16/2022 10:52:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=182
03/16/2022 10:52:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=184
03/16/2022 10:52:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=187
03/16/2022 10:52:55 - INFO - __main__ - Global step 750 Train loss 0.17 Classification-F1 0.699286600496278 on epoch=187
03/16/2022 10:52:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6933470814284767 -> 0.699286600496278 on epoch=187, global_step=750
03/16/2022 10:52:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=189
03/16/2022 10:53:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=192
03/16/2022 10:53:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.14 on epoch=194
03/16/2022 10:53:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=197
03/16/2022 10:53:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=199
03/16/2022 10:53:09 - INFO - __main__ - Global step 800 Train loss 0.17 Classification-F1 0.69443867959997 on epoch=199
03/16/2022 10:53:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=202
03/16/2022 10:53:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=204
03/16/2022 10:53:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=207
03/16/2022 10:53:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=209
03/16/2022 10:53:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=212
03/16/2022 10:53:23 - INFO - __main__ - Global step 850 Train loss 0.13 Classification-F1 0.8265533486121722 on epoch=212
03/16/2022 10:53:23 - INFO - __main__ - Saving model with best Classification-F1: 0.699286600496278 -> 0.8265533486121722 on epoch=212, global_step=850
03/16/2022 10:53:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=214
03/16/2022 10:53:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=217
03/16/2022 10:53:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=219
03/16/2022 10:53:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=222
03/16/2022 10:53:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=224
03/16/2022 10:53:37 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.7747926093514329 on epoch=224
03/16/2022 10:53:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=227
03/16/2022 10:53:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=229
03/16/2022 10:53:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=232
03/16/2022 10:53:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=234
03/16/2022 10:53:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=237
03/16/2022 10:53:51 - INFO - __main__ - Global step 950 Train loss 0.13 Classification-F1 0.7120098039215687 on epoch=237
03/16/2022 10:53:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=239
03/16/2022 10:53:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=242
03/16/2022 10:53:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=244
03/16/2022 10:54:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
03/16/2022 10:54:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
03/16/2022 10:54:04 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.6926118082368082 on epoch=249
03/16/2022 10:54:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=252
03/16/2022 10:54:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
03/16/2022 10:54:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
03/16/2022 10:54:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
03/16/2022 10:54:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=262
03/16/2022 10:54:18 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.6874842690699468 on epoch=262
03/16/2022 10:54:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
03/16/2022 10:54:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=267
03/16/2022 10:54:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
03/16/2022 10:54:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=272
03/16/2022 10:54:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
03/16/2022 10:54:31 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.7443066801619433 on epoch=274
03/16/2022 10:54:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
03/16/2022 10:54:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
03/16/2022 10:54:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
03/16/2022 10:54:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=284
03/16/2022 10:54:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=287
03/16/2022 10:54:45 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.7215306884424532 on epoch=287
03/16/2022 10:54:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=289
03/16/2022 10:54:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=292
03/16/2022 10:54:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=294
03/16/2022 10:54:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
03/16/2022 10:54:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=299
03/16/2022 10:54:58 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.757004173290938 on epoch=299
03/16/2022 10:55:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
03/16/2022 10:55:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=304
03/16/2022 10:55:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
03/16/2022 10:55:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=309
03/16/2022 10:55:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
03/16/2022 10:55:11 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.7571726190476191 on epoch=312
03/16/2022 10:55:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=314
03/16/2022 10:55:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=317
03/16/2022 10:55:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=319
03/16/2022 10:55:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
03/16/2022 10:55:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
03/16/2022 10:55:24 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.7223039215686274 on epoch=324
03/16/2022 10:55:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
03/16/2022 10:55:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
03/16/2022 10:55:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=332
03/16/2022 10:55:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=334
03/16/2022 10:55:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
03/16/2022 10:55:38 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.7106981981981981 on epoch=337
03/16/2022 10:55:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
03/16/2022 10:55:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
03/16/2022 10:55:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=344
03/16/2022 10:55:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
03/16/2022 10:55:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
03/16/2022 10:55:51 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.7355994152046784 on epoch=349
03/16/2022 10:55:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
03/16/2022 10:55:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=354
03/16/2022 10:55:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
03/16/2022 10:56:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/16/2022 10:56:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
03/16/2022 10:56:05 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.7585591206179442 on epoch=362
03/16/2022 10:56:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
03/16/2022 10:56:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
03/16/2022 10:56:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
03/16/2022 10:56:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=372
03/16/2022 10:56:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
03/16/2022 10:56:18 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.7712554112554113 on epoch=374
03/16/2022 10:56:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
03/16/2022 10:56:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
03/16/2022 10:56:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
03/16/2022 10:56:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
03/16/2022 10:56:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
03/16/2022 10:56:32 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.7760180995475112 on epoch=387
03/16/2022 10:56:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
03/16/2022 10:56:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=392
03/16/2022 10:56:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
03/16/2022 10:56:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
03/16/2022 10:56:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
03/16/2022 10:56:45 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.7763746057863705 on epoch=399
03/16/2022 10:56:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
03/16/2022 10:56:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
03/16/2022 10:56:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
03/16/2022 10:56:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
03/16/2022 10:56:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
03/16/2022 10:56:58 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7164525230382008 on epoch=412
03/16/2022 10:57:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
03/16/2022 10:57:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
03/16/2022 10:57:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
03/16/2022 10:57:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=422
03/16/2022 10:57:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
03/16/2022 10:57:12 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.7078012125902993 on epoch=424
03/16/2022 10:57:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
03/16/2022 10:57:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
03/16/2022 10:57:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
03/16/2022 10:57:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=434
03/16/2022 10:57:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
03/16/2022 10:57:26 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.7233585858585859 on epoch=437
03/16/2022 10:57:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
03/16/2022 10:57:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
03/16/2022 10:57:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/16/2022 10:57:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
03/16/2022 10:57:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
03/16/2022 10:57:39 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.7391347687400318 on epoch=449
03/16/2022 10:57:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/16/2022 10:57:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
03/16/2022 10:57:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
03/16/2022 10:57:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
03/16/2022 10:57:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/16/2022 10:57:53 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6954163343965976 on epoch=462
03/16/2022 10:57:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=464
03/16/2022 10:57:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
03/16/2022 10:58:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=469
03/16/2022 10:58:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
03/16/2022 10:58:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
03/16/2022 10:58:06 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.6972930201565883 on epoch=474
03/16/2022 10:58:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
03/16/2022 10:58:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
03/16/2022 10:58:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
03/16/2022 10:58:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
03/16/2022 10:58:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
03/16/2022 10:58:20 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7520424836601307 on epoch=487
03/16/2022 10:58:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
03/16/2022 10:58:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/16/2022 10:58:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=494
03/16/2022 10:58:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=497
03/16/2022 10:58:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 10:58:33 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.6845473367212497 on epoch=499
03/16/2022 10:58:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
03/16/2022 10:58:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
03/16/2022 10:58:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
03/16/2022 10:58:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/16/2022 10:58:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/16/2022 10:58:47 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7189295273534404 on epoch=512
03/16/2022 10:58:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=514
03/16/2022 10:58:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
03/16/2022 10:58:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/16/2022 10:58:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
03/16/2022 10:58:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 10:59:00 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7440711462450593 on epoch=524
03/16/2022 10:59:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/16/2022 10:59:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 10:59:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
03/16/2022 10:59:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/16/2022 10:59:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
03/16/2022 10:59:13 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7507395382395383 on epoch=537
03/16/2022 10:59:16 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
03/16/2022 10:59:18 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
03/16/2022 10:59:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
03/16/2022 10:59:23 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/16/2022 10:59:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
03/16/2022 10:59:27 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7650252525252526 on epoch=549
03/16/2022 10:59:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/16/2022 10:59:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/16/2022 10:59:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/16/2022 10:59:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/16/2022 10:59:39 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=562
03/16/2022 10:59:40 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7304347826086957 on epoch=562
03/16/2022 10:59:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
03/16/2022 10:59:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/16/2022 10:59:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
03/16/2022 10:59:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
03/16/2022 10:59:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
03/16/2022 10:59:54 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.706465103523927 on epoch=574
03/16/2022 10:59:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/16/2022 10:59:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
03/16/2022 11:00:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/16/2022 11:00:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
03/16/2022 11:00:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
03/16/2022 11:00:07 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7284610983981693 on epoch=587
03/16/2022 11:00:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 11:00:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
03/16/2022 11:00:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
03/16/2022 11:00:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/16/2022 11:00:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
03/16/2022 11:00:20 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7512955182072829 on epoch=599
03/16/2022 11:00:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/16/2022 11:00:25 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/16/2022 11:00:28 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/16/2022 11:00:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
03/16/2022 11:00:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=612
03/16/2022 11:00:34 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.6992794795783925 on epoch=612
03/16/2022 11:00:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
03/16/2022 11:00:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 11:00:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
03/16/2022 11:00:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/16/2022 11:00:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
03/16/2022 11:00:48 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7525804547863372 on epoch=624
03/16/2022 11:00:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/16/2022 11:00:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
03/16/2022 11:00:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/16/2022 11:00:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=634
03/16/2022 11:01:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/16/2022 11:01:01 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7076839826839827 on epoch=637
03/16/2022 11:01:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/16/2022 11:01:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/16/2022 11:01:08 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/16/2022 11:01:11 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/16/2022 11:01:13 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/16/2022 11:01:15 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7172928591726546 on epoch=649
03/16/2022 11:01:17 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
03/16/2022 11:01:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/16/2022 11:01:22 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/16/2022 11:01:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/16/2022 11:01:27 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/16/2022 11:01:28 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7164525230382008 on epoch=662
03/16/2022 11:01:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/16/2022 11:01:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
03/16/2022 11:01:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/16/2022 11:01:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
03/16/2022 11:01:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
03/16/2022 11:01:42 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7589698077813528 on epoch=674
03/16/2022 11:01:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/16/2022 11:01:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=679
03/16/2022 11:01:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/16/2022 11:01:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/16/2022 11:01:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/16/2022 11:01:55 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7149044795783925 on epoch=687
03/16/2022 11:01:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
03/16/2022 11:02:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
03/16/2022 11:02:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
03/16/2022 11:02:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/16/2022 11:02:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/16/2022 11:02:09 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6884633994608419 on epoch=699
03/16/2022 11:02:11 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
03/16/2022 11:02:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/16/2022 11:02:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/16/2022 11:02:18 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=709
03/16/2022 11:02:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/16/2022 11:02:22 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7150650410504991 on epoch=712
03/16/2022 11:02:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
03/16/2022 11:02:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 11:02:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
03/16/2022 11:02:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
03/16/2022 11:02:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
03/16/2022 11:02:36 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7031692818137831 on epoch=724
03/16/2022 11:02:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/16/2022 11:02:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=729
03/16/2022 11:02:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 11:02:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
03/16/2022 11:02:48 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/16/2022 11:02:49 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7170517260367845 on epoch=737
03/16/2022 11:02:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
03/16/2022 11:02:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
03/16/2022 11:02:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.10 on epoch=744
03/16/2022 11:02:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 11:03:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
03/16/2022 11:03:03 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6566280566280567 on epoch=749
03/16/2022 11:03:03 - INFO - __main__ - save last model!
03/16/2022 11:03:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 11:03:03 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 11:03:03 - INFO - __main__ - Printing 3 examples
03/16/2022 11:03:03 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 11:03:03 - INFO - __main__ - ['others']
03/16/2022 11:03:03 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 11:03:03 - INFO - __main__ - ['others']
03/16/2022 11:03:03 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 11:03:03 - INFO - __main__ - ['others']
03/16/2022 11:03:03 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:03:03 - INFO - __main__ - Printing 3 examples
03/16/2022 11:03:03 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/16/2022 11:03:03 - INFO - __main__ - ['happy']
03/16/2022 11:03:03 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/16/2022 11:03:03 - INFO - __main__ - ['happy']
03/16/2022 11:03:03 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/16/2022 11:03:03 - INFO - __main__ - ['happy']
03/16/2022 11:03:03 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:03:03 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:03:03 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:03:03 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 11:03:03 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:03:03 - INFO - __main__ - Printing 3 examples
03/16/2022 11:03:03 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/16/2022 11:03:03 - INFO - __main__ - ['happy']
03/16/2022 11:03:03 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/16/2022 11:03:03 - INFO - __main__ - ['happy']
03/16/2022 11:03:03 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/16/2022 11:03:03 - INFO - __main__ - ['happy']
03/16/2022 11:03:03 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:03:03 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:03:03 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 11:03:05 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:03:10 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 11:03:21 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 11:03:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 11:03:22 - INFO - __main__ - Starting training!
03/16/2022 11:04:46 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_42_0.4_8_predictions.txt
03/16/2022 11:04:46 - INFO - __main__ - Classification-F1 on test data: 0.1468
03/16/2022 11:04:48 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.8265533486121722, test_performance=0.1467817068229894
03/16/2022 11:04:48 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
03/16/2022 11:04:49 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:04:49 - INFO - __main__ - Printing 3 examples
03/16/2022 11:04:49 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/16/2022 11:04:49 - INFO - __main__ - ['happy']
03/16/2022 11:04:49 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/16/2022 11:04:49 - INFO - __main__ - ['happy']
03/16/2022 11:04:49 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/16/2022 11:04:49 - INFO - __main__ - ['happy']
03/16/2022 11:04:49 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:04:49 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:04:49 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 11:04:49 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:04:49 - INFO - __main__ - Printing 3 examples
03/16/2022 11:04:49 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/16/2022 11:04:49 - INFO - __main__ - ['happy']
03/16/2022 11:04:49 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/16/2022 11:04:49 - INFO - __main__ - ['happy']
03/16/2022 11:04:49 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/16/2022 11:04:49 - INFO - __main__ - ['happy']
03/16/2022 11:04:49 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:04:50 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:04:50 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 11:05:05 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 11:05:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 11:05:05 - INFO - __main__ - Starting training!
03/16/2022 11:05:08 - INFO - __main__ - Step 10 Global step 10 Train loss 2.83 on epoch=2
03/16/2022 11:05:11 - INFO - __main__ - Step 20 Global step 20 Train loss 1.71 on epoch=4
03/16/2022 11:05:13 - INFO - __main__ - Step 30 Global step 30 Train loss 1.14 on epoch=7
03/16/2022 11:05:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.99 on epoch=9
03/16/2022 11:05:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.91 on epoch=12
03/16/2022 11:05:20 - INFO - __main__ - Global step 50 Train loss 1.51 Classification-F1 0.405993265993266 on epoch=12
03/16/2022 11:05:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.405993265993266 on epoch=12, global_step=50
03/16/2022 11:05:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.92 on epoch=14
03/16/2022 11:05:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=17
03/16/2022 11:05:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.81 on epoch=19
03/16/2022 11:05:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=22
03/16/2022 11:05:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=24
03/16/2022 11:05:33 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.3752886002886003 on epoch=24
03/16/2022 11:05:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=27
03/16/2022 11:05:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.88 on epoch=29
03/16/2022 11:05:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.83 on epoch=32
03/16/2022 11:05:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.84 on epoch=34
03/16/2022 11:05:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=37
03/16/2022 11:05:46 - INFO - __main__ - Global step 150 Train loss 0.84 Classification-F1 0.6001929392446634 on epoch=37
03/16/2022 11:05:46 - INFO - __main__ - Saving model with best Classification-F1: 0.405993265993266 -> 0.6001929392446634 on epoch=37, global_step=150
03/16/2022 11:05:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.72 on epoch=39
03/16/2022 11:05:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.72 on epoch=42
03/16/2022 11:05:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.84 on epoch=44
03/16/2022 11:05:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.73 on epoch=47
03/16/2022 11:05:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.78 on epoch=49
03/16/2022 11:05:59 - INFO - __main__ - Global step 200 Train loss 0.76 Classification-F1 0.6059297477590161 on epoch=49
03/16/2022 11:05:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6001929392446634 -> 0.6059297477590161 on epoch=49, global_step=200
03/16/2022 11:06:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=52
03/16/2022 11:06:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.74 on epoch=54
03/16/2022 11:06:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.62 on epoch=57
03/16/2022 11:06:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.65 on epoch=59
03/16/2022 11:06:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.70 on epoch=62
03/16/2022 11:06:13 - INFO - __main__ - Global step 250 Train loss 0.68 Classification-F1 0.4352272727272728 on epoch=62
03/16/2022 11:06:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.63 on epoch=64
03/16/2022 11:06:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=67
03/16/2022 11:06:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=69
03/16/2022 11:06:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.64 on epoch=72
03/16/2022 11:06:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=74
03/16/2022 11:06:26 - INFO - __main__ - Global step 300 Train loss 0.62 Classification-F1 0.6353868797308663 on epoch=74
03/16/2022 11:06:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6059297477590161 -> 0.6353868797308663 on epoch=74, global_step=300
03/16/2022 11:06:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.59 on epoch=77
03/16/2022 11:06:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=79
03/16/2022 11:06:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=82
03/16/2022 11:06:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.54 on epoch=84
03/16/2022 11:06:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=87
03/16/2022 11:06:39 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.6424242424242425 on epoch=87
03/16/2022 11:06:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6353868797308663 -> 0.6424242424242425 on epoch=87, global_step=350
03/16/2022 11:06:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=89
03/16/2022 11:06:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=92
03/16/2022 11:06:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=94
03/16/2022 11:06:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=97
03/16/2022 11:06:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=99
03/16/2022 11:06:52 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.49753743104806936 on epoch=99
03/16/2022 11:06:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=102
03/16/2022 11:06:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=104
03/16/2022 11:07:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=107
03/16/2022 11:07:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=109
03/16/2022 11:07:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=112
03/16/2022 11:07:06 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.5916666666666667 on epoch=112
03/16/2022 11:07:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=114
03/16/2022 11:07:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=117
03/16/2022 11:07:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=119
03/16/2022 11:07:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=122
03/16/2022 11:07:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=124
03/16/2022 11:07:19 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.6479173000912131 on epoch=124
03/16/2022 11:07:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6424242424242425 -> 0.6479173000912131 on epoch=124, global_step=500
03/16/2022 11:07:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=127
03/16/2022 11:07:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=129
03/16/2022 11:07:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=132
03/16/2022 11:07:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=134
03/16/2022 11:07:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=137
03/16/2022 11:07:32 - INFO - __main__ - Global step 550 Train loss 0.28 Classification-F1 0.6636183261183262 on epoch=137
03/16/2022 11:07:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6479173000912131 -> 0.6636183261183262 on epoch=137, global_step=550
03/16/2022 11:07:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=139
03/16/2022 11:07:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=142
03/16/2022 11:07:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=144
03/16/2022 11:07:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=147
03/16/2022 11:07:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=149
03/16/2022 11:07:45 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.6330059523809524 on epoch=149
03/16/2022 11:07:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=152
03/16/2022 11:07:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=154
03/16/2022 11:07:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=157
03/16/2022 11:07:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=159
03/16/2022 11:07:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=162
03/16/2022 11:07:58 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.6841491841491841 on epoch=162
03/16/2022 11:07:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6636183261183262 -> 0.6841491841491841 on epoch=162, global_step=650
03/16/2022 11:08:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=164
03/16/2022 11:08:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=167
03/16/2022 11:08:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
03/16/2022 11:08:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=172
03/16/2022 11:08:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.17 on epoch=174
03/16/2022 11:08:11 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.7327787155373362 on epoch=174
03/16/2022 11:08:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6841491841491841 -> 0.7327787155373362 on epoch=174, global_step=700
03/16/2022 11:08:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=177
03/16/2022 11:08:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=179
03/16/2022 11:08:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=182
03/16/2022 11:08:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
03/16/2022 11:08:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=187
03/16/2022 11:08:24 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.6652136327503975 on epoch=187
03/16/2022 11:08:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=189
03/16/2022 11:08:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=192
03/16/2022 11:08:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=194
03/16/2022 11:08:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=197
03/16/2022 11:08:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=199
03/16/2022 11:08:37 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.6237079070758739 on epoch=199
03/16/2022 11:08:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=202
03/16/2022 11:08:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=204
03/16/2022 11:08:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=207
03/16/2022 11:08:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=209
03/16/2022 11:08:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.15 on epoch=212
03/16/2022 11:08:51 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.6503615300109857 on epoch=212
03/16/2022 11:08:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=214
03/16/2022 11:08:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=217
03/16/2022 11:08:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=219
03/16/2022 11:09:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=222
03/16/2022 11:09:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=224
03/16/2022 11:09:04 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.718734335839599 on epoch=224
03/16/2022 11:09:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=227
03/16/2022 11:09:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=229
03/16/2022 11:09:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
03/16/2022 11:09:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=234
03/16/2022 11:09:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=237
03/16/2022 11:09:17 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.6549364613880743 on epoch=237
03/16/2022 11:09:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=239
03/16/2022 11:09:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=242
03/16/2022 11:09:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=244
03/16/2022 11:09:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=247
03/16/2022 11:09:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
03/16/2022 11:09:30 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.6069174069174069 on epoch=249
03/16/2022 11:09:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=252
03/16/2022 11:09:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
03/16/2022 11:09:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=257
03/16/2022 11:09:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=259
03/16/2022 11:09:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
03/16/2022 11:09:43 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.6596681096681097 on epoch=262
03/16/2022 11:09:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
03/16/2022 11:09:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=267
03/16/2022 11:09:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=269
03/16/2022 11:09:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
03/16/2022 11:09:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
03/16/2022 11:09:56 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.6465229460960011 on epoch=274
03/16/2022 11:09:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
03/16/2022 11:10:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=279
03/16/2022 11:10:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
03/16/2022 11:10:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
03/16/2022 11:10:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
03/16/2022 11:10:09 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.6447458133971291 on epoch=287
03/16/2022 11:10:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
03/16/2022 11:10:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
03/16/2022 11:10:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
03/16/2022 11:10:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=297
03/16/2022 11:10:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
03/16/2022 11:10:23 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.6089285714285715 on epoch=299
03/16/2022 11:10:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
03/16/2022 11:10:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
03/16/2022 11:10:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
03/16/2022 11:10:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=309
03/16/2022 11:10:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
03/16/2022 11:10:36 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.6482866595769822 on epoch=312
03/16/2022 11:10:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=314
03/16/2022 11:10:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
03/16/2022 11:10:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
03/16/2022 11:10:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
03/16/2022 11:10:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
03/16/2022 11:10:49 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.6361196016052232 on epoch=324
03/16/2022 11:10:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=327
03/16/2022 11:10:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
03/16/2022 11:10:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
03/16/2022 11:10:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
03/16/2022 11:11:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=337
03/16/2022 11:11:02 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.6974987974987975 on epoch=337
03/16/2022 11:11:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=339
03/16/2022 11:11:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
03/16/2022 11:11:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
03/16/2022 11:11:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=347
03/16/2022 11:11:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
03/16/2022 11:11:15 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.6599389712292938 on epoch=349
03/16/2022 11:11:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=352
03/16/2022 11:11:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
03/16/2022 11:11:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
03/16/2022 11:11:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
03/16/2022 11:11:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
03/16/2022 11:11:28 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.680082726957727 on epoch=362
03/16/2022 11:11:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
03/16/2022 11:11:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=367
03/16/2022 11:11:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
03/16/2022 11:11:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
03/16/2022 11:11:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=374
03/16/2022 11:11:41 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.6324918324918325 on epoch=374
03/16/2022 11:11:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
03/16/2022 11:11:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
03/16/2022 11:11:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
03/16/2022 11:11:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
03/16/2022 11:11:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
03/16/2022 11:11:54 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.6730552232854865 on epoch=387
03/16/2022 11:11:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=389
03/16/2022 11:11:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
03/16/2022 11:12:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
03/16/2022 11:12:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
03/16/2022 11:12:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
03/16/2022 11:12:07 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6827876984126985 on epoch=399
03/16/2022 11:12:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/16/2022 11:12:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
03/16/2022 11:12:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
03/16/2022 11:12:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
03/16/2022 11:12:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
03/16/2022 11:12:21 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6666666666666667 on epoch=412
03/16/2022 11:12:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
03/16/2022 11:12:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
03/16/2022 11:12:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 11:12:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
03/16/2022 11:12:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
03/16/2022 11:12:34 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.68551267281106 on epoch=424
03/16/2022 11:12:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
03/16/2022 11:12:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/16/2022 11:12:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
03/16/2022 11:12:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/16/2022 11:12:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/16/2022 11:12:47 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6431623931623931 on epoch=437
03/16/2022 11:12:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=439
03/16/2022 11:12:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=442
03/16/2022 11:12:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
03/16/2022 11:12:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
03/16/2022 11:13:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
03/16/2022 11:13:00 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.7014705882352942 on epoch=449
03/16/2022 11:13:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=452
03/16/2022 11:13:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/16/2022 11:13:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
03/16/2022 11:13:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
03/16/2022 11:13:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
03/16/2022 11:13:14 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.6937699555346615 on epoch=462
03/16/2022 11:13:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
03/16/2022 11:13:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/16/2022 11:13:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
03/16/2022 11:13:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/16/2022 11:13:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
03/16/2022 11:13:27 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.629166770457093 on epoch=474
03/16/2022 11:13:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
03/16/2022 11:13:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/16/2022 11:13:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
03/16/2022 11:13:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
03/16/2022 11:13:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
03/16/2022 11:13:40 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6695746596747709 on epoch=487
03/16/2022 11:13:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/16/2022 11:13:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/16/2022 11:13:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
03/16/2022 11:13:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
03/16/2022 11:13:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 11:13:54 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6387108918802631 on epoch=499
03/16/2022 11:13:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/16/2022 11:13:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=504
03/16/2022 11:14:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
03/16/2022 11:14:03 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
03/16/2022 11:14:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/16/2022 11:14:07 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6833040811797696 on epoch=512
03/16/2022 11:14:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=514
03/16/2022 11:14:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/16/2022 11:14:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/16/2022 11:14:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/16/2022 11:14:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
03/16/2022 11:14:20 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6559139784946236 on epoch=524
03/16/2022 11:14:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/16/2022 11:14:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
03/16/2022 11:14:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 11:14:30 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
03/16/2022 11:14:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
03/16/2022 11:14:34 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6544488916256157 on epoch=537
03/16/2022 11:14:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
03/16/2022 11:14:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
03/16/2022 11:14:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
03/16/2022 11:14:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/16/2022 11:14:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=549
03/16/2022 11:14:47 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6360791826309068 on epoch=549
03/16/2022 11:14:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
03/16/2022 11:14:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
03/16/2022 11:14:55 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/16/2022 11:14:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
03/16/2022 11:15:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
03/16/2022 11:15:01 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.6452958027151576 on epoch=562
03/16/2022 11:15:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/16/2022 11:15:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/16/2022 11:15:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
03/16/2022 11:15:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 11:15:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
03/16/2022 11:15:14 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6302083333333334 on epoch=574
03/16/2022 11:15:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/16/2022 11:15:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/16/2022 11:15:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/16/2022 11:15:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=584
03/16/2022 11:15:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
03/16/2022 11:15:28 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6246438746438746 on epoch=587
03/16/2022 11:15:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 11:15:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/16/2022 11:15:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=594
03/16/2022 11:15:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/16/2022 11:15:40 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/16/2022 11:15:41 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6394423558897242 on epoch=599
03/16/2022 11:15:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/16/2022 11:15:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/16/2022 11:15:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=607
03/16/2022 11:15:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
03/16/2022 11:15:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
03/16/2022 11:15:54 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.680978705978706 on epoch=612
03/16/2022 11:15:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
03/16/2022 11:15:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 11:16:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 11:16:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/16/2022 11:16:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/16/2022 11:16:08 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6636836343732896 on epoch=624
03/16/2022 11:16:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/16/2022 11:16:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/16/2022 11:16:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/16/2022 11:16:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/16/2022 11:16:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/16/2022 11:16:21 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.6366719660837308 on epoch=637
03/16/2022 11:16:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/16/2022 11:16:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
03/16/2022 11:16:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=644
03/16/2022 11:16:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/16/2022 11:16:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/16/2022 11:16:34 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6479433878814684 on epoch=649
03/16/2022 11:16:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/16/2022 11:16:39 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/16/2022 11:16:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/16/2022 11:16:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/16/2022 11:16:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
03/16/2022 11:16:48 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.664793092212447 on epoch=662
03/16/2022 11:16:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/16/2022 11:16:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/16/2022 11:16:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/16/2022 11:16:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/16/2022 11:17:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
03/16/2022 11:17:01 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6657467532467531 on epoch=674
03/16/2022 11:17:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/16/2022 11:17:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/16/2022 11:17:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/16/2022 11:17:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/16/2022 11:17:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
03/16/2022 11:17:14 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6479433878814684 on epoch=687
03/16/2022 11:17:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/16/2022 11:17:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/16/2022 11:17:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/16/2022 11:17:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/16/2022 11:17:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/16/2022 11:17:28 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6654761904761906 on epoch=699
03/16/2022 11:17:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/16/2022 11:17:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/16/2022 11:17:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/16/2022 11:17:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/16/2022 11:17:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/16/2022 11:17:41 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6630952380952381 on epoch=712
03/16/2022 11:17:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/16/2022 11:17:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 11:17:48 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/16/2022 11:17:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
03/16/2022 11:17:53 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
03/16/2022 11:17:54 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6497947454844006 on epoch=724
03/16/2022 11:17:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=727
03/16/2022 11:17:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
03/16/2022 11:18:02 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 11:18:04 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/16/2022 11:18:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
03/16/2022 11:18:08 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.570422130204739 on epoch=737
03/16/2022 11:18:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/16/2022 11:18:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 11:18:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/16/2022 11:18:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 11:18:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 11:18:21 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6347751710654936 on epoch=749
03/16/2022 11:18:21 - INFO - __main__ - save last model!
03/16/2022 11:18:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 11:18:21 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 11:18:21 - INFO - __main__ - Printing 3 examples
03/16/2022 11:18:21 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 11:18:21 - INFO - __main__ - ['others']
03/16/2022 11:18:21 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 11:18:21 - INFO - __main__ - ['others']
03/16/2022 11:18:21 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 11:18:21 - INFO - __main__ - ['others']
03/16/2022 11:18:21 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:18:22 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:18:22 - INFO - __main__ - Printing 3 examples
03/16/2022 11:18:22 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/16/2022 11:18:22 - INFO - __main__ - ['happy']
03/16/2022 11:18:22 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/16/2022 11:18:22 - INFO - __main__ - ['happy']
03/16/2022 11:18:22 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/16/2022 11:18:22 - INFO - __main__ - ['happy']
03/16/2022 11:18:22 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:18:22 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:18:22 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 11:18:22 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:18:22 - INFO - __main__ - Printing 3 examples
03/16/2022 11:18:22 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/16/2022 11:18:22 - INFO - __main__ - ['happy']
03/16/2022 11:18:22 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/16/2022 11:18:22 - INFO - __main__ - ['happy']
03/16/2022 11:18:22 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/16/2022 11:18:22 - INFO - __main__ - ['happy']
03/16/2022 11:18:22 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:18:22 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:18:22 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 11:18:23 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:18:28 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 11:18:37 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 11:18:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 11:18:38 - INFO - __main__ - Starting training!
03/16/2022 11:19:53 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_42_0.3_8_predictions.txt
03/16/2022 11:19:53 - INFO - __main__ - Classification-F1 on test data: 0.2573
03/16/2022 11:19:53 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.7327787155373362, test_performance=0.25728205394361064
03/16/2022 11:19:53 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
03/16/2022 11:19:54 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:19:54 - INFO - __main__ - Printing 3 examples
03/16/2022 11:19:54 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/16/2022 11:19:54 - INFO - __main__ - ['happy']
03/16/2022 11:19:54 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/16/2022 11:19:54 - INFO - __main__ - ['happy']
03/16/2022 11:19:54 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/16/2022 11:19:54 - INFO - __main__ - ['happy']
03/16/2022 11:19:54 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:19:54 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:19:54 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 11:19:54 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:19:54 - INFO - __main__ - Printing 3 examples
03/16/2022 11:19:54 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/16/2022 11:19:54 - INFO - __main__ - ['happy']
03/16/2022 11:19:54 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/16/2022 11:19:54 - INFO - __main__ - ['happy']
03/16/2022 11:19:54 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/16/2022 11:19:54 - INFO - __main__ - ['happy']
03/16/2022 11:19:54 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:19:54 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:19:54 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 11:20:09 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 11:20:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 11:20:10 - INFO - __main__ - Starting training!
03/16/2022 11:20:13 - INFO - __main__ - Step 10 Global step 10 Train loss 3.06 on epoch=2
03/16/2022 11:20:15 - INFO - __main__ - Step 20 Global step 20 Train loss 1.80 on epoch=4
03/16/2022 11:20:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.41 on epoch=7
03/16/2022 11:20:20 - INFO - __main__ - Step 40 Global step 40 Train loss 1.17 on epoch=9
03/16/2022 11:20:23 - INFO - __main__ - Step 50 Global step 50 Train loss 1.08 on epoch=12
03/16/2022 11:20:24 - INFO - __main__ - Global step 50 Train loss 1.71 Classification-F1 0.31853991596638653 on epoch=12
03/16/2022 11:20:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.31853991596638653 on epoch=12, global_step=50
03/16/2022 11:20:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=14
03/16/2022 11:20:29 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=17
03/16/2022 11:20:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=19
03/16/2022 11:20:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.96 on epoch=22
03/16/2022 11:20:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.95 on epoch=24
03/16/2022 11:20:37 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.16108564637976402 on epoch=24
03/16/2022 11:20:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.96 on epoch=27
03/16/2022 11:20:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.94 on epoch=29
03/16/2022 11:20:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.94 on epoch=32
03/16/2022 11:20:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
03/16/2022 11:20:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.78 on epoch=37
03/16/2022 11:20:50 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.36588315217391304 on epoch=37
03/16/2022 11:20:50 - INFO - __main__ - Saving model with best Classification-F1: 0.31853991596638653 -> 0.36588315217391304 on epoch=37, global_step=150
03/16/2022 11:20:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=39
03/16/2022 11:20:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.90 on epoch=42
03/16/2022 11:20:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.82 on epoch=44
03/16/2022 11:21:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.83 on epoch=47
03/16/2022 11:21:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.81 on epoch=49
03/16/2022 11:21:03 - INFO - __main__ - Global step 200 Train loss 0.87 Classification-F1 0.36404761904761906 on epoch=49
03/16/2022 11:21:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=52
03/16/2022 11:21:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=54
03/16/2022 11:21:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.78 on epoch=57
03/16/2022 11:21:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.73 on epoch=59
03/16/2022 11:21:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.77 on epoch=62
03/16/2022 11:21:16 - INFO - __main__ - Global step 250 Train loss 0.79 Classification-F1 0.5288795660649207 on epoch=62
03/16/2022 11:21:16 - INFO - __main__ - Saving model with best Classification-F1: 0.36588315217391304 -> 0.5288795660649207 on epoch=62, global_step=250
03/16/2022 11:21:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.76 on epoch=64
03/16/2022 11:21:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.69 on epoch=67
03/16/2022 11:21:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.75 on epoch=69
03/16/2022 11:21:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.76 on epoch=72
03/16/2022 11:21:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=74
03/16/2022 11:21:30 - INFO - __main__ - Global step 300 Train loss 0.73 Classification-F1 0.5171274961597543 on epoch=74
03/16/2022 11:21:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.74 on epoch=77
03/16/2022 11:21:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=79
03/16/2022 11:21:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.65 on epoch=82
03/16/2022 11:21:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.61 on epoch=84
03/16/2022 11:21:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.60 on epoch=87
03/16/2022 11:21:43 - INFO - __main__ - Global step 350 Train loss 0.65 Classification-F1 0.48929738562091496 on epoch=87
03/16/2022 11:21:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.66 on epoch=89
03/16/2022 11:21:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.57 on epoch=92
03/16/2022 11:21:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.69 on epoch=94
03/16/2022 11:21:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.55 on epoch=97
03/16/2022 11:21:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.58 on epoch=99
03/16/2022 11:21:56 - INFO - __main__ - Global step 400 Train loss 0.61 Classification-F1 0.600755278174633 on epoch=99
03/16/2022 11:21:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5288795660649207 -> 0.600755278174633 on epoch=99, global_step=400
03/16/2022 11:21:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.66 on epoch=102
03/16/2022 11:22:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.56 on epoch=104
03/16/2022 11:22:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=107
03/16/2022 11:22:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.70 on epoch=109
03/16/2022 11:22:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=112
03/16/2022 11:22:09 - INFO - __main__ - Global step 450 Train loss 0.60 Classification-F1 0.4898520161678056 on epoch=112
03/16/2022 11:22:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=114
03/16/2022 11:22:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=117
03/16/2022 11:22:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.67 on epoch=119
03/16/2022 11:22:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=122
03/16/2022 11:22:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=124
03/16/2022 11:22:22 - INFO - __main__ - Global step 500 Train loss 0.54 Classification-F1 0.6499183006535948 on epoch=124
03/16/2022 11:22:22 - INFO - __main__ - Saving model with best Classification-F1: 0.600755278174633 -> 0.6499183006535948 on epoch=124, global_step=500
03/16/2022 11:22:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=127
03/16/2022 11:22:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=129
03/16/2022 11:22:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=132
03/16/2022 11:22:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.54 on epoch=134
03/16/2022 11:22:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=137
03/16/2022 11:22:35 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.563278589594379 on epoch=137
03/16/2022 11:22:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=139
03/16/2022 11:22:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=142
03/16/2022 11:22:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=144
03/16/2022 11:22:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=147
03/16/2022 11:22:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=149
03/16/2022 11:22:48 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.6403430860327411 on epoch=149
03/16/2022 11:22:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=152
03/16/2022 11:22:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=154
03/16/2022 11:22:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=157
03/16/2022 11:22:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=159
03/16/2022 11:23:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=162
03/16/2022 11:23:01 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.5759969028261711 on epoch=162
03/16/2022 11:23:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=164
03/16/2022 11:23:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=167
03/16/2022 11:23:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=169
03/16/2022 11:23:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=172
03/16/2022 11:23:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=174
03/16/2022 11:23:14 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.6341666666666667 on epoch=174
03/16/2022 11:23:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=177
03/16/2022 11:23:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=179
03/16/2022 11:23:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=182
03/16/2022 11:23:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=184
03/16/2022 11:23:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=187
03/16/2022 11:23:27 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.6242833742833742 on epoch=187
03/16/2022 11:23:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.30 on epoch=189
03/16/2022 11:23:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=192
03/16/2022 11:23:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=194
03/16/2022 11:23:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=197
03/16/2022 11:23:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=199
03/16/2022 11:23:40 - INFO - __main__ - Global step 800 Train loss 0.29 Classification-F1 0.6522727272727272 on epoch=199
03/16/2022 11:23:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6499183006535948 -> 0.6522727272727272 on epoch=199, global_step=800
03/16/2022 11:23:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=202
03/16/2022 11:23:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=204
03/16/2022 11:23:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=207
03/16/2022 11:23:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=209
03/16/2022 11:23:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=212
03/16/2022 11:23:53 - INFO - __main__ - Global step 850 Train loss 0.28 Classification-F1 0.6044659248435802 on epoch=212
03/16/2022 11:23:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=214
03/16/2022 11:23:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=217
03/16/2022 11:24:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=219
03/16/2022 11:24:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=222
03/16/2022 11:24:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.27 on epoch=224
03/16/2022 11:24:06 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.604040404040404 on epoch=224
03/16/2022 11:24:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=227
03/16/2022 11:24:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=229
03/16/2022 11:24:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=232
03/16/2022 11:24:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=234
03/16/2022 11:24:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=237
03/16/2022 11:24:19 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.6245475113122172 on epoch=237
03/16/2022 11:24:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=239
03/16/2022 11:24:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=242
03/16/2022 11:24:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=244
03/16/2022 11:24:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.31 on epoch=247
03/16/2022 11:24:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=249
03/16/2022 11:24:32 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.6506674898515505 on epoch=249
03/16/2022 11:24:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=252
03/16/2022 11:24:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=254
03/16/2022 11:24:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=257
03/16/2022 11:24:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=259
03/16/2022 11:24:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=262
03/16/2022 11:24:45 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.6683695507708667 on epoch=262
03/16/2022 11:24:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6522727272727272 -> 0.6683695507708667 on epoch=262, global_step=1050
03/16/2022 11:24:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=264
03/16/2022 11:24:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=267
03/16/2022 11:24:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=269
03/16/2022 11:24:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=272
03/16/2022 11:24:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=274
03/16/2022 11:24:58 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.620069745069745 on epoch=274
03/16/2022 11:25:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=277
03/16/2022 11:25:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=279
03/16/2022 11:25:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=282
03/16/2022 11:25:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=284
03/16/2022 11:25:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=287
03/16/2022 11:25:11 - INFO - __main__ - Global step 1150 Train loss 0.16 Classification-F1 0.6066295546558704 on epoch=287
03/16/2022 11:25:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=289
03/16/2022 11:25:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=292
03/16/2022 11:25:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=294
03/16/2022 11:25:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=297
03/16/2022 11:25:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=299
03/16/2022 11:25:24 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.6978431372549019 on epoch=299
03/16/2022 11:25:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6683695507708667 -> 0.6978431372549019 on epoch=299, global_step=1200
03/16/2022 11:25:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
03/16/2022 11:25:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=304
03/16/2022 11:25:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=307
03/16/2022 11:25:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=309
03/16/2022 11:25:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=312
03/16/2022 11:25:37 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.6493243243243243 on epoch=312
03/16/2022 11:25:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
03/16/2022 11:25:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=317
03/16/2022 11:25:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
03/16/2022 11:25:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
03/16/2022 11:25:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
03/16/2022 11:25:50 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.7372722997722998 on epoch=324
03/16/2022 11:25:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6978431372549019 -> 0.7372722997722998 on epoch=324, global_step=1300
03/16/2022 11:25:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=327
03/16/2022 11:25:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=329
03/16/2022 11:25:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=332
03/16/2022 11:26:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
03/16/2022 11:26:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=337
03/16/2022 11:26:04 - INFO - __main__ - Global step 1350 Train loss 0.12 Classification-F1 0.632926632191338 on epoch=337
03/16/2022 11:26:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=339
03/16/2022 11:26:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
03/16/2022 11:26:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=344
03/16/2022 11:26:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
03/16/2022 11:26:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
03/16/2022 11:26:17 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.6415286938574056 on epoch=349
03/16/2022 11:26:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
03/16/2022 11:26:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=354
03/16/2022 11:26:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=357
03/16/2022 11:26:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=359
03/16/2022 11:26:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=362
03/16/2022 11:26:30 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.6075523594909862 on epoch=362
03/16/2022 11:26:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
03/16/2022 11:26:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
03/16/2022 11:26:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
03/16/2022 11:26:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=372
03/16/2022 11:26:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
03/16/2022 11:26:44 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.6493233082706766 on epoch=374
03/16/2022 11:26:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
03/16/2022 11:26:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
03/16/2022 11:26:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
03/16/2022 11:26:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=384
03/16/2022 11:26:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=387
03/16/2022 11:26:57 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.6736111111111112 on epoch=387
03/16/2022 11:26:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
03/16/2022 11:27:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=392
03/16/2022 11:27:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
03/16/2022 11:27:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=397
03/16/2022 11:27:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=399
03/16/2022 11:27:10 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.7160138571829069 on epoch=399
03/16/2022 11:27:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
03/16/2022 11:27:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=404
03/16/2022 11:27:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=407
03/16/2022 11:27:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
03/16/2022 11:27:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=412
03/16/2022 11:27:23 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.6264200532493216 on epoch=412
03/16/2022 11:27:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=414
03/16/2022 11:27:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=417
03/16/2022 11:27:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=419
03/16/2022 11:27:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
03/16/2022 11:27:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
03/16/2022 11:27:37 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.6937815126050422 on epoch=424
03/16/2022 11:27:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
03/16/2022 11:27:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
03/16/2022 11:27:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
03/16/2022 11:27:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
03/16/2022 11:27:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=437
03/16/2022 11:27:50 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.6513637506284565 on epoch=437
03/16/2022 11:27:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=439
03/16/2022 11:27:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
03/16/2022 11:27:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
03/16/2022 11:28:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
03/16/2022 11:28:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
03/16/2022 11:28:03 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.6264915264915265 on epoch=449
03/16/2022 11:28:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
03/16/2022 11:28:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
03/16/2022 11:28:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
03/16/2022 11:28:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=459
03/16/2022 11:28:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/16/2022 11:28:17 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6233122620436438 on epoch=462
03/16/2022 11:28:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=464
03/16/2022 11:28:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
03/16/2022 11:28:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
03/16/2022 11:28:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
03/16/2022 11:28:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=474
03/16/2022 11:28:30 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.6257960449136919 on epoch=474
03/16/2022 11:28:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
03/16/2022 11:28:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
03/16/2022 11:28:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
03/16/2022 11:28:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
03/16/2022 11:28:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
03/16/2022 11:28:43 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7077547199251053 on epoch=487
03/16/2022 11:28:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
03/16/2022 11:28:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/16/2022 11:28:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=494
03/16/2022 11:28:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=497
03/16/2022 11:28:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 11:28:56 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.6930148555148555 on epoch=499
03/16/2022 11:28:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
03/16/2022 11:29:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 11:29:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
03/16/2022 11:29:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
03/16/2022 11:29:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/16/2022 11:29:10 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6138181993445151 on epoch=512
03/16/2022 11:29:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/16/2022 11:29:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=517
03/16/2022 11:29:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
03/16/2022 11:29:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
03/16/2022 11:29:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
03/16/2022 11:29:23 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.5966373587743824 on epoch=524
03/16/2022 11:29:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
03/16/2022 11:29:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 11:29:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 11:29:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
03/16/2022 11:29:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/16/2022 11:29:36 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6586505860699409 on epoch=537
03/16/2022 11:29:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
03/16/2022 11:29:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=542
03/16/2022 11:29:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
03/16/2022 11:29:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
03/16/2022 11:29:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
03/16/2022 11:29:50 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6661661661661661 on epoch=549
03/16/2022 11:29:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/16/2022 11:29:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
03/16/2022 11:29:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
03/16/2022 11:30:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/16/2022 11:30:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
03/16/2022 11:30:03 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6593990632690323 on epoch=562
03/16/2022 11:30:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/16/2022 11:30:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
03/16/2022 11:30:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
03/16/2022 11:30:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
03/16/2022 11:30:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
03/16/2022 11:30:17 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6600233100233099 on epoch=574
03/16/2022 11:30:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/16/2022 11:30:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/16/2022 11:30:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/16/2022 11:30:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 11:30:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=587
03/16/2022 11:30:30 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.658961038961039 on epoch=587
03/16/2022 11:30:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 11:30:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=592
03/16/2022 11:30:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
03/16/2022 11:30:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
03/16/2022 11:30:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
03/16/2022 11:30:43 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.6510936431989063 on epoch=599
03/16/2022 11:30:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
03/16/2022 11:30:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
03/16/2022 11:30:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/16/2022 11:30:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
03/16/2022 11:30:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
03/16/2022 11:30:57 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6840848806366049 on epoch=612
03/16/2022 11:30:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/16/2022 11:31:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
03/16/2022 11:31:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=619
03/16/2022 11:31:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/16/2022 11:31:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/16/2022 11:31:10 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6447616971810519 on epoch=624
03/16/2022 11:31:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/16/2022 11:31:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 11:31:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/16/2022 11:31:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
03/16/2022 11:31:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/16/2022 11:31:23 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6723441668687848 on epoch=637
03/16/2022 11:31:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
03/16/2022 11:31:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/16/2022 11:31:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/16/2022 11:31:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/16/2022 11:31:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
03/16/2022 11:31:36 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.632051282051282 on epoch=649
03/16/2022 11:31:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/16/2022 11:31:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/16/2022 11:31:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
03/16/2022 11:31:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/16/2022 11:31:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/16/2022 11:31:50 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6501217532467531 on epoch=662
03/16/2022 11:31:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
03/16/2022 11:31:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
03/16/2022 11:31:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/16/2022 11:32:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
03/16/2022 11:32:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=674
03/16/2022 11:32:03 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6548387096774193 on epoch=674
03/16/2022 11:32:05 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
03/16/2022 11:32:08 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/16/2022 11:32:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
03/16/2022 11:32:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
03/16/2022 11:32:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
03/16/2022 11:32:16 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6089285714285715 on epoch=687
03/16/2022 11:32:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
03/16/2022 11:32:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
03/16/2022 11:32:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=694
03/16/2022 11:32:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
03/16/2022 11:32:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=699
03/16/2022 11:32:29 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.6302083333333333 on epoch=699
03/16/2022 11:32:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
03/16/2022 11:32:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/16/2022 11:32:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/16/2022 11:32:38 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/16/2022 11:32:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/16/2022 11:32:41 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6298039215686274 on epoch=712
03/16/2022 11:32:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/16/2022 11:32:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=717
03/16/2022 11:32:48 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
03/16/2022 11:32:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=722
03/16/2022 11:32:53 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/16/2022 11:32:54 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6359616074986094 on epoch=724
03/16/2022 11:32:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
03/16/2022 11:32:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 11:33:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
03/16/2022 11:33:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=734
03/16/2022 11:33:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
03/16/2022 11:33:07 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.6213406611479634 on epoch=737
03/16/2022 11:33:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
03/16/2022 11:33:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 11:33:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/16/2022 11:33:16 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
03/16/2022 11:33:18 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.25 on epoch=749
03/16/2022 11:33:19 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.6293859649122807 on epoch=749
03/16/2022 11:33:19 - INFO - __main__ - save last model!
03/16/2022 11:33:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 11:33:19 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 11:33:19 - INFO - __main__ - Printing 3 examples
03/16/2022 11:33:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 11:33:19 - INFO - __main__ - ['others']
03/16/2022 11:33:19 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 11:33:19 - INFO - __main__ - ['others']
03/16/2022 11:33:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 11:33:19 - INFO - __main__ - ['others']
03/16/2022 11:33:19 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:33:20 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:33:20 - INFO - __main__ - Printing 3 examples
03/16/2022 11:33:20 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/16/2022 11:33:20 - INFO - __main__ - ['others']
03/16/2022 11:33:20 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/16/2022 11:33:20 - INFO - __main__ - ['others']
03/16/2022 11:33:20 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/16/2022 11:33:20 - INFO - __main__ - ['others']
03/16/2022 11:33:20 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:33:20 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:33:20 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 11:33:20 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:33:20 - INFO - __main__ - Printing 3 examples
03/16/2022 11:33:20 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/16/2022 11:33:20 - INFO - __main__ - ['others']
03/16/2022 11:33:20 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/16/2022 11:33:20 - INFO - __main__ - ['others']
03/16/2022 11:33:20 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/16/2022 11:33:20 - INFO - __main__ - ['others']
03/16/2022 11:33:20 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:33:20 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:33:20 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 11:33:21 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:33:27 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 11:33:36 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 11:33:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 11:33:36 - INFO - __main__ - Starting training!
03/16/2022 11:35:00 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_42_0.2_8_predictions.txt
03/16/2022 11:35:00 - INFO - __main__ - Classification-F1 on test data: 0.1445
03/16/2022 11:35:00 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.7372722997722998, test_performance=0.14450394117263018
03/16/2022 11:35:00 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
03/16/2022 11:35:01 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:35:01 - INFO - __main__ - Printing 3 examples
03/16/2022 11:35:01 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/16/2022 11:35:01 - INFO - __main__ - ['others']
03/16/2022 11:35:01 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/16/2022 11:35:01 - INFO - __main__ - ['others']
03/16/2022 11:35:01 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/16/2022 11:35:01 - INFO - __main__ - ['others']
03/16/2022 11:35:01 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:35:01 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:35:01 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 11:35:01 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:35:01 - INFO - __main__ - Printing 3 examples
03/16/2022 11:35:01 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/16/2022 11:35:01 - INFO - __main__ - ['others']
03/16/2022 11:35:01 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/16/2022 11:35:01 - INFO - __main__ - ['others']
03/16/2022 11:35:01 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/16/2022 11:35:01 - INFO - __main__ - ['others']
03/16/2022 11:35:01 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:35:01 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:35:01 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 11:35:20 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 11:35:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 11:35:21 - INFO - __main__ - Starting training!
03/16/2022 11:35:24 - INFO - __main__ - Step 10 Global step 10 Train loss 2.35 on epoch=2
03/16/2022 11:35:26 - INFO - __main__ - Step 20 Global step 20 Train loss 1.07 on epoch=4
03/16/2022 11:35:28 - INFO - __main__ - Step 30 Global step 30 Train loss 1.07 on epoch=7
03/16/2022 11:35:31 - INFO - __main__ - Step 40 Global step 40 Train loss 1.02 on epoch=9
03/16/2022 11:35:33 - INFO - __main__ - Step 50 Global step 50 Train loss 1.07 on epoch=12
03/16/2022 11:35:34 - INFO - __main__ - Global step 50 Train loss 1.32 Classification-F1 0.13067758749069247 on epoch=12
03/16/2022 11:35:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=12, global_step=50
03/16/2022 11:35:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.99 on epoch=14
03/16/2022 11:35:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=17
03/16/2022 11:35:41 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=19
03/16/2022 11:35:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.95 on epoch=22
03/16/2022 11:35:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.88 on epoch=24
03/16/2022 11:35:46 - INFO - __main__ - Global step 100 Train loss 0.93 Classification-F1 0.2709576584256891 on epoch=24
03/16/2022 11:35:46 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.2709576584256891 on epoch=24, global_step=100
03/16/2022 11:35:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=27
03/16/2022 11:35:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=29
03/16/2022 11:35:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=32
03/16/2022 11:35:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=34
03/16/2022 11:35:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.75 on epoch=37
03/16/2022 11:35:58 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.13067758749069247 on epoch=37
03/16/2022 11:36:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.70 on epoch=39
03/16/2022 11:36:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.66 on epoch=42
03/16/2022 11:36:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.62 on epoch=44
03/16/2022 11:36:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.63 on epoch=47
03/16/2022 11:36:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.66 on epoch=49
03/16/2022 11:36:11 - INFO - __main__ - Global step 200 Train loss 0.65 Classification-F1 0.4639289446185998 on epoch=49
03/16/2022 11:36:11 - INFO - __main__ - Saving model with best Classification-F1: 0.2709576584256891 -> 0.4639289446185998 on epoch=49, global_step=200
03/16/2022 11:36:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.60 on epoch=52
03/16/2022 11:36:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.60 on epoch=54
03/16/2022 11:36:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=57
03/16/2022 11:36:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.64 on epoch=59
03/16/2022 11:36:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.78 on epoch=62
03/16/2022 11:36:23 - INFO - __main__ - Global step 250 Train loss 0.63 Classification-F1 0.42948717948717946 on epoch=62
03/16/2022 11:36:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.71 on epoch=64
03/16/2022 11:36:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=67
03/16/2022 11:36:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=69
03/16/2022 11:36:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=72
03/16/2022 11:36:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=74
03/16/2022 11:36:35 - INFO - __main__ - Global step 300 Train loss 0.56 Classification-F1 0.5208405629458261 on epoch=74
03/16/2022 11:36:35 - INFO - __main__ - Saving model with best Classification-F1: 0.4639289446185998 -> 0.5208405629458261 on epoch=74, global_step=300
03/16/2022 11:36:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=77
03/16/2022 11:36:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=79
03/16/2022 11:36:42 - INFO - __main__ - Step 330 Global step 330 Train loss 1.00 on epoch=82
03/16/2022 11:36:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=84
03/16/2022 11:36:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=87
03/16/2022 11:36:48 - INFO - __main__ - Global step 350 Train loss 0.54 Classification-F1 0.7275725232621784 on epoch=87
03/16/2022 11:36:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5208405629458261 -> 0.7275725232621784 on epoch=87, global_step=350
03/16/2022 11:36:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=89
03/16/2022 11:36:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=92
03/16/2022 11:36:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=94
03/16/2022 11:36:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=97
03/16/2022 11:36:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=99
03/16/2022 11:37:00 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.7517156862745098 on epoch=99
03/16/2022 11:37:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7275725232621784 -> 0.7517156862745098 on epoch=99, global_step=400
03/16/2022 11:37:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=102
03/16/2022 11:37:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=104
03/16/2022 11:37:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=107
03/16/2022 11:37:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=109
03/16/2022 11:37:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=112
03/16/2022 11:37:12 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.7116155252997358 on epoch=112
03/16/2022 11:37:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=114
03/16/2022 11:37:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=117
03/16/2022 11:37:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=119
03/16/2022 11:37:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=122
03/16/2022 11:37:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=124
03/16/2022 11:37:25 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.7768928803411562 on epoch=124
03/16/2022 11:37:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7517156862745098 -> 0.7768928803411562 on epoch=124, global_step=500
03/16/2022 11:37:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=127
03/16/2022 11:37:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=129
03/16/2022 11:37:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=132
03/16/2022 11:37:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=134
03/16/2022 11:37:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=137
03/16/2022 11:37:37 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.7798132183908045 on epoch=137
03/16/2022 11:37:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7768928803411562 -> 0.7798132183908045 on epoch=137, global_step=550
03/16/2022 11:37:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=139
03/16/2022 11:37:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=142
03/16/2022 11:37:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=144
03/16/2022 11:37:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=147
03/16/2022 11:37:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=149
03/16/2022 11:37:49 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.680479242979243 on epoch=149
03/16/2022 11:37:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=152
03/16/2022 11:37:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=154
03/16/2022 11:37:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=157
03/16/2022 11:37:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=159
03/16/2022 11:38:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=162
03/16/2022 11:38:01 - INFO - __main__ - Global step 650 Train loss 0.27 Classification-F1 0.7513880859469095 on epoch=162
03/16/2022 11:38:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=164
03/16/2022 11:38:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=167
03/16/2022 11:38:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=169
03/16/2022 11:38:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=172
03/16/2022 11:38:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=174
03/16/2022 11:38:14 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.6670954681990335 on epoch=174
03/16/2022 11:38:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=177
03/16/2022 11:38:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=179
03/16/2022 11:38:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=182
03/16/2022 11:38:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=184
03/16/2022 11:38:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=187
03/16/2022 11:38:26 - INFO - __main__ - Global step 750 Train loss 0.19 Classification-F1 0.8007563025210085 on epoch=187
03/16/2022 11:38:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7798132183908045 -> 0.8007563025210085 on epoch=187, global_step=750
03/16/2022 11:38:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=189
03/16/2022 11:38:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=192
03/16/2022 11:38:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=194
03/16/2022 11:38:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=197
03/16/2022 11:38:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=199
03/16/2022 11:38:38 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.6830827067669173 on epoch=199
03/16/2022 11:38:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=202
03/16/2022 11:38:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=204
03/16/2022 11:38:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=207
03/16/2022 11:38:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=209
03/16/2022 11:38:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=212
03/16/2022 11:38:50 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.7663349490795144 on epoch=212
03/16/2022 11:38:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
03/16/2022 11:38:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.11 on epoch=217
03/16/2022 11:38:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=219
03/16/2022 11:39:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=222
03/16/2022 11:39:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=224
03/16/2022 11:39:03 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.7359660080248316 on epoch=224
03/16/2022 11:39:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=227
03/16/2022 11:39:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=229
03/16/2022 11:39:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
03/16/2022 11:39:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=234
03/16/2022 11:39:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=237
03/16/2022 11:39:15 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.7156949221377533 on epoch=237
03/16/2022 11:39:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
03/16/2022 11:39:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=242
03/16/2022 11:39:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
03/16/2022 11:39:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
03/16/2022 11:39:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=249
03/16/2022 11:39:27 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.773940288646171 on epoch=249
03/16/2022 11:39:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
03/16/2022 11:39:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
03/16/2022 11:39:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
03/16/2022 11:39:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=259
03/16/2022 11:39:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
03/16/2022 11:39:40 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.7807453416149068 on epoch=262
03/16/2022 11:39:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=264
03/16/2022 11:39:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=267
03/16/2022 11:39:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
03/16/2022 11:39:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
03/16/2022 11:39:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=274
03/16/2022 11:39:52 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.7581523648886078 on epoch=274
03/16/2022 11:39:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
03/16/2022 11:39:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
03/16/2022 11:39:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
03/16/2022 11:40:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
03/16/2022 11:40:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
03/16/2022 11:40:05 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.7669530483818241 on epoch=287
03/16/2022 11:40:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
03/16/2022 11:40:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
03/16/2022 11:40:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
03/16/2022 11:40:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
03/16/2022 11:40:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=299
03/16/2022 11:40:17 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6215986394557823 on epoch=299
03/16/2022 11:40:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=302
03/16/2022 11:40:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
03/16/2022 11:40:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
03/16/2022 11:40:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=309
03/16/2022 11:40:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=312
03/16/2022 11:40:30 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.703886787530977 on epoch=312
03/16/2022 11:40:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
03/16/2022 11:40:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
03/16/2022 11:40:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
03/16/2022 11:40:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
03/16/2022 11:40:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=324
03/16/2022 11:40:42 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.6822368421052631 on epoch=324
03/16/2022 11:40:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
03/16/2022 11:40:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
03/16/2022 11:40:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
03/16/2022 11:40:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
03/16/2022 11:40:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
03/16/2022 11:40:54 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.7016768292682927 on epoch=337
03/16/2022 11:40:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
03/16/2022 11:40:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
03/16/2022 11:41:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
03/16/2022 11:41:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=347
03/16/2022 11:41:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
03/16/2022 11:41:07 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.7378687428029533 on epoch=349
03/16/2022 11:41:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
03/16/2022 11:41:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
03/16/2022 11:41:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
03/16/2022 11:41:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/16/2022 11:41:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
03/16/2022 11:41:19 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.7595208845208845 on epoch=362
03/16/2022 11:41:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=364
03/16/2022 11:41:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
03/16/2022 11:41:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=369
03/16/2022 11:41:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
03/16/2022 11:41:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
03/16/2022 11:41:32 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.7449874686716793 on epoch=374
03/16/2022 11:41:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
03/16/2022 11:41:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
03/16/2022 11:41:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=382
03/16/2022 11:41:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
03/16/2022 11:41:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
03/16/2022 11:41:44 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.7458139563494992 on epoch=387
03/16/2022 11:41:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
03/16/2022 11:41:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=392
03/16/2022 11:41:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
03/16/2022 11:41:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
03/16/2022 11:41:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
03/16/2022 11:41:57 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.731077745783628 on epoch=399
03/16/2022 11:41:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
03/16/2022 11:42:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
03/16/2022 11:42:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
03/16/2022 11:42:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
03/16/2022 11:42:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
03/16/2022 11:42:09 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7377403846153846 on epoch=412
03/16/2022 11:42:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
03/16/2022 11:42:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
03/16/2022 11:42:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 11:42:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
03/16/2022 11:42:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
03/16/2022 11:42:22 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6857822410147991 on epoch=424
03/16/2022 11:42:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
03/16/2022 11:42:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
03/16/2022 11:42:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=432
03/16/2022 11:42:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/16/2022 11:42:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/16/2022 11:42:34 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.731951871657754 on epoch=437
03/16/2022 11:42:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
03/16/2022 11:42:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
03/16/2022 11:42:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/16/2022 11:42:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/16/2022 11:42:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
03/16/2022 11:42:46 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7596545743604567 on epoch=449
03/16/2022 11:42:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
03/16/2022 11:42:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/16/2022 11:42:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
03/16/2022 11:42:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
03/16/2022 11:42:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
03/16/2022 11:42:59 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7608153907496013 on epoch=462
03/16/2022 11:43:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
03/16/2022 11:43:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/16/2022 11:43:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/16/2022 11:43:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/16/2022 11:43:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=474
03/16/2022 11:43:11 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7752904989747095 on epoch=474
03/16/2022 11:43:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
03/16/2022 11:43:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
03/16/2022 11:43:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
03/16/2022 11:43:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/16/2022 11:43:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
03/16/2022 11:43:23 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7509805118500771 on epoch=487
03/16/2022 11:43:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
03/16/2022 11:43:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/16/2022 11:43:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
03/16/2022 11:43:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/16/2022 11:43:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 11:43:36 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7659694312507612 on epoch=499
03/16/2022 11:43:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/16/2022 11:43:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
03/16/2022 11:43:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
03/16/2022 11:43:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/16/2022 11:43:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/16/2022 11:43:48 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7266068012570882 on epoch=512
03/16/2022 11:43:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/16/2022 11:43:53 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
03/16/2022 11:43:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/16/2022 11:43:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/16/2022 11:44:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 11:44:01 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7961133069828722 on epoch=524
03/16/2022 11:44:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/16/2022 11:44:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
03/16/2022 11:44:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 11:44:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
03/16/2022 11:44:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/16/2022 11:44:13 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7960530863344163 on epoch=537
03/16/2022 11:44:16 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/16/2022 11:44:18 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/16/2022 11:44:20 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/16/2022 11:44:23 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/16/2022 11:44:25 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
03/16/2022 11:44:26 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7378687428029533 on epoch=549
03/16/2022 11:44:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/16/2022 11:44:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/16/2022 11:44:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/16/2022 11:44:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/16/2022 11:44:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/16/2022 11:44:38 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7819599490795144 on epoch=562
03/16/2022 11:44:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/16/2022 11:44:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/16/2022 11:44:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/16/2022 11:44:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=572
03/16/2022 11:44:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.09 on epoch=574
03/16/2022 11:44:51 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7449486920075155 on epoch=574
03/16/2022 11:44:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/16/2022 11:44:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/16/2022 11:44:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/16/2022 11:45:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
03/16/2022 11:45:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
03/16/2022 11:45:03 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7302428096545743 on epoch=587
03/16/2022 11:45:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=589
03/16/2022 11:45:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/16/2022 11:45:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/16/2022 11:45:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/16/2022 11:45:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=599
03/16/2022 11:45:16 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.7443693693693694 on epoch=599
03/16/2022 11:45:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
03/16/2022 11:45:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/16/2022 11:45:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/16/2022 11:45:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/16/2022 11:45:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/16/2022 11:45:28 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.8020966420966421 on epoch=612
03/16/2022 11:45:28 - INFO - __main__ - Saving model with best Classification-F1: 0.8007563025210085 -> 0.8020966420966421 on epoch=612, global_step=2450
03/16/2022 11:45:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/16/2022 11:45:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 11:45:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 11:45:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/16/2022 11:45:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
03/16/2022 11:45:40 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.8165584415584416 on epoch=624
03/16/2022 11:45:40 - INFO - __main__ - Saving model with best Classification-F1: 0.8020966420966421 -> 0.8165584415584416 on epoch=624, global_step=2500
03/16/2022 11:45:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/16/2022 11:45:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/16/2022 11:45:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/16/2022 11:45:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=634
03/16/2022 11:45:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/16/2022 11:45:53 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7746723996723998 on epoch=637
03/16/2022 11:45:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/16/2022 11:45:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=642
03/16/2022 11:46:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
03/16/2022 11:46:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/16/2022 11:46:04 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
03/16/2022 11:46:06 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.758982683982684 on epoch=649
03/16/2022 11:46:08 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/16/2022 11:46:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/16/2022 11:46:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/16/2022 11:46:15 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
03/16/2022 11:46:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/16/2022 11:46:18 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7746723996723998 on epoch=662
03/16/2022 11:46:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/16/2022 11:46:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=667
03/16/2022 11:46:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/16/2022 11:46:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/16/2022 11:46:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
03/16/2022 11:46:30 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6830683624801271 on epoch=674
03/16/2022 11:46:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/16/2022 11:46:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/16/2022 11:46:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/16/2022 11:46:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/16/2022 11:46:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=687
03/16/2022 11:46:43 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7746723996723998 on epoch=687
03/16/2022 11:46:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
03/16/2022 11:46:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/16/2022 11:46:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
03/16/2022 11:46:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/16/2022 11:46:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
03/16/2022 11:46:55 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7765322912381736 on epoch=699
03/16/2022 11:46:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/16/2022 11:47:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=704
03/16/2022 11:47:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=707
03/16/2022 11:47:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
03/16/2022 11:47:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/16/2022 11:47:08 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7292690417690417 on epoch=712
03/16/2022 11:47:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/16/2022 11:47:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
03/16/2022 11:47:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/16/2022 11:47:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/16/2022 11:47:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
03/16/2022 11:47:20 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7320560232101994 on epoch=724
03/16/2022 11:47:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=727
03/16/2022 11:47:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
03/16/2022 11:47:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 11:47:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=734
03/16/2022 11:47:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
03/16/2022 11:47:33 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7608153907496013 on epoch=737
03/16/2022 11:47:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/16/2022 11:47:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 11:47:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/16/2022 11:47:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
03/16/2022 11:47:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/16/2022 11:47:45 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7752904989747095 on epoch=749
03/16/2022 11:47:45 - INFO - __main__ - save last model!
03/16/2022 11:47:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 11:47:45 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 11:47:45 - INFO - __main__ - Printing 3 examples
03/16/2022 11:47:45 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 11:47:45 - INFO - __main__ - ['others']
03/16/2022 11:47:45 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 11:47:45 - INFO - __main__ - ['others']
03/16/2022 11:47:45 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 11:47:45 - INFO - __main__ - ['others']
03/16/2022 11:47:45 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:47:45 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:47:45 - INFO - __main__ - Printing 3 examples
03/16/2022 11:47:45 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/16/2022 11:47:45 - INFO - __main__ - ['others']
03/16/2022 11:47:45 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/16/2022 11:47:45 - INFO - __main__ - ['others']
03/16/2022 11:47:45 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/16/2022 11:47:45 - INFO - __main__ - ['others']
03/16/2022 11:47:45 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:47:45 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:47:45 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 11:47:45 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:47:45 - INFO - __main__ - Printing 3 examples
03/16/2022 11:47:45 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/16/2022 11:47:45 - INFO - __main__ - ['others']
03/16/2022 11:47:45 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/16/2022 11:47:45 - INFO - __main__ - ['others']
03/16/2022 11:47:45 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/16/2022 11:47:45 - INFO - __main__ - ['others']
03/16/2022 11:47:45 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:47:45 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:47:46 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 11:47:47 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:47:53 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 11:48:01 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 11:48:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 11:48:02 - INFO - __main__ - Starting training!
03/16/2022 11:49:07 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_87_0.5_8_predictions.txt
03/16/2022 11:49:07 - INFO - __main__ - Classification-F1 on test data: 0.3716
03/16/2022 11:49:07 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.8165584415584416, test_performance=0.37160201777480717
03/16/2022 11:49:07 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
03/16/2022 11:49:08 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:49:08 - INFO - __main__ - Printing 3 examples
03/16/2022 11:49:08 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/16/2022 11:49:08 - INFO - __main__ - ['others']
03/16/2022 11:49:08 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/16/2022 11:49:08 - INFO - __main__ - ['others']
03/16/2022 11:49:08 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/16/2022 11:49:08 - INFO - __main__ - ['others']
03/16/2022 11:49:08 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:49:08 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:49:08 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 11:49:08 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 11:49:08 - INFO - __main__ - Printing 3 examples
03/16/2022 11:49:08 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/16/2022 11:49:08 - INFO - __main__ - ['others']
03/16/2022 11:49:08 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/16/2022 11:49:08 - INFO - __main__ - ['others']
03/16/2022 11:49:08 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/16/2022 11:49:08 - INFO - __main__ - ['others']
03/16/2022 11:49:08 - INFO - __main__ - Tokenizing Input ...
03/16/2022 11:49:08 - INFO - __main__ - Tokenizing Output ...
03/16/2022 11:49:08 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 11:49:23 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 11:49:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 11:49:24 - INFO - __main__ - Starting training!
03/16/2022 11:49:27 - INFO - __main__ - Step 10 Global step 10 Train loss 2.60 on epoch=2
03/16/2022 11:49:29 - INFO - __main__ - Step 20 Global step 20 Train loss 1.27 on epoch=4
03/16/2022 11:49:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.13 on epoch=7
03/16/2022 11:49:34 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=9
03/16/2022 11:49:37 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=12
03/16/2022 11:49:38 - INFO - __main__ - Global step 50 Train loss 1.40 Classification-F1 0.1 on epoch=12
03/16/2022 11:49:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/16/2022 11:49:40 - INFO - __main__ - Step 60 Global step 60 Train loss 1.00 on epoch=14
03/16/2022 11:49:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=17
03/16/2022 11:49:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=19
03/16/2022 11:49:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=22
03/16/2022 11:49:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=24
03/16/2022 11:49:51 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.23819163292847506 on epoch=24
03/16/2022 11:49:51 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.23819163292847506 on epoch=24, global_step=100
03/16/2022 11:49:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.87 on epoch=27
03/16/2022 11:49:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.88 on epoch=29
03/16/2022 11:49:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.87 on epoch=32
03/16/2022 11:50:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.85 on epoch=34
03/16/2022 11:50:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.76 on epoch=37
03/16/2022 11:50:03 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.13067758749069247 on epoch=37
03/16/2022 11:50:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=39
03/16/2022 11:50:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.73 on epoch=42
03/16/2022 11:50:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.76 on epoch=44
03/16/2022 11:50:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.75 on epoch=47
03/16/2022 11:50:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.73 on epoch=49
03/16/2022 11:50:16 - INFO - __main__ - Global step 200 Train loss 0.77 Classification-F1 0.49768953634085217 on epoch=49
03/16/2022 11:50:16 - INFO - __main__ - Saving model with best Classification-F1: 0.23819163292847506 -> 0.49768953634085217 on epoch=49, global_step=200
03/16/2022 11:50:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.62 on epoch=52
03/16/2022 11:50:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=54
03/16/2022 11:50:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=57
03/16/2022 11:50:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=59
03/16/2022 11:50:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.66 on epoch=62
03/16/2022 11:50:28 - INFO - __main__ - Global step 250 Train loss 0.61 Classification-F1 0.6715170929599763 on epoch=62
03/16/2022 11:50:28 - INFO - __main__ - Saving model with best Classification-F1: 0.49768953634085217 -> 0.6715170929599763 on epoch=62, global_step=250
03/16/2022 11:50:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.59 on epoch=64
03/16/2022 11:50:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.58 on epoch=67
03/16/2022 11:50:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.55 on epoch=69
03/16/2022 11:50:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=72
03/16/2022 11:50:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=74
03/16/2022 11:50:41 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.5503034547152195 on epoch=74
03/16/2022 11:50:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=77
03/16/2022 11:50:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=79
03/16/2022 11:50:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=82
03/16/2022 11:50:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=84
03/16/2022 11:50:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=87
03/16/2022 11:50:54 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.7447129807530252 on epoch=87
03/16/2022 11:50:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6715170929599763 -> 0.7447129807530252 on epoch=87, global_step=350
03/16/2022 11:50:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=89
03/16/2022 11:50:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=92
03/16/2022 11:51:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=94
03/16/2022 11:51:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=97
03/16/2022 11:51:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=99
03/16/2022 11:51:06 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.483234126984127 on epoch=99
03/16/2022 11:51:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=102
03/16/2022 11:51:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=104
03/16/2022 11:51:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=107
03/16/2022 11:51:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=109
03/16/2022 11:51:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=112
03/16/2022 11:51:19 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.6447447447447447 on epoch=112
03/16/2022 11:51:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=114
03/16/2022 11:51:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=117
03/16/2022 11:51:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=119
03/16/2022 11:51:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=122
03/16/2022 11:51:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=124
03/16/2022 11:51:32 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.6292144417144417 on epoch=124
03/16/2022 11:51:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=127
03/16/2022 11:51:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=129
03/16/2022 11:51:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.31 on epoch=132
03/16/2022 11:51:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=134
03/16/2022 11:51:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=137
03/16/2022 11:51:45 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.6273378950798306 on epoch=137
03/16/2022 11:51:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=139
03/16/2022 11:51:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=142
03/16/2022 11:51:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=144
03/16/2022 11:51:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=147
03/16/2022 11:51:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=149
03/16/2022 11:51:57 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.7075925106884859 on epoch=149
03/16/2022 11:52:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=152
03/16/2022 11:52:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=154
03/16/2022 11:52:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=157
03/16/2022 11:52:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=159
03/16/2022 11:52:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=162
03/16/2022 11:52:10 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.737058371735791 on epoch=162
03/16/2022 11:52:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=164
03/16/2022 11:52:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=167
03/16/2022 11:52:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=169
03/16/2022 11:52:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=172
03/16/2022 11:52:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=174
03/16/2022 11:52:23 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.7506718903777728 on epoch=174
03/16/2022 11:52:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7447129807530252 -> 0.7506718903777728 on epoch=174, global_step=700
03/16/2022 11:52:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=177
03/16/2022 11:52:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=179
03/16/2022 11:52:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=182
03/16/2022 11:52:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=184
03/16/2022 11:52:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=187
03/16/2022 11:52:36 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.7304112554112554 on epoch=187
03/16/2022 11:52:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=189
03/16/2022 11:52:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=192
03/16/2022 11:52:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=194
03/16/2022 11:52:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=197
03/16/2022 11:52:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=199
03/16/2022 11:52:49 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.7292690417690417 on epoch=199
03/16/2022 11:52:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
03/16/2022 11:52:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=204
03/16/2022 11:52:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=207
03/16/2022 11:52:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=209
03/16/2022 11:53:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=212
03/16/2022 11:53:02 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.7601389838231944 on epoch=212
03/16/2022 11:53:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7506718903777728 -> 0.7601389838231944 on epoch=212, global_step=850
03/16/2022 11:53:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=214
03/16/2022 11:53:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=217
03/16/2022 11:53:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=219
03/16/2022 11:53:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=222
03/16/2022 11:53:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=224
03/16/2022 11:53:14 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.75 on epoch=224
03/16/2022 11:53:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=227
03/16/2022 11:53:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=229
03/16/2022 11:53:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=232
03/16/2022 11:53:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
03/16/2022 11:53:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=237
03/16/2022 11:53:27 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.7818362193362194 on epoch=237
03/16/2022 11:53:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7601389838231944 -> 0.7818362193362194 on epoch=237, global_step=950
03/16/2022 11:53:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
03/16/2022 11:53:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
03/16/2022 11:53:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.16 on epoch=244
03/16/2022 11:53:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=247
03/16/2022 11:53:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=249
03/16/2022 11:53:40 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.7593563512361465 on epoch=249
03/16/2022 11:53:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=252
03/16/2022 11:53:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
03/16/2022 11:53:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=257
03/16/2022 11:53:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=259
03/16/2022 11:53:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=262
03/16/2022 11:53:53 - INFO - __main__ - Global step 1050 Train loss 0.13 Classification-F1 0.7114328614328614 on epoch=262
03/16/2022 11:53:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
03/16/2022 11:53:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=267
03/16/2022 11:54:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=269
03/16/2022 11:54:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
03/16/2022 11:54:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=274
03/16/2022 11:54:06 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.7511853832442069 on epoch=274
03/16/2022 11:54:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=277
03/16/2022 11:54:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
03/16/2022 11:54:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
03/16/2022 11:54:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
03/16/2022 11:54:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=287
03/16/2022 11:54:19 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.7660014005602241 on epoch=287
03/16/2022 11:54:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
03/16/2022 11:54:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
03/16/2022 11:54:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
03/16/2022 11:54:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
03/16/2022 11:54:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=299
03/16/2022 11:54:32 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.7595444117183248 on epoch=299
03/16/2022 11:54:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
03/16/2022 11:54:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
03/16/2022 11:54:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
03/16/2022 11:54:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
03/16/2022 11:54:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=312
03/16/2022 11:54:45 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.7454967632986209 on epoch=312
03/16/2022 11:54:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=314
03/16/2022 11:54:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=317
03/16/2022 11:54:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=319
03/16/2022 11:54:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
03/16/2022 11:54:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
03/16/2022 11:54:59 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.7446504688832055 on epoch=324
03/16/2022 11:55:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
03/16/2022 11:55:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
03/16/2022 11:55:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=332
03/16/2022 11:55:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
03/16/2022 11:55:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
03/16/2022 11:55:12 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.758982683982684 on epoch=337
03/16/2022 11:55:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
03/16/2022 11:55:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
03/16/2022 11:55:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
03/16/2022 11:55:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
03/16/2022 11:55:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
03/16/2022 11:55:25 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.766336898395722 on epoch=349
03/16/2022 11:55:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
03/16/2022 11:55:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
03/16/2022 11:55:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
03/16/2022 11:55:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
03/16/2022 11:55:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
03/16/2022 11:55:38 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.7454545454545454 on epoch=362
03/16/2022 11:55:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
03/16/2022 11:55:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
03/16/2022 11:55:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
03/16/2022 11:55:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=372
03/16/2022 11:55:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=374
03/16/2022 11:55:51 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7240676382889333 on epoch=374
03/16/2022 11:55:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
03/16/2022 11:55:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
03/16/2022 11:55:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
03/16/2022 11:56:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
03/16/2022 11:56:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
03/16/2022 11:56:04 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.6982675953264189 on epoch=387
03/16/2022 11:56:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
03/16/2022 11:56:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
03/16/2022 11:56:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
03/16/2022 11:56:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
03/16/2022 11:56:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
03/16/2022 11:56:17 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.7447204968944099 on epoch=399
03/16/2022 11:56:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/16/2022 11:56:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
03/16/2022 11:56:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
03/16/2022 11:56:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
03/16/2022 11:56:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
03/16/2022 11:56:30 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7220771191359426 on epoch=412
03/16/2022 11:56:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
03/16/2022 11:56:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
03/16/2022 11:56:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 11:56:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
03/16/2022 11:56:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
03/16/2022 11:56:44 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7219696969696969 on epoch=424
03/16/2022 11:56:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
03/16/2022 11:56:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
03/16/2022 11:56:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
03/16/2022 11:56:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
03/16/2022 11:56:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
03/16/2022 11:56:57 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.7465532036613272 on epoch=437
03/16/2022 11:56:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
03/16/2022 11:57:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
03/16/2022 11:57:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/16/2022 11:57:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/16/2022 11:57:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/16/2022 11:57:10 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7602208186447318 on epoch=449
03/16/2022 11:57:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/16/2022 11:57:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/16/2022 11:57:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
03/16/2022 11:57:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
03/16/2022 11:57:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/16/2022 11:57:23 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7377917690417691 on epoch=462
03/16/2022 11:57:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
03/16/2022 11:57:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/16/2022 11:57:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
03/16/2022 11:57:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
03/16/2022 11:57:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
03/16/2022 11:57:36 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.709140498304585 on epoch=474
03/16/2022 11:57:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=477
03/16/2022 11:57:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
03/16/2022 11:57:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
03/16/2022 11:57:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
03/16/2022 11:57:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
03/16/2022 11:57:49 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7224524945113181 on epoch=487
03/16/2022 11:57:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/16/2022 11:57:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/16/2022 11:57:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/16/2022 11:57:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/16/2022 11:58:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 11:58:02 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7663368983957219 on epoch=499
03/16/2022 11:58:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
03/16/2022 11:58:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 11:58:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
03/16/2022 11:58:12 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=509
03/16/2022 11:58:14 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=512
03/16/2022 11:58:16 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.7301822573561704 on epoch=512
03/16/2022 11:58:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=514
03/16/2022 11:58:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=517
03/16/2022 11:58:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/16/2022 11:58:25 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
03/16/2022 11:58:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 11:58:29 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.7077532077532077 on epoch=524
03/16/2022 11:58:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
03/16/2022 11:58:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/16/2022 11:58:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 11:58:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/16/2022 11:58:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/16/2022 11:58:42 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7231200172376644 on epoch=537
03/16/2022 11:58:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/16/2022 11:58:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
03/16/2022 11:58:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/16/2022 11:58:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=547
03/16/2022 11:58:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
03/16/2022 11:58:55 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7085742291334396 on epoch=549
03/16/2022 11:58:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
03/16/2022 11:59:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
03/16/2022 11:59:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/16/2022 11:59:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/16/2022 11:59:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/16/2022 11:59:08 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7222214192802427 on epoch=562
03/16/2022 11:59:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/16/2022 11:59:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/16/2022 11:59:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
03/16/2022 11:59:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 11:59:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=574
03/16/2022 11:59:21 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7229113384484228 on epoch=574
03/16/2022 11:59:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
03/16/2022 11:59:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=579
03/16/2022 11:59:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/16/2022 11:59:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/16/2022 11:59:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
03/16/2022 11:59:34 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.7229113384484228 on epoch=587
03/16/2022 11:59:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
03/16/2022 11:59:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
03/16/2022 11:59:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/16/2022 11:59:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
03/16/2022 11:59:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
03/16/2022 11:59:48 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7451680672268908 on epoch=599
03/16/2022 11:59:50 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/16/2022 11:59:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/16/2022 11:59:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/16/2022 11:59:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/16/2022 12:00:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
03/16/2022 12:00:01 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7306998556998557 on epoch=612
03/16/2022 12:00:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
03/16/2022 12:00:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 12:00:08 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
03/16/2022 12:00:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/16/2022 12:00:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
03/16/2022 12:00:14 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6992424242424242 on epoch=624
03/16/2022 12:00:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
03/16/2022 12:00:19 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 12:00:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/16/2022 12:00:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
03/16/2022 12:00:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/16/2022 12:00:27 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7451680672268908 on epoch=637
03/16/2022 12:00:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/16/2022 12:00:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/16/2022 12:00:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/16/2022 12:00:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=647
03/16/2022 12:00:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
03/16/2022 12:00:40 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.722296494355318 on epoch=649
03/16/2022 12:00:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/16/2022 12:00:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
03/16/2022 12:00:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/16/2022 12:00:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=659
03/16/2022 12:00:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/16/2022 12:00:53 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.7447204968944099 on epoch=662
03/16/2022 12:00:56 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/16/2022 12:00:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/16/2022 12:01:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/16/2022 12:01:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/16/2022 12:01:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
03/16/2022 12:01:07 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7458067687237159 on epoch=674
03/16/2022 12:01:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
03/16/2022 12:01:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/16/2022 12:01:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/16/2022 12:01:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/16/2022 12:01:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/16/2022 12:01:20 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7518551587301587 on epoch=687
03/16/2022 12:01:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
03/16/2022 12:01:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/16/2022 12:01:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
03/16/2022 12:01:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/16/2022 12:01:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/16/2022 12:01:33 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7451680672268908 on epoch=699
03/16/2022 12:01:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
03/16/2022 12:01:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
03/16/2022 12:01:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/16/2022 12:01:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=709
03/16/2022 12:01:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/16/2022 12:01:46 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.7670833333333333 on epoch=712
03/16/2022 12:01:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/16/2022 12:01:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 12:01:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
03/16/2022 12:01:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
03/16/2022 12:01:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
03/16/2022 12:01:59 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7454545454545454 on epoch=724
03/16/2022 12:02:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=727
03/16/2022 12:02:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
03/16/2022 12:02:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
03/16/2022 12:02:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/16/2022 12:02:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
03/16/2022 12:02:12 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.736927301633184 on epoch=737
03/16/2022 12:02:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
03/16/2022 12:02:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 12:02:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/16/2022 12:02:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 12:02:25 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/16/2022 12:02:26 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7366522366522367 on epoch=749
03/16/2022 12:02:26 - INFO - __main__ - save last model!
03/16/2022 12:02:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 12:02:26 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 12:02:26 - INFO - __main__ - Printing 3 examples
03/16/2022 12:02:26 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 12:02:26 - INFO - __main__ - ['others']
03/16/2022 12:02:26 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 12:02:26 - INFO - __main__ - ['others']
03/16/2022 12:02:26 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 12:02:26 - INFO - __main__ - ['others']
03/16/2022 12:02:26 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:02:26 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 12:02:26 - INFO - __main__ - Printing 3 examples
03/16/2022 12:02:26 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/16/2022 12:02:26 - INFO - __main__ - ['others']
03/16/2022 12:02:26 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/16/2022 12:02:26 - INFO - __main__ - ['others']
03/16/2022 12:02:26 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/16/2022 12:02:26 - INFO - __main__ - ['others']
03/16/2022 12:02:26 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:02:26 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:02:26 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 12:02:26 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 12:02:26 - INFO - __main__ - Printing 3 examples
03/16/2022 12:02:26 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/16/2022 12:02:26 - INFO - __main__ - ['others']
03/16/2022 12:02:26 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/16/2022 12:02:26 - INFO - __main__ - ['others']
03/16/2022 12:02:26 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/16/2022 12:02:26 - INFO - __main__ - ['others']
03/16/2022 12:02:26 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:02:26 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:02:26 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 12:02:28 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:02:33 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 12:02:44 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 12:02:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 12:02:45 - INFO - __main__ - Starting training!
03/16/2022 12:04:05 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_87_0.4_8_predictions.txt
03/16/2022 12:04:05 - INFO - __main__ - Classification-F1 on test data: 0.1383
03/16/2022 12:04:05 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.7818362193362194, test_performance=0.13829649375604028
03/16/2022 12:04:05 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
03/16/2022 12:04:06 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 12:04:06 - INFO - __main__ - Printing 3 examples
03/16/2022 12:04:06 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/16/2022 12:04:06 - INFO - __main__ - ['others']
03/16/2022 12:04:06 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/16/2022 12:04:06 - INFO - __main__ - ['others']
03/16/2022 12:04:06 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/16/2022 12:04:06 - INFO - __main__ - ['others']
03/16/2022 12:04:06 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:04:06 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:04:06 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 12:04:06 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 12:04:06 - INFO - __main__ - Printing 3 examples
03/16/2022 12:04:06 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/16/2022 12:04:06 - INFO - __main__ - ['others']
03/16/2022 12:04:06 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/16/2022 12:04:06 - INFO - __main__ - ['others']
03/16/2022 12:04:06 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/16/2022 12:04:06 - INFO - __main__ - ['others']
03/16/2022 12:04:06 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:04:06 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:04:06 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 12:04:25 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 12:04:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 12:04:26 - INFO - __main__ - Starting training!
03/16/2022 12:04:29 - INFO - __main__ - Step 10 Global step 10 Train loss 2.75 on epoch=2
03/16/2022 12:04:31 - INFO - __main__ - Step 20 Global step 20 Train loss 1.42 on epoch=4
03/16/2022 12:04:34 - INFO - __main__ - Step 30 Global step 30 Train loss 1.12 on epoch=7
03/16/2022 12:04:36 - INFO - __main__ - Step 40 Global step 40 Train loss 1.03 on epoch=9
03/16/2022 12:04:38 - INFO - __main__ - Step 50 Global step 50 Train loss 1.03 on epoch=12
03/16/2022 12:04:39 - INFO - __main__ - Global step 50 Train loss 1.47 Classification-F1 0.1 on epoch=12
03/16/2022 12:04:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/16/2022 12:04:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=14
03/16/2022 12:04:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=17
03/16/2022 12:04:46 - INFO - __main__ - Step 80 Global step 80 Train loss 1.01 on epoch=19
03/16/2022 12:04:48 - INFO - __main__ - Step 90 Global step 90 Train loss 1.02 on epoch=22
03/16/2022 12:04:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.81 on epoch=24
03/16/2022 12:04:52 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.20526315789473687 on epoch=24
03/16/2022 12:04:52 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.20526315789473687 on epoch=24, global_step=100
03/16/2022 12:04:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.81 on epoch=27
03/16/2022 12:04:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.86 on epoch=29
03/16/2022 12:04:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=32
03/16/2022 12:05:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.86 on epoch=34
03/16/2022 12:05:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.76 on epoch=37
03/16/2022 12:05:05 - INFO - __main__ - Global step 150 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=37
03/16/2022 12:05:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.88 on epoch=39
03/16/2022 12:05:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.74 on epoch=42
03/16/2022 12:05:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.80 on epoch=44
03/16/2022 12:05:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.77 on epoch=47
03/16/2022 12:05:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.71 on epoch=49
03/16/2022 12:05:17 - INFO - __main__ - Global step 200 Train loss 0.78 Classification-F1 0.34487179487179487 on epoch=49
03/16/2022 12:05:17 - INFO - __main__ - Saving model with best Classification-F1: 0.20526315789473687 -> 0.34487179487179487 on epoch=49, global_step=200
03/16/2022 12:05:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.69 on epoch=52
03/16/2022 12:05:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.64 on epoch=54
03/16/2022 12:05:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.66 on epoch=57
03/16/2022 12:05:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.59 on epoch=59
03/16/2022 12:05:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.65 on epoch=62
03/16/2022 12:05:30 - INFO - __main__ - Global step 250 Train loss 0.65 Classification-F1 0.5239097075611854 on epoch=62
03/16/2022 12:05:30 - INFO - __main__ - Saving model with best Classification-F1: 0.34487179487179487 -> 0.5239097075611854 on epoch=62, global_step=250
03/16/2022 12:05:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.66 on epoch=64
03/16/2022 12:05:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.60 on epoch=67
03/16/2022 12:05:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.61 on epoch=69
03/16/2022 12:05:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.59 on epoch=72
03/16/2022 12:05:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=74
03/16/2022 12:05:43 - INFO - __main__ - Global step 300 Train loss 0.59 Classification-F1 0.6056678569267082 on epoch=74
03/16/2022 12:05:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5239097075611854 -> 0.6056678569267082 on epoch=74, global_step=300
03/16/2022 12:05:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.54 on epoch=77
03/16/2022 12:05:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=79
03/16/2022 12:05:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=82
03/16/2022 12:05:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=84
03/16/2022 12:05:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=87
03/16/2022 12:05:56 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.5960847240051348 on epoch=87
03/16/2022 12:05:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=89
03/16/2022 12:06:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=92
03/16/2022 12:06:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=94
03/16/2022 12:06:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=97
03/16/2022 12:06:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=99
03/16/2022 12:06:09 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.5030881195908733 on epoch=99
03/16/2022 12:06:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=102
03/16/2022 12:06:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=104
03/16/2022 12:06:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=107
03/16/2022 12:06:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=109
03/16/2022 12:06:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=112
03/16/2022 12:06:22 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.6540419884169885 on epoch=112
03/16/2022 12:06:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6056678569267082 -> 0.6540419884169885 on epoch=112, global_step=450
03/16/2022 12:06:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=114
03/16/2022 12:06:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=117
03/16/2022 12:06:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=119
03/16/2022 12:06:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=122
03/16/2022 12:06:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=124
03/16/2022 12:06:34 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.6850459482038429 on epoch=124
03/16/2022 12:06:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6540419884169885 -> 0.6850459482038429 on epoch=124, global_step=500
03/16/2022 12:06:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=127
03/16/2022 12:06:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=129
03/16/2022 12:06:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=132
03/16/2022 12:06:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=134
03/16/2022 12:06:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=137
03/16/2022 12:06:47 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.766748366013072 on epoch=137
03/16/2022 12:06:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6850459482038429 -> 0.766748366013072 on epoch=137, global_step=550
03/16/2022 12:06:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=139
03/16/2022 12:06:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=142
03/16/2022 12:06:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=144
03/16/2022 12:06:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=147
03/16/2022 12:06:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=149
03/16/2022 12:07:00 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.6285714285714286 on epoch=149
03/16/2022 12:07:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=152
03/16/2022 12:07:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=154
03/16/2022 12:07:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=157
03/16/2022 12:07:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=159
03/16/2022 12:07:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=162
03/16/2022 12:07:13 - INFO - __main__ - Global step 650 Train loss 0.24 Classification-F1 0.7116541353383459 on epoch=162
03/16/2022 12:07:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=164
03/16/2022 12:07:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=167
03/16/2022 12:07:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
03/16/2022 12:07:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=172
03/16/2022 12:07:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=174
03/16/2022 12:07:26 - INFO - __main__ - Global step 700 Train loss 0.24 Classification-F1 0.7116541353383459 on epoch=174
03/16/2022 12:07:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
03/16/2022 12:07:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=179
03/16/2022 12:07:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=182
03/16/2022 12:07:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=184
03/16/2022 12:07:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=187
03/16/2022 12:07:39 - INFO - __main__ - Global step 750 Train loss 0.15 Classification-F1 0.6850459482038429 on epoch=187
03/16/2022 12:07:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=189
03/16/2022 12:07:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=192
03/16/2022 12:07:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=194
03/16/2022 12:07:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=197
03/16/2022 12:07:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=199
03/16/2022 12:07:52 - INFO - __main__ - Global step 800 Train loss 0.17 Classification-F1 0.5621001202062895 on epoch=199
03/16/2022 12:07:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.13 on epoch=202
03/16/2022 12:07:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=204
03/16/2022 12:07:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=207
03/16/2022 12:08:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=209
03/16/2022 12:08:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=212
03/16/2022 12:08:04 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.7228136446886447 on epoch=212
03/16/2022 12:08:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=214
03/16/2022 12:08:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=217
03/16/2022 12:08:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=219
03/16/2022 12:08:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=222
03/16/2022 12:08:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=224
03/16/2022 12:08:17 - INFO - __main__ - Global step 900 Train loss 0.14 Classification-F1 0.7361111111111112 on epoch=224
03/16/2022 12:08:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=227
03/16/2022 12:08:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
03/16/2022 12:08:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=232
03/16/2022 12:08:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
03/16/2022 12:08:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=237
03/16/2022 12:08:30 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.6703634085213033 on epoch=237
03/16/2022 12:08:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
03/16/2022 12:08:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=242
03/16/2022 12:08:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=244
03/16/2022 12:08:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=247
03/16/2022 12:08:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=249
03/16/2022 12:08:43 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.7503217503217504 on epoch=249
03/16/2022 12:08:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=252
03/16/2022 12:08:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=254
03/16/2022 12:08:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=257
03/16/2022 12:08:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
03/16/2022 12:08:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=262
03/16/2022 12:08:56 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.6402027027027027 on epoch=262
03/16/2022 12:08:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=264
03/16/2022 12:09:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
03/16/2022 12:09:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=269
03/16/2022 12:09:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=272
03/16/2022 12:09:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
03/16/2022 12:09:09 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.7746723996723998 on epoch=274
03/16/2022 12:09:09 - INFO - __main__ - Saving model with best Classification-F1: 0.766748366013072 -> 0.7746723996723998 on epoch=274, global_step=1100
03/16/2022 12:09:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
03/16/2022 12:09:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
03/16/2022 12:09:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
03/16/2022 12:09:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
03/16/2022 12:09:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
03/16/2022 12:09:22 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.7453688600747425 on epoch=287
03/16/2022 12:09:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
03/16/2022 12:09:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
03/16/2022 12:09:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
03/16/2022 12:09:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=297
03/16/2022 12:09:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=299
03/16/2022 12:09:35 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.7116541353383459 on epoch=299
03/16/2022 12:09:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
03/16/2022 12:09:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
03/16/2022 12:09:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
03/16/2022 12:09:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
03/16/2022 12:09:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
03/16/2022 12:09:48 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.7601389838231944 on epoch=312
03/16/2022 12:09:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
03/16/2022 12:09:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
03/16/2022 12:09:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
03/16/2022 12:09:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
03/16/2022 12:10:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=324
03/16/2022 12:10:02 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.745093795093795 on epoch=324
03/16/2022 12:10:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
03/16/2022 12:10:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
03/16/2022 12:10:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
03/16/2022 12:10:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
03/16/2022 12:10:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
03/16/2022 12:10:15 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.7371583768642592 on epoch=337
03/16/2022 12:10:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
03/16/2022 12:10:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=342
03/16/2022 12:10:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
03/16/2022 12:10:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
03/16/2022 12:10:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
03/16/2022 12:10:28 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6968782249742002 on epoch=349
03/16/2022 12:10:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
03/16/2022 12:10:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
03/16/2022 12:10:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
03/16/2022 12:10:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/16/2022 12:10:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
03/16/2022 12:10:41 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.7361111111111112 on epoch=362
03/16/2022 12:10:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
03/16/2022 12:10:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
03/16/2022 12:10:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
03/16/2022 12:10:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
03/16/2022 12:10:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
03/16/2022 12:10:54 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.721969696969697 on epoch=374
03/16/2022 12:10:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
03/16/2022 12:10:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
03/16/2022 12:11:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
03/16/2022 12:11:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
03/16/2022 12:11:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
03/16/2022 12:11:07 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.7360360360360361 on epoch=387
03/16/2022 12:11:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=389
03/16/2022 12:11:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
03/16/2022 12:11:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
03/16/2022 12:11:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
03/16/2022 12:11:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=399
03/16/2022 12:11:20 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7357142857142857 on epoch=399
03/16/2022 12:11:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/16/2022 12:11:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
03/16/2022 12:11:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
03/16/2022 12:11:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=409
03/16/2022 12:11:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
03/16/2022 12:11:33 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.7360360360360361 on epoch=412
03/16/2022 12:11:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
03/16/2022 12:11:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
03/16/2022 12:11:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=419
03/16/2022 12:11:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
03/16/2022 12:11:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
03/16/2022 12:11:47 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7064905814905814 on epoch=424
03/16/2022 12:11:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
03/16/2022 12:11:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
03/16/2022 12:11:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
03/16/2022 12:11:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
03/16/2022 12:11:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/16/2022 12:12:00 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7357142857142857 on epoch=437
03/16/2022 12:12:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
03/16/2022 12:12:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
03/16/2022 12:12:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/16/2022 12:12:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
03/16/2022 12:12:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/16/2022 12:12:13 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7212601256718904 on epoch=449
03/16/2022 12:12:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
03/16/2022 12:12:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/16/2022 12:12:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
03/16/2022 12:12:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
03/16/2022 12:12:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
03/16/2022 12:12:26 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.7445285239402887 on epoch=462
03/16/2022 12:12:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
03/16/2022 12:12:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/16/2022 12:12:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
03/16/2022 12:12:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/16/2022 12:12:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
03/16/2022 12:12:39 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7602026456515621 on epoch=474
03/16/2022 12:12:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
03/16/2022 12:12:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/16/2022 12:12:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
03/16/2022 12:12:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
03/16/2022 12:12:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=487
03/16/2022 12:12:52 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7502705627705628 on epoch=487
03/16/2022 12:12:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=489
03/16/2022 12:12:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/16/2022 12:13:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=494
03/16/2022 12:13:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
03/16/2022 12:13:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
03/16/2022 12:13:05 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.7450237670825905 on epoch=499
03/16/2022 12:13:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/16/2022 12:13:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 12:13:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/16/2022 12:13:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
03/16/2022 12:13:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
03/16/2022 12:13:18 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7116541353383459 on epoch=512
03/16/2022 12:13:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/16/2022 12:13:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/16/2022 12:13:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
03/16/2022 12:13:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/16/2022 12:13:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
03/16/2022 12:13:32 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.721719070403281 on epoch=524
03/16/2022 12:13:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/16/2022 12:13:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=529
03/16/2022 12:13:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
03/16/2022 12:13:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/16/2022 12:13:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=537
03/16/2022 12:13:45 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7360360360360361 on epoch=537
03/16/2022 12:13:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
03/16/2022 12:13:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/16/2022 12:13:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
03/16/2022 12:13:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/16/2022 12:13:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/16/2022 12:13:58 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7212601256718904 on epoch=549
03/16/2022 12:14:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
03/16/2022 12:14:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
03/16/2022 12:14:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/16/2022 12:14:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
03/16/2022 12:14:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/16/2022 12:14:11 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7212601256718904 on epoch=562
03/16/2022 12:14:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/16/2022 12:14:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/16/2022 12:14:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/16/2022 12:14:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 12:14:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
03/16/2022 12:14:25 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7217503217503218 on epoch=574
03/16/2022 12:14:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
03/16/2022 12:14:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/16/2022 12:14:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
03/16/2022 12:14:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/16/2022 12:14:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/16/2022 12:14:38 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7212601256718904 on epoch=587
03/16/2022 12:14:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 12:14:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/16/2022 12:14:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/16/2022 12:14:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/16/2022 12:14:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/16/2022 12:14:51 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7360360360360361 on epoch=599
03/16/2022 12:14:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=602
03/16/2022 12:14:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/16/2022 12:14:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/16/2022 12:15:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
03/16/2022 12:15:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
03/16/2022 12:15:04 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7214285714285714 on epoch=612
03/16/2022 12:15:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/16/2022 12:15:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/16/2022 12:15:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 12:15:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/16/2022 12:15:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/16/2022 12:15:17 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.7212601256718904 on epoch=624
03/16/2022 12:15:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/16/2022 12:15:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=629
03/16/2022 12:15:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/16/2022 12:15:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
03/16/2022 12:15:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/16/2022 12:15:31 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.7361111111111112 on epoch=637
03/16/2022 12:15:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/16/2022 12:15:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/16/2022 12:15:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/16/2022 12:15:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/16/2022 12:15:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/16/2022 12:15:44 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7512899896800826 on epoch=649
03/16/2022 12:15:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/16/2022 12:15:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/16/2022 12:15:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/16/2022 12:15:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/16/2022 12:15:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/16/2022 12:15:57 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7361111111111112 on epoch=662
03/16/2022 12:15:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/16/2022 12:16:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/16/2022 12:16:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/16/2022 12:16:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/16/2022 12:16:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/16/2022 12:16:10 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.7366522366522367 on epoch=674
03/16/2022 12:16:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/16/2022 12:16:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/16/2022 12:16:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/16/2022 12:16:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=684
03/16/2022 12:16:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/16/2022 12:16:23 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7214285714285714 on epoch=687
03/16/2022 12:16:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/16/2022 12:16:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/16/2022 12:16:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/16/2022 12:16:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/16/2022 12:16:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=699
03/16/2022 12:16:37 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.721969696969697 on epoch=699
03/16/2022 12:16:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/16/2022 12:16:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/16/2022 12:16:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/16/2022 12:16:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/16/2022 12:16:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/16/2022 12:16:50 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7512899896800826 on epoch=712
03/16/2022 12:16:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/16/2022 12:16:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 12:16:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/16/2022 12:17:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/16/2022 12:17:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
03/16/2022 12:17:03 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7360360360360361 on epoch=724
03/16/2022 12:17:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/16/2022 12:17:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 12:17:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
03/16/2022 12:17:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
03/16/2022 12:17:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/16/2022 12:17:16 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7217503217503218 on epoch=737
03/16/2022 12:17:18 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=739
03/16/2022 12:17:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/16/2022 12:17:23 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
03/16/2022 12:17:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 12:17:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/16/2022 12:17:28 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7212601256718904 on epoch=749
03/16/2022 12:17:28 - INFO - __main__ - save last model!
03/16/2022 12:17:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 12:17:28 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 12:17:28 - INFO - __main__ - Printing 3 examples
03/16/2022 12:17:28 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 12:17:28 - INFO - __main__ - ['others']
03/16/2022 12:17:28 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 12:17:28 - INFO - __main__ - ['others']
03/16/2022 12:17:28 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 12:17:28 - INFO - __main__ - ['others']
03/16/2022 12:17:28 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:17:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 12:17:29 - INFO - __main__ - Printing 3 examples
03/16/2022 12:17:29 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/16/2022 12:17:29 - INFO - __main__ - ['others']
03/16/2022 12:17:29 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/16/2022 12:17:29 - INFO - __main__ - ['others']
03/16/2022 12:17:29 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/16/2022 12:17:29 - INFO - __main__ - ['others']
03/16/2022 12:17:29 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:17:29 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:17:29 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 12:17:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 12:17:29 - INFO - __main__ - Printing 3 examples
03/16/2022 12:17:29 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/16/2022 12:17:29 - INFO - __main__ - ['others']
03/16/2022 12:17:29 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/16/2022 12:17:29 - INFO - __main__ - ['others']
03/16/2022 12:17:29 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/16/2022 12:17:29 - INFO - __main__ - ['others']
03/16/2022 12:17:29 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:17:29 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:17:29 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 12:17:30 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:17:36 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 12:17:45 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 12:17:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 12:17:45 - INFO - __main__ - Starting training!
03/16/2022 12:19:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_87_0.3_8_predictions.txt
03/16/2022 12:19:09 - INFO - __main__ - Classification-F1 on test data: 0.3006
03/16/2022 12:19:09 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.7746723996723998, test_performance=0.3006027051668499
03/16/2022 12:19:09 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
03/16/2022 12:19:10 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 12:19:10 - INFO - __main__ - Printing 3 examples
03/16/2022 12:19:10 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/16/2022 12:19:10 - INFO - __main__ - ['others']
03/16/2022 12:19:10 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/16/2022 12:19:10 - INFO - __main__ - ['others']
03/16/2022 12:19:10 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/16/2022 12:19:10 - INFO - __main__ - ['others']
03/16/2022 12:19:10 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:19:10 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:19:10 - INFO - __main__ - Loaded 64 examples from train data
03/16/2022 12:19:10 - INFO - __main__ - Start tokenizing ... 64 instances
03/16/2022 12:19:10 - INFO - __main__ - Printing 3 examples
03/16/2022 12:19:10 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/16/2022 12:19:10 - INFO - __main__ - ['others']
03/16/2022 12:19:10 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/16/2022 12:19:10 - INFO - __main__ - ['others']
03/16/2022 12:19:10 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/16/2022 12:19:10 - INFO - __main__ - ['others']
03/16/2022 12:19:10 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:19:10 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:19:10 - INFO - __main__ - Loaded 64 examples from dev data
03/16/2022 12:19:29 - INFO - __main__ - load prompt embedding from ckpt
03/16/2022 12:19:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/16/2022 12:19:30 - INFO - __main__ - Starting training!
03/16/2022 12:19:33 - INFO - __main__ - Step 10 Global step 10 Train loss 3.14 on epoch=2
03/16/2022 12:19:35 - INFO - __main__ - Step 20 Global step 20 Train loss 1.73 on epoch=4
03/16/2022 12:19:37 - INFO - __main__ - Step 30 Global step 30 Train loss 1.38 on epoch=7
03/16/2022 12:19:40 - INFO - __main__ - Step 40 Global step 40 Train loss 1.13 on epoch=9
03/16/2022 12:19:42 - INFO - __main__ - Step 50 Global step 50 Train loss 1.13 on epoch=12
03/16/2022 12:19:43 - INFO - __main__ - Global step 50 Train loss 1.70 Classification-F1 0.13067758749069247 on epoch=12
03/16/2022 12:19:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=12, global_step=50
03/16/2022 12:19:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.93 on epoch=14
03/16/2022 12:19:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=17
03/16/2022 12:19:50 - INFO - __main__ - Step 80 Global step 80 Train loss 1.00 on epoch=19
03/16/2022 12:19:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=22
03/16/2022 12:19:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=24
03/16/2022 12:19:55 - INFO - __main__ - Global step 100 Train loss 0.93 Classification-F1 0.18166666666666667 on epoch=24
03/16/2022 12:19:55 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.18166666666666667 on epoch=24, global_step=100
03/16/2022 12:19:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=27
03/16/2022 12:20:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.94 on epoch=29
03/16/2022 12:20:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=32
03/16/2022 12:20:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.78 on epoch=34
03/16/2022 12:20:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.84 on epoch=37
03/16/2022 12:20:08 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.3527667984189723 on epoch=37
03/16/2022 12:20:08 - INFO - __main__ - Saving model with best Classification-F1: 0.18166666666666667 -> 0.3527667984189723 on epoch=37, global_step=150
03/16/2022 12:20:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.78 on epoch=39
03/16/2022 12:20:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=42
03/16/2022 12:20:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=44
03/16/2022 12:20:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.74 on epoch=47
03/16/2022 12:20:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.85 on epoch=49
03/16/2022 12:20:20 - INFO - __main__ - Global step 200 Train loss 0.83 Classification-F1 0.3537878787878788 on epoch=49
03/16/2022 12:20:20 - INFO - __main__ - Saving model with best Classification-F1: 0.3527667984189723 -> 0.3537878787878788 on epoch=49, global_step=200
03/16/2022 12:20:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=52
03/16/2022 12:20:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.77 on epoch=54
03/16/2022 12:20:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.80 on epoch=57
03/16/2022 12:20:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.77 on epoch=59
03/16/2022 12:20:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.70 on epoch=62
03/16/2022 12:20:32 - INFO - __main__ - Global step 250 Train loss 0.77 Classification-F1 0.5152525252525253 on epoch=62
03/16/2022 12:20:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3537878787878788 -> 0.5152525252525253 on epoch=62, global_step=250
03/16/2022 12:20:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.75 on epoch=64
03/16/2022 12:20:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.69 on epoch=67
03/16/2022 12:20:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.72 on epoch=69
03/16/2022 12:20:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.68 on epoch=72
03/16/2022 12:20:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.66 on epoch=74
03/16/2022 12:20:45 - INFO - __main__ - Global step 300 Train loss 0.70 Classification-F1 0.45385696040868456 on epoch=74
03/16/2022 12:20:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.68 on epoch=77
03/16/2022 12:20:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.66 on epoch=79
03/16/2022 12:20:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=82
03/16/2022 12:20:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.63 on epoch=84
03/16/2022 12:20:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.69 on epoch=87
03/16/2022 12:20:57 - INFO - __main__ - Global step 350 Train loss 0.65 Classification-F1 0.6361873553197247 on epoch=87
03/16/2022 12:20:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5152525252525253 -> 0.6361873553197247 on epoch=87, global_step=350
03/16/2022 12:21:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.57 on epoch=89
03/16/2022 12:21:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.61 on epoch=92
03/16/2022 12:21:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.57 on epoch=94
03/16/2022 12:21:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=97
03/16/2022 12:21:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=99
03/16/2022 12:21:10 - INFO - __main__ - Global step 400 Train loss 0.56 Classification-F1 0.4904970760233919 on epoch=99
03/16/2022 12:21:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=102
03/16/2022 12:21:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.52 on epoch=104
03/16/2022 12:21:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=107
03/16/2022 12:21:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=109
03/16/2022 12:21:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=112
03/16/2022 12:21:23 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.5968628231481855 on epoch=112
03/16/2022 12:21:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.58 on epoch=114
03/16/2022 12:21:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=117
03/16/2022 12:21:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=119
03/16/2022 12:21:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=122
03/16/2022 12:21:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=124
03/16/2022 12:21:36 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.4976426499032881 on epoch=124
03/16/2022 12:21:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=127
03/16/2022 12:21:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=129
03/16/2022 12:21:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=132
03/16/2022 12:21:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=134
03/16/2022 12:21:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=137
03/16/2022 12:21:49 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.5612087912087913 on epoch=137
03/16/2022 12:21:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=139
03/16/2022 12:21:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=142
03/16/2022 12:21:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=144
03/16/2022 12:21:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=147
03/16/2022 12:22:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=149
03/16/2022 12:22:01 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.5718754981667464 on epoch=149
03/16/2022 12:22:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.28 on epoch=152
03/16/2022 12:22:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=154
03/16/2022 12:22:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=157
03/16/2022 12:22:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=159
03/16/2022 12:22:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=162
03/16/2022 12:22:14 - INFO - __main__ - Global step 650 Train loss 0.27 Classification-F1 0.5958453311394488 on epoch=162
03/16/2022 12:22:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=164
03/16/2022 12:22:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=167
03/16/2022 12:22:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=169
03/16/2022 12:22:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.30 on epoch=172
03/16/2022 12:22:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=174
03/16/2022 12:22:27 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.5991596638655462 on epoch=174
03/16/2022 12:22:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=177
03/16/2022 12:22:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=179
03/16/2022 12:22:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=182
03/16/2022 12:22:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=184
03/16/2022 12:22:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=187
03/16/2022 12:22:40 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.5733096490628772 on epoch=187
03/16/2022 12:22:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=189
03/16/2022 12:22:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=192
03/16/2022 12:22:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=194
03/16/2022 12:22:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=197
03/16/2022 12:22:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=199
03/16/2022 12:22:53 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.6160714285714286 on epoch=199
03/16/2022 12:22:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=202
03/16/2022 12:22:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=204
03/16/2022 12:23:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=207
03/16/2022 12:23:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=209
03/16/2022 12:23:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=212
03/16/2022 12:23:05 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.6724671195259431 on epoch=212
03/16/2022 12:23:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6361873553197247 -> 0.6724671195259431 on epoch=212, global_step=850
03/16/2022 12:23:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=214
03/16/2022 12:23:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=217
03/16/2022 12:23:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=219
03/16/2022 12:23:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=222
03/16/2022 12:23:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=224
03/16/2022 12:23:18 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.6592607313195549 on epoch=224
03/16/2022 12:23:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=227
03/16/2022 12:23:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=229
03/16/2022 12:23:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=232
03/16/2022 12:23:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=234
03/16/2022 12:23:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=237
03/16/2022 12:23:31 - INFO - __main__ - Global step 950 Train loss 0.14 Classification-F1 0.6847540818129053 on epoch=237
03/16/2022 12:23:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6724671195259431 -> 0.6847540818129053 on epoch=237, global_step=950
03/16/2022 12:23:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
03/16/2022 12:23:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.14 on epoch=242
03/16/2022 12:23:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=244
03/16/2022 12:23:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
03/16/2022 12:23:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=249
03/16/2022 12:23:44 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.6714912280701755 on epoch=249
03/16/2022 12:23:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=252
03/16/2022 12:23:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
03/16/2022 12:23:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
03/16/2022 12:23:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=259
03/16/2022 12:23:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
03/16/2022 12:23:57 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.6175324675324676 on epoch=262
03/16/2022 12:24:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=264
03/16/2022 12:24:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
03/16/2022 12:24:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=269
03/16/2022 12:24:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
03/16/2022 12:24:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
03/16/2022 12:24:10 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.6025018826876412 on epoch=274
03/16/2022 12:24:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
03/16/2022 12:24:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
03/16/2022 12:24:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
03/16/2022 12:24:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=284
03/16/2022 12:24:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=287
03/16/2022 12:24:23 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.6415055532702592 on epoch=287
03/16/2022 12:24:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=289
03/16/2022 12:24:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
03/16/2022 12:24:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
03/16/2022 12:24:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
03/16/2022 12:24:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
03/16/2022 12:24:36 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.6561747506019953 on epoch=299
03/16/2022 12:24:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
03/16/2022 12:24:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=304
03/16/2022 12:24:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
03/16/2022 12:24:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=309
03/16/2022 12:24:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
03/16/2022 12:24:49 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.6440554629409119 on epoch=312
03/16/2022 12:24:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
03/16/2022 12:24:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=317
03/16/2022 12:24:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
03/16/2022 12:24:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
03/16/2022 12:25:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
03/16/2022 12:25:02 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.6158730158730159 on epoch=324
03/16/2022 12:25:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
03/16/2022 12:25:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
03/16/2022 12:25:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=332
03/16/2022 12:25:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=334
03/16/2022 12:25:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
03/16/2022 12:25:15 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6354304479304479 on epoch=337
03/16/2022 12:25:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=339
03/16/2022 12:25:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
03/16/2022 12:25:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
03/16/2022 12:25:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
03/16/2022 12:25:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
03/16/2022 12:25:28 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.6022321428571429 on epoch=349
03/16/2022 12:25:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=352
03/16/2022 12:25:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
03/16/2022 12:25:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
03/16/2022 12:25:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=359
03/16/2022 12:25:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
03/16/2022 12:25:41 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.6031119465329993 on epoch=362
03/16/2022 12:25:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
03/16/2022 12:25:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=367
03/16/2022 12:25:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=369
03/16/2022 12:25:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
03/16/2022 12:25:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
03/16/2022 12:25:54 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.6412186884323108 on epoch=374
03/16/2022 12:25:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=377
03/16/2022 12:25:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
03/16/2022 12:26:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
03/16/2022 12:26:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
03/16/2022 12:26:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
03/16/2022 12:26:07 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6304590304590305 on epoch=387
03/16/2022 12:26:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
03/16/2022 12:26:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=392
03/16/2022 12:26:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
03/16/2022 12:26:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
03/16/2022 12:26:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
03/16/2022 12:26:20 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6007025124672184 on epoch=399
03/16/2022 12:26:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
03/16/2022 12:26:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
03/16/2022 12:26:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
03/16/2022 12:26:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
03/16/2022 12:26:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
03/16/2022 12:26:34 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.6179917643332277 on epoch=412
03/16/2022 12:26:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
03/16/2022 12:26:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
03/16/2022 12:26:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/16/2022 12:26:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
03/16/2022 12:26:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
03/16/2022 12:26:47 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6437969924812031 on epoch=424
03/16/2022 12:26:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
03/16/2022 12:26:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/16/2022 12:26:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
03/16/2022 12:26:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/16/2022 12:26:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/16/2022 12:27:00 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6309818664307828 on epoch=437
03/16/2022 12:27:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
03/16/2022 12:27:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=442
03/16/2022 12:27:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
03/16/2022 12:27:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
03/16/2022 12:27:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=449
03/16/2022 12:27:13 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.6564745196324143 on epoch=449
03/16/2022 12:27:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
03/16/2022 12:27:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
03/16/2022 12:27:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=457
03/16/2022 12:27:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
03/16/2022 12:27:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/16/2022 12:27:26 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6570370680277801 on epoch=462
03/16/2022 12:27:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
03/16/2022 12:27:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
03/16/2022 12:27:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/16/2022 12:27:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/16/2022 12:27:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/16/2022 12:27:39 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6610644257703081 on epoch=474
03/16/2022 12:27:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
03/16/2022 12:27:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/16/2022 12:27:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=482
03/16/2022 12:27:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
03/16/2022 12:27:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
03/16/2022 12:27:52 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6693693693693694 on epoch=487
03/16/2022 12:27:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
03/16/2022 12:27:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=492
03/16/2022 12:28:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
03/16/2022 12:28:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
03/16/2022 12:28:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/16/2022 12:28:05 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.655952380952381 on epoch=499
03/16/2022 12:28:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/16/2022 12:28:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/16/2022 12:28:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/16/2022 12:28:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
03/16/2022 12:28:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
03/16/2022 12:28:18 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.5969793322734499 on epoch=512
03/16/2022 12:28:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/16/2022 12:28:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
03/16/2022 12:28:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/16/2022 12:28:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/16/2022 12:28:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/16/2022 12:28:32 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6675386500754148 on epoch=524
03/16/2022 12:28:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
03/16/2022 12:28:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=529
03/16/2022 12:28:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/16/2022 12:28:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/16/2022 12:28:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/16/2022 12:28:45 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6580683624801271 on epoch=537
03/16/2022 12:28:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/16/2022 12:28:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
03/16/2022 12:28:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
03/16/2022 12:28:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/16/2022 12:28:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/16/2022 12:28:58 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6819476562123621 on epoch=549
03/16/2022 12:29:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/16/2022 12:29:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/16/2022 12:29:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/16/2022 12:29:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/16/2022 12:29:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
03/16/2022 12:29:11 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6801470588235294 on epoch=562
03/16/2022 12:29:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/16/2022 12:29:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/16/2022 12:29:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
03/16/2022 12:29:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/16/2022 12:29:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
03/16/2022 12:29:24 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6853535353535354 on epoch=574
03/16/2022 12:29:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6847540818129053 -> 0.6853535353535354 on epoch=574, global_step=2300
03/16/2022 12:29:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/16/2022 12:29:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
03/16/2022 12:29:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/16/2022 12:29:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
03/16/2022 12:29:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/16/2022 12:29:37 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6984126984126984 on epoch=587
03/16/2022 12:29:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6853535353535354 -> 0.6984126984126984 on epoch=587, global_step=2350
03/16/2022 12:29:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/16/2022 12:29:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/16/2022 12:29:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/16/2022 12:29:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/16/2022 12:29:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
03/16/2022 12:29:50 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.683128583128583 on epoch=599
03/16/2022 12:29:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/16/2022 12:29:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/16/2022 12:29:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/16/2022 12:30:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
03/16/2022 12:30:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=612
03/16/2022 12:30:04 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6795165701415702 on epoch=612
03/16/2022 12:30:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/16/2022 12:30:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/16/2022 12:30:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/16/2022 12:30:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/16/2022 12:30:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/16/2022 12:30:17 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6968508615567438 on epoch=624
03/16/2022 12:30:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
03/16/2022 12:30:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/16/2022 12:30:24 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/16/2022 12:30:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
03/16/2022 12:30:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/16/2022 12:30:30 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6877034871919783 on epoch=637
03/16/2022 12:30:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
03/16/2022 12:30:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
03/16/2022 12:30:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
03/16/2022 12:30:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/16/2022 12:30:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/16/2022 12:30:43 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6706709956709958 on epoch=649
03/16/2022 12:30:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/16/2022 12:30:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
03/16/2022 12:30:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/16/2022 12:30:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=659
03/16/2022 12:30:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/16/2022 12:30:56 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.6168609168609168 on epoch=662
03/16/2022 12:30:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=664
03/16/2022 12:31:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/16/2022 12:31:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/16/2022 12:31:06 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/16/2022 12:31:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
03/16/2022 12:31:10 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6847540818129053 on epoch=674
03/16/2022 12:31:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/16/2022 12:31:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/16/2022 12:31:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/16/2022 12:31:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/16/2022 12:31:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/16/2022 12:31:23 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7218253968253968 on epoch=687
03/16/2022 12:31:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6984126984126984 -> 0.7218253968253968 on epoch=687, global_step=2750
03/16/2022 12:31:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/16/2022 12:31:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/16/2022 12:31:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/16/2022 12:31:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/16/2022 12:31:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/16/2022 12:31:36 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6932916117698726 on epoch=699
03/16/2022 12:31:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
03/16/2022 12:31:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
03/16/2022 12:31:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/16/2022 12:31:45 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
03/16/2022 12:31:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/16/2022 12:31:49 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6789308224090833 on epoch=712
03/16/2022 12:31:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/16/2022 12:31:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/16/2022 12:31:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=719
03/16/2022 12:31:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
03/16/2022 12:32:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
03/16/2022 12:32:02 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.6661181955299602 on epoch=724
03/16/2022 12:32:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
03/16/2022 12:32:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/16/2022 12:32:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/16/2022 12:32:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/16/2022 12:32:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/16/2022 12:32:15 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6968508615567438 on epoch=737
03/16/2022 12:32:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/16/2022 12:32:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
03/16/2022 12:32:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
03/16/2022 12:32:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/16/2022 12:32:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/16/2022 12:32:28 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6892292490118577 on epoch=749
03/16/2022 12:32:28 - INFO - __main__ - save last model!
03/16/2022 12:32:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/16/2022 12:32:28 - INFO - __main__ - Start tokenizing ... 5509 instances
03/16/2022 12:32:28 - INFO - __main__ - Printing 3 examples
03/16/2022 12:32:28 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/16/2022 12:32:28 - INFO - __main__ - ['others']
03/16/2022 12:32:28 - INFO - __main__ -  [emo] what you like very little things ok
03/16/2022 12:32:28 - INFO - __main__ - ['others']
03/16/2022 12:32:28 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/16/2022 12:32:28 - INFO - __main__ - ['others']
03/16/2022 12:32:28 - INFO - __main__ - Tokenizing Input ...
03/16/2022 12:32:30 - INFO - __main__ - Tokenizing Output ...
03/16/2022 12:32:36 - INFO - __main__ - Loaded 5509 examples from test data
03/16/2022 12:34:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1/singletask-emo/emo_16_87_0.2_8_predictions.txt
03/16/2022 12:34:09 - INFO - __main__ - Classification-F1 on test data: 0.4105
03/16/2022 12:34:09 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.7218253968253968, test_performance=0.4105259736672074
